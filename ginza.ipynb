{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 依存 依存 NOUN 名詞-普通名詞-サ変可能 compound 2\n",
      "1 構造 構造 NOUN 名詞-普通名詞-一般 compound 2\n",
      "2 文 文 NOUN 名詞-普通名詞-一般 nmod 4\n",
      "3 の の ADP 助詞-格助詞 case 2\n",
      "4 解析 解析 NOUN 名詞-普通名詞-サ変可能 obj 8\n",
      "5 を を ADP 助詞-格助詞 case 4\n",
      "6 Ginza ginza NOUN 名詞-普通名詞-一般 nmod 8\n",
      "7 で で ADP 助詞-格助詞 case 6\n",
      "8 行い 行う VERB 動詞-一般 ROOT 8\n",
      "9 ます ます AUX 助動詞 aux 8\n",
      "10 。 。 PUNCT 補助記号-句点 punct 8\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('ja_ginza')\n",
    "doc = nlp('依存構造文の解析をGinzaで行います。')\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, token.tag_, token.dep_, token.head.i)\n",
    "    print('EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from elasticsearch import Elasticsearch\n",
    "import os\n",
    "es = Elasticsearch(\"localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('qa_train2.json', 'r',encoding=\"utf-8\") as f:\n",
    "    qa_train = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "for qa in qa_train['data']:\n",
    "    es.index(index=\"qa_train2\", \n",
    "             doc_type=\"test\", \n",
    "             body={\n",
    "                 \"question\":qa['paragraphs'][0]['qas'][0]['question'], \n",
    "                 \"context\":qa['paragraphs'][0]['context'],\n",
    "                 \"answer\":qa['paragraphs'][0]['qas'][0]['answers'][0]['text'],\n",
    "                 \"id\":qa['title']\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index=\"qa_train1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'バイアスとはなんですか'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('qa_dev2.json', 'r',encoding=\"utf-8\") as f:\n",
    "    qa_dev = json.load(f)\n",
    "    \n",
    "qa_dev = qa_dev['data']\n",
    "qa_dev[0]['paragraphs'][0]['qas'][0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------devの質問---------------\n",
      "0610 バイアスとはなんですか\n",
      "------------trainの検索結果------------\n",
      "0610 バイアスってなんですか\n",
      "0610 バイアスと分散はどのような関係ですか\n",
      "0305 バイアスって何ですか\n",
      "------------devの質問---------------\n",
      "0306 候補削除アルゴリズムとはなんですか\n",
      "------------trainの検索結果------------\n",
      "0306 候補削除アルゴリズムってなんですか\n",
      "0204 次元削減とはなんですか\n",
      "1116 EMアルゴリズムとはなんですか\n",
      "------------devの質問---------------\n",
      "1502 強化学習とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1502 強化学習とはなんですか\n",
      "1501 強化学習はなぜ中間的学習なのですか\n",
      "1502 強化学習は例えば何に使われていますか\n",
      "------------devの質問---------------\n",
      "0605 決定係数の定義は何ですか\n",
      "------------trainの検索結果------------\n",
      "0605 決定係数ってなんですか\n",
      "0701 マージンの定義はなんですか\n",
      "0901 深層学習の定義はなにか\n",
      "------------devの質問---------------\n",
      "0916 時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか\n",
      "------------trainの検索結果------------\n",
      "1301 入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか\n",
      "1301 系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか\n",
      "1301 系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか\n",
      "------------devの質問---------------\n",
      "1502 強化学習はどのような学習法ですか\n",
      "------------trainの検索結果------------\n",
      "1503 強化学習で、報酬が決定的な場合の学習はどのようなものですか\n",
      "1501 強化学習はなぜ中間的学習なのですか\n",
      "1502 強化学習とはなんですか\n",
      "------------devの質問---------------\n",
      "0411 ゼロ頻度問題って何ですか\n",
      "------------trainの検索結果------------\n",
      "0411 ゼロ頻度問題とは何ですか\n",
      "0411 ゼロ頻度問題はどのような問題ですか\n",
      "0411 ゼロ頻度問題はどうすれば回避できますか\n",
      "------------devの質問---------------\n",
      "0810 階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか\n",
      "------------trainの検索結果------------\n",
      "0811 勾配が消失しない関数ってなんだったっけ\n",
      "0906 多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか\n",
      "0906 多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか\n",
      "------------devの質問---------------\n",
      "0514 最急勾配法の問題点の対処法は何がありますか\n",
      "------------trainの検索結果------------\n",
      "0514 最急勾配法の問題点とはなんですか\n",
      "0917 リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか\n",
      "0514 最急勾配法の欠点はなんですか\n",
      "------------devの質問---------------\n",
      "1103 階層的手法とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1104 階層的クラスタリングとは何ですか\n",
      "1104 階層的クラスタリングとはなんですか\n",
      "1104 階層的クラスタリングってなんですか\n",
      "------------devの質問---------------\n",
      "0805 なぜ重みを更新するのですか\n",
      "------------trainの検索結果------------\n",
      "0513 最急勾配法の重みの更新はどうなったら止まりますか\n",
      "1012 バギングでは、個々のデータに対してどのように重みを設定しますか\n",
      "0717 スラック変数の重みは離散値ですか、連続値ですか\n",
      "------------devの質問---------------\n",
      "0412 ベイジアンネットワークって何ですか\n",
      "------------trainの検索結果------------\n",
      "0412 ベイジアンネットワークってなんですか\n",
      "0413 ベイジアンネットワークのメリットは何ですか\n",
      "0417 ベイジアンネットワークで重要な項目は何ですか\n",
      "------------devの質問---------------\n",
      "0614 モデル木って何ですか\n",
      "------------trainの検索結果------------\n",
      "0614 モデル木ってなんですか\n",
      "0611 回帰木って何ですか\n",
      "0505 識別モデルってなんですか\n",
      "------------devの質問---------------\n",
      "0601 回帰問題の学習目的はなんですか\n",
      "------------trainの検索結果------------\n",
      "0601 回帰問題とはなんですか\n",
      "0601 回帰問題ってなんですか\n",
      "0602 回帰問題は識別問題とどう違うんですか\n",
      "------------devの質問---------------\n",
      "0717 グリッドサーチはなんのためにやるんですか\n",
      "------------trainの検索結果------------\n",
      "0115 データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか\n",
      "0402 条件付き確率ってどうやって求めるんですか\n",
      "1507 マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか\n",
      "------------devの質問---------------\n",
      "1412 ラベル伝搬法の考え方はどのようなものですか\n",
      "------------trainの検索結果------------\n",
      "1412 ラベル伝搬法の考え方にある仮定はどのようなものですか\n",
      "1412 ラベル伝搬法の考え方は何ですか\n",
      "0611 決定木はどのような考えですか\n",
      "------------devの質問---------------\n",
      "0602 回帰問題が識別問題と異なるのはどのような点ですか\n",
      "------------trainの検索結果------------\n",
      "0602 回帰問題は識別問題とどう違うんですか\n",
      "0601 回帰問題とはなんですか\n",
      "0411 ゼロ頻度問題はどのような問題ですか\n",
      "------------devの質問---------------\n",
      "0805 出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか\n",
      "------------trainの検索結果------------\n",
      "0805 出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か\n",
      "0805 出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか\n",
      "0810 多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか\n",
      "------------devの質問---------------\n",
      "0413 ベイジアンネットワークの利点とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0413 ベイジアンネットワークの利点はなんですか\n",
      "0811 ReLUを用いる利点はなんですか\n",
      "0412 ベイジアンネットワークってなんですか\n",
      "------------devの質問---------------\n",
      "0611 回帰木とはなに\n",
      "------------trainの検索結果------------\n",
      "0611 回帰木とはなんですか\n",
      "0614 モデル木は回帰木とどう違いますか\n",
      "0611 回帰木って何ですか\n",
      "------------devの質問---------------\n",
      "0811 ReLUとは何ですか\n",
      "------------trainの検索結果------------\n",
      "0811 ReLu関数の良さは何ですか\n",
      "0811 ReLUを用いる利点はなんですか\n",
      "0811 ReLUを用いると誤差はどうなりますか\n",
      "------------devの質問---------------\n",
      "0803 ノードを階層的に組むと非線形識別面が実現できるのはなぜか\n",
      "------------trainの検索結果------------\n",
      "0803 なぜノードを階層的に組むと非線形識別面が実現できるのか\n",
      "0803 なぜノードを階層的に組むと非線形識別面が実現できるのですか\n",
      "0803 なぜノードを階層的に組むと非線形識別面を実現できるんですか\n",
      "------------devの質問---------------\n",
      "0302 クラスとはなんですか\n",
      "------------trainの検索結果------------\n",
      "0302 クラスって何ですか\n",
      "0402 最も確率が高いクラスを出力とする手法はなに\n",
      "0405 事後確率が最大となるクラスは、何を求めることで得られますか\n",
      "------------devの質問---------------\n",
      "1219 協調フィルタリングではどのようなデータが必要ですか\n",
      "------------trainの検索結果------------\n",
      "1219 協調フィルタリングとはどのようなものですか\n",
      "1219 協調フィルタリングとは何ですか\n",
      "1506 なぜ割引率が必要なのですか\n",
      "------------devの質問---------------\n",
      "0606 汎化能力という点で望ましい線形回帰式の性質はなんですか\n",
      "------------trainの検索結果------------\n",
      "0614 回帰木と線形回帰の双方のよいところを取った方法はなんですか\n",
      "0502 線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに\n",
      "0810 ディープニューラルネットワークの性能が向上しない原因はなんですか\n",
      "------------devの質問---------------\n",
      "0917 中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか\n",
      "------------trainの検索結果------------\n",
      "0917 リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか\n",
      "0916 中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか\n",
      "0917 LSTMで置き換えられるメモリユニットの名前はなんですか\n",
      "------------devの質問---------------\n",
      "0113 教師なし学習では何を学習しますか\n",
      "------------trainの検索結果------------\n",
      "0109 教師なし学習とはどういう学習法ですか\n",
      "0113 教師なし学習の目的はなんですか\n",
      "0109 教師なし学習はどういうものですか\n",
      "------------devの質問---------------\n",
      "0406 i.i.d. (independent and identically distributed) とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0311 エントロピーとはなんですか\n",
      "1010 ブースティングとはなんですか\n",
      "0114 クラスタリングとはなんですか\n",
      "------------devの質問---------------\n",
      "0713 SVMの方法は例えば何に使われていますか\n",
      "------------trainの検索結果------------\n",
      "1502 強化学習は例えば何に使われていますか\n",
      "0713 高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか\n",
      "0114 クラスタリングはどのようなことに使われますか\n",
      "------------devの質問---------------\n",
      "1108 k-平均法って何ですか\n",
      "------------trainの検索結果------------\n",
      "1108 k-平均法とはなんですか\n",
      "1108 k-平均法とは、どのようなアルゴリズムなのですか\n",
      "0208 k-NN法とはなんですか\n",
      "------------devの質問---------------\n",
      "1111 異常検出とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1112 局所異常因子では、どのように外れ値を検知するのですか\n",
      "1112 局所異常因子とは何ですか\n",
      "0917 LSTMと通常のユニットの違いは何ですか\n",
      "------------devの質問---------------\n",
      "0410 ナイーブベイズ識別法とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0410 ナイーブベイズ識別って何ですか\n",
      "0505 識別関数法とはなんですか\n",
      "0410 ナイーブベイス識別法ってなんですか\n",
      "------------devの質問---------------\n",
      "0901 DNNとは何ですか\n",
      "------------trainの検索結果------------\n",
      "0901 DNNってなんですか\n",
      "1404 オーバーラップとは何ですか\n",
      "0717 グリッドサーチとは何ですか\n",
      "------------devの質問---------------\n",
      "0508 二乗誤差とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0508 二乗誤差とは何ですか\n",
      "0508 なぜ最小二乗法では正解との誤差を二乗するんですか\n",
      "0508 最小二乗法とはなんですか\n",
      "------------devの質問---------------\n",
      "0407 対数尤度とはなんですか\n",
      "------------trainの検索結果------------\n",
      "1110 モデルの対数尤度とは何ですか\n",
      "0407 何故尤度の対数をとって計算するんですか\n",
      "0406 尤度とはなんですか\n",
      "------------devの質問---------------\n",
      "0708 スラック変数が小さいほうがいいのはなぜか\n",
      "------------trainの検索結果------------\n",
      "0708 なぜスラック変数が小さい方が良いのですか\n",
      "0701 マージンは大きいほうがいいんですか\n",
      "1510 なぜTD学習において温度を学習が進むにつれて小さくするのですか\n",
      "------------devの質問---------------\n",
      "0511 入力が負例のときは、確率はどう求められますか\n",
      "------------trainの検索結果------------\n",
      "1114 事前確率はどのように求めますか\n",
      "0405 事後確率が最大となるクラスは、何を求めることで得られますか\n",
      "0402 条件付き確率ってどうやって求めるんですか\n",
      "------------devの質問---------------\n",
      "1110 X-meansアルゴリズムとはなんですか\n",
      "------------trainの検索結果------------\n",
      "1110 X-meansアルゴリズムは、どのよなアルゴリズムなのですか\n",
      "1110 k-means法の問題点はなんですか\n",
      "1116 EMアルゴリズムとはなんですか\n",
      "------------devの質問---------------\n",
      "0313 過学習を避けるためにはどのような方法がありますか\n",
      "------------trainの検索結果------------\n",
      "1507 マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか\n",
      "0715 複雑な非線形変換を求めるという操作を避ける方法をなんといいますか\n",
      "0911 なぜドロップアウトによって過学習が回避できるのですか\n",
      "------------devの質問---------------\n",
      "0117 半教師あり学習に適した状況はどのように作られますか\n",
      "------------trainの検索結果------------\n",
      "1402 半教師あり学習に適した数値特徴データとはどのようなものですか\n",
      "1406 半教師あり学習の識別器に適切なのはどのようなものですか\n",
      "0117 半教師あり学習はどんなときに使われますか\n",
      "------------devの質問---------------\n",
      "0109 教師あり学習って何ですか\n",
      "------------trainの検索結果------------\n",
      "0109 教師あり学習とはなんですか\n",
      "0116 半教師あり学習とはなんですか\n",
      "1402 半教師あり学習が成立する条件は何ですか\n",
      "------------devの質問---------------\n",
      "0204 主成分分析ってどういうことをしているんですか\n",
      "------------trainの検索結果------------\n",
      "0205 主成分分析って何ですか\n",
      "0205 主成分分析とはなんですか\n",
      "0205 主成分分析とは何ですか\n",
      "------------devの質問---------------\n",
      "0901 深層学習とはどのようなものか\n",
      "------------trainの検索結果------------\n",
      "0901 深層学習はどのようなものですか\n",
      "0903 深層学習の中で特化した構造にはどのようなものがありますか\n",
      "0901 深層学習はどのような場面で応用されますか\n",
      "------------devの質問---------------\n",
      "1012 ブースティングで、個々のデータに対してどのように重みを設定するのですか\n",
      "------------trainの検索結果------------\n",
      "1012 バギングでは、個々のデータに対してどのように重みを設定しますか\n",
      "0717 SVMのハイパーパラメータの設定はどのように行いますか\n",
      "0508 線形分離不可能なデータにはどのように対処しますか\n",
      "------------devの質問---------------\n",
      "1110 BICとは何ですか\n",
      "------------trainの検索結果------------\n",
      "1404 オーバーラップとは何ですか\n",
      "0717 グリッドサーチとは何ですか\n",
      "0701 サポートベクトルマシンとは何ですか\n",
      "------------devの質問---------------\n",
      "0707 ソフトマージンとは何ですか\n",
      "------------trainの検索結果------------\n",
      "0707 ソフトマージンによる際の識別面の設定の仕方\n",
      "1404 オーバーラップとは何ですか\n",
      "0717 グリッドサーチとは何ですか\n",
      "------------devの質問---------------\n",
      "0507 パーセプトロンの学習アルゴリズムはどのようなときに停止しますか\n",
      "------------trainの検索結果------------\n",
      "1306 条件付き確率場の学習にはどのような性質を利用しますか\n",
      "0506 パーセプトロンの出力はどのようになっていますか\n",
      "0507 なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか\n",
      "------------devの質問---------------\n",
      "0811 ReLuってなんですか\n",
      "------------trainの検索結果------------\n",
      "0811 ReLUを用いる利点はなんですか\n",
      "0612 CARTってなんですか\n",
      "0114 クラスタリングってなんですか\n",
      "------------devの質問---------------\n",
      "0504 なぜ生成モデルアプローチは問題を難しくしているといえるのですか\n",
      "------------trainの検索結果------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0917 リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか\n",
      "0504 生成モデルアプローチが有効なのはどのようなときですか\n",
      "1001 なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか\n",
      "------------devの質問---------------\n",
      "0908 自己写像の学習において使われる入力と出力の距離は何ですか\n",
      "------------trainの検索結果------------\n",
      "0908 深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに\n",
      "0908 入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか\n",
      "1309 入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか\n",
      "------------devの質問---------------\n",
      "0907 事前学習法の考え方は何ですか\n",
      "------------trainの検索結果------------\n",
      "1412 ラベル伝搬法の考え方は何ですか\n",
      "0907 事前学習法とは何ですか\n",
      "0907 事前学習とは何ですか\n",
      "------------devの質問---------------\n",
      "0611 回帰木ってなんですか\n",
      "------------trainの検索結果------------\n",
      "0611 回帰木って何ですか\n",
      "0606 Ridge回帰ってなんですか\n",
      "0607 Lasso回帰ってなんですか\n",
      "------------devの質問---------------\n",
      "1209 リフト値とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1209 リフト値ってなんですか\n",
      "1209 リフト値の値からどのようなことがわかりますか\n",
      "1111 外れ値とは何ですか\n",
      "------------devの質問---------------\n",
      "1209 確信度の値からどのようなことがわかりますか\n",
      "------------trainの検索結果------------\n",
      "1209 リフト値の値からどのようなことがわかりますか\n",
      "0114 クラスタリングはどのようなことに使われますか\n",
      "1209 確信度とはなんですか\n",
      "------------devの質問---------------\n",
      "1303 形態素解析とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1303 日本語の形態素列の特徴は何ですか\n",
      "1304 観測素性とは何ですか\n",
      "1304 遷移素性とは何ですか\n",
      "------------devの質問---------------\n",
      "1506 政策とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1507 マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか\n",
      "1506 マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか\n",
      "1404 オーバーラップとは何ですか\n",
      "------------devの質問---------------\n",
      "0305 バイアスとはなんですか\n",
      "------------trainの検索結果------------\n",
      "0610 バイアスってなんですか\n",
      "0610 バイアスと分散はどのような関係ですか\n",
      "0305 バイアスって何ですか\n",
      "------------devの質問---------------\n",
      "1110 x-meansアルゴリズムとは何ですか\n",
      "------------trainの検索結果------------\n",
      "1110 X-meansアルゴリズムは、どのよなアルゴリズムなのですか\n",
      "1411 YATSIアルゴリズムとは何ですか\n",
      "1214 FP-Growthアルゴリズムとは何ですか\n",
      "------------devの質問---------------\n",
      "0907 誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか\n",
      "------------trainの検索結果------------\n",
      "0810 多段階に誤差逆伝播法を適用すると問題はありますか\n",
      "0508 二乗誤差を最小にするように識別関数を調整する方法を何と言いますか\n",
      "0805 出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か\n",
      "------------devの質問---------------\n",
      "0805 誤差逆伝播法とはどういう手法ですか\n",
      "------------trainの検索結果------------\n",
      "0805 誤差逆伝播法とは何か\n",
      "0805 誤差逆伝播法とは何ですか\n",
      "0810 誤差逆伝播法における問題点は何か\n",
      "------------devの質問---------------\n",
      "0911 ドロップアウトによって過学習が生じにくくなっている理由はなんですか\n",
      "------------trainの検索結果------------\n",
      "0911 なぜドロップアウトによって過学習が回避できるのですか\n",
      "1510 なぜTD学習において温度を学習が進むにつれて小さくするのですか\n",
      "1501 強化学習が中間的学習という位置づけにある理由はなんですか\n",
      "------------devの質問---------------\n",
      "0206 分割学習法とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0206 分割学習法の際学習データが足りない場合どうしたらいいですか\n",
      "1103 分割最適化手法とは何ですか\n",
      "0911 ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという\n",
      "------------devの質問---------------\n",
      "1506 政策の良さはどのように評価されますか\n",
      "------------trainの検索結果------------\n",
      "1506 マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか\n",
      "0901 深層学習はどのような場面で応用されますか\n",
      "0803 基底関数ベクトルによる非線形識別面はどのように実現されていますか\n",
      "------------devの質問---------------\n",
      "0109 回帰とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0611 回帰木とはなんですか\n",
      "0616 カーネル回帰とはなんですか\n",
      "0601 回帰問題とはなんですか\n",
      "------------devの質問---------------\n",
      "1108 k-meansアルゴリズムとは何ですか\n",
      "------------trainの検索結果------------\n",
      "1110 k-means法の問題点はなんですか\n",
      "1110 X-meansアルゴリズムは、どのよなアルゴリズムなのですか\n",
      "1108 k-平均法とは、どのようなアルゴリズムなのですか\n",
      "------------devの質問---------------\n",
      "0508 最小二乗法って何ですか\n",
      "------------trainの検索結果------------\n",
      "0508 最小二乗法ってなんですか\n",
      "0508 最小二乗法とはなんですか\n",
      "0508 なぜ最小二乗法では正解との誤差を二乗するんですか\n",
      "------------devの質問---------------\n",
      "0315 Gini Inpurityは何を表しますか\n",
      "------------trainの検索結果------------\n",
      "0412 ベイジアンネットワークはどのような仮定を表現したものですか\n",
      "0612 Gini Impurityってなんですか\n",
      "0115 パターンマイニングの代表的な手法には何がありますか\n",
      "------------devの質問---------------\n",
      "0506 単層パーセプトロンはどうやって出力を決めるんですか\n",
      "------------trainの検索結果------------\n",
      "0506 パーセプトロンの出力はどのようになっていますか\n",
      "0402 条件付き確率ってどうやって求めるんですか\n",
      "0402 統計的識別ってどうやってやるんですか\n",
      "------------devの質問---------------\n",
      "0802 中間層を言い換えるとなんといいますか\n",
      "------------trainの検索結果------------\n",
      "0901 深層学習を言い換えると何になりますか\n",
      "0917 リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか\n",
      "0717 複数のパラメータを組合わせる空間を何と言いますか\n",
      "------------devの質問---------------\n",
      "0611 出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか\n",
      "------------trainの検索結果------------\n",
      "0509 学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか\n",
      "0316 特徴が数値であるときの決定木の学習はどのように行いますか\n",
      "0506 パーセプトロンの出力はどのようになっていますか\n",
      "------------devの質問---------------\n",
      "0111 識別ではどのようなことができますか\n",
      "------------trainの検索結果------------\n",
      "0803 ノードを階層的に組むとどのような識別面ができるか\n",
      "1209 リフト値の値からどのようなことがわかりますか\n",
      "0510 識別モデルはどのように作りますか\n",
      "------------devの質問---------------\n",
      "0811 ReLUだと勾配消失が起こらないのはなぜですか\n",
      "------------trainの検索結果------------\n",
      "0811 勾配が消失しない関数ってなんだったっけ\n",
      "0810 勾配消失問題とはなんですか\n",
      "0810 勾配消失問題とは何か\n",
      "------------devの質問---------------\n",
      "0209 再現率って何ですか\n",
      "------------trainの検索結果------------\n",
      "0901 表現学習ってなんですか\n",
      "0402 最大事後確率則って何ですか\n",
      "0402 条件付き確率ってどうやって求めるんですか\n",
      "------------devの質問---------------\n",
      "0701 サポートベクターマシンとはなんですか\n",
      "------------trainの検索結果------------\n",
      "0311 エントロピーとはなんですか\n",
      "1010 ブースティングとはなんですか\n",
      "0114 クラスタリングとはなんですか\n",
      "------------devの質問---------------\n",
      "1004 バギングとは何ですか\n",
      "------------trainの検索結果------------\n",
      "1004 バギングとはなんですか\n",
      "1004 バギングはどういう手法ですか\n",
      "1010 バギングやランダムフォレストとブースティングではどのような点で異なりますか\n",
      "------------devの質問---------------\n",
      "0105 機械学習の基本的な定義はなんですか\n",
      "------------trainの検索結果------------\n",
      "0104 深層学習と機械学習の違いはなんですか\n",
      "1406 半教師ある学習の基本的な進め方はどういったものですか\n",
      "0104 深層学習が他の機械学習手法と異なる点はなんですか\n",
      "------------devの質問---------------\n",
      "0712 非線形変換が存在するのはどういう条件の時ですか\n",
      "------------trainの検索結果------------\n",
      "0710 サポートベクトルマシンで高次元に非線形変換する際の条件はなに\n",
      "0715 複雑な非線形変換を求めるという操作を避ける方法をなんといいますか\n",
      "0803 基底関数ベクトルによる非線形識別面はどういう風に実現されているのか\n",
      "------------devの質問---------------\n",
      "0701 サポートベクトルマシンはどういう手法ですか\n",
      "------------trainの検索結果------------\n",
      "1004 バギングはどういう手法ですか\n",
      "0114 クラスタリングはどのような手法ですか\n",
      "1106 Ward法はどういうものですか\n",
      "------------devの質問---------------\n",
      "0105 機械学習では何ができますか\n",
      "------------trainの検索結果------------\n",
      "0103 機械学習はどんな時に利用されますか\n",
      "0103 機械学習で用いられるビッグデータとは何ですか\n",
      "0104 深層学習と機械学習の違いはなんですか\n",
      "------------devの質問---------------\n",
      "0701 なぜペナルティを設定するのですか\n",
      "------------trainの検索結果------------\n",
      "0707 ソフトマージンによる際の識別面の設定の仕方\n",
      "1012 バギングでは、個々のデータに対してどのように重みを設定しますか\n",
      "0717 SVMのハイパーパラメータの設定はどのように行いますか\n",
      "------------devの質問---------------\n",
      "1405 半教師あり学習は何によく使われますか\n",
      "------------trainの検索結果------------\n",
      "0117 半教師あり学習はどんなときに使われますか\n",
      "0116 半教師あり学習とはなんですか\n",
      "1406 半教師あり学習の識別器には何が適切ですか\n",
      "------------devの質問---------------\n",
      "0701 線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに\n",
      "------------trainの検索結果------------\n",
      "0502 線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに\n",
      "0713 高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか\n",
      "0701 学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか\n",
      "------------devの質問---------------\n",
      "0209 再現率とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0402 事後確率とはなんですか\n",
      "0404 事前確率とはなんですか\n",
      "0901 表現学習ってなんですか\n",
      "------------devの質問---------------\n",
      "0109 教師なし学習って何ですか\n",
      "------------trainの検索結果------------\n",
      "0109 教師なし学習とはどういう学習法ですか\n",
      "0113 教師なし学習の目的はなんですか\n",
      "0109 教師なし学習はどういうものですか\n",
      "------------devの質問---------------\n",
      "0911 多階層学習においてドロップアウトを用いるとどうなるか\n",
      "------------trainの検索結果------------\n",
      "0906 多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか\n",
      "0901 深層学習に用いるニューラルネットワークをなんとよぶか\n",
      "0906 多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか\n",
      "------------devの質問---------------\n",
      "0616 なぜ回帰問題にカーネル法を使うんですか\n",
      "------------trainの検索結果------------\n",
      "0616 回帰にカーネル法を取り入れる理由はなんですか\n",
      "0602 回帰問題は識別問題とどう違うんですか\n",
      "0601 回帰問題ってなんですか\n",
      "------------devの質問---------------\n",
      "0901 ディープニューラルネットワークとは何ですか\n",
      "------------trainの検索結果------------\n",
      "0912 画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか\n",
      "0810 ディープニューラルネットワークの性能が向上しない原因はなんですか\n",
      "1404 オーバーラップとは何ですか\n",
      "------------devの質問---------------\n",
      "0204 次元削減って何ですか\n",
      "------------trainの検索結果------------\n",
      "0204 次元削減とはなんですか\n",
      "0204 なぜ次元を削減するんですか\n",
      "0204 なぜ前処理で次元削減を行うのですか\n",
      "------------devの質問---------------\n",
      "0105 パターン認識とはどういうものですか\n",
      "------------trainの検索結果------------\n",
      "0105 パターン認識とはなんですか\n",
      "0105 パターン認識ってなんですか\n",
      "0505 識別モデルとはどういうものですか\n",
      "------------devの質問---------------\n",
      "1007 ランダムフォレストとは何ですか\n",
      "------------trainの検索結果------------\n",
      "1007 ランダムフォレストとはなんですか\n",
      "1009 通常の決定木とランダムフォレストとの違いは何ですか\n",
      "1008 ランダムフォレストの学習手順は\n",
      "------------devの質問---------------\n",
      "0109 教師なし学習とはなんですか\n",
      "------------trainの検索結果------------\n",
      "0113 教師なし学習の目的はなんですか\n",
      "0109 教師なし学習とはどういう学習法ですか\n",
      "0109 教師あり学習とはなんですか\n",
      "------------devの質問---------------\n",
      "0416 ここでいう目的変数って何ですか\n",
      "------------trainの検索結果------------\n",
      "0416 目的変数ってなんですか\n",
      "0708 スラック変数ってなんですか\n",
      "0311 分類能力が高いとはどういうことですか\n",
      "------------devの質問---------------\n",
      "0710 特徴空間の次元数$d$が大きい場合はどうなりますか\n",
      "------------trainの検索結果------------\n",
      "0509 学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか\n",
      "0710 特徴次元が多いとどうなるのか\n",
      "0509 学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか\n",
      "------------devの質問---------------\n",
      "0801 ニューラルとはどういう意味か\n",
      "------------trainの検索結果------------\n",
      "0406 尤度とはどういう意味ですか\n",
      "0606 Ridgeってどういう意味ですか\n",
      "0607 lassoってどういう意味ですか\n",
      "------------devの質問---------------\n",
      "0811 勾配消失問題を解決する手法はあるのか\n",
      "------------trainの検索結果------------\n",
      "0917 リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか\n",
      "0811 どのようにして勾配消失問題を解決しますか\n",
      "0810 勾配消失問題とは何か\n",
      "------------devの質問---------------\n",
      "1106 Ward法とはなんですか\n",
      "------------trainの検索結果------------\n",
      "1106 Ward法とは何ですか\n",
      "1106 Ward法はどういうものですか\n",
      "0208 k-NN法とはなんですか\n",
      "------------devの質問---------------\n",
      "0701 超平面とは何ですか\n",
      "------------trainの検索結果------------\n",
      "0701 学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか\n",
      "1108 k-平均法とはなんですか\n",
      "1108 k-平均法とは、どのようなアルゴリズムなのですか\n",
      "------------devの質問---------------\n",
      "0610 トレードオフってなんですか\n",
      "------------trainの検索結果------------\n",
      "0612 CARTってなんですか\n",
      "0114 クラスタリングってなんですか\n",
      "0901 DNNってなんですか\n",
      "------------devの質問---------------\n",
      "1409 自己学習の問題点は何ですか\n",
      "------------trainの検索結果------------\n",
      "1409 自己学習の問題点は何がありますか\n",
      "1407 自己学習とは何ですか\n",
      "1407 自己学習の狙いは何ですか\n",
      "------------devの質問---------------\n",
      "1205 a priori な原理の対偶とは何ですか\n",
      "------------trainの検索結果------------\n",
      "1205 a prioriな原理とは何ですか\n",
      "1207 a prioriアルゴリズムとは何ですか\n",
      "1110 モデルの対数尤度とは何ですか\n",
      "------結果-------\n",
      "54\n",
      "46\n",
      "精度： 0.54\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "\n",
    "\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "\n",
    "search_top_5 = []  # { 'id':'0101', \n",
    "                    #    'q':'多階層ニューラルネットワークとは何か', \n",
    "                    #    'ruizido' : [\n",
    "                    #       { 'top1_id': '0102, 'top1_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #       { 'top2_id': '0102, 'top2_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #          ...\n",
    "                    #     ]\n",
    "\n",
    "for idx in range(len(qa_dev)):\n",
    "    \n",
    "# sudachiを使うver    \n",
    "#     body = {\n",
    "#         \"query\" : {\n",
    "#             \"bool\": {\n",
    "#                 \"must\": [\n",
    "#                     {\n",
    "#                       \"query_string\": {\n",
    "#                         \"analyzer\": \"sudachi_analyzer\",\n",
    "#                         \"query\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question']\n",
    "#                       }\n",
    "#                     }\n",
    "#                 ]            \n",
    "#             }\n",
    "#         },\n",
    "#         \"highlight\": {\n",
    "#             \"fields\": {\n",
    "#                 \"itemCaption\": {}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#  sudachiを使わないver\n",
    "    body = {\n",
    "        \"query\" : {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                ]            \n",
    "            }\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"itemCaption\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    body['query']['bool']['must'].append(\n",
    "        { \n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"question\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question'] } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"------------devの質問---------------\")\n",
    "    print(qa_dev[idx]['title'], qa_dev[idx]['paragraphs'][0]['qas'][0]['question'])\n",
    "    print(\"------------trainの検索結果------------\")\n",
    "    result = es.search(index='qa_train2', body=body, size=1000) # ここをかえる！！！！\n",
    "    result_num = result['hits']['total']['value']\n",
    "    get_qa_train = result['hits']['hits']\n",
    "    print(get_qa_train[0]['_source']['id'],get_qa_train[0]['_source']['question'])\n",
    "    print(get_qa_train[1]['_source']['id'],get_qa_train[1]['_source']['question'])\n",
    "    print(get_qa_train[2]['_source']['id'],get_qa_train[2]['_source']['question'])\n",
    "    \n",
    "    \n",
    "    # ruizido_top_5に格納する辞書型を作る\n",
    "    mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_qa_train[0]['_source']['id'], 'top1_ans':  get_qa_train[0]['_source']['question'] },\n",
    "                        { 'top2_id':  get_qa_train[1]['_source']['id'], 'top2_ans':  get_qa_train[1]['_source']['question'] },\n",
    "                        { 'top3_id':  get_qa_train[2]['_source']['id'], 'top3_ans':  get_qa_train[2]['_source']['question'] },\n",
    "                        { 'top4_id':  get_qa_train[3]['_source']['id'], 'top4_ans':  get_qa_train[3]['_source']['question'] },\n",
    "                        { 'top5_id':  get_qa_train[4]['_source']['id'], 'top5_ans':  get_qa_train[4]['_source']['question'] }\n",
    "          ]\n",
    "          }\n",
    "    search_top_5.append(mydict)\n",
    "    \n",
    "    if qa_dev[idx]['title'] == get_qa_train[0]['_source']['id']:\n",
    "        true_count += 1\n",
    "    else:\n",
    "        false_count += 1\n",
    "\n",
    "\n",
    "print(\"------結果-------\")\n",
    "print(true_count)\n",
    "print(false_count)\n",
    "print(\"精度：\", true_count / (true_count + false_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0610',\n",
       "  'q': 'バイアスとはなんですか',\n",
       "  'search': [{'top1_id': '0610', 'top1_ans': 'バイアスってなんですか'},\n",
       "   {'top2_id': '0610', 'top2_ans': 'バイアスと分散はどのような関係ですか'},\n",
       "   {'top3_id': '0305', 'top3_ans': 'バイアスって何ですか'},\n",
       "   {'top4_id': '0311', 'top4_ans': 'エントロピーとはなんですか'},\n",
       "   {'top5_id': '1010', 'top5_ans': 'ブースティングとはなんですか'}]},\n",
       " {'id': '0306',\n",
       "  'q': '候補削除アルゴリズムとはなんですか',\n",
       "  'search': [{'top1_id': '0306', 'top1_ans': '候補削除アルゴリズムってなんですか'},\n",
       "   {'top2_id': '0204', 'top2_ans': '次元削減とはなんですか'},\n",
       "   {'top3_id': '1116', 'top3_ans': 'EMアルゴリズムとはなんですか'},\n",
       "   {'top4_id': '0305', 'top4_ans': 'FIND-Sアルゴリズムとはなんですか'},\n",
       "   {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとはなんですか'}]},\n",
       " {'id': '1502',\n",
       "  'q': '強化学習とは何ですか',\n",
       "  'search': [{'top1_id': '1502', 'top1_ans': '強化学習とはなんですか'},\n",
       "   {'top2_id': '1501', 'top2_ans': '強化学習はなぜ中間的学習なのですか'},\n",
       "   {'top3_id': '1502', 'top3_ans': '強化学習は例えば何に使われていますか'},\n",
       "   {'top4_id': '1501', 'top4_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'},\n",
       "   {'top5_id': '1503', 'top5_ans': '強化学習で、報酬が決定的な場合の学習はどのようなものですか'}]},\n",
       " {'id': '0605',\n",
       "  'q': '決定係数の定義は何ですか',\n",
       "  'search': [{'top1_id': '0605', 'top1_ans': '決定係数ってなんですか'},\n",
       "   {'top2_id': '0701', 'top2_ans': 'マージンの定義はなんですか'},\n",
       "   {'top3_id': '0901', 'top3_ans': '深層学習の定義はなにか'},\n",
       "   {'top4_id': '0102', 'top4_ans': '人工知能の定義はなんですか'},\n",
       "   {'top5_id': '1502', 'top5_ans': '意思決定エージェントの例は何ですか'}]},\n",
       " {'id': '0916',\n",
       "  'q': '時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか',\n",
       "  'search': [{'top1_id': '1301',\n",
       "    'top1_ans': '入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか'},\n",
       "   {'top2_id': '1301', 'top2_ans': '系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか'},\n",
       "   {'top3_id': '1301',\n",
       "    'top3_ans': '系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか'},\n",
       "   {'top4_id': '1301', 'top4_ans': '系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか'},\n",
       "   {'top5_id': '1301',\n",
       "    'top5_ans': '系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか'}]},\n",
       " {'id': '1502',\n",
       "  'q': '強化学習はどのような学習法ですか',\n",
       "  'search': [{'top1_id': '1503', 'top1_ans': '強化学習で、報酬が決定的な場合の学習はどのようなものですか'},\n",
       "   {'top2_id': '1501', 'top2_ans': '強化学習はなぜ中間的学習なのですか'},\n",
       "   {'top3_id': '1502', 'top3_ans': '強化学習とはなんですか'},\n",
       "   {'top4_id': '1501', 'top4_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'},\n",
       "   {'top5_id': '0307', 'top5_ans': '決定木とはどのような学習法ですか'}]},\n",
       " {'id': '0411',\n",
       "  'q': 'ゼロ頻度問題って何ですか',\n",
       "  'search': [{'top1_id': '0411', 'top1_ans': 'ゼロ頻度問題とは何ですか'},\n",
       "   {'top2_id': '0411', 'top2_ans': 'ゼロ頻度問題はどのような問題ですか'},\n",
       "   {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどうすれば回避できますか'},\n",
       "   {'top4_id': '0209', 'top4_ans': '精度って何ですか'},\n",
       "   {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]},\n",
       " {'id': '0810',\n",
       "  'q': '階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか',\n",
       "  'search': [{'top1_id': '0811', 'top1_ans': '勾配が消失しない関数ってなんだったっけ'},\n",
       "   {'top2_id': '0906',\n",
       "    'top2_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'},\n",
       "   {'top3_id': '0906', 'top3_ans': '多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか'},\n",
       "   {'top4_id': '0906',\n",
       "    'top4_ans': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか'},\n",
       "   {'top5_id': '0901', 'top5_ans': '深層学習に用いるニューラルネットワークをなんとよぶか'}]},\n",
       " {'id': '0514',\n",
       "  'q': '最急勾配法の問題点の対処法は何がありますか',\n",
       "  'search': [{'top1_id': '0514', 'top1_ans': '最急勾配法の問題点とはなんですか'},\n",
       "   {'top2_id': '0917', 'top2_ans': 'リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか'},\n",
       "   {'top3_id': '0514', 'top3_ans': '最急勾配法の欠点はなんですか'},\n",
       "   {'top4_id': '0906', 'top4_ans': '誤差逆伝播法の問題点は何がありますか'},\n",
       "   {'top5_id': '0811', 'top5_ans': '勾配消失問題への取組みにはどのような方法がありますか'}]},\n",
       " {'id': '1103',\n",
       "  'q': '階層的手法とは何ですか',\n",
       "  'search': [{'top1_id': '1104', 'top1_ans': '階層的クラスタリングとは何ですか'},\n",
       "   {'top2_id': '1104', 'top2_ans': '階層的クラスタリングとはなんですか'},\n",
       "   {'top3_id': '1104', 'top3_ans': '階層的クラスタリングってなんですか'},\n",
       "   {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークとは何か'},\n",
       "   {'top5_id': '1104', 'top5_ans': '階層的クラスタリングは、どのようなアルゴリズムですか'}]},\n",
       " {'id': '0805',\n",
       "  'q': 'なぜ重みを更新するのですか',\n",
       "  'search': [{'top1_id': '0513', 'top1_ans': '最急勾配法の重みの更新はどうなったら止まりますか'},\n",
       "   {'top2_id': '1012', 'top2_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'},\n",
       "   {'top3_id': '0717', 'top3_ans': 'スラック変数の重みは離散値ですか、連続値ですか'},\n",
       "   {'top4_id': '0906',\n",
       "    'top4_ans': '多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか'},\n",
       "   {'top5_id': '0204', 'top5_ans': 'なぜ次元を削減するんですか'}]},\n",
       " {'id': '0412',\n",
       "  'q': 'ベイジアンネットワークって何ですか',\n",
       "  'search': [{'top1_id': '0412', 'top1_ans': 'ベイジアンネットワークってなんですか'},\n",
       "   {'top2_id': '0413', 'top2_ans': 'ベイジアンネットワークのメリットは何ですか'},\n",
       "   {'top3_id': '0417', 'top3_ans': 'ベイジアンネットワークで重要な項目は何ですか'},\n",
       "   {'top4_id': '0612', 'top4_ans': 'CARTって何ですか'},\n",
       "   {'top5_id': '0115', 'top5_ans': 'パターンマイニングって何ですか'}]},\n",
       " {'id': '0614',\n",
       "  'q': 'モデル木って何ですか',\n",
       "  'search': [{'top1_id': '0614', 'top1_ans': 'モデル木ってなんですか'},\n",
       "   {'top2_id': '0611', 'top2_ans': '回帰木って何ですか'},\n",
       "   {'top3_id': '0505', 'top3_ans': '識別モデルってなんですか'},\n",
       "   {'top4_id': '0114', 'top4_ans': 'モデル推定ってなんですか'},\n",
       "   {'top5_id': '0614', 'top5_ans': 'モデル木とはなんですか'}]},\n",
       " {'id': '0601',\n",
       "  'q': '回帰問題の学習目的はなんですか',\n",
       "  'search': [{'top1_id': '0601', 'top1_ans': '回帰問題とはなんですか'},\n",
       "   {'top2_id': '0601', 'top2_ans': '回帰問題ってなんですか'},\n",
       "   {'top3_id': '0602', 'top3_ans': '回帰問題は識別問題とどう違うんですか'},\n",
       "   {'top4_id': '0113', 'top4_ans': '教師なし学習の目的はなんですか'},\n",
       "   {'top5_id': '0104', 'top5_ans': '深層学習が得意な問題はなんですか'}]},\n",
       " {'id': '0717',\n",
       "  'q': 'グリッドサーチはなんのためにやるんですか',\n",
       "  'search': [{'top1_id': '0115',\n",
       "    'top1_ans': 'データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか'},\n",
       "   {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'},\n",
       "   {'top3_id': '1507',\n",
       "    'top3_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'},\n",
       "   {'top4_id': '0402', 'top4_ans': '統計的識別ってどうやってやるんですか'},\n",
       "   {'top5_id': '0505', 'top5_ans': 'パーセプトロンを多層に重ねたものはなんですか'}]},\n",
       " {'id': '1412',\n",
       "  'q': 'ラベル伝搬法の考え方はどのようなものですか',\n",
       "  'search': [{'top1_id': '1412', 'top1_ans': 'ラベル伝搬法の考え方にある仮定はどのようなものですか'},\n",
       "   {'top2_id': '1412', 'top2_ans': 'ラベル伝搬法の考え方は何ですか'},\n",
       "   {'top3_id': '0611', 'top3_ans': '決定木はどのような考えですか'},\n",
       "   {'top4_id': '1507',\n",
       "    'top4_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'},\n",
       "   {'top5_id': '0614', 'top5_ans': 'モデル木はどのような方法ですか'}]},\n",
       " {'id': '0602',\n",
       "  'q': '回帰問題が識別問題と異なるのはどのような点ですか',\n",
       "  'search': [{'top1_id': '0602', 'top1_ans': '回帰問題は識別問題とどう違うんですか'},\n",
       "   {'top2_id': '0601', 'top2_ans': '回帰問題とはなんですか'},\n",
       "   {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどのような問題ですか'},\n",
       "   {'top4_id': '0810', 'top4_ans': '勾配消失問題とはどのような問題ですか'},\n",
       "   {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]},\n",
       " {'id': '0805',\n",
       "  'q': '出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか',\n",
       "  'search': [{'top1_id': '0805',\n",
       "    'top1_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'},\n",
       "   {'top2_id': '0805', 'top2_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'},\n",
       "   {'top3_id': '0810',\n",
       "    'top3_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'},\n",
       "   {'top4_id': '0810', 'top4_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'},\n",
       "   {'top5_id': '0717',\n",
       "    'top5_ans': 'パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか'}]},\n",
       " {'id': '0413',\n",
       "  'q': 'ベイジアンネットワークの利点とはなんですか',\n",
       "  'search': [{'top1_id': '0413', 'top1_ans': 'ベイジアンネットワークの利点はなんですか'},\n",
       "   {'top2_id': '0811', 'top2_ans': 'ReLUを用いる利点はなんですか'},\n",
       "   {'top3_id': '0412', 'top3_ans': 'ベイジアンネットワークってなんですか'},\n",
       "   {'top4_id': '0413', 'top4_ans': 'ベイジアンネットワークのメリットは何ですか'},\n",
       "   {'top5_id': '0514', 'top5_ans': '最急勾配法の問題点とはなんですか'}]},\n",
       " {'id': '0611',\n",
       "  'q': '回帰木とはなに',\n",
       "  'search': [{'top1_id': '0611', 'top1_ans': '回帰木とはなんですか'},\n",
       "   {'top2_id': '0614', 'top2_ans': 'モデル木は回帰木とどう違いますか'},\n",
       "   {'top3_id': '0611', 'top3_ans': '回帰木って何ですか'},\n",
       "   {'top4_id': '0611', 'top4_ans': '回帰木の特徴はなんですか'},\n",
       "   {'top5_id': '0612', 'top5_ans': 'CARTは回帰木とどう違いますか'}]},\n",
       " {'id': '0811',\n",
       "  'q': 'ReLUとは何ですか',\n",
       "  'search': [{'top1_id': '0811', 'top1_ans': 'ReLu関数の良さは何ですか'},\n",
       "   {'top2_id': '0811', 'top2_ans': 'ReLUを用いる利点はなんですか'},\n",
       "   {'top3_id': '0811', 'top3_ans': 'ReLUを用いると誤差はどうなりますか'},\n",
       "   {'top4_id': '0811', 'top4_ans': '活性化関数にReLUを用いると誤差が失われないのはなぜですか'},\n",
       "   {'top5_id': '1404', 'top5_ans': 'オーバーラップとは何ですか'}]},\n",
       " {'id': '0803',\n",
       "  'q': 'ノードを階層的に組むと非線形識別面が実現できるのはなぜか',\n",
       "  'search': [{'top1_id': '0803', 'top1_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのか'},\n",
       "   {'top2_id': '0803', 'top2_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのですか'},\n",
       "   {'top3_id': '0803', 'top3_ans': 'なぜノードを階層的に組むと非線形識別面を実現できるんですか'},\n",
       "   {'top4_id': '0803', 'top4_ans': 'ノードを階層的に組むとどのような識別面ができるか'},\n",
       "   {'top5_id': '0803', 'top5_ans': '基底関数ベクトルによる非線形識別面はどういう風に実現されているのか'}]},\n",
       " {'id': '0302',\n",
       "  'q': 'クラスとはなんですか',\n",
       "  'search': [{'top1_id': '0302', 'top1_ans': 'クラスって何ですか'},\n",
       "   {'top2_id': '0402', 'top2_ans': '最も確率が高いクラスを出力とする手法はなに'},\n",
       "   {'top3_id': '0405', 'top3_ans': '事後確率が最大となるクラスは、何を求めることで得られますか'},\n",
       "   {'top4_id': '0402', 'top4_ans': '事後確率が最大となるクラスを識別結果とする方法を何と言いますか'},\n",
       "   {'top5_id': '0802', 'top5_ans': 'フィードフォワード型モデルの出力層のクラスの数はいくつ'}]},\n",
       " {'id': '1219',\n",
       "  'q': '協調フィルタリングではどのようなデータが必要ですか',\n",
       "  'search': [{'top1_id': '1219', 'top1_ans': '協調フィルタリングとはどのようなものですか'},\n",
       "   {'top2_id': '1219', 'top2_ans': '協調フィルタリングとは何ですか'},\n",
       "   {'top3_id': '1506', 'top3_ans': 'なぜ割引率が必要なのですか'},\n",
       "   {'top4_id': '0802', 'top4_ans': 'フィードフォワード型ニューラルネットワークを構成するには何が必要ですか'},\n",
       "   {'top5_id': '1114', 'top5_ans': '事前確率を求めるには何が必要ですか'}]},\n",
       " {'id': '0606',\n",
       "  'q': '汎化能力という点で望ましい線形回帰式の性質はなんですか',\n",
       "  'search': [{'top1_id': '0614', 'top1_ans': '回帰木と線形回帰の双方のよいところを取った方法はなんですか'},\n",
       "   {'top2_id': '0502', 'top2_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'},\n",
       "   {'top3_id': '0810', 'top3_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'},\n",
       "   {'top4_id': '0707', 'top4_ans': '線形分離可能でない場合はどうすれば良いですか'},\n",
       "   {'top5_id': '0508', 'top5_ans': '線形分離不可能なデータにはどのように対処しますか'}]},\n",
       " {'id': '0917',\n",
       "  'q': '中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか',\n",
       "  'search': [{'top1_id': '0917',\n",
       "    'top1_ans': 'リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか'},\n",
       "   {'top2_id': '0916',\n",
       "    'top2_ans': '中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか'},\n",
       "   {'top3_id': '0917', 'top3_ans': 'LSTMで置き換えられるメモリユニットの名前はなんですか'},\n",
       "   {'top4_id': '0901', 'top4_ans': '深層学習を言い換えると何になりますか'},\n",
       "   {'top5_id': '0911',\n",
       "    'top5_ans': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという'}]},\n",
       " {'id': '0113',\n",
       "  'q': '教師なし学習では何を学習しますか',\n",
       "  'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'},\n",
       "   {'top2_id': '0113', 'top2_ans': '教師なし学習の目的はなんですか'},\n",
       "   {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'},\n",
       "   {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'},\n",
       "   {'top5_id': '0109', 'top5_ans': '教師あり学習とはなんですか'}]},\n",
       " {'id': '0406',\n",
       "  'q': 'i.i.d. (independent and identically distributed) とはなんですか',\n",
       "  'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'},\n",
       "   {'top2_id': '1010', 'top2_ans': 'ブースティングとはなんですか'},\n",
       "   {'top3_id': '0114', 'top3_ans': 'クラスタリングとはなんですか'},\n",
       "   {'top4_id': '1004', 'top4_ans': 'バギングとはなんですか'},\n",
       "   {'top5_id': '0612', 'top5_ans': 'CARTとはなんですか'}]},\n",
       " {'id': '0713',\n",
       "  'q': 'SVMの方法は例えば何に使われていますか',\n",
       "  'search': [{'top1_id': '1502', 'top1_ans': '強化学習は例えば何に使われていますか'},\n",
       "   {'top2_id': '0713', 'top2_ans': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか'},\n",
       "   {'top3_id': '0114', 'top3_ans': 'クラスタリングはどのようなことに使われますか'},\n",
       "   {'top4_id': '0908', 'top4_ans': '入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか'},\n",
       "   {'top5_id': '0117', 'top5_ans': '半教師あり学習はどんなときに使われますか'}]},\n",
       " {'id': '1108',\n",
       "  'q': 'k-平均法って何ですか',\n",
       "  'search': [{'top1_id': '1108', 'top1_ans': 'k-平均法とはなんですか'},\n",
       "   {'top2_id': '1108', 'top2_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'},\n",
       "   {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'},\n",
       "   {'top4_id': '0512', 'top4_ans': '最急勾配法って何ですか'},\n",
       "   {'top5_id': '1110', 'top5_ans': 'k-means法の問題点はなんですか'}]},\n",
       " {'id': '1111',\n",
       "  'q': '異常検出とは何ですか',\n",
       "  'search': [{'top1_id': '1112', 'top1_ans': '局所異常因子では、どのように外れ値を検知するのですか'},\n",
       "   {'top2_id': '1112', 'top2_ans': '局所異常因子とは何ですか'},\n",
       "   {'top3_id': '0917', 'top3_ans': 'LSTMと通常のユニットの違いは何ですか'},\n",
       "   {'top4_id': '1201', 'top4_ans': '頻出項目抽出とは何ですか'},\n",
       "   {'top5_id': '1009', 'top5_ans': '通常の決定木とランダムフォレストとの違いは何ですか'}]},\n",
       " {'id': '0410',\n",
       "  'q': 'ナイーブベイズ識別法とはなんですか',\n",
       "  'search': [{'top1_id': '0410', 'top1_ans': 'ナイーブベイズ識別って何ですか'},\n",
       "   {'top2_id': '0505', 'top2_ans': '識別関数法とはなんですか'},\n",
       "   {'top3_id': '0410', 'top3_ans': 'ナイーブベイス識別法ってなんですか'},\n",
       "   {'top4_id': '0505', 'top4_ans': '識別関数法ってなんですか'},\n",
       "   {'top5_id': '0510', 'top5_ans': '識別面とはなんですか'}]},\n",
       " {'id': '0901',\n",
       "  'q': 'DNNとは何ですか',\n",
       "  'search': [{'top1_id': '0901', 'top1_ans': 'DNNってなんですか'},\n",
       "   {'top2_id': '1404', 'top2_ans': 'オーバーラップとは何ですか'},\n",
       "   {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'},\n",
       "   {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'},\n",
       "   {'top5_id': '1306', 'top5_ans': 'CRFとは何ですか'}]},\n",
       " {'id': '0508',\n",
       "  'q': '二乗誤差とはなんですか',\n",
       "  'search': [{'top1_id': '0508', 'top1_ans': '二乗誤差とは何ですか'},\n",
       "   {'top2_id': '0508', 'top2_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'},\n",
       "   {'top3_id': '0508', 'top3_ans': '最小二乗法とはなんですか'},\n",
       "   {'top4_id': '0508', 'top4_ans': '最小二乗法ってなんですか'},\n",
       "   {'top5_id': '0508', 'top5_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}]},\n",
       " {'id': '0407',\n",
       "  'q': '対数尤度とはなんですか',\n",
       "  'search': [{'top1_id': '1110', 'top1_ans': 'モデルの対数尤度とは何ですか'},\n",
       "   {'top2_id': '0407', 'top2_ans': '何故尤度の対数をとって計算するんですか'},\n",
       "   {'top3_id': '0406', 'top3_ans': '尤度とはなんですか'},\n",
       "   {'top4_id': '0405', 'top4_ans': '尤度とは何ですか'},\n",
       "   {'top5_id': '0406', 'top5_ans': '尤度とはどういう意味ですか'}]},\n",
       " {'id': '0708',\n",
       "  'q': 'スラック変数が小さいほうがいいのはなぜか',\n",
       "  'search': [{'top1_id': '0708', 'top1_ans': 'なぜスラック変数が小さい方が良いのですか'},\n",
       "   {'top2_id': '0701', 'top2_ans': 'マージンは大きいほうがいいんですか'},\n",
       "   {'top3_id': '1510', 'top3_ans': 'なぜTD学習において温度を学習が進むにつれて小さくするのですか'},\n",
       "   {'top4_id': '0810',\n",
       "    'top4_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'},\n",
       "   {'top5_id': '0811', 'top5_ans': '活性化関数にReLUを用いると誤差が失われないのはなぜですか'}]},\n",
       " {'id': '0511',\n",
       "  'q': '入力が負例のときは、確率はどう求められますか',\n",
       "  'search': [{'top1_id': '1114', 'top1_ans': '事前確率はどのように求めますか'},\n",
       "   {'top2_id': '0405', 'top2_ans': '事後確率が最大となるクラスは、何を求めることで得られますか'},\n",
       "   {'top3_id': '0402', 'top3_ans': '条件付き確率ってどうやって求めるんですか'},\n",
       "   {'top4_id': '0405', 'top4_ans': '事後確率が最大になるクラスはどうやって得られますか'},\n",
       "   {'top5_id': '0402', 'top5_ans': '入力を観測した後で計算される確率は何と言いますか'}]},\n",
       " {'id': '1110',\n",
       "  'q': 'X-meansアルゴリズムとはなんですか',\n",
       "  'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'},\n",
       "   {'top2_id': '1110', 'top2_ans': 'k-means法の問題点はなんですか'},\n",
       "   {'top3_id': '1116', 'top3_ans': 'EMアルゴリズムとはなんですか'},\n",
       "   {'top4_id': '0305', 'top4_ans': 'FIND-Sアルゴリズムとはなんですか'},\n",
       "   {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとはなんですか'}]},\n",
       " {'id': '0313',\n",
       "  'q': '過学習を避けるためにはどのような方法がありますか',\n",
       "  'search': [{'top1_id': '1507',\n",
       "    'top1_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'},\n",
       "   {'top2_id': '0715', 'top2_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'},\n",
       "   {'top3_id': '0911', 'top3_ans': 'なぜドロップアウトによって過学習が回避できるのですか'},\n",
       "   {'top4_id': '0903', 'top4_ans': '深層学習の中で特化した構造にはどのようなものがありますか'},\n",
       "   {'top5_id': '1408', 'top5_ans': '自己学習の性質にはどのようなものがありますか'}]},\n",
       " {'id': '0117',\n",
       "  'q': '半教師あり学習に適した状況はどのように作られますか',\n",
       "  'search': [{'top1_id': '1402', 'top1_ans': '半教師あり学習に適した数値特徴データとはどのようなものですか'},\n",
       "   {'top2_id': '1406', 'top2_ans': '半教師あり学習の識別器に適切なのはどのようなものですか'},\n",
       "   {'top3_id': '0117', 'top3_ans': '半教師あり学習はどんなときに使われますか'},\n",
       "   {'top4_id': '1406', 'top4_ans': '半教師あり学習の識別器には何が適切ですか'},\n",
       "   {'top5_id': '0116', 'top5_ans': '半教師あり学習とはどのようなものですか'}]},\n",
       " {'id': '0109',\n",
       "  'q': '教師あり学習って何ですか',\n",
       "  'search': [{'top1_id': '0109', 'top1_ans': '教師あり学習とはなんですか'},\n",
       "   {'top2_id': '0116', 'top2_ans': '半教師あり学習とはなんですか'},\n",
       "   {'top3_id': '1402', 'top3_ans': '半教師あり学習が成立する条件は何ですか'},\n",
       "   {'top4_id': '1406', 'top4_ans': '半教師あり学習の識別器には何が適切ですか'},\n",
       "   {'top5_id': '0116', 'top5_ans': '半教師あり学習とはどのようなものですか'}]},\n",
       " {'id': '0204',\n",
       "  'q': '主成分分析ってどういうことをしているんですか',\n",
       "  'search': [{'top1_id': '0205', 'top1_ans': '主成分分析って何ですか'},\n",
       "   {'top2_id': '0205', 'top2_ans': '主成分分析とはなんですか'},\n",
       "   {'top3_id': '0205', 'top3_ans': '主成分分析とは何ですか'},\n",
       "   {'top4_id': '0311', 'top4_ans': '分類能力が高いとはどういうことですか'},\n",
       "   {'top5_id': '1220',\n",
       "    'top5_ans': '推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか'}]},\n",
       " {'id': '0901',\n",
       "  'q': '深層学習とはどのようなものか',\n",
       "  'search': [{'top1_id': '0901', 'top1_ans': '深層学習はどのようなものですか'},\n",
       "   {'top2_id': '0903', 'top2_ans': '深層学習の中で特化した構造にはどのようなものがありますか'},\n",
       "   {'top3_id': '0901', 'top3_ans': '深層学習はどのような場面で応用されますか'},\n",
       "   {'top4_id': '0901', 'top4_ans': '深層学習はどのような分野で用いられていますか'},\n",
       "   {'top5_id': '0901', 'top5_ans': '深層学習のポイントは何'}]},\n",
       " {'id': '1012',\n",
       "  'q': 'ブースティングで、個々のデータに対してどのように重みを設定するのですか',\n",
       "  'search': [{'top1_id': '1012',\n",
       "    'top1_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'},\n",
       "   {'top2_id': '0717', 'top2_ans': 'SVMのハイパーパラメータの設定はどのように行いますか'},\n",
       "   {'top3_id': '0508', 'top3_ans': '線形分離不可能なデータにはどのように対処しますか'},\n",
       "   {'top4_id': '0707', 'top4_ans': 'ソフトマージンによる際の識別面の設定の仕方'},\n",
       "   {'top5_id': '1012', 'top5_ans': 'ブースティングでは、どのように識別器を作成するのですか'}]},\n",
       " {'id': '1110',\n",
       "  'q': 'BICとは何ですか',\n",
       "  'search': [{'top1_id': '1404', 'top1_ans': 'オーバーラップとは何ですか'},\n",
       "   {'top2_id': '0717', 'top2_ans': 'グリッドサーチとは何ですか'},\n",
       "   {'top3_id': '0701', 'top3_ans': 'サポートベクトルマシンとは何ですか'},\n",
       "   {'top4_id': '1306', 'top4_ans': 'CRFとは何ですか'},\n",
       "   {'top5_id': '1306', 'top5_ans': 'ビタビアルゴリズムとは何ですか'}]},\n",
       " {'id': '0707',\n",
       "  'q': 'ソフトマージンとは何ですか',\n",
       "  'search': [{'top1_id': '0707', 'top1_ans': 'ソフトマージンによる際の識別面の設定の仕方'},\n",
       "   {'top2_id': '1404', 'top2_ans': 'オーバーラップとは何ですか'},\n",
       "   {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'},\n",
       "   {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'},\n",
       "   {'top5_id': '1306', 'top5_ans': 'CRFとは何ですか'}]},\n",
       " {'id': '0507',\n",
       "  'q': 'パーセプトロンの学習アルゴリズムはどのようなときに停止しますか',\n",
       "  'search': [{'top1_id': '1306', 'top1_ans': '条件付き確率場の学習にはどのような性質を利用しますか'},\n",
       "   {'top2_id': '0506', 'top2_ans': 'パーセプトロンの出力はどのようになっていますか'},\n",
       "   {'top3_id': '0507', 'top3_ans': 'なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか'},\n",
       "   {'top4_id': '0316', 'top4_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'},\n",
       "   {'top5_id': '1207', 'top5_ans': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか'}]},\n",
       " {'id': '0811',\n",
       "  'q': 'ReLuってなんですか',\n",
       "  'search': [{'top1_id': '0811', 'top1_ans': 'ReLUを用いる利点はなんですか'},\n",
       "   {'top2_id': '0612', 'top2_ans': 'CARTってなんですか'},\n",
       "   {'top3_id': '0114', 'top3_ans': 'クラスタリングってなんですか'},\n",
       "   {'top4_id': '0901', 'top4_ans': 'DNNってなんですか'},\n",
       "   {'top5_id': '0715', 'top5_ans': 'カーネルトリックってなんですか'}]},\n",
       " {'id': '0504',\n",
       "  'q': 'なぜ生成モデルアプローチは問題を難しくしているといえるのですか',\n",
       "  'search': [{'top1_id': '0917',\n",
       "    'top1_ans': 'リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか'},\n",
       "   {'top2_id': '0504', 'top2_ans': '生成モデルアプローチが有効なのはどのようなときですか'},\n",
       "   {'top3_id': '1001', 'top3_ans': 'なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか'},\n",
       "   {'top4_id': '1309', 'top4_ans': '入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか'},\n",
       "   {'top5_id': '0509',\n",
       "    'top5_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}]},\n",
       " {'id': '0908',\n",
       "  'q': '自己写像の学習において使われる入力と出力の距離は何ですか',\n",
       "  'search': [{'top1_id': '0908',\n",
       "    'top1_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'},\n",
       "   {'top2_id': '0908', 'top2_ans': '入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか'},\n",
       "   {'top3_id': '1309', 'top3_ans': '入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか'},\n",
       "   {'top4_id': '1309', 'top4_ans': '入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか'},\n",
       "   {'top5_id': '1407', 'top5_ans': '自己学習の狙いは何ですか'}]},\n",
       " {'id': '0907',\n",
       "  'q': '事前学習法の考え方は何ですか',\n",
       "  'search': [{'top1_id': '1412', 'top1_ans': 'ラベル伝搬法の考え方は何ですか'},\n",
       "   {'top2_id': '0907', 'top2_ans': '事前学習法とは何ですか'},\n",
       "   {'top3_id': '0907', 'top3_ans': '事前学習とは何ですか'},\n",
       "   {'top4_id': '1412', 'top4_ans': 'ラベル伝搬法の考え方にある仮定はどのようなものですか'},\n",
       "   {'top5_id': '0404', 'top5_ans': '事前確率とは何ですか'}]},\n",
       " {'id': '0611',\n",
       "  'q': '回帰木ってなんですか',\n",
       "  'search': [{'top1_id': '0611', 'top1_ans': '回帰木って何ですか'},\n",
       "   {'top2_id': '0606', 'top2_ans': 'Ridge回帰ってなんですか'},\n",
       "   {'top3_id': '0607', 'top3_ans': 'Lasso回帰ってなんですか'},\n",
       "   {'top4_id': '0611', 'top4_ans': '回帰木とはなんですか'},\n",
       "   {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]},\n",
       " {'id': '1209',\n",
       "  'q': 'リフト値とは何ですか',\n",
       "  'search': [{'top1_id': '1209', 'top1_ans': 'リフト値ってなんですか'},\n",
       "   {'top2_id': '1209', 'top2_ans': 'リフト値の値からどのようなことがわかりますか'},\n",
       "   {'top3_id': '1111', 'top3_ans': '外れ値とは何ですか'},\n",
       "   {'top4_id': '0209', 'top4_ans': 'F値って何ですか'},\n",
       "   {'top5_id': '0209', 'top5_ans': 'F値とはなんですか'}]},\n",
       " {'id': '1209',\n",
       "  'q': '確信度の値からどのようなことがわかりますか',\n",
       "  'search': [{'top1_id': '1209', 'top1_ans': 'リフト値の値からどのようなことがわかりますか'},\n",
       "   {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのようなことに使われますか'},\n",
       "   {'top3_id': '1209', 'top3_ans': '確信度とはなんですか'},\n",
       "   {'top4_id': '1209', 'top4_ans': '確信度とは何ですか'},\n",
       "   {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]},\n",
       " {'id': '1303',\n",
       "  'q': '形態素解析とは何ですか',\n",
       "  'search': [{'top1_id': '1303', 'top1_ans': '日本語の形態素列の特徴は何ですか'},\n",
       "   {'top2_id': '1304', 'top2_ans': '観測素性とは何ですか'},\n",
       "   {'top3_id': '1304', 'top3_ans': '遷移素性とは何ですか'},\n",
       "   {'top4_id': '0205', 'top4_ans': '主成分分析とは何ですか'},\n",
       "   {'top5_id': '0205', 'top5_ans': '主成分分析って何ですか'}]},\n",
       " {'id': '1506',\n",
       "  'q': '政策とは何ですか',\n",
       "  'search': [{'top1_id': '1507',\n",
       "    'top1_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'},\n",
       "   {'top2_id': '1506', 'top2_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'},\n",
       "   {'top3_id': '1404', 'top3_ans': 'オーバーラップとは何ですか'},\n",
       "   {'top4_id': '0717', 'top4_ans': 'グリッドサーチとは何ですか'},\n",
       "   {'top5_id': '0701', 'top5_ans': 'サポートベクトルマシンとは何ですか'}]},\n",
       " {'id': '0305',\n",
       "  'q': 'バイアスとはなんですか',\n",
       "  'search': [{'top1_id': '0610', 'top1_ans': 'バイアスってなんですか'},\n",
       "   {'top2_id': '0610', 'top2_ans': 'バイアスと分散はどのような関係ですか'},\n",
       "   {'top3_id': '0305', 'top3_ans': 'バイアスって何ですか'},\n",
       "   {'top4_id': '0311', 'top4_ans': 'エントロピーとはなんですか'},\n",
       "   {'top5_id': '1010', 'top5_ans': 'ブースティングとはなんですか'}]},\n",
       " {'id': '1110',\n",
       "  'q': 'x-meansアルゴリズムとは何ですか',\n",
       "  'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'},\n",
       "   {'top2_id': '1411', 'top2_ans': 'YATSIアルゴリズムとは何ですか'},\n",
       "   {'top3_id': '1214', 'top3_ans': 'FP-Growthアルゴリズムとは何ですか'},\n",
       "   {'top4_id': '1207', 'top4_ans': 'a prioriアルゴリズムとは何ですか'},\n",
       "   {'top5_id': '1207', 'top5_ans': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか'}]},\n",
       " {'id': '0907',\n",
       "  'q': '誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか',\n",
       "  'search': [{'top1_id': '0810', 'top1_ans': '多段階に誤差逆伝播法を適用すると問題はありますか'},\n",
       "   {'top2_id': '0508', 'top2_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'},\n",
       "   {'top3_id': '0805', 'top3_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'},\n",
       "   {'top4_id': '0810',\n",
       "    'top4_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'},\n",
       "   {'top5_id': '0908',\n",
       "    'top5_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'}]},\n",
       " {'id': '0805',\n",
       "  'q': '誤差逆伝播法とはどういう手法ですか',\n",
       "  'search': [{'top1_id': '0805', 'top1_ans': '誤差逆伝播法とは何か'},\n",
       "   {'top2_id': '0805', 'top2_ans': '誤差逆伝播法とは何ですか'},\n",
       "   {'top3_id': '0810', 'top3_ans': '誤差逆伝播法における問題点は何か'},\n",
       "   {'top4_id': '0906', 'top4_ans': '誤差逆伝播法の問題点は何がありますか'},\n",
       "   {'top5_id': '0810',\n",
       "    'top5_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}]},\n",
       " {'id': '0911',\n",
       "  'q': 'ドロップアウトによって過学習が生じにくくなっている理由はなんですか',\n",
       "  'search': [{'top1_id': '0911', 'top1_ans': 'なぜドロップアウトによって過学習が回避できるのですか'},\n",
       "   {'top2_id': '1510', 'top2_ans': 'なぜTD学習において温度を学習が進むにつれて小さくするのですか'},\n",
       "   {'top3_id': '1501', 'top3_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'},\n",
       "   {'top4_id': '0911', 'top4_ans': 'ドロップアウトで過学習が防げるのはなぜですか'},\n",
       "   {'top5_id': '0313', 'top5_ans': '過学習って何ですか'}]},\n",
       " {'id': '0206',\n",
       "  'q': '分割学習法とはなんですか',\n",
       "  'search': [{'top1_id': '0206', 'top1_ans': '分割学習法の際学習データが足りない場合どうしたらいいですか'},\n",
       "   {'top2_id': '1103', 'top2_ans': '分割最適化手法とは何ですか'},\n",
       "   {'top3_id': '0911',\n",
       "    'top3_ans': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという'},\n",
       "   {'top4_id': '0104', 'top4_ans': '深層学習が他の機械学習手法と異なる点はなんですか'},\n",
       "   {'top5_id': '0109', 'top5_ans': '教師なし学習とはどういう学習法ですか'}]},\n",
       " {'id': '1506',\n",
       "  'q': '政策の良さはどのように評価されますか',\n",
       "  'search': [{'top1_id': '1506',\n",
       "    'top1_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'},\n",
       "   {'top2_id': '0901', 'top2_ans': '深層学習はどのような場面で応用されますか'},\n",
       "   {'top3_id': '0803', 'top3_ans': '基底関数ベクトルによる非線形識別面はどのように実現されていますか'},\n",
       "   {'top4_id': '0811', 'top4_ans': 'ReLu関数の良さは何ですか'},\n",
       "   {'top5_id': '0103', 'top5_ans': '機械学習はどんな時に利用されますか'}]},\n",
       " {'id': '0109',\n",
       "  'q': '回帰とはなんですか',\n",
       "  'search': [{'top1_id': '0611', 'top1_ans': '回帰木とはなんですか'},\n",
       "   {'top2_id': '0616', 'top2_ans': 'カーネル回帰とはなんですか'},\n",
       "   {'top3_id': '0601', 'top3_ans': '回帰問題とはなんですか'},\n",
       "   {'top4_id': '0606', 'top4_ans': 'Ridge回帰ってなんですか'},\n",
       "   {'top5_id': '0607', 'top5_ans': 'Lasso回帰ってなんですか'}]},\n",
       " {'id': '1108',\n",
       "  'q': 'k-meansアルゴリズムとは何ですか',\n",
       "  'search': [{'top1_id': '1110', 'top1_ans': 'k-means法の問題点はなんですか'},\n",
       "   {'top2_id': '1110', 'top2_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'},\n",
       "   {'top3_id': '1108', 'top3_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'},\n",
       "   {'top4_id': '1411', 'top4_ans': 'YATSIアルゴリズムとは何ですか'},\n",
       "   {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとは何ですか'}]},\n",
       " {'id': '0508',\n",
       "  'q': '最小二乗法って何ですか',\n",
       "  'search': [{'top1_id': '0508', 'top1_ans': '最小二乗法ってなんですか'},\n",
       "   {'top2_id': '0508', 'top2_ans': '最小二乗法とはなんですか'},\n",
       "   {'top3_id': '0508', 'top3_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'},\n",
       "   {'top4_id': '0508', 'top4_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'},\n",
       "   {'top5_id': '0508', 'top5_ans': '二乗誤差とは何ですか'}]},\n",
       " {'id': '0315',\n",
       "  'q': 'Gini Inpurityは何を表しますか',\n",
       "  'search': [{'top1_id': '0412', 'top1_ans': 'ベイジアンネットワークはどのような仮定を表現したものですか'},\n",
       "   {'top2_id': '0612', 'top2_ans': 'Gini Impurityってなんですか'},\n",
       "   {'top3_id': '0115', 'top3_ans': 'パターンマイニングの代表的な手法には何がありますか'},\n",
       "   {'top4_id': '0114', 'top4_ans': 'クラスタリングの代表的な手法には何がありますか'},\n",
       "   {'top5_id': '0112', 'top5_ans': '回帰の代表的な手法には何がありますか'}]},\n",
       " {'id': '0506',\n",
       "  'q': '単層パーセプトロンはどうやって出力を決めるんですか',\n",
       "  'search': [{'top1_id': '0506', 'top1_ans': 'パーセプトロンの出力はどのようになっていますか'},\n",
       "   {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'},\n",
       "   {'top3_id': '0402', 'top3_ans': '統計的識別ってどうやってやるんですか'},\n",
       "   {'top4_id': '0805', 'top4_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'},\n",
       "   {'top5_id': '0313', 'top5_ans': '決定木ではどうやって過学習を回避しますか'}]},\n",
       " {'id': '0802',\n",
       "  'q': '中間層を言い換えるとなんといいますか',\n",
       "  'search': [{'top1_id': '0901', 'top1_ans': '深層学習を言い換えると何になりますか'},\n",
       "   {'top2_id': '0917',\n",
       "    'top2_ans': 'リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか'},\n",
       "   {'top3_id': '0717', 'top3_ans': '複数のパラメータを組合わせる空間を何と言いますか'},\n",
       "   {'top4_id': '0802', 'top4_ans': '中間層はいくつ用意しますか'},\n",
       "   {'top5_id': '0715', 'top5_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}]},\n",
       " {'id': '0611',\n",
       "  'q': '出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか',\n",
       "  'search': [{'top1_id': '0509',\n",
       "    'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'},\n",
       "   {'top2_id': '0316', 'top2_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'},\n",
       "   {'top3_id': '0506', 'top3_ans': 'パーセプトロンの出力はどのようになっていますか'},\n",
       "   {'top4_id': '0906', 'top4_ans': '特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか'},\n",
       "   {'top5_id': '0701', 'top5_ans': '識別境界線と最も近いデータとの距離のことをなんといいますか'}]},\n",
       " {'id': '0111',\n",
       "  'q': '識別ではどのようなことができますか',\n",
       "  'search': [{'top1_id': '0803', 'top1_ans': 'ノードを階層的に組むとどのような識別面ができるか'},\n",
       "   {'top2_id': '1209', 'top2_ans': 'リフト値の値からどのようなことがわかりますか'},\n",
       "   {'top3_id': '0510', 'top3_ans': '識別モデルはどのように作りますか'},\n",
       "   {'top4_id': '0702', 'top4_ans': 'サポートベクトルマシンの識別面はどのような形になりますか'},\n",
       "   {'top5_id': '0114', 'top5_ans': 'クラスタリングはどのようなことに使われますか'}]},\n",
       " {'id': '0811',\n",
       "  'q': 'ReLUだと勾配消失が起こらないのはなぜですか',\n",
       "  'search': [{'top1_id': '0811', 'top1_ans': '勾配が消失しない関数ってなんだったっけ'},\n",
       "   {'top2_id': '0810', 'top2_ans': '勾配消失問題とはなんですか'},\n",
       "   {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何か'},\n",
       "   {'top4_id': '0810', 'top4_ans': '勾配消失問題とは何ですか'},\n",
       "   {'top5_id': '0810', 'top5_ans': '勾配消失問題とはどのような問題ですか'}]},\n",
       " {'id': '0209',\n",
       "  'q': '再現率って何ですか',\n",
       "  'search': [{'top1_id': '0901', 'top1_ans': '表現学習ってなんですか'},\n",
       "   {'top2_id': '0402', 'top2_ans': '最大事後確率則って何ですか'},\n",
       "   {'top3_id': '0402', 'top3_ans': '条件付き確率ってどうやって求めるんですか'},\n",
       "   {'top4_id': '0411', 'top4_ans': '確率のm推定ってなんですか'},\n",
       "   {'top5_id': '0402', 'top5_ans': '最大事後確率則ってなんですか'}]},\n",
       " {'id': '0701',\n",
       "  'q': 'サポートベクターマシンとはなんですか',\n",
       "  'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'},\n",
       "   {'top2_id': '1010', 'top2_ans': 'ブースティングとはなんですか'},\n",
       "   {'top3_id': '0114', 'top3_ans': 'クラスタリングとはなんですか'},\n",
       "   {'top4_id': '1004', 'top4_ans': 'バギングとはなんですか'},\n",
       "   {'top5_id': '0612', 'top5_ans': 'CARTとはなんですか'}]},\n",
       " {'id': '1004',\n",
       "  'q': 'バギングとは何ですか',\n",
       "  'search': [{'top1_id': '1004', 'top1_ans': 'バギングとはなんですか'},\n",
       "   {'top2_id': '1004', 'top2_ans': 'バギングはどういう手法ですか'},\n",
       "   {'top3_id': '1010', 'top3_ans': 'バギングやランダムフォレストとブースティングではどのような点で異なりますか'},\n",
       "   {'top4_id': '1012', 'top4_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'},\n",
       "   {'top5_id': '1404', 'top5_ans': 'オーバーラップとは何ですか'}]},\n",
       " {'id': '0105',\n",
       "  'q': '機械学習の基本的な定義はなんですか',\n",
       "  'search': [{'top1_id': '0104', 'top1_ans': '深層学習と機械学習の違いはなんですか'},\n",
       "   {'top2_id': '1406', 'top2_ans': '半教師ある学習の基本的な進め方はどういったものですか'},\n",
       "   {'top3_id': '0104', 'top3_ans': '深層学習が他の機械学習手法と異なる点はなんですか'},\n",
       "   {'top4_id': '0103', 'top4_ans': '機械学習はどんな時に利用されますか'},\n",
       "   {'top5_id': '0901', 'top5_ans': '深層学習の定義はなにか'}]},\n",
       " {'id': '0712',\n",
       "  'q': '非線形変換が存在するのはどういう条件の時ですか',\n",
       "  'search': [{'top1_id': '0710', 'top1_ans': 'サポートベクトルマシンで高次元に非線形変換する際の条件はなに'},\n",
       "   {'top2_id': '0715', 'top2_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'},\n",
       "   {'top3_id': '0803', 'top3_ans': '基底関数ベクトルによる非線形識別面はどういう風に実現されているのか'},\n",
       "   {'top4_id': '0704', 'top4_ans': '制約条件ではどのような式を用いますか'},\n",
       "   {'top5_id': '0710', 'top5_ans': '特徴空間を高次元に変換する際に重要な条件はなんですか'}]},\n",
       " {'id': '0701',\n",
       "  'q': 'サポートベクトルマシンはどういう手法ですか',\n",
       "  'search': [{'top1_id': '1004', 'top1_ans': 'バギングはどういう手法ですか'},\n",
       "   {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのような手法ですか'},\n",
       "   {'top3_id': '1106', 'top3_ans': 'Ward法はどういうものですか'},\n",
       "   {'top4_id': '0115', 'top4_ans': 'パターンマイニングとはどういう方法ですか'},\n",
       "   {'top5_id': '1509', 'top5_ans': 'モデルフリーの手法とはどのような場合ですか'}]},\n",
       " {'id': '0105',\n",
       "  'q': '機械学習では何ができますか',\n",
       "  'search': [{'top1_id': '0103', 'top1_ans': '機械学習はどんな時に利用されますか'},\n",
       "   {'top2_id': '0103', 'top2_ans': '機械学習で用いられるビッグデータとは何ですか'},\n",
       "   {'top3_id': '0104', 'top3_ans': '深層学習と機械学習の違いはなんですか'},\n",
       "   {'top4_id': '0104', 'top4_ans': '深層学習が他の機械学習手法と異なる点はなんですか'},\n",
       "   {'top5_id': '0102', 'top5_ans': '機械学習と人工知能の意味の差はありますか'}]},\n",
       " {'id': '0701',\n",
       "  'q': 'なぜペナルティを設定するのですか',\n",
       "  'search': [{'top1_id': '0707', 'top1_ans': 'ソフトマージンによる際の識別面の設定の仕方'},\n",
       "   {'top2_id': '1012', 'top2_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'},\n",
       "   {'top3_id': '0717', 'top3_ans': 'SVMのハイパーパラメータの設定はどのように行いますか'},\n",
       "   {'top4_id': '0204', 'top4_ans': 'なぜ次元を削減するんですか'},\n",
       "   {'top5_id': '1313', 'top5_ans': 'なぜHMMで最尤状態系列を求めるのですか'}]},\n",
       " {'id': '1405',\n",
       "  'q': '半教師あり学習は何によく使われますか',\n",
       "  'search': [{'top1_id': '0117', 'top1_ans': '半教師あり学習はどんなときに使われますか'},\n",
       "   {'top2_id': '0116', 'top2_ans': '半教師あり学習とはなんですか'},\n",
       "   {'top3_id': '1406', 'top3_ans': '半教師あり学習の識別器には何が適切ですか'},\n",
       "   {'top4_id': '0116', 'top4_ans': '半教師あり学習とはどのようなものですか'},\n",
       "   {'top5_id': '1402', 'top5_ans': '半教師あり学習が成立する条件は何ですか'}]},\n",
       " {'id': '0701',\n",
       "  'q': '線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに',\n",
       "  'search': [{'top1_id': '0502',\n",
       "    'top1_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'},\n",
       "   {'top2_id': '0713', 'top2_ans': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか'},\n",
       "   {'top3_id': '0701',\n",
       "    'top3_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'},\n",
       "   {'top4_id': '0502', 'top4_ans': '非線形性を持つデータに対して識別を行うにはどんな方法がありますか'},\n",
       "   {'top5_id': '0715', 'top5_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}]},\n",
       " {'id': '0209',\n",
       "  'q': '再現率とはなんですか',\n",
       "  'search': [{'top1_id': '0402', 'top1_ans': '事後確率とはなんですか'},\n",
       "   {'top2_id': '0404', 'top2_ans': '事前確率とはなんですか'},\n",
       "   {'top3_id': '0901', 'top3_ans': '表現学習ってなんですか'},\n",
       "   {'top4_id': '0901', 'top4_ans': '表現学習とは何ですか'},\n",
       "   {'top5_id': '0514', 'top5_ans': '確率的最急勾配法とはなんですか'}]},\n",
       " {'id': '0109',\n",
       "  'q': '教師なし学習って何ですか',\n",
       "  'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'},\n",
       "   {'top2_id': '0113', 'top2_ans': '教師なし学習の目的はなんですか'},\n",
       "   {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'},\n",
       "   {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'},\n",
       "   {'top5_id': '0109', 'top5_ans': '教師あり学習とはなんですか'}]},\n",
       " {'id': '0911',\n",
       "  'q': '多階層学習においてドロップアウトを用いるとどうなるか',\n",
       "  'search': [{'top1_id': '0906',\n",
       "    'top1_ans': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか'},\n",
       "   {'top2_id': '0901', 'top2_ans': '深層学習に用いるニューラルネットワークをなんとよぶか'},\n",
       "   {'top3_id': '0906',\n",
       "    'top3_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'},\n",
       "   {'top4_id': '0810', 'top4_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'},\n",
       "   {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]},\n",
       " {'id': '0616',\n",
       "  'q': 'なぜ回帰問題にカーネル法を使うんですか',\n",
       "  'search': [{'top1_id': '0616', 'top1_ans': '回帰にカーネル法を取り入れる理由はなんですか'},\n",
       "   {'top2_id': '0602', 'top2_ans': '回帰問題は識別問題とどう違うんですか'},\n",
       "   {'top3_id': '0601', 'top3_ans': '回帰問題ってなんですか'},\n",
       "   {'top4_id': '0601', 'top4_ans': '回帰問題とはなんですか'},\n",
       "   {'top5_id': '0616', 'top5_ans': 'カーネル回帰とはなんですか'}]},\n",
       " {'id': '0901',\n",
       "  'q': 'ディープニューラルネットワークとは何ですか',\n",
       "  'search': [{'top1_id': '0912',\n",
       "    'top1_ans': '画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか'},\n",
       "   {'top2_id': '0810', 'top2_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'},\n",
       "   {'top3_id': '1404', 'top3_ans': 'オーバーラップとは何ですか'},\n",
       "   {'top4_id': '0717', 'top4_ans': 'グリッドサーチとは何ですか'},\n",
       "   {'top5_id': '0701', 'top5_ans': 'サポートベクトルマシンとは何ですか'}]},\n",
       " {'id': '0204',\n",
       "  'q': '次元削減って何ですか',\n",
       "  'search': [{'top1_id': '0204', 'top1_ans': '次元削減とはなんですか'},\n",
       "   {'top2_id': '0204', 'top2_ans': 'なぜ次元を削減するんですか'},\n",
       "   {'top3_id': '0204', 'top3_ans': 'なぜ前処理で次元削減を行うのですか'},\n",
       "   {'top4_id': '0306', 'top4_ans': '候補削除アルゴリズムってなんですか'},\n",
       "   {'top5_id': '0710', 'top5_ans': '二次元から三次元の変換・写像で気をつけることはなんですか'}]},\n",
       " {'id': '0105',\n",
       "  'q': 'パターン認識とはどういうものですか',\n",
       "  'search': [{'top1_id': '0105', 'top1_ans': 'パターン認識とはなんですか'},\n",
       "   {'top2_id': '0105', 'top2_ans': 'パターン認識ってなんですか'},\n",
       "   {'top3_id': '0505', 'top3_ans': '識別モデルとはどういうものですか'},\n",
       "   {'top4_id': '0717', 'top4_ans': 'グリッドサーチとはどういうものですか'},\n",
       "   {'top5_id': '0114', 'top5_ans': 'クラスタリングとはどういうものですか'}]},\n",
       " {'id': '1007',\n",
       "  'q': 'ランダムフォレストとは何ですか',\n",
       "  'search': [{'top1_id': '1007', 'top1_ans': 'ランダムフォレストとはなんですか'},\n",
       "   {'top2_id': '1009', 'top2_ans': '通常の決定木とランダムフォレストとの違いは何ですか'},\n",
       "   {'top3_id': '1008', 'top3_ans': 'ランダムフォレストの学習手順は'},\n",
       "   {'top4_id': '1009', 'top4_ans': '通常の決定木とランダムフォレストの違いはなんですか'},\n",
       "   {'top5_id': '1010', 'top5_ans': 'バギングやランダムフォレストとブースティングではどのような点で異なりますか'}]},\n",
       " {'id': '0109',\n",
       "  'q': '教師なし学習とはなんですか',\n",
       "  'search': [{'top1_id': '0113', 'top1_ans': '教師なし学習の目的はなんですか'},\n",
       "   {'top2_id': '0109', 'top2_ans': '教師なし学習とはどういう学習法ですか'},\n",
       "   {'top3_id': '0109', 'top3_ans': '教師あり学習とはなんですか'},\n",
       "   {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'},\n",
       "   {'top5_id': '0116', 'top5_ans': '半教師あり学習とはなんですか'}]},\n",
       " {'id': '0416',\n",
       "  'q': 'ここでいう目的変数って何ですか',\n",
       "  'search': [{'top1_id': '0416', 'top1_ans': '目的変数ってなんですか'},\n",
       "   {'top2_id': '0708', 'top2_ans': 'スラック変数ってなんですか'},\n",
       "   {'top3_id': '0311', 'top3_ans': '分類能力が高いとはどういうことですか'},\n",
       "   {'top4_id': '1403', 'top4_ans': '多様体仮定とはどういうことですか'},\n",
       "   {'top5_id': '0703', 'top5_ans': '目的関数と距離の式が違うのはなぜですか'}]},\n",
       " {'id': '0710',\n",
       "  'q': '特徴空間の次元数$d$が大きい場合はどうなりますか',\n",
       "  'search': [{'top1_id': '0509',\n",
       "    'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'},\n",
       "   {'top2_id': '0710', 'top2_ans': '特徴次元が多いとどうなるのか'},\n",
       "   {'top3_id': '0509', 'top3_ans': '学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか'},\n",
       "   {'top4_id': '0204', 'top4_ans': '特徴ベクトルの次元数が増えるとどんな問題がありますか'},\n",
       "   {'top5_id': '0710', 'top5_ans': '特徴ベクトルの次元が増えるとどうなるのか'}]},\n",
       " {'id': '0801',\n",
       "  'q': 'ニューラルとはどういう意味か',\n",
       "  'search': [{'top1_id': '0406', 'top1_ans': '尤度とはどういう意味ですか'},\n",
       "   {'top2_id': '0606', 'top2_ans': 'Ridgeってどういう意味ですか'},\n",
       "   {'top3_id': '0607', 'top3_ans': 'lassoってどういう意味ですか'},\n",
       "   {'top4_id': '0110', 'top4_ans': '識別と回帰の意味の違いは何ですか'},\n",
       "   {'top5_id': '0102', 'top5_ans': '人工知能と機械学習は同じ意味ですか'}]},\n",
       " {'id': '0811',\n",
       "  'q': '勾配消失問題を解決する手法はあるのか',\n",
       "  'search': [{'top1_id': '0917',\n",
       "    'top1_ans': 'リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか'},\n",
       "   {'top2_id': '0811', 'top2_ans': 'どのようにして勾配消失問題を解決しますか'},\n",
       "   {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何か'},\n",
       "   {'top4_id': '0810', 'top4_ans': '勾配消失問題とは何ですか'},\n",
       "   {'top5_id': '0810', 'top5_ans': '勾配消失問題とはどのような問題ですか'}]},\n",
       " {'id': '1106',\n",
       "  'q': 'Ward法とはなんですか',\n",
       "  'search': [{'top1_id': '1106', 'top1_ans': 'Ward法とは何ですか'},\n",
       "   {'top2_id': '1106', 'top2_ans': 'Ward法はどういうものですか'},\n",
       "   {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'},\n",
       "   {'top4_id': '1106', 'top4_ans': '重心法とはなんですか'},\n",
       "   {'top5_id': '1108', 'top5_ans': 'k-平均法とはなんですか'}]},\n",
       " {'id': '0701',\n",
       "  'q': '超平面とは何ですか',\n",
       "  'search': [{'top1_id': '0701',\n",
       "    'top1_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'},\n",
       "   {'top2_id': '1108', 'top2_ans': 'k-平均法とはなんですか'},\n",
       "   {'top3_id': '1108', 'top3_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'},\n",
       "   {'top4_id': '0510', 'top4_ans': '識別面とはなんですか'},\n",
       "   {'top5_id': '0510', 'top5_ans': '識別面ってなんですか'}]},\n",
       " {'id': '0610',\n",
       "  'q': 'トレードオフってなんですか',\n",
       "  'search': [{'top1_id': '0612', 'top1_ans': 'CARTってなんですか'},\n",
       "   {'top2_id': '0114', 'top2_ans': 'クラスタリングってなんですか'},\n",
       "   {'top3_id': '0901', 'top3_ans': 'DNNってなんですか'},\n",
       "   {'top4_id': '0715', 'top4_ans': 'カーネルトリックってなんですか'},\n",
       "   {'top5_id': '0505', 'top5_ans': 'パーセプトロンってなんですか'}]},\n",
       " {'id': '1409',\n",
       "  'q': '自己学習の問題点は何ですか',\n",
       "  'search': [{'top1_id': '1409', 'top1_ans': '自己学習の問題点は何がありますか'},\n",
       "   {'top2_id': '1407', 'top2_ans': '自己学習とは何ですか'},\n",
       "   {'top3_id': '1407', 'top3_ans': '自己学習の狙いは何ですか'},\n",
       "   {'top4_id': '1407', 'top4_ans': '自己学習とはなんですか'},\n",
       "   {'top5_id': '0906', 'top5_ans': '階層が多いニューラルネットワークの学習の問題点は？'}]},\n",
       " {'id': '1205',\n",
       "  'q': 'a priori な原理の対偶とは何ですか',\n",
       "  'search': [{'top1_id': '1205', 'top1_ans': 'a prioriな原理とは何ですか'},\n",
       "   {'top2_id': '1207', 'top2_ans': 'a prioriアルゴリズムとは何ですか'},\n",
       "   {'top3_id': '1110', 'top3_ans': 'モデルの対数尤度とは何ですか'},\n",
       "   {'top4_id': '0810', 'top4_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'},\n",
       "   {'top5_id': '0507', 'top5_ans': 'パーセプトロンの収束定理ってなんですか'}]}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '0610', 'q': 'バイアスとはなんですか', 'search': [{'top1_id': '0610', 'top1_ans': 'バイアスってなんですか'}, {'top2_id': '0610', 'top2_ans': 'バイアスと分散はどのような関係ですか'}, {'top3_id': '0305', 'top3_ans': 'バイアスって何ですか'}, {'top4_id': '0311', 'top4_ans': 'エントロピーとはなんですか'}, {'top5_id': '1010', 'top5_ans': 'ブースティングとはなんですか'}]}, {'id': '0306', 'q': '候補削除アルゴリズムとはなんですか', 'search': [{'top1_id': '0306', 'top1_ans': '候補削除アルゴリズムってなんですか'}, {'top2_id': '0204', 'top2_ans': '次元削減とはなんですか'}, {'top3_id': '1116', 'top3_ans': 'EMアルゴリズムとはなんですか'}, {'top4_id': '0305', 'top4_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとはなんですか'}]}, {'id': '1502', 'q': '強化学習とは何ですか', 'search': [{'top1_id': '1502', 'top1_ans': '強化学習とはなんですか'}, {'top2_id': '1501', 'top2_ans': '強化学習はなぜ中間的学習なのですか'}, {'top3_id': '1502', 'top3_ans': '強化学習は例えば何に使われていますか'}, {'top4_id': '1501', 'top4_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'}, {'top5_id': '1503', 'top5_ans': '強化学習で、報酬が決定的な場合の学習はどのようなものですか'}]}, {'id': '0605', 'q': '決定係数の定義は何ですか', 'search': [{'top1_id': '0605', 'top1_ans': '決定係数ってなんですか'}, {'top2_id': '0701', 'top2_ans': 'マージンの定義はなんですか'}, {'top3_id': '0901', 'top3_ans': '深層学習の定義はなにか'}, {'top4_id': '0102', 'top4_ans': '人工知能の定義はなんですか'}, {'top5_id': '1502', 'top5_ans': '意思決定エージェントの例は何ですか'}]}, {'id': '0916', 'q': '時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか', 'search': [{'top1_id': '1301', 'top1_ans': '入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか'}, {'top2_id': '1301', 'top2_ans': '系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか'}, {'top3_id': '1301', 'top3_ans': '系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか'}, {'top4_id': '1301', 'top4_ans': '系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか'}, {'top5_id': '1301', 'top5_ans': '系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか'}]}, {'id': '1502', 'q': '強化学習はどのような学習法ですか', 'search': [{'top1_id': '1503', 'top1_ans': '強化学習で、報酬が決定的な場合の学習はどのようなものですか'}, {'top2_id': '1501', 'top2_ans': '強化学習はなぜ中間的学習なのですか'}, {'top3_id': '1502', 'top3_ans': '強化学習とはなんですか'}, {'top4_id': '1501', 'top4_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'}, {'top5_id': '0307', 'top5_ans': '決定木とはどのような学習法ですか'}]}, {'id': '0411', 'q': 'ゼロ頻度問題って何ですか', 'search': [{'top1_id': '0411', 'top1_ans': 'ゼロ頻度問題とは何ですか'}, {'top2_id': '0411', 'top2_ans': 'ゼロ頻度問題はどのような問題ですか'}, {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどうすれば回避できますか'}, {'top4_id': '0209', 'top4_ans': '精度って何ですか'}, {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]}, {'id': '0810', 'q': '階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか', 'search': [{'top1_id': '0811', 'top1_ans': '勾配が消失しない関数ってなんだったっけ'}, {'top2_id': '0906', 'top2_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'}, {'top3_id': '0906', 'top3_ans': '多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか'}, {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか'}, {'top5_id': '0901', 'top5_ans': '深層学習に用いるニューラルネットワークをなんとよぶか'}]}, {'id': '0514', 'q': '最急勾配法の問題点の対処法は何がありますか', 'search': [{'top1_id': '0514', 'top1_ans': '最急勾配法の問題点とはなんですか'}, {'top2_id': '0917', 'top2_ans': 'リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか'}, {'top3_id': '0514', 'top3_ans': '最急勾配法の欠点はなんですか'}, {'top4_id': '0906', 'top4_ans': '誤差逆伝播法の問題点は何がありますか'}, {'top5_id': '0811', 'top5_ans': '勾配消失問題への取組みにはどのような方法がありますか'}]}, {'id': '1103', 'q': '階層的手法とは何ですか', 'search': [{'top1_id': '1104', 'top1_ans': '階層的クラスタリングとは何ですか'}, {'top2_id': '1104', 'top2_ans': '階層的クラスタリングとはなんですか'}, {'top3_id': '1104', 'top3_ans': '階層的クラスタリングってなんですか'}, {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークとは何か'}, {'top5_id': '1104', 'top5_ans': '階層的クラスタリングは、どのようなアルゴリズムですか'}]}, {'id': '0805', 'q': 'なぜ重みを更新するのですか', 'search': [{'top1_id': '0513', 'top1_ans': '最急勾配法の重みの更新はどうなったら止まりますか'}, {'top2_id': '1012', 'top2_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'}, {'top3_id': '0717', 'top3_ans': 'スラック変数の重みは離散値ですか、連続値ですか'}, {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか'}, {'top5_id': '0204', 'top5_ans': 'なぜ次元を削減するんですか'}]}, {'id': '0412', 'q': 'ベイジアンネットワークって何ですか', 'search': [{'top1_id': '0412', 'top1_ans': 'ベイジアンネットワークってなんですか'}, {'top2_id': '0413', 'top2_ans': 'ベイジアンネットワークのメリットは何ですか'}, {'top3_id': '0417', 'top3_ans': 'ベイジアンネットワークで重要な項目は何ですか'}, {'top4_id': '0612', 'top4_ans': 'CARTって何ですか'}, {'top5_id': '0115', 'top5_ans': 'パターンマイニングって何ですか'}]}, {'id': '0614', 'q': 'モデル木って何ですか', 'search': [{'top1_id': '0614', 'top1_ans': 'モデル木ってなんですか'}, {'top2_id': '0611', 'top2_ans': '回帰木って何ですか'}, {'top3_id': '0505', 'top3_ans': '識別モデルってなんですか'}, {'top4_id': '0114', 'top4_ans': 'モデル推定ってなんですか'}, {'top5_id': '0614', 'top5_ans': 'モデル木とはなんですか'}]}, {'id': '0601', 'q': '回帰問題の学習目的はなんですか', 'search': [{'top1_id': '0601', 'top1_ans': '回帰問題とはなんですか'}, {'top2_id': '0601', 'top2_ans': '回帰問題ってなんですか'}, {'top3_id': '0602', 'top3_ans': '回帰問題は識別問題とどう違うんですか'}, {'top4_id': '0113', 'top4_ans': '教師なし学習の目的はなんですか'}, {'top5_id': '0104', 'top5_ans': '深層学習が得意な問題はなんですか'}]}, {'id': '0717', 'q': 'グリッドサーチはなんのためにやるんですか', 'search': [{'top1_id': '0115', 'top1_ans': 'データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか'}, {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'}, {'top3_id': '1507', 'top3_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'}, {'top4_id': '0402', 'top4_ans': '統計的識別ってどうやってやるんですか'}, {'top5_id': '0505', 'top5_ans': 'パーセプトロンを多層に重ねたものはなんですか'}]}, {'id': '1412', 'q': 'ラベル伝搬法の考え方はどのようなものですか', 'search': [{'top1_id': '1412', 'top1_ans': 'ラベル伝搬法の考え方にある仮定はどのようなものですか'}, {'top2_id': '1412', 'top2_ans': 'ラベル伝搬法の考え方は何ですか'}, {'top3_id': '0611', 'top3_ans': '決定木はどのような考えですか'}, {'top4_id': '1507', 'top4_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'}, {'top5_id': '0614', 'top5_ans': 'モデル木はどのような方法ですか'}]}, {'id': '0602', 'q': '回帰問題が識別問題と異なるのはどのような点ですか', 'search': [{'top1_id': '0602', 'top1_ans': '回帰問題は識別問題とどう違うんですか'}, {'top2_id': '0601', 'top2_ans': '回帰問題とはなんですか'}, {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどのような問題ですか'}, {'top4_id': '0810', 'top4_ans': '勾配消失問題とはどのような問題ですか'}, {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]}, {'id': '0805', 'q': '出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか', 'search': [{'top1_id': '0805', 'top1_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'}, {'top2_id': '0805', 'top2_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'}, {'top3_id': '0810', 'top3_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}, {'top4_id': '0810', 'top4_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'}, {'top5_id': '0717', 'top5_ans': 'パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか'}]}, {'id': '0413', 'q': 'ベイジアンネットワークの利点とはなんですか', 'search': [{'top1_id': '0413', 'top1_ans': 'ベイジアンネットワークの利点はなんですか'}, {'top2_id': '0811', 'top2_ans': 'ReLUを用いる利点はなんですか'}, {'top3_id': '0412', 'top3_ans': 'ベイジアンネットワークってなんですか'}, {'top4_id': '0413', 'top4_ans': 'ベイジアンネットワークのメリットは何ですか'}, {'top5_id': '0514', 'top5_ans': '最急勾配法の問題点とはなんですか'}]}, {'id': '0611', 'q': '回帰木とはなに', 'search': [{'top1_id': '0611', 'top1_ans': '回帰木とはなんですか'}, {'top2_id': '0614', 'top2_ans': 'モデル木は回帰木とどう違いますか'}, {'top3_id': '0611', 'top3_ans': '回帰木って何ですか'}, {'top4_id': '0611', 'top4_ans': '回帰木の特徴はなんですか'}, {'top5_id': '0612', 'top5_ans': 'CARTは回帰木とどう違いますか'}]}, {'id': '0811', 'q': 'ReLUとは何ですか', 'search': [{'top1_id': '0811', 'top1_ans': 'ReLu関数の良さは何ですか'}, {'top2_id': '0811', 'top2_ans': 'ReLUを用いる利点はなんですか'}, {'top3_id': '0811', 'top3_ans': 'ReLUを用いると誤差はどうなりますか'}, {'top4_id': '0811', 'top4_ans': '活性化関数にReLUを用いると誤差が失われないのはなぜですか'}, {'top5_id': '1404', 'top5_ans': 'オーバーラップとは何ですか'}]}, {'id': '0803', 'q': 'ノードを階層的に組むと非線形識別面が実現できるのはなぜか', 'search': [{'top1_id': '0803', 'top1_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのか'}, {'top2_id': '0803', 'top2_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのですか'}, {'top3_id': '0803', 'top3_ans': 'なぜノードを階層的に組むと非線形識別面を実現できるんですか'}, {'top4_id': '0803', 'top4_ans': 'ノードを階層的に組むとどのような識別面ができるか'}, {'top5_id': '0803', 'top5_ans': '基底関数ベクトルによる非線形識別面はどういう風に実現されているのか'}]}, {'id': '0302', 'q': 'クラスとはなんですか', 'search': [{'top1_id': '0302', 'top1_ans': 'クラスって何ですか'}, {'top2_id': '0402', 'top2_ans': '最も確率が高いクラスを出力とする手法はなに'}, {'top3_id': '0405', 'top3_ans': '事後確率が最大となるクラスは、何を求めることで得られますか'}, {'top4_id': '0402', 'top4_ans': '事後確率が最大となるクラスを識別結果とする方法を何と言いますか'}, {'top5_id': '0802', 'top5_ans': 'フィードフォワード型モデルの出力層のクラスの数はいくつ'}]}, {'id': '1219', 'q': '協調フィルタリングではどのようなデータが必要ですか', 'search': [{'top1_id': '1219', 'top1_ans': '協調フィルタリングとはどのようなものですか'}, {'top2_id': '1219', 'top2_ans': '協調フィルタリングとは何ですか'}, {'top3_id': '1506', 'top3_ans': 'なぜ割引率が必要なのですか'}, {'top4_id': '0802', 'top4_ans': 'フィードフォワード型ニューラルネットワークを構成するには何が必要ですか'}, {'top5_id': '1114', 'top5_ans': '事前確率を求めるには何が必要ですか'}]}, {'id': '0606', 'q': '汎化能力という点で望ましい線形回帰式の性質はなんですか', 'search': [{'top1_id': '0614', 'top1_ans': '回帰木と線形回帰の双方のよいところを取った方法はなんですか'}, {'top2_id': '0502', 'top2_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top3_id': '0810', 'top3_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}, {'top4_id': '0707', 'top4_ans': '線形分離可能でない場合はどうすれば良いですか'}, {'top5_id': '0508', 'top5_ans': '線形分離不可能なデータにはどのように対処しますか'}]}, {'id': '0917', 'q': '中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか', 'search': [{'top1_id': '0917', 'top1_ans': 'リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか'}, {'top2_id': '0916', 'top2_ans': '中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか'}, {'top3_id': '0917', 'top3_ans': 'LSTMで置き換えられるメモリユニットの名前はなんですか'}, {'top4_id': '0901', 'top4_ans': '深層学習を言い換えると何になりますか'}, {'top5_id': '0911', 'top5_ans': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという'}]}, {'id': '0113', 'q': '教師なし学習では何を学習しますか', 'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'}, {'top2_id': '0113', 'top2_ans': '教師なし学習の目的はなんですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'}, {'top5_id': '0109', 'top5_ans': '教師あり学習とはなんですか'}]}, {'id': '0406', 'q': 'i.i.d. (independent and identically distributed) とはなんですか', 'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'}, {'top2_id': '1010', 'top2_ans': 'ブースティングとはなんですか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングとはなんですか'}, {'top4_id': '1004', 'top4_ans': 'バギングとはなんですか'}, {'top5_id': '0612', 'top5_ans': 'CARTとはなんですか'}]}, {'id': '0713', 'q': 'SVMの方法は例えば何に使われていますか', 'search': [{'top1_id': '1502', 'top1_ans': '強化学習は例えば何に使われていますか'}, {'top2_id': '0713', 'top2_ans': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングはどのようなことに使われますか'}, {'top4_id': '0908', 'top4_ans': '入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか'}, {'top5_id': '0117', 'top5_ans': '半教師あり学習はどんなときに使われますか'}]}, {'id': '1108', 'q': 'k-平均法って何ですか', 'search': [{'top1_id': '1108', 'top1_ans': 'k-平均法とはなんですか'}, {'top2_id': '1108', 'top2_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'}, {'top4_id': '0512', 'top4_ans': '最急勾配法って何ですか'}, {'top5_id': '1110', 'top5_ans': 'k-means法の問題点はなんですか'}]}, {'id': '1111', 'q': '異常検出とは何ですか', 'search': [{'top1_id': '1112', 'top1_ans': '局所異常因子では、どのように外れ値を検知するのですか'}, {'top2_id': '1112', 'top2_ans': '局所異常因子とは何ですか'}, {'top3_id': '0917', 'top3_ans': 'LSTMと通常のユニットの違いは何ですか'}, {'top4_id': '1201', 'top4_ans': '頻出項目抽出とは何ですか'}, {'top5_id': '1009', 'top5_ans': '通常の決定木とランダムフォレストとの違いは何ですか'}]}, {'id': '0410', 'q': 'ナイーブベイズ識別法とはなんですか', 'search': [{'top1_id': '0410', 'top1_ans': 'ナイーブベイズ識別って何ですか'}, {'top2_id': '0505', 'top2_ans': '識別関数法とはなんですか'}, {'top3_id': '0410', 'top3_ans': 'ナイーブベイス識別法ってなんですか'}, {'top4_id': '0505', 'top4_ans': '識別関数法ってなんですか'}, {'top5_id': '0510', 'top5_ans': '識別面とはなんですか'}]}, {'id': '0901', 'q': 'DNNとは何ですか', 'search': [{'top1_id': '0901', 'top1_ans': 'DNNってなんですか'}, {'top2_id': '1404', 'top2_ans': 'オーバーラップとは何ですか'}, {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'}, {'top5_id': '1306', 'top5_ans': 'CRFとは何ですか'}]}, {'id': '0508', 'q': '二乗誤差とはなんですか', 'search': [{'top1_id': '0508', 'top1_ans': '二乗誤差とは何ですか'}, {'top2_id': '0508', 'top2_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'}, {'top3_id': '0508', 'top3_ans': '最小二乗法とはなんですか'}, {'top4_id': '0508', 'top4_ans': '最小二乗法ってなんですか'}, {'top5_id': '0508', 'top5_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}]}, {'id': '0407', 'q': '対数尤度とはなんですか', 'search': [{'top1_id': '1110', 'top1_ans': 'モデルの対数尤度とは何ですか'}, {'top2_id': '0407', 'top2_ans': '何故尤度の対数をとって計算するんですか'}, {'top3_id': '0406', 'top3_ans': '尤度とはなんですか'}, {'top4_id': '0405', 'top4_ans': '尤度とは何ですか'}, {'top5_id': '0406', 'top5_ans': '尤度とはどういう意味ですか'}]}, {'id': '0708', 'q': 'スラック変数が小さいほうがいいのはなぜか', 'search': [{'top1_id': '0708', 'top1_ans': 'なぜスラック変数が小さい方が良いのですか'}, {'top2_id': '0701', 'top2_ans': 'マージンは大きいほうがいいんですか'}, {'top3_id': '1510', 'top3_ans': 'なぜTD学習において温度を学習が進むにつれて小さくするのですか'}, {'top4_id': '0810', 'top4_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}, {'top5_id': '0811', 'top5_ans': '活性化関数にReLUを用いると誤差が失われないのはなぜですか'}]}, {'id': '0511', 'q': '入力が負例のときは、確率はどう求められますか', 'search': [{'top1_id': '1114', 'top1_ans': '事前確率はどのように求めますか'}, {'top2_id': '0405', 'top2_ans': '事後確率が最大となるクラスは、何を求めることで得られますか'}, {'top3_id': '0402', 'top3_ans': '条件付き確率ってどうやって求めるんですか'}, {'top4_id': '0405', 'top4_ans': '事後確率が最大になるクラスはどうやって得られますか'}, {'top5_id': '0402', 'top5_ans': '入力を観測した後で計算される確率は何と言いますか'}]}, {'id': '1110', 'q': 'X-meansアルゴリズムとはなんですか', 'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'}, {'top2_id': '1110', 'top2_ans': 'k-means法の問題点はなんですか'}, {'top3_id': '1116', 'top3_ans': 'EMアルゴリズムとはなんですか'}, {'top4_id': '0305', 'top4_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとはなんですか'}]}, {'id': '0313', 'q': '過学習を避けるためにはどのような方法がありますか', 'search': [{'top1_id': '1507', 'top1_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'}, {'top2_id': '0715', 'top2_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}, {'top3_id': '0911', 'top3_ans': 'なぜドロップアウトによって過学習が回避できるのですか'}, {'top4_id': '0903', 'top4_ans': '深層学習の中で特化した構造にはどのようなものがありますか'}, {'top5_id': '1408', 'top5_ans': '自己学習の性質にはどのようなものがありますか'}]}, {'id': '0117', 'q': '半教師あり学習に適した状況はどのように作られますか', 'search': [{'top1_id': '1402', 'top1_ans': '半教師あり学習に適した数値特徴データとはどのようなものですか'}, {'top2_id': '1406', 'top2_ans': '半教師あり学習の識別器に適切なのはどのようなものですか'}, {'top3_id': '0117', 'top3_ans': '半教師あり学習はどんなときに使われますか'}, {'top4_id': '1406', 'top4_ans': '半教師あり学習の識別器には何が適切ですか'}, {'top5_id': '0116', 'top5_ans': '半教師あり学習とはどのようなものですか'}]}, {'id': '0109', 'q': '教師あり学習って何ですか', 'search': [{'top1_id': '0109', 'top1_ans': '教師あり学習とはなんですか'}, {'top2_id': '0116', 'top2_ans': '半教師あり学習とはなんですか'}, {'top3_id': '1402', 'top3_ans': '半教師あり学習が成立する条件は何ですか'}, {'top4_id': '1406', 'top4_ans': '半教師あり学習の識別器には何が適切ですか'}, {'top5_id': '0116', 'top5_ans': '半教師あり学習とはどのようなものですか'}]}, {'id': '0204', 'q': '主成分分析ってどういうことをしているんですか', 'search': [{'top1_id': '0205', 'top1_ans': '主成分分析って何ですか'}, {'top2_id': '0205', 'top2_ans': '主成分分析とはなんですか'}, {'top3_id': '0205', 'top3_ans': '主成分分析とは何ですか'}, {'top4_id': '0311', 'top4_ans': '分類能力が高いとはどういうことですか'}, {'top5_id': '1220', 'top5_ans': '推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか'}]}, {'id': '0901', 'q': '深層学習とはどのようなものか', 'search': [{'top1_id': '0901', 'top1_ans': '深層学習はどのようなものですか'}, {'top2_id': '0903', 'top2_ans': '深層学習の中で特化した構造にはどのようなものがありますか'}, {'top3_id': '0901', 'top3_ans': '深層学習はどのような場面で応用されますか'}, {'top4_id': '0901', 'top4_ans': '深層学習はどのような分野で用いられていますか'}, {'top5_id': '0901', 'top5_ans': '深層学習のポイントは何'}]}, {'id': '1012', 'q': 'ブースティングで、個々のデータに対してどのように重みを設定するのですか', 'search': [{'top1_id': '1012', 'top1_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'}, {'top2_id': '0717', 'top2_ans': 'SVMのハイパーパラメータの設定はどのように行いますか'}, {'top3_id': '0508', 'top3_ans': '線形分離不可能なデータにはどのように対処しますか'}, {'top4_id': '0707', 'top4_ans': 'ソフトマージンによる際の識別面の設定の仕方'}, {'top5_id': '1012', 'top5_ans': 'ブースティングでは、どのように識別器を作成するのですか'}]}, {'id': '1110', 'q': 'BICとは何ですか', 'search': [{'top1_id': '1404', 'top1_ans': 'オーバーラップとは何ですか'}, {'top2_id': '0717', 'top2_ans': 'グリッドサーチとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'サポートベクトルマシンとは何ですか'}, {'top4_id': '1306', 'top4_ans': 'CRFとは何ですか'}, {'top5_id': '1306', 'top5_ans': 'ビタビアルゴリズムとは何ですか'}]}, {'id': '0707', 'q': 'ソフトマージンとは何ですか', 'search': [{'top1_id': '0707', 'top1_ans': 'ソフトマージンによる際の識別面の設定の仕方'}, {'top2_id': '1404', 'top2_ans': 'オーバーラップとは何ですか'}, {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'}, {'top5_id': '1306', 'top5_ans': 'CRFとは何ですか'}]}, {'id': '0507', 'q': 'パーセプトロンの学習アルゴリズムはどのようなときに停止しますか', 'search': [{'top1_id': '1306', 'top1_ans': '条件付き確率場の学習にはどのような性質を利用しますか'}, {'top2_id': '0506', 'top2_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top3_id': '0507', 'top3_ans': 'なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか'}, {'top4_id': '0316', 'top4_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'}, {'top5_id': '1207', 'top5_ans': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか'}]}, {'id': '0811', 'q': 'ReLuってなんですか', 'search': [{'top1_id': '0811', 'top1_ans': 'ReLUを用いる利点はなんですか'}, {'top2_id': '0612', 'top2_ans': 'CARTってなんですか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングってなんですか'}, {'top4_id': '0901', 'top4_ans': 'DNNってなんですか'}, {'top5_id': '0715', 'top5_ans': 'カーネルトリックってなんですか'}]}, {'id': '0504', 'q': 'なぜ生成モデルアプローチは問題を難しくしているといえるのですか', 'search': [{'top1_id': '0917', 'top1_ans': 'リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか'}, {'top2_id': '0504', 'top2_ans': '生成モデルアプローチが有効なのはどのようなときですか'}, {'top3_id': '1001', 'top3_ans': 'なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか'}, {'top4_id': '1309', 'top4_ans': '入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか'}, {'top5_id': '0509', 'top5_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}]}, {'id': '0908', 'q': '自己写像の学習において使われる入力と出力の距離は何ですか', 'search': [{'top1_id': '0908', 'top1_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'}, {'top2_id': '0908', 'top2_ans': '入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか'}, {'top3_id': '1309', 'top3_ans': '入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか'}, {'top4_id': '1309', 'top4_ans': '入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか'}, {'top5_id': '1407', 'top5_ans': '自己学習の狙いは何ですか'}]}, {'id': '0907', 'q': '事前学習法の考え方は何ですか', 'search': [{'top1_id': '1412', 'top1_ans': 'ラベル伝搬法の考え方は何ですか'}, {'top2_id': '0907', 'top2_ans': '事前学習法とは何ですか'}, {'top3_id': '0907', 'top3_ans': '事前学習とは何ですか'}, {'top4_id': '1412', 'top4_ans': 'ラベル伝搬法の考え方にある仮定はどのようなものですか'}, {'top5_id': '0404', 'top5_ans': '事前確率とは何ですか'}]}, {'id': '0611', 'q': '回帰木ってなんですか', 'search': [{'top1_id': '0611', 'top1_ans': '回帰木って何ですか'}, {'top2_id': '0606', 'top2_ans': 'Ridge回帰ってなんですか'}, {'top3_id': '0607', 'top3_ans': 'Lasso回帰ってなんですか'}, {'top4_id': '0611', 'top4_ans': '回帰木とはなんですか'}, {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]}, {'id': '1209', 'q': 'リフト値とは何ですか', 'search': [{'top1_id': '1209', 'top1_ans': 'リフト値ってなんですか'}, {'top2_id': '1209', 'top2_ans': 'リフト値の値からどのようなことがわかりますか'}, {'top3_id': '1111', 'top3_ans': '外れ値とは何ですか'}, {'top4_id': '0209', 'top4_ans': 'F値って何ですか'}, {'top5_id': '0209', 'top5_ans': 'F値とはなんですか'}]}, {'id': '1209', 'q': '確信度の値からどのようなことがわかりますか', 'search': [{'top1_id': '1209', 'top1_ans': 'リフト値の値からどのようなことがわかりますか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのようなことに使われますか'}, {'top3_id': '1209', 'top3_ans': '確信度とはなんですか'}, {'top4_id': '1209', 'top4_ans': '確信度とは何ですか'}, {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]}, {'id': '1303', 'q': '形態素解析とは何ですか', 'search': [{'top1_id': '1303', 'top1_ans': '日本語の形態素列の特徴は何ですか'}, {'top2_id': '1304', 'top2_ans': '観測素性とは何ですか'}, {'top3_id': '1304', 'top3_ans': '遷移素性とは何ですか'}, {'top4_id': '0205', 'top4_ans': '主成分分析とは何ですか'}, {'top5_id': '0205', 'top5_ans': '主成分分析って何ですか'}]}, {'id': '1506', 'q': '政策とは何ですか', 'search': [{'top1_id': '1507', 'top1_ans': 'マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか'}, {'top2_id': '1506', 'top2_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'}, {'top3_id': '1404', 'top3_ans': 'オーバーラップとは何ですか'}, {'top4_id': '0717', 'top4_ans': 'グリッドサーチとは何ですか'}, {'top5_id': '0701', 'top5_ans': 'サポートベクトルマシンとは何ですか'}]}, {'id': '0305', 'q': 'バイアスとはなんですか', 'search': [{'top1_id': '0610', 'top1_ans': 'バイアスってなんですか'}, {'top2_id': '0610', 'top2_ans': 'バイアスと分散はどのような関係ですか'}, {'top3_id': '0305', 'top3_ans': 'バイアスって何ですか'}, {'top4_id': '0311', 'top4_ans': 'エントロピーとはなんですか'}, {'top5_id': '1010', 'top5_ans': 'ブースティングとはなんですか'}]}, {'id': '1110', 'q': 'x-meansアルゴリズムとは何ですか', 'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'}, {'top2_id': '1411', 'top2_ans': 'YATSIアルゴリズムとは何ですか'}, {'top3_id': '1214', 'top3_ans': 'FP-Growthアルゴリズムとは何ですか'}, {'top4_id': '1207', 'top4_ans': 'a prioriアルゴリズムとは何ですか'}, {'top5_id': '1207', 'top5_ans': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか'}]}, {'id': '0907', 'q': '誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか', 'search': [{'top1_id': '0810', 'top1_ans': '多段階に誤差逆伝播法を適用すると問題はありますか'}, {'top2_id': '0508', 'top2_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}, {'top3_id': '0805', 'top3_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'}, {'top4_id': '0810', 'top4_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}, {'top5_id': '0908', 'top5_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'}]}, {'id': '0805', 'q': '誤差逆伝播法とはどういう手法ですか', 'search': [{'top1_id': '0805', 'top1_ans': '誤差逆伝播法とは何か'}, {'top2_id': '0805', 'top2_ans': '誤差逆伝播法とは何ですか'}, {'top3_id': '0810', 'top3_ans': '誤差逆伝播法における問題点は何か'}, {'top4_id': '0906', 'top4_ans': '誤差逆伝播法の問題点は何がありますか'}, {'top5_id': '0810', 'top5_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}]}, {'id': '0911', 'q': 'ドロップアウトによって過学習が生じにくくなっている理由はなんですか', 'search': [{'top1_id': '0911', 'top1_ans': 'なぜドロップアウトによって過学習が回避できるのですか'}, {'top2_id': '1510', 'top2_ans': 'なぜTD学習において温度を学習が進むにつれて小さくするのですか'}, {'top3_id': '1501', 'top3_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'}, {'top4_id': '0911', 'top4_ans': 'ドロップアウトで過学習が防げるのはなぜですか'}, {'top5_id': '0313', 'top5_ans': '過学習って何ですか'}]}, {'id': '0206', 'q': '分割学習法とはなんですか', 'search': [{'top1_id': '0206', 'top1_ans': '分割学習法の際学習データが足りない場合どうしたらいいですか'}, {'top2_id': '1103', 'top2_ans': '分割最適化手法とは何ですか'}, {'top3_id': '0911', 'top3_ans': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという'}, {'top4_id': '0104', 'top4_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top5_id': '0109', 'top5_ans': '教師なし学習とはどういう学習法ですか'}]}, {'id': '1506', 'q': '政策の良さはどのように評価されますか', 'search': [{'top1_id': '1506', 'top1_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'}, {'top2_id': '0901', 'top2_ans': '深層学習はどのような場面で応用されますか'}, {'top3_id': '0803', 'top3_ans': '基底関数ベクトルによる非線形識別面はどのように実現されていますか'}, {'top4_id': '0811', 'top4_ans': 'ReLu関数の良さは何ですか'}, {'top5_id': '0103', 'top5_ans': '機械学習はどんな時に利用されますか'}]}, {'id': '0109', 'q': '回帰とはなんですか', 'search': [{'top1_id': '0611', 'top1_ans': '回帰木とはなんですか'}, {'top2_id': '0616', 'top2_ans': 'カーネル回帰とはなんですか'}, {'top3_id': '0601', 'top3_ans': '回帰問題とはなんですか'}, {'top4_id': '0606', 'top4_ans': 'Ridge回帰ってなんですか'}, {'top5_id': '0607', 'top5_ans': 'Lasso回帰ってなんですか'}]}, {'id': '1108', 'q': 'k-meansアルゴリズムとは何ですか', 'search': [{'top1_id': '1110', 'top1_ans': 'k-means法の問題点はなんですか'}, {'top2_id': '1110', 'top2_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'}, {'top3_id': '1108', 'top3_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top4_id': '1411', 'top4_ans': 'YATSIアルゴリズムとは何ですか'}, {'top5_id': '1214', 'top5_ans': 'FP-Growthアルゴリズムとは何ですか'}]}, {'id': '0508', 'q': '最小二乗法って何ですか', 'search': [{'top1_id': '0508', 'top1_ans': '最小二乗法ってなんですか'}, {'top2_id': '0508', 'top2_ans': '最小二乗法とはなんですか'}, {'top3_id': '0508', 'top3_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'}, {'top4_id': '0508', 'top4_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}, {'top5_id': '0508', 'top5_ans': '二乗誤差とは何ですか'}]}, {'id': '0315', 'q': 'Gini Inpurityは何を表しますか', 'search': [{'top1_id': '0412', 'top1_ans': 'ベイジアンネットワークはどのような仮定を表現したものですか'}, {'top2_id': '0612', 'top2_ans': 'Gini Impurityってなんですか'}, {'top3_id': '0115', 'top3_ans': 'パターンマイニングの代表的な手法には何がありますか'}, {'top4_id': '0114', 'top4_ans': 'クラスタリングの代表的な手法には何がありますか'}, {'top5_id': '0112', 'top5_ans': '回帰の代表的な手法には何がありますか'}]}, {'id': '0506', 'q': '単層パーセプトロンはどうやって出力を決めるんですか', 'search': [{'top1_id': '0506', 'top1_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'}, {'top3_id': '0402', 'top3_ans': '統計的識別ってどうやってやるんですか'}, {'top4_id': '0805', 'top4_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'}, {'top5_id': '0313', 'top5_ans': '決定木ではどうやって過学習を回避しますか'}]}, {'id': '0802', 'q': '中間層を言い換えるとなんといいますか', 'search': [{'top1_id': '0901', 'top1_ans': '深層学習を言い換えると何になりますか'}, {'top2_id': '0917', 'top2_ans': 'リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか'}, {'top3_id': '0717', 'top3_ans': '複数のパラメータを組合わせる空間を何と言いますか'}, {'top4_id': '0802', 'top4_ans': '中間層はいくつ用意しますか'}, {'top5_id': '0715', 'top5_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}]}, {'id': '0611', 'q': '出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか', 'search': [{'top1_id': '0509', 'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top2_id': '0316', 'top2_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'}, {'top3_id': '0506', 'top3_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top4_id': '0906', 'top4_ans': '特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか'}, {'top5_id': '0701', 'top5_ans': '識別境界線と最も近いデータとの距離のことをなんといいますか'}]}, {'id': '0111', 'q': '識別ではどのようなことができますか', 'search': [{'top1_id': '0803', 'top1_ans': 'ノードを階層的に組むとどのような識別面ができるか'}, {'top2_id': '1209', 'top2_ans': 'リフト値の値からどのようなことがわかりますか'}, {'top3_id': '0510', 'top3_ans': '識別モデルはどのように作りますか'}, {'top4_id': '0702', 'top4_ans': 'サポートベクトルマシンの識別面はどのような形になりますか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングはどのようなことに使われますか'}]}, {'id': '0811', 'q': 'ReLUだと勾配消失が起こらないのはなぜですか', 'search': [{'top1_id': '0811', 'top1_ans': '勾配が消失しない関数ってなんだったっけ'}, {'top2_id': '0810', 'top2_ans': '勾配消失問題とはなんですか'}, {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何か'}, {'top4_id': '0810', 'top4_ans': '勾配消失問題とは何ですか'}, {'top5_id': '0810', 'top5_ans': '勾配消失問題とはどのような問題ですか'}]}, {'id': '0209', 'q': '再現率って何ですか', 'search': [{'top1_id': '0901', 'top1_ans': '表現学習ってなんですか'}, {'top2_id': '0402', 'top2_ans': '最大事後確率則って何ですか'}, {'top3_id': '0402', 'top3_ans': '条件付き確率ってどうやって求めるんですか'}, {'top4_id': '0411', 'top4_ans': '確率のm推定ってなんですか'}, {'top5_id': '0402', 'top5_ans': '最大事後確率則ってなんですか'}]}, {'id': '0701', 'q': 'サポートベクターマシンとはなんですか', 'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'}, {'top2_id': '1010', 'top2_ans': 'ブースティングとはなんですか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングとはなんですか'}, {'top4_id': '1004', 'top4_ans': 'バギングとはなんですか'}, {'top5_id': '0612', 'top5_ans': 'CARTとはなんですか'}]}, {'id': '1004', 'q': 'バギングとは何ですか', 'search': [{'top1_id': '1004', 'top1_ans': 'バギングとはなんですか'}, {'top2_id': '1004', 'top2_ans': 'バギングはどういう手法ですか'}, {'top3_id': '1010', 'top3_ans': 'バギングやランダムフォレストとブースティングではどのような点で異なりますか'}, {'top4_id': '1012', 'top4_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'}, {'top5_id': '1404', 'top5_ans': 'オーバーラップとは何ですか'}]}, {'id': '0105', 'q': '機械学習の基本的な定義はなんですか', 'search': [{'top1_id': '0104', 'top1_ans': '深層学習と機械学習の違いはなんですか'}, {'top2_id': '1406', 'top2_ans': '半教師ある学習の基本的な進め方はどういったものですか'}, {'top3_id': '0104', 'top3_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top4_id': '0103', 'top4_ans': '機械学習はどんな時に利用されますか'}, {'top5_id': '0901', 'top5_ans': '深層学習の定義はなにか'}]}, {'id': '0712', 'q': '非線形変換が存在するのはどういう条件の時ですか', 'search': [{'top1_id': '0710', 'top1_ans': 'サポートベクトルマシンで高次元に非線形変換する際の条件はなに'}, {'top2_id': '0715', 'top2_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}, {'top3_id': '0803', 'top3_ans': '基底関数ベクトルによる非線形識別面はどういう風に実現されているのか'}, {'top4_id': '0704', 'top4_ans': '制約条件ではどのような式を用いますか'}, {'top5_id': '0710', 'top5_ans': '特徴空間を高次元に変換する際に重要な条件はなんですか'}]}, {'id': '0701', 'q': 'サポートベクトルマシンはどういう手法ですか', 'search': [{'top1_id': '1004', 'top1_ans': 'バギングはどういう手法ですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのような手法ですか'}, {'top3_id': '1106', 'top3_ans': 'Ward法はどういうものですか'}, {'top4_id': '0115', 'top4_ans': 'パターンマイニングとはどういう方法ですか'}, {'top5_id': '1509', 'top5_ans': 'モデルフリーの手法とはどのような場合ですか'}]}, {'id': '0105', 'q': '機械学習では何ができますか', 'search': [{'top1_id': '0103', 'top1_ans': '機械学習はどんな時に利用されますか'}, {'top2_id': '0103', 'top2_ans': '機械学習で用いられるビッグデータとは何ですか'}, {'top3_id': '0104', 'top3_ans': '深層学習と機械学習の違いはなんですか'}, {'top4_id': '0104', 'top4_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top5_id': '0102', 'top5_ans': '機械学習と人工知能の意味の差はありますか'}]}, {'id': '0701', 'q': 'なぜペナルティを設定するのですか', 'search': [{'top1_id': '0707', 'top1_ans': 'ソフトマージンによる際の識別面の設定の仕方'}, {'top2_id': '1012', 'top2_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'}, {'top3_id': '0717', 'top3_ans': 'SVMのハイパーパラメータの設定はどのように行いますか'}, {'top4_id': '0204', 'top4_ans': 'なぜ次元を削減するんですか'}, {'top5_id': '1313', 'top5_ans': 'なぜHMMで最尤状態系列を求めるのですか'}]}, {'id': '1405', 'q': '半教師あり学習は何によく使われますか', 'search': [{'top1_id': '0117', 'top1_ans': '半教師あり学習はどんなときに使われますか'}, {'top2_id': '0116', 'top2_ans': '半教師あり学習とはなんですか'}, {'top3_id': '1406', 'top3_ans': '半教師あり学習の識別器には何が適切ですか'}, {'top4_id': '0116', 'top4_ans': '半教師あり学習とはどのようなものですか'}, {'top5_id': '1402', 'top5_ans': '半教師あり学習が成立する条件は何ですか'}]}, {'id': '0701', 'q': '線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに', 'search': [{'top1_id': '0502', 'top1_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top2_id': '0713', 'top2_ans': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか'}, {'top3_id': '0701', 'top3_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'}, {'top4_id': '0502', 'top4_ans': '非線形性を持つデータに対して識別を行うにはどんな方法がありますか'}, {'top5_id': '0715', 'top5_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}]}, {'id': '0209', 'q': '再現率とはなんですか', 'search': [{'top1_id': '0402', 'top1_ans': '事後確率とはなんですか'}, {'top2_id': '0404', 'top2_ans': '事前確率とはなんですか'}, {'top3_id': '0901', 'top3_ans': '表現学習ってなんですか'}, {'top4_id': '0901', 'top4_ans': '表現学習とは何ですか'}, {'top5_id': '0514', 'top5_ans': '確率的最急勾配法とはなんですか'}]}, {'id': '0109', 'q': '教師なし学習って何ですか', 'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'}, {'top2_id': '0113', 'top2_ans': '教師なし学習の目的はなんですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'}, {'top5_id': '0109', 'top5_ans': '教師あり学習とはなんですか'}]}, {'id': '0911', 'q': '多階層学習においてドロップアウトを用いるとどうなるか', 'search': [{'top1_id': '0906', 'top1_ans': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか'}, {'top2_id': '0901', 'top2_ans': '深層学習に用いるニューラルネットワークをなんとよぶか'}, {'top3_id': '0906', 'top3_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'}, {'top4_id': '0810', 'top4_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'}, {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]}, {'id': '0616', 'q': 'なぜ回帰問題にカーネル法を使うんですか', 'search': [{'top1_id': '0616', 'top1_ans': '回帰にカーネル法を取り入れる理由はなんですか'}, {'top2_id': '0602', 'top2_ans': '回帰問題は識別問題とどう違うんですか'}, {'top3_id': '0601', 'top3_ans': '回帰問題ってなんですか'}, {'top4_id': '0601', 'top4_ans': '回帰問題とはなんですか'}, {'top5_id': '0616', 'top5_ans': 'カーネル回帰とはなんですか'}]}, {'id': '0901', 'q': 'ディープニューラルネットワークとは何ですか', 'search': [{'top1_id': '0912', 'top1_ans': '画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか'}, {'top2_id': '0810', 'top2_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}, {'top3_id': '1404', 'top3_ans': 'オーバーラップとは何ですか'}, {'top4_id': '0717', 'top4_ans': 'グリッドサーチとは何ですか'}, {'top5_id': '0701', 'top5_ans': 'サポートベクトルマシンとは何ですか'}]}, {'id': '0204', 'q': '次元削減って何ですか', 'search': [{'top1_id': '0204', 'top1_ans': '次元削減とはなんですか'}, {'top2_id': '0204', 'top2_ans': 'なぜ次元を削減するんですか'}, {'top3_id': '0204', 'top3_ans': 'なぜ前処理で次元削減を行うのですか'}, {'top4_id': '0306', 'top4_ans': '候補削除アルゴリズムってなんですか'}, {'top5_id': '0710', 'top5_ans': '二次元から三次元の変換・写像で気をつけることはなんですか'}]}, {'id': '0105', 'q': 'パターン認識とはどういうものですか', 'search': [{'top1_id': '0105', 'top1_ans': 'パターン認識とはなんですか'}, {'top2_id': '0105', 'top2_ans': 'パターン認識ってなんですか'}, {'top3_id': '0505', 'top3_ans': '識別モデルとはどういうものですか'}, {'top4_id': '0717', 'top4_ans': 'グリッドサーチとはどういうものですか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングとはどういうものですか'}]}, {'id': '1007', 'q': 'ランダムフォレストとは何ですか', 'search': [{'top1_id': '1007', 'top1_ans': 'ランダムフォレストとはなんですか'}, {'top2_id': '1009', 'top2_ans': '通常の決定木とランダムフォレストとの違いは何ですか'}, {'top3_id': '1008', 'top3_ans': 'ランダムフォレストの学習手順は'}, {'top4_id': '1009', 'top4_ans': '通常の決定木とランダムフォレストの違いはなんですか'}, {'top5_id': '1010', 'top5_ans': 'バギングやランダムフォレストとブースティングではどのような点で異なりますか'}]}, {'id': '0109', 'q': '教師なし学習とはなんですか', 'search': [{'top1_id': '0113', 'top1_ans': '教師なし学習の目的はなんですか'}, {'top2_id': '0109', 'top2_ans': '教師なし学習とはどういう学習法ですか'}, {'top3_id': '0109', 'top3_ans': '教師あり学習とはなんですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'}, {'top5_id': '0116', 'top5_ans': '半教師あり学習とはなんですか'}]}, {'id': '0416', 'q': 'ここでいう目的変数って何ですか', 'search': [{'top1_id': '0416', 'top1_ans': '目的変数ってなんですか'}, {'top2_id': '0708', 'top2_ans': 'スラック変数ってなんですか'}, {'top3_id': '0311', 'top3_ans': '分類能力が高いとはどういうことですか'}, {'top4_id': '1403', 'top4_ans': '多様体仮定とはどういうことですか'}, {'top5_id': '0703', 'top5_ans': '目的関数と距離の式が違うのはなぜですか'}]}, {'id': '0710', 'q': '特徴空間の次元数$d$が大きい場合はどうなりますか', 'search': [{'top1_id': '0509', 'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top2_id': '0710', 'top2_ans': '特徴次元が多いとどうなるのか'}, {'top3_id': '0509', 'top3_ans': '学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか'}, {'top4_id': '0204', 'top4_ans': '特徴ベクトルの次元数が増えるとどんな問題がありますか'}, {'top5_id': '0710', 'top5_ans': '特徴ベクトルの次元が増えるとどうなるのか'}]}, {'id': '0801', 'q': 'ニューラルとはどういう意味か', 'search': [{'top1_id': '0406', 'top1_ans': '尤度とはどういう意味ですか'}, {'top2_id': '0606', 'top2_ans': 'Ridgeってどういう意味ですか'}, {'top3_id': '0607', 'top3_ans': 'lassoってどういう意味ですか'}, {'top4_id': '0110', 'top4_ans': '識別と回帰の意味の違いは何ですか'}, {'top5_id': '0102', 'top5_ans': '人工知能と機械学習は同じ意味ですか'}]}, {'id': '0811', 'q': '勾配消失問題を解決する手法はあるのか', 'search': [{'top1_id': '0917', 'top1_ans': 'リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか'}, {'top2_id': '0811', 'top2_ans': 'どのようにして勾配消失問題を解決しますか'}, {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何か'}, {'top4_id': '0810', 'top4_ans': '勾配消失問題とは何ですか'}, {'top5_id': '0810', 'top5_ans': '勾配消失問題とはどのような問題ですか'}]}, {'id': '1106', 'q': 'Ward法とはなんですか', 'search': [{'top1_id': '1106', 'top1_ans': 'Ward法とは何ですか'}, {'top2_id': '1106', 'top2_ans': 'Ward法はどういうものですか'}, {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'}, {'top4_id': '1106', 'top4_ans': '重心法とはなんですか'}, {'top5_id': '1108', 'top5_ans': 'k-平均法とはなんですか'}]}, {'id': '0701', 'q': '超平面とは何ですか', 'search': [{'top1_id': '0701', 'top1_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'}, {'top2_id': '1108', 'top2_ans': 'k-平均法とはなんですか'}, {'top3_id': '1108', 'top3_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top4_id': '0510', 'top4_ans': '識別面とはなんですか'}, {'top5_id': '0510', 'top5_ans': '識別面ってなんですか'}]}, {'id': '0610', 'q': 'トレードオフってなんですか', 'search': [{'top1_id': '0612', 'top1_ans': 'CARTってなんですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングってなんですか'}, {'top3_id': '0901', 'top3_ans': 'DNNってなんですか'}, {'top4_id': '0715', 'top4_ans': 'カーネルトリックってなんですか'}, {'top5_id': '0505', 'top5_ans': 'パーセプトロンってなんですか'}]}, {'id': '1409', 'q': '自己学習の問題点は何ですか', 'search': [{'top1_id': '1409', 'top1_ans': '自己学習の問題点は何がありますか'}, {'top2_id': '1407', 'top2_ans': '自己学習とは何ですか'}, {'top3_id': '1407', 'top3_ans': '自己学習の狙いは何ですか'}, {'top4_id': '1407', 'top4_ans': '自己学習とはなんですか'}, {'top5_id': '0906', 'top5_ans': '階層が多いニューラルネットワークの学習の問題点は？'}]}, {'id': '1205', 'q': 'a priori な原理の対偶とは何ですか', 'search': [{'top1_id': '1205', 'top1_ans': 'a prioriな原理とは何ですか'}, {'top2_id': '1207', 'top2_ans': 'a prioriアルゴリズムとは何ですか'}, {'top3_id': '1110', 'top3_ans': 'モデルの対数尤度とは何ですか'}, {'top4_id': '0810', 'top4_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}, {'top5_id': '0507', 'top5_ans': 'パーセプトロンの収束定理ってなんですか'}]}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "pickle_dump(search_top_5, './search2_top_5.pickle')\n",
    "print(pickle_load('./search2_top_5.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#\n",
    "#                                     以上フラットな検索\n",
    "#\n",
    "#                    以下ではチュータリングシステムにおける検索精度を測る\n",
    "#\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------結果-------\n",
      "66\n",
      "34\n",
      "精度： 0.66\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "\n",
    "\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "\n",
    "tutoring_search_top_5 = []  # { 'id':'0101', \n",
    "                    #    'q':'多階層ニューラルネットワークとは何か', \n",
    "                    #    'ruizido' : [\n",
    "                    #       { 'top1_id': '0102, 'top1_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #       { 'top2_id': '0102, 'top2_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #          ...\n",
    "                    #     ]\n",
    "\n",
    "for idx in range(len(qa_dev)):\n",
    "    \n",
    "# sudachiを使うver    \n",
    "#     body = {\n",
    "#         \"query\" : {\n",
    "#             \"bool\": {\n",
    "#                 \"must\": [\n",
    "#                     {\n",
    "#                       \"query_string\": {\n",
    "#                         \"analyzer\": \"sudachi_analyzer\",\n",
    "#                         \"query\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question']\n",
    "#                       }\n",
    "#                     }\n",
    "#                 ]            \n",
    "#             }\n",
    "#         },\n",
    "#         \"highlight\": {\n",
    "#             \"fields\": {\n",
    "#                 \"itemCaption\": {}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#  sudachiを使わないver\n",
    "    body = {\n",
    "        \"query\" : {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                ]            \n",
    "            }\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"itemCaption\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    body['query']['bool']['must'].append(\n",
    "        { \n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"question\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question'] } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "#     print(\"------------devの質問---------------\")\n",
    "#     print(qa_dev[idx]['title'], qa_dev[idx]['paragraphs'][0]['qas'][0]['question'])\n",
    "#     print(\"------------trainの検索結果------------\")\n",
    "    result = es.search(index='qa_train2', body=body, size=1000)\n",
    "    result_num = result['hits']['total']['value']\n",
    "    get_qa_train = result['hits']['hits']\n",
    "    \n",
    "    get_id_and_ans = []\n",
    "    \n",
    "    for one_get_qa_train in get_qa_train:\n",
    "        \n",
    "        if int(qa_dev[idx]['title']) >= int(one_get_qa_train['_source']['id']):\n",
    "            \n",
    "            get_id_and_ans.append([one_get_qa_train['_source']['id'], one_get_qa_train['_source']['question']])\n",
    "            \n",
    "        if len(get_id_and_ans) == 5:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    if len(get_id_and_ans) == 1:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  get_id_and_ans[0][1] },\n",
    "                        { 'top2_id':  None, 'top2_ans':  None },\n",
    "                        { 'top3_id':  None, 'top3_ans':  None },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 2:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  get_id_and_ans[0][1] },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  get_id_and_ans[1][1] },\n",
    "                        { 'top3_id':  None, 'top3_ans':  None },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 3:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  get_id_and_ans[0][1] },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  get_id_and_ans[1][1] },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans':  get_id_and_ans[2][1] },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 4:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  get_id_and_ans[0][1] },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  get_id_and_ans[1][1] },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans':  get_id_and_ans[2][1] },\n",
    "                        { 'top4_id':  get_id_and_ans[3][0], 'top4_ans':  get_id_and_ans[3][1] },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "        \n",
    "    if len(get_id_and_ans) == 5:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  get_id_and_ans[0][1] },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  get_id_and_ans[1][1] },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans':  get_id_and_ans[2][1] },\n",
    "                        { 'top4_id':  get_id_and_ans[3][0], 'top4_ans':  get_id_and_ans[3][1] },\n",
    "                        { 'top5_id':  get_id_and_ans[4][0], 'top5_ans':  get_id_and_ans[4][1] }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    # ruizido_top_5に格納する辞書型を作る\n",
    "#     mydict = { \n",
    "#           'id': qa_dev[idx]['title'],\n",
    "#           'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "#           'search' : [\n",
    "#                         { 'top1_id':  get_qa_train[0]['_source']['id'], 'top1_ans':  get_qa_train[0]['_source']['question'] },\n",
    "#                         { 'top2_id':  get_qa_train[1]['_source']['id'], 'top2_ans':  get_qa_train[1]['_source']['question'] },\n",
    "#                         { 'top3_id':  get_qa_train[2]['_source']['id'], 'top3_ans':  get_qa_train[2]['_source']['question'] },\n",
    "#                         { 'top4_id':  get_qa_train[3]['_source']['id'], 'top4_ans':  get_qa_train[3]['_source']['question'] },\n",
    "#                         { 'top5_id':  get_qa_train[4]['_source']['id'], 'top5_ans':  get_qa_train[4]['_source']['question'] }\n",
    "#           ]\n",
    "#           }\n",
    "    tutoring_search_top_5.append(mydict)\n",
    "    \n",
    "    if qa_dev[idx]['title'] == get_id_and_ans[0][0]:\n",
    "        true_count += 1\n",
    "    else:\n",
    "        false_count += 1\n",
    "\n",
    "\n",
    "print(\"------結果-------\")\n",
    "print(true_count)\n",
    "print(false_count)\n",
    "print(\"精度：\", true_count / (true_count + false_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '0610', 'q': 'バイアスとはなんですか', 'search': [{'top1_id': '0610', 'top1_ans': 'バイアスってなんですか'}, {'top2_id': '0610', 'top2_ans': 'バイアスと分散はどのような関係ですか'}, {'top3_id': '0305', 'top3_ans': 'バイアスって何ですか'}, {'top4_id': '0311', 'top4_ans': 'エントロピーとはなんですか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングとはなんですか'}]}, {'id': '0306', 'q': '候補削除アルゴリズムとはなんですか', 'search': [{'top1_id': '0306', 'top1_ans': '候補削除アルゴリズムってなんですか'}, {'top2_id': '0204', 'top2_ans': '次元削減とはなんですか'}, {'top3_id': '0305', 'top3_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top4_id': '0204', 'top4_ans': 'なぜ次元を削減するんですか'}, {'top5_id': '0204', 'top5_ans': 'なぜ前処理で次元削減を行うのですか'}]}, {'id': '1502', 'q': '強化学習とは何ですか', 'search': [{'top1_id': '1502', 'top1_ans': '強化学習とはなんですか'}, {'top2_id': '1501', 'top2_ans': '強化学習はなぜ中間的学習なのですか'}, {'top3_id': '1502', 'top3_ans': '強化学習は例えば何に使われていますか'}, {'top4_id': '1501', 'top4_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'}, {'top5_id': '1001', 'top5_ans': 'アンサンブル学習とは何ですか'}]}, {'id': '0605', 'q': '決定係数の定義は何ですか', 'search': [{'top1_id': '0605', 'top1_ans': '決定係数ってなんですか'}, {'top2_id': '0102', 'top2_ans': '人工知能の定義はなんですか'}, {'top3_id': '0307', 'top3_ans': '決定木とはなんですか'}, {'top4_id': '0307', 'top4_ans': '決定木ってなんですか'}, {'top5_id': '0316', 'top5_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'}]}, {'id': '0916', 'q': '時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか', 'search': [{'top1_id': '0509', 'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top2_id': '0916', 'top2_ans': '中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか'}, {'top3_id': '0404', 'top3_ans': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか'}, {'top4_id': '0115', 'top4_ans': 'データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか'}, {'top5_id': '0509', 'top5_ans': '学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか'}]}, {'id': '1502', 'q': '強化学習はどのような学習法ですか', 'search': [{'top1_id': '1501', 'top1_ans': '強化学習はなぜ中間的学習なのですか'}, {'top2_id': '1502', 'top2_ans': '強化学習とはなんですか'}, {'top3_id': '1501', 'top3_ans': '強化学習が中間的学習という位置づけにある理由はなんですか'}, {'top4_id': '0307', 'top4_ans': '決定木とはどのような学習法ですか'}, {'top5_id': '0109', 'top5_ans': '教師なし学習とはどういう学習法ですか'}]}, {'id': '0411', 'q': 'ゼロ頻度問題って何ですか', 'search': [{'top1_id': '0411', 'top1_ans': 'ゼロ頻度問題とは何ですか'}, {'top2_id': '0411', 'top2_ans': 'ゼロ頻度問題はどのような問題ですか'}, {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどうすれば回避できますか'}, {'top4_id': '0209', 'top4_ans': '精度って何ですか'}, {'top5_id': '0407', 'top5_ans': '何故尤度の対数をとって計算するんですか'}]}, {'id': '0810', 'q': '階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか', 'search': [{'top1_id': '0802', 'top1_ans': 'パーセプトロンをノードとして，階層状に結合したものをなんといいますか'}, {'top2_id': '0803', 'top2_ans': 'ノードを階層的に組むとどのような識別面ができるか'}, {'top3_id': '0803', 'top3_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのか'}, {'top4_id': '0803', 'top4_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのですか'}, {'top5_id': '0810', 'top5_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}]}, {'id': '0514', 'q': '最急勾配法の問題点の対処法は何がありますか', 'search': [{'top1_id': '0514', 'top1_ans': '最急勾配法の問題点とはなんですか'}, {'top2_id': '0514', 'top2_ans': '最急勾配法の欠点はなんですか'}, {'top3_id': '0512', 'top3_ans': '最急勾配法って何ですか'}, {'top4_id': '0513', 'top4_ans': '最急勾配法の重みの更新はどうなったら止まりますか'}, {'top5_id': '0513', 'top5_ans': '最急勾配法はいつ止まるんですか'}]}, {'id': '1103', 'q': '階層的手法とは何ですか', 'search': [{'top1_id': '0906', 'top1_ans': '多階層ニューラルネットワークとは何か'}, {'top2_id': '0115', 'top2_ans': 'パターンマイニングの代表的な手法には何がありますか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングの代表的な手法には何がありますか'}, {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークとは何ですか'}, {'top5_id': '0112', 'top5_ans': '回帰の代表的な手法には何がありますか'}]}, {'id': '0805', 'q': 'なぜ重みを更新するのですか', 'search': [{'top1_id': '0513', 'top1_ans': '最急勾配法の重みの更新はどうなったら止まりますか'}, {'top2_id': '0717', 'top2_ans': 'スラック変数の重みは離散値ですか、連続値ですか'}, {'top3_id': '0204', 'top3_ans': 'なぜ次元を削減するんですか'}, {'top4_id': '0606', 'top4_ans': 'なぜパラメータを0に近づけるんですか'}, {'top5_id': '0505', 'top5_ans': 'パーセプトロンを多層に重ねたものはなんですか'}]}, {'id': '0412', 'q': 'ベイジアンネットワークって何ですか', 'search': [{'top1_id': '0412', 'top1_ans': 'ベイジアンネットワークってなんですか'}, {'top2_id': '0115', 'top2_ans': 'パターンマイニングって何ですか'}, {'top3_id': '0311', 'top3_ans': 'エントロピーって何ですか'}, {'top4_id': '0114', 'top4_ans': 'クラスタリングって何ですか'}, {'top5_id': '0305', 'top5_ans': 'バイアスって何ですか'}]}, {'id': '0614', 'q': 'モデル木って何ですか', 'search': [{'top1_id': '0614', 'top1_ans': 'モデル木ってなんですか'}, {'top2_id': '0611', 'top2_ans': '回帰木って何ですか'}, {'top3_id': '0505', 'top3_ans': '識別モデルってなんですか'}, {'top4_id': '0114', 'top4_ans': 'モデル推定ってなんですか'}, {'top5_id': '0614', 'top5_ans': 'モデル木とはなんですか'}]}, {'id': '0601', 'q': '回帰問題の学習目的はなんですか', 'search': [{'top1_id': '0601', 'top1_ans': '回帰問題とはなんですか'}, {'top2_id': '0601', 'top2_ans': '回帰問題ってなんですか'}, {'top3_id': '0113', 'top3_ans': '教師なし学習の目的はなんですか'}, {'top4_id': '0104', 'top4_ans': '深層学習が得意な問題はなんですか'}, {'top5_id': '0112', 'top5_ans': '回帰の代表的な手法には何がありますか'}]}, {'id': '0717', 'q': 'グリッドサーチはなんのためにやるんですか', 'search': [{'top1_id': '0115', 'top1_ans': 'データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか'}, {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'}, {'top3_id': '0402', 'top3_ans': '統計的識別ってどうやってやるんですか'}, {'top4_id': '0505', 'top4_ans': 'パーセプトロンを多層に重ねたものはなんですか'}, {'top5_id': '0717', 'top5_ans': 'グリッドサーチとは何ですか'}]}, {'id': '1412', 'q': 'ラベル伝搬法の考え方はどのようなものですか', 'search': [{'top1_id': '1412', 'top1_ans': 'ラベル伝搬法の考え方にある仮定はどのようなものですか'}, {'top2_id': '1412', 'top2_ans': 'ラベル伝搬法の考え方は何ですか'}, {'top3_id': '0611', 'top3_ans': '決定木はどのような考えですか'}, {'top4_id': '0614', 'top4_ans': 'モデル木はどのような方法ですか'}, {'top5_id': '1405', 'top5_ans': '特徴の伝搬とは何ですか'}]}, {'id': '0602', 'q': '回帰問題が識別問題と異なるのはどのような点ですか', 'search': [{'top1_id': '0602', 'top1_ans': '回帰問題は識別問題とどう違うんですか'}, {'top2_id': '0601', 'top2_ans': '回帰問題とはなんですか'}, {'top3_id': '0411', 'top3_ans': 'ゼロ頻度問題はどのような問題ですか'}, {'top4_id': '0601', 'top4_ans': '回帰問題ってなんですか'}, {'top5_id': '0111', 'top5_ans': '識別問題にはどんな種類がありますか'}]}, {'id': '0805', 'q': '出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか', 'search': [{'top1_id': '0805', 'top1_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'}, {'top2_id': '0805', 'top2_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'}, {'top3_id': '0717', 'top3_ans': 'パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか'}, {'top4_id': '0717', 'top4_ans': 'パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに'}, {'top5_id': '0805', 'top5_ans': '誤差逆伝播法とは何か'}]}, {'id': '0413', 'q': 'ベイジアンネットワークの利点とはなんですか', 'search': [{'top1_id': '0413', 'top1_ans': 'ベイジアンネットワークの利点はなんですか'}, {'top2_id': '0412', 'top2_ans': 'ベイジアンネットワークってなんですか'}, {'top3_id': '0413', 'top3_ans': 'ベイジアンネットワークのメリットは何ですか'}, {'top4_id': '0103', 'top4_ans': '機械学習はどんな時に利用されますか'}, {'top5_id': '0412', 'top5_ans': 'ベイジアンネットワークはどのような仮定を表現したものですか'}]}, {'id': '0611', 'q': '回帰木とはなに', 'search': [{'top1_id': '0611', 'top1_ans': '回帰木とはなんですか'}, {'top2_id': '0611', 'top2_ans': '回帰木って何ですか'}, {'top3_id': '0611', 'top3_ans': '回帰木の特徴はなんですか'}, {'top4_id': '0611', 'top4_ans': '回帰木の特徴はどんものですか'}, {'top5_id': '0601', 'top5_ans': '回帰問題とはなんですか'}]}, {'id': '0811', 'q': 'ReLUとは何ですか', 'search': [{'top1_id': '0811', 'top1_ans': 'ReLu関数の良さは何ですか'}, {'top2_id': '0811', 'top2_ans': 'ReLUを用いる利点はなんですか'}, {'top3_id': '0811', 'top3_ans': 'ReLUを用いると誤差はどうなりますか'}, {'top4_id': '0811', 'top4_ans': '活性化関数にReLUを用いると誤差が失われないのはなぜですか'}, {'top5_id': '0717', 'top5_ans': 'グリッドサーチとは何ですか'}]}, {'id': '0803', 'q': 'ノードを階層的に組むと非線形識別面が実現できるのはなぜか', 'search': [{'top1_id': '0803', 'top1_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのか'}, {'top2_id': '0803', 'top2_ans': 'なぜノードを階層的に組むと非線形識別面が実現できるのですか'}, {'top3_id': '0803', 'top3_ans': 'なぜノードを階層的に組むと非線形識別面を実現できるんですか'}, {'top4_id': '0803', 'top4_ans': 'ノードを階層的に組むとどのような識別面ができるか'}, {'top5_id': '0803', 'top5_ans': '基底関数ベクトルによる非線形識別面はどういう風に実現されているのか'}]}, {'id': '0302', 'q': 'クラスとはなんですか', 'search': [{'top1_id': '0302', 'top1_ans': 'クラスって何ですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングとはなんですか'}, {'top3_id': '0115', 'top3_ans': 'パターンマイニングとはなんですか'}, {'top4_id': '0209', 'top4_ans': 'F値とはなんですか'}, {'top5_id': '0209', 'top5_ans': '精度とはなんですか'}]}, {'id': '1219', 'q': '協調フィルタリングではどのようなデータが必要ですか', 'search': [{'top1_id': '1219', 'top1_ans': '協調フィルタリングとはどのようなものですか'}, {'top2_id': '1219', 'top2_ans': '協調フィルタリングとは何ですか'}, {'top3_id': '0802', 'top3_ans': 'フィードフォワード型ニューラルネットワークを構成するには何が必要ですか'}, {'top4_id': '1114', 'top4_ans': '事前確率を求めるには何が必要ですか'}, {'top5_id': '0906', 'top5_ans': '特徴抽出を学習するには何が必要か'}]}, {'id': '0606', 'q': '汎化能力という点で望ましい線形回帰式の性質はなんですか', 'search': [{'top1_id': '0502', 'top1_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top2_id': '0508', 'top2_ans': '線形分離不可能なデータにはどのように対処しますか'}, {'top3_id': '0110', 'top3_ans': '識別と回帰の違いはなんですか'}, {'top4_id': '0311', 'top4_ans': '分類能力が高いとはどういうことですか'}, {'top5_id': '0507', 'top5_ans': 'なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか'}]}, {'id': '0917', 'q': '中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか', 'search': [{'top1_id': '0917', 'top1_ans': 'リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか'}, {'top2_id': '0916', 'top2_ans': '中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか'}, {'top3_id': '0917', 'top3_ans': 'LSTMで置き換えられるメモリユニットの名前はなんですか'}, {'top4_id': '0901', 'top4_ans': '深層学習を言い換えると何になりますか'}, {'top5_id': '0911', 'top5_ans': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという'}]}, {'id': '0113', 'q': '教師なし学習では何を学習しますか', 'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'}, {'top2_id': '0113', 'top2_ans': '教師なし学習の目的はなんですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'}, {'top5_id': '0109', 'top5_ans': '教師あり学習とはなんですか'}]}, {'id': '0406', 'q': 'i.i.d. (independent and identically distributed) とはなんですか', 'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングとはなんですか'}, {'top3_id': '0115', 'top3_ans': 'パターンマイニングとはなんですか'}, {'top4_id': '0209', 'top4_ans': 'F値とはなんですか'}, {'top5_id': '0209', 'top5_ans': '精度とはなんですか'}]}, {'id': '0713', 'q': 'SVMの方法は例えば何に使われていますか', 'search': [{'top1_id': '0713', 'top1_ans': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのようなことに使われますか'}, {'top3_id': '0117', 'top3_ans': '半教師あり学習はどんなときに使われますか'}, {'top4_id': '0404', 'top4_ans': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか'}, {'top5_id': '0411', 'top5_ans': 'ゼロ頻度問題はどうすれば回避できますか'}]}, {'id': '1108', 'q': 'k-平均法って何ですか', 'search': [{'top1_id': '1108', 'top1_ans': 'k-平均法とはなんですか'}, {'top2_id': '1108', 'top2_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'}, {'top4_id': '0512', 'top4_ans': '最急勾配法って何ですか'}, {'top5_id': '0410', 'top5_ans': 'ナイーブベイス識別法ってなんですか'}]}, {'id': '1111', 'q': '異常検出とは何ですか', 'search': [{'top1_id': '0917', 'top1_ans': 'LSTMと通常のユニットの違いは何ですか'}, {'top2_id': '1009', 'top2_ans': '通常の決定木とランダムフォレストとの違いは何ですか'}, {'top3_id': '0912', 'top3_ans': '畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか'}, {'top4_id': '1010', 'top4_ans': 'バギングやランダムフォレストとブースティングではどのような点で異なりますか'}, {'top5_id': '1009', 'top5_ans': '通常の決定木とランダムフォレストの違いはなんですか'}]}, {'id': '0410', 'q': 'ナイーブベイズ識別法とはなんですか', 'search': [{'top1_id': '0410', 'top1_ans': 'ナイーブベイズ識別って何ですか'}, {'top2_id': '0410', 'top2_ans': 'ナイーブベイス識別法ってなんですか'}, {'top3_id': '0402', 'top3_ans': '統計的識別とはなんですか'}, {'top4_id': '0110', 'top4_ans': '識別と回帰の違いはなんですか'}, {'top5_id': '0111', 'top5_ans': '識別の代表的な手法には何がありますか'}]}, {'id': '0901', 'q': 'DNNとは何ですか', 'search': [{'top1_id': '0901', 'top1_ans': 'DNNってなんですか'}, {'top2_id': '0717', 'top2_ans': 'グリッドサーチとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'サポートベクトルマシンとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'SVMとは何ですか'}, {'top5_id': '0801', 'top5_ans': 'ニューラルネットワークとは何ですか'}]}, {'id': '0508', 'q': '二乗誤差とはなんですか', 'search': [{'top1_id': '0508', 'top1_ans': '二乗誤差とは何ですか'}, {'top2_id': '0508', 'top2_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'}, {'top3_id': '0508', 'top3_ans': '最小二乗法とはなんですか'}, {'top4_id': '0508', 'top4_ans': '最小二乗法ってなんですか'}, {'top5_id': '0508', 'top5_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}]}, {'id': '0407', 'q': '対数尤度とはなんですか', 'search': [{'top1_id': '0407', 'top1_ans': '何故尤度の対数をとって計算するんですか'}, {'top2_id': '0406', 'top2_ans': '尤度とはなんですか'}, {'top3_id': '0405', 'top3_ans': '尤度とは何ですか'}, {'top4_id': '0406', 'top4_ans': '尤度とはどういう意味ですか'}, {'top5_id': '0209', 'top5_ans': '精度とはなんですか'}]}, {'id': '0708', 'q': 'スラック変数が小さいほうがいいのはなぜか', 'search': [{'top1_id': '0708', 'top1_ans': 'なぜスラック変数が小さい方が良いのですか'}, {'top2_id': '0701', 'top2_ans': 'マージンは大きいほうがいいんですか'}, {'top3_id': '0708', 'top3_ans': 'スラック変数はなにをするものか'}, {'top4_id': '0708', 'top4_ans': 'スラック変数とはなんですか'}, {'top5_id': '0708', 'top5_ans': 'スラック変数とは何ですか'}]}, {'id': '0511', 'q': '入力が負例のときは、確率はどう求められますか', 'search': [{'top1_id': '0405', 'top1_ans': '事後確率が最大となるクラスは、何を求めることで得られますか'}, {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'}, {'top3_id': '0405', 'top3_ans': '事後確率が最大になるクラスはどうやって得られますか'}, {'top4_id': '0402', 'top4_ans': '入力を観測した後で計算される確率は何と言いますか'}, {'top5_id': '0416', 'top5_ans': 'どういうときに確率伝播を使うんですか'}]}, {'id': '1110', 'q': 'X-meansアルゴリズムとはなんですか', 'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'}, {'top2_id': '1110', 'top2_ans': 'k-means法の問題点はなんですか'}, {'top3_id': '0305', 'top3_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top4_id': '0306', 'top4_ans': '候補削除アルゴリズムってなんですか'}, {'top5_id': '1108', 'top5_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}]}, {'id': '0313', 'q': '過学習を避けるためにはどのような方法がありますか', 'search': [{'top1_id': '0313', 'top1_ans': '決定木ではどうやって過学習を回避しますか'}, {'top2_id': '0112', 'top2_ans': '回帰にはどのような種類がありますか'}, {'top3_id': '0206', 'top3_ans': '分割学習法の際学習データが足りない場合どうしたらいいですか'}, {'top4_id': '0116', 'top4_ans': '半教師あり学習とはどのようなものですか'}, {'top5_id': '0313', 'top5_ans': '過学習とはどのような状態ですか'}]}, {'id': '0117', 'q': '半教師あり学習に適した状況はどのように作られますか', 'search': [{'top1_id': '0117', 'top1_ans': '半教師あり学習はどんなときに使われますか'}, {'top2_id': '0116', 'top2_ans': '半教師あり学習とはどのようなものですか'}, {'top3_id': '0116', 'top3_ans': '半教師あり学習とはなんですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習はどういうものですか'}, {'top5_id': '0109', 'top5_ans': '教師なし学習とはどういう学習法ですか'}]}, {'id': '0109', 'q': '教師あり学習って何ですか', 'search': [{'top1_id': '0109', 'top1_ans': '教師あり学習とはなんですか'}, {'top2_id': '0109', 'top2_ans': '教師なし学習とはどういう学習法ですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習はどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習とはどういうものですか'}, {'top5_id': '0102', 'top5_ans': '機械学習と人工知能の意味の差はありますか'}]}, {'id': '0204', 'q': '主成分分析ってどういうことをしているんですか', 'search': [{'top1_id': '0111', 'top1_ans': '識別では、なぜすべてのデータをきれいに分離しないのですか'}, {'top2_id': '0109', 'top2_ans': '教師なし学習とはどういうものですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習とはどういう学習法ですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習はどういうものですか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングとはどういうものですか'}]}, {'id': '0901', 'q': '深層学習とはどのようなものか', 'search': [{'top1_id': '0901', 'top1_ans': '深層学習はどのようなものですか'}, {'top2_id': '0901', 'top2_ans': '深層学習はどのような場面で応用されますか'}, {'top3_id': '0901', 'top3_ans': '深層学習はどのような分野で用いられていますか'}, {'top4_id': '0901', 'top4_ans': '深層学習のポイントは何'}, {'top5_id': '0116', 'top5_ans': '半教師あり学習とはどのようなものですか'}]}, {'id': '1012', 'q': 'ブースティングで、個々のデータに対してどのように重みを設定するのですか', 'search': [{'top1_id': '1012', 'top1_ans': 'バギングでは、個々のデータに対してどのように重みを設定しますか'}, {'top2_id': '0717', 'top2_ans': 'SVMのハイパーパラメータの設定はどのように行いますか'}, {'top3_id': '0508', 'top3_ans': '線形分離不可能なデータにはどのように対処しますか'}, {'top4_id': '0707', 'top4_ans': 'ソフトマージンによる際の識別面の設定の仕方'}, {'top5_id': '1012', 'top5_ans': 'ブースティングでは、どのように識別器を作成するのですか'}]}, {'id': '1110', 'q': 'BICとは何ですか', 'search': [{'top1_id': '0717', 'top1_ans': 'グリッドサーチとは何ですか'}, {'top2_id': '0701', 'top2_ans': 'サポートベクトルマシンとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'SVMとは何ですか'}, {'top4_id': '0917', 'top4_ans': 'LSTMとは何ですか'}, {'top5_id': '0911', 'top5_ans': 'ドロップアウトとは何ですか'}]}, {'id': '0707', 'q': 'ソフトマージンとは何ですか', 'search': [{'top1_id': '0707', 'top1_ans': 'ソフトマージンによる際の識別面の設定の仕方'}, {'top2_id': '0701', 'top2_ans': 'サポートベクトルマシンとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'SVMとは何ですか'}, {'top4_id': '0505', 'top4_ans': 'パーセプトロンとは何ですか'}, {'top5_id': '0701', 'top5_ans': 'マージンとは何ですか'}]}, {'id': '0507', 'q': 'パーセプトロンの学習アルゴリズムはどのようなときに停止しますか', 'search': [{'top1_id': '0506', 'top1_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top2_id': '0507', 'top2_ans': 'なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか'}, {'top3_id': '0316', 'top3_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'}, {'top4_id': '0507', 'top4_ans': '学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか'}, {'top5_id': '0502', 'top5_ans': '統計モデルによるアプローチはどのようなときに有効ですか'}]}, {'id': '0811', 'q': 'ReLuってなんですか', 'search': [{'top1_id': '0811', 'top1_ans': 'ReLUを用いる利点はなんですか'}, {'top2_id': '0612', 'top2_ans': 'CARTってなんですか'}, {'top3_id': '0114', 'top3_ans': 'クラスタリングってなんですか'}, {'top4_id': '0715', 'top4_ans': 'カーネルトリックってなんですか'}, {'top5_id': '0505', 'top5_ans': 'パーセプトロンってなんですか'}]}, {'id': '0504', 'q': 'なぜ生成モデルアプローチは問題を難しくしているといえるのですか', 'search': [{'top1_id': '0504', 'top1_ans': '生成モデルアプローチが有効なのはどのようなときですか'}, {'top2_id': '0111', 'top2_ans': '識別では、なぜすべてのデータをきれいに分離しないのですか'}, {'top3_id': '0502', 'top3_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top4_id': '0204', 'top4_ans': '特徴ベクトルの次元数が増えるとどんな問題がありますか'}, {'top5_id': '0402', 'top5_ans': '入力を観測した後で計算される確率は何と言いますか'}]}, {'id': '0908', 'q': '自己写像の学習において使われる入力と出力の距離は何ですか', 'search': [{'top1_id': '0908', 'top1_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'}, {'top2_id': '0908', 'top2_ans': '入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか'}, {'top3_id': '0404', 'top3_ans': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか'}, {'top4_id': '0906', 'top4_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'}, {'top5_id': '0506', 'top5_ans': 'パーセプトロンの出力はどのようになっていますか'}]}, {'id': '0907', 'q': '事前学習法の考え方は何ですか', 'search': [{'top1_id': '0907', 'top1_ans': '事前学習法とは何ですか'}, {'top2_id': '0907', 'top2_ans': '事前学習とは何ですか'}, {'top3_id': '0404', 'top3_ans': '事前確率とは何ですか'}, {'top4_id': '0611', 'top4_ans': '決定木はどのような考えですか'}, {'top5_id': '0404', 'top5_ans': '事前確率とはなんですか'}]}, {'id': '0611', 'q': '回帰木ってなんですか', 'search': [{'top1_id': '0611', 'top1_ans': '回帰木って何ですか'}, {'top2_id': '0606', 'top2_ans': 'Ridge回帰ってなんですか'}, {'top3_id': '0607', 'top3_ans': 'Lasso回帰ってなんですか'}, {'top4_id': '0611', 'top4_ans': '回帰木とはなんですか'}, {'top5_id': '0601', 'top5_ans': '回帰問題ってなんですか'}]}, {'id': '1209', 'q': 'リフト値とは何ですか', 'search': [{'top1_id': '1209', 'top1_ans': 'リフト値ってなんですか'}, {'top2_id': '1209', 'top2_ans': 'リフト値の値からどのようなことがわかりますか'}, {'top3_id': '1111', 'top3_ans': '外れ値とは何ですか'}, {'top4_id': '0209', 'top4_ans': 'F値って何ですか'}, {'top5_id': '0209', 'top5_ans': 'F値とはなんですか'}]}, {'id': '1209', 'q': '確信度の値からどのようなことがわかりますか', 'search': [{'top1_id': '1209', 'top1_ans': 'リフト値の値からどのようなことがわかりますか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングはどのようなことに使われますか'}, {'top3_id': '1209', 'top3_ans': '確信度とはなんですか'}, {'top4_id': '1209', 'top4_ans': '確信度とは何ですか'}, {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]}, {'id': '1303', 'q': '形態素解析とは何ですか', 'search': [{'top1_id': '1303', 'top1_ans': '日本語の形態素列の特徴は何ですか'}, {'top2_id': '0205', 'top2_ans': '主成分分析とは何ですか'}, {'top3_id': '0205', 'top3_ans': '主成分分析って何ですか'}, {'top4_id': '0205', 'top4_ans': '主成分分析とはなんですか'}, {'top5_id': '0313', 'top5_ans': '過学習とはどのような状態ですか'}]}, {'id': '1506', 'q': '政策とは何ですか', 'search': [{'top1_id': '1506', 'top1_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'}, {'top2_id': '1404', 'top2_ans': 'オーバーラップとは何ですか'}, {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'}, {'top5_id': '1306', 'top5_ans': 'CRFとは何ですか'}]}, {'id': '0305', 'q': 'バイアスとはなんですか', 'search': [{'top1_id': '0305', 'top1_ans': 'バイアスって何ですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングとはなんですか'}, {'top3_id': '0115', 'top3_ans': 'パターンマイニングとはなんですか'}, {'top4_id': '0209', 'top4_ans': 'F値とはなんですか'}, {'top5_id': '0209', 'top5_ans': '精度とはなんですか'}]}, {'id': '1110', 'q': 'x-meansアルゴリズムとは何ですか', 'search': [{'top1_id': '1110', 'top1_ans': 'X-meansアルゴリズムは、どのよなアルゴリズムなのですか'}, {'top2_id': '1110', 'top2_ans': 'k-means法の問題点はなんですか'}, {'top3_id': '0305', 'top3_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top4_id': '1108', 'top4_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top5_id': '1104', 'top5_ans': '階層的クラスタリングは、どのようなアルゴリズムですか'}]}, {'id': '0907', 'q': '誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか', 'search': [{'top1_id': '0810', 'top1_ans': '多段階に誤差逆伝播法を適用すると問題はありますか'}, {'top2_id': '0508', 'top2_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}, {'top3_id': '0805', 'top3_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'}, {'top4_id': '0810', 'top4_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}, {'top5_id': '0810', 'top5_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'}]}, {'id': '0805', 'q': '誤差逆伝播法とはどういう手法ですか', 'search': [{'top1_id': '0805', 'top1_ans': '誤差逆伝播法とは何か'}, {'top2_id': '0805', 'top2_ans': '誤差逆伝播法とは何ですか'}, {'top3_id': '0805', 'top3_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か'}, {'top4_id': '0805', 'top4_ans': '出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか'}, {'top5_id': '0416', 'top5_ans': 'どういうときに確率伝播を使うんですか'}]}, {'id': '0911', 'q': 'ドロップアウトによって過学習が生じにくくなっている理由はなんですか', 'search': [{'top1_id': '0911', 'top1_ans': 'なぜドロップアウトによって過学習が回避できるのですか'}, {'top2_id': '0911', 'top2_ans': 'ドロップアウトで過学習が防げるのはなぜですか'}, {'top3_id': '0313', 'top3_ans': '過学習って何ですか'}, {'top4_id': '0908', 'top4_ans': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに'}, {'top5_id': '0810', 'top5_ans': '多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか'}]}, {'id': '0206', 'q': '分割学習法とはなんですか', 'search': [{'top1_id': '0206', 'top1_ans': '分割学習法の際学習データが足りない場合どうしたらいいですか'}, {'top2_id': '0104', 'top2_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習とはどういう学習法ですか'}, {'top4_id': '0205', 'top4_ans': '主成分分析とはなんですか'}, {'top5_id': '0104', 'top5_ans': '深層学習と機械学習の違いはなんですか'}]}, {'id': '1506', 'q': '政策の良さはどのように評価されますか', 'search': [{'top1_id': '1506', 'top1_ans': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか'}, {'top2_id': '0901', 'top2_ans': '深層学習はどのような場面で応用されますか'}, {'top3_id': '0803', 'top3_ans': '基底関数ベクトルによる非線形識別面はどのように実現されていますか'}, {'top4_id': '0811', 'top4_ans': 'ReLu関数の良さは何ですか'}, {'top5_id': '0103', 'top5_ans': '機械学習はどんな時に利用されますか'}]}, {'id': '0109', 'q': '回帰とはなんですか', 'search': [{'top1_id': '0105', 'top1_ans': 'パターン認識とはなんですか'}, {'top2_id': '0105', 'top2_ans': '特徴抽出とはなんですか'}, {'top3_id': '0109', 'top3_ans': '教師あり学習とはなんですか'}, {'top4_id': '0104', 'top4_ans': '深層学習と機械学習の違いはなんですか'}, {'top5_id': '0104', 'top5_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}]}, {'id': '1108', 'q': 'k-meansアルゴリズムとは何ですか', 'search': [{'top1_id': '1108', 'top1_ans': 'k-平均法とは、どのようなアルゴリズムなのですか'}, {'top2_id': '0208', 'top2_ans': 'k-NN法とはなんですか'}, {'top3_id': '1108', 'top3_ans': 'k-平均法とはなんですか'}, {'top4_id': '0305', 'top4_ans': 'FIND-Sアルゴリズムとはなんですか'}, {'top5_id': '1104', 'top5_ans': '階層的クラスタリングは、どのようなアルゴリズムですか'}]}, {'id': '0508', 'q': '最小二乗法って何ですか', 'search': [{'top1_id': '0508', 'top1_ans': '最小二乗法ってなんですか'}, {'top2_id': '0508', 'top2_ans': '最小二乗法とはなんですか'}, {'top3_id': '0508', 'top3_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'}, {'top4_id': '0508', 'top4_ans': '二乗誤差を最小にするように識別関数を調整する方法を何と言いますか'}, {'top5_id': '0508', 'top5_ans': '二乗誤差とは何ですか'}]}, {'id': '0315', 'q': 'Gini Inpurityは何を表しますか', 'search': [{'top1_id': '0115', 'top1_ans': 'パターンマイニングの代表的な手法には何がありますか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングの代表的な手法には何がありますか'}, {'top3_id': '0112', 'top3_ans': '回帰の代表的な手法には何がありますか'}, {'top4_id': '0111', 'top4_ans': '識別の代表的な手法には何がありますか'}, {'top5_id': '0313', 'top5_ans': '決定木ではどうやって過学習を回避しますか'}]}, {'id': '0506', 'q': '単層パーセプトロンはどうやって出力を決めるんですか', 'search': [{'top1_id': '0506', 'top1_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top2_id': '0402', 'top2_ans': '条件付き確率ってどうやって求めるんですか'}, {'top3_id': '0402', 'top3_ans': '統計的識別ってどうやってやるんですか'}, {'top4_id': '0313', 'top4_ans': '決定木ではどうやって過学習を回避しますか'}, {'top5_id': '0505', 'top5_ans': 'パーセプトロンってなんですか'}]}, {'id': '0802', 'q': '中間層を言い換えるとなんといいますか', 'search': [{'top1_id': '0717', 'top1_ans': '複数のパラメータを組合わせる空間を何と言いますか'}, {'top2_id': '0802', 'top2_ans': '中間層はいくつ用意しますか'}, {'top3_id': '0715', 'top3_ans': '複雑な非線形変換を求めるという操作を避ける方法をなんといいますか'}, {'top4_id': '0711', 'top4_ans': 'もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか'}, {'top5_id': '0402', 'top5_ans': '事後確率が最大となるクラスを識別結果とする方法を何と言いますか'}]}, {'id': '0611', 'q': '出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか', 'search': [{'top1_id': '0509', 'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top2_id': '0316', 'top2_ans': '特徴が数値であるときの決定木の学習はどのように行いますか'}, {'top3_id': '0506', 'top3_ans': 'パーセプトロンの出力はどのようになっていますか'}, {'top4_id': '0502', 'top4_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top5_id': '0404', 'top5_ans': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか'}]}, {'id': '0111', 'q': '識別ではどのようなことができますか', 'search': [{'top1_id': '0111', 'top1_ans': '識別問題にはどんな種類がありますか'}, {'top2_id': '0111', 'top2_ans': '識別の代表的な手法には何がありますか'}, {'top3_id': '0111', 'top3_ans': '識別では、なぜすべてのデータをきれいに分離しないのですか'}, {'top4_id': '0110', 'top4_ans': '識別と回帰の違いはなんですか'}, {'top5_id': '0110', 'top5_ans': '識別と回帰の意味の違いは何ですか'}]}, {'id': '0811', 'q': 'ReLUだと勾配消失が起こらないのはなぜですか', 'search': [{'top1_id': '0811', 'top1_ans': '勾配が消失しない関数ってなんだったっけ'}, {'top2_id': '0810', 'top2_ans': '勾配消失問題とはなんですか'}, {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何か'}, {'top4_id': '0810', 'top4_ans': '勾配消失問題とは何ですか'}, {'top5_id': '0810', 'top5_ans': '勾配消失問題とはどのような問題ですか'}]}, {'id': '0209', 'q': '再現率って何ですか', 'search': [{'top1_id': '0115', 'top1_ans': 'パターンマイニングって何ですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングって何ですか'}, {'top3_id': '0209', 'top3_ans': 'F値って何ですか'}, {'top4_id': '0209', 'top4_ans': '精度って何ですか'}, {'top5_id': '0205', 'top5_ans': '主成分分析って何ですか'}]}, {'id': '0701', 'q': 'サポートベクターマシンとはなんですか', 'search': [{'top1_id': '0311', 'top1_ans': 'エントロピーとはなんですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングとはなんですか'}, {'top3_id': '0612', 'top3_ans': 'CARTとはなんですか'}, {'top4_id': '0115', 'top4_ans': 'パターンマイニングとはなんですか'}, {'top5_id': '0209', 'top5_ans': 'F値とはなんですか'}]}, {'id': '1004', 'q': 'バギングとは何ですか', 'search': [{'top1_id': '1004', 'top1_ans': 'バギングとはなんですか'}, {'top2_id': '1004', 'top2_ans': 'バギングはどういう手法ですか'}, {'top3_id': '0717', 'top3_ans': 'グリッドサーチとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'サポートベクトルマシンとは何ですか'}, {'top5_id': '0701', 'top5_ans': 'SVMとは何ですか'}]}, {'id': '0105', 'q': '機械学習の基本的な定義はなんですか', 'search': [{'top1_id': '0104', 'top1_ans': '深層学習と機械学習の違いはなんですか'}, {'top2_id': '0104', 'top2_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top3_id': '0103', 'top3_ans': '機械学習はどんな時に利用されますか'}, {'top4_id': '0103', 'top4_ans': '機械学習で用いられるビッグデータとは何ですか'}, {'top5_id': '0102', 'top5_ans': '人工知能と機械学習は同じ意味ですか'}]}, {'id': '0712', 'q': '非線形変換が存在するのはどういう条件の時ですか', 'search': [{'top1_id': '0710', 'top1_ans': 'サポートベクトルマシンで高次元に非線形変換する際の条件はなに'}, {'top2_id': '0704', 'top2_ans': '制約条件ではどのような式を用いますか'}, {'top3_id': '0710', 'top3_ans': '特徴空間を高次元に変換する際に重要な条件はなんですか'}, {'top4_id': '0402', 'top4_ans': '条件付き確率ってどうやって求めるんですか'}, {'top5_id': '0502', 'top5_ans': '非線形性を持つデータに対して識別を行うにはどんな方法がありますか'}]}, {'id': '0701', 'q': 'サポートベクトルマシンはどういう手法ですか', 'search': [{'top1_id': '0114', 'top1_ans': 'クラスタリングはどのような手法ですか'}, {'top2_id': '0115', 'top2_ans': 'パターンマイニングとはどういう方法ですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習とはどういう学習法ですか'}, {'top4_id': '0514', 'top4_ans': '確率的最急勾配法とはどういうものですか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングとはどういうものですか'}]}, {'id': '0105', 'q': '機械学習では何ができますか', 'search': [{'top1_id': '0103', 'top1_ans': '機械学習はどんな時に利用されますか'}, {'top2_id': '0103', 'top2_ans': '機械学習で用いられるビッグデータとは何ですか'}, {'top3_id': '0104', 'top3_ans': '深層学習と機械学習の違いはなんですか'}, {'top4_id': '0104', 'top4_ans': '深層学習が他の機械学習手法と異なる点はなんですか'}, {'top5_id': '0102', 'top5_ans': '機械学習と人工知能の意味の差はありますか'}]}, {'id': '0701', 'q': 'なぜペナルティを設定するのですか', 'search': [{'top1_id': '0204', 'top1_ans': 'なぜ次元を削減するんですか'}, {'top2_id': '0606', 'top2_ans': 'なぜパラメータを0に近づけるんですか'}, {'top3_id': '0508', 'top3_ans': 'なぜ最小二乗法では正解との誤差を二乗するんですか'}, {'top4_id': '0204', 'top4_ans': 'なぜ前処理で次元削減を行うのですか'}, {'top5_id': '0111', 'top5_ans': '識別では、なぜすべてのデータをきれいに分離しないのですか'}]}, {'id': '1405', 'q': '半教師あり学習は何によく使われますか', 'search': [{'top1_id': '0117', 'top1_ans': '半教師あり学習はどんなときに使われますか'}, {'top2_id': '0116', 'top2_ans': '半教師あり学習とはなんですか'}, {'top3_id': '0116', 'top3_ans': '半教師あり学習とはどのようなものですか'}, {'top4_id': '1402', 'top4_ans': '半教師あり学習が成立する条件は何ですか'}, {'top5_id': '1402', 'top5_ans': '半教師あり学習に適した数値特徴データとはどのようなものですか'}]}, {'id': '0701', 'q': '線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに', 'search': [{'top1_id': '0502', 'top1_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top2_id': '0701', 'top2_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'}, {'top3_id': '0502', 'top3_ans': '非線形性を持つデータに対して識別を行うにはどんな方法がありますか'}, {'top4_id': '0509', 'top4_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top5_id': '0111', 'top5_ans': '識別では、なぜすべてのデータをきれいに分離しないのですか'}]}, {'id': '0209', 'q': '再現率とはなんですか', 'search': [{'top1_id': '0115', 'top1_ans': 'データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか'}, {'top2_id': '0114', 'top2_ans': 'クラスタリングとはなんですか'}, {'top3_id': '0115', 'top3_ans': 'パターンマイニングとはなんですか'}, {'top4_id': '0209', 'top4_ans': 'F値とはなんですか'}, {'top5_id': '0209', 'top5_ans': '精度とはなんですか'}]}, {'id': '0109', 'q': '教師なし学習って何ですか', 'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'}, {'top2_id': '0109', 'top2_ans': '教師なし学習はどういうものですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習とはどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師あり学習とはなんですか'}, {'top5_id': '0104', 'top5_ans': '深層学習と機械学習の違いはなんですか'}]}, {'id': '0911', 'q': '多階層学習においてドロップアウトを用いるとどうなるか', 'search': [{'top1_id': '0906', 'top1_ans': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか'}, {'top2_id': '0901', 'top2_ans': '深層学習に用いるニューラルネットワークをなんとよぶか'}, {'top3_id': '0906', 'top3_ans': '多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか'}, {'top4_id': '0810', 'top4_ans': '誤差逆伝播法による多階層ネットワークの学習は何故難しいのか'}, {'top5_id': '0903', 'top5_ans': '深層学習に用いられるニューラルネットワークはどのような種類がありますか'}]}, {'id': '0616', 'q': 'なぜ回帰問題にカーネル法を使うんですか', 'search': [{'top1_id': '0616', 'top1_ans': '回帰にカーネル法を取り入れる理由はなんですか'}, {'top2_id': '0602', 'top2_ans': '回帰問題は識別問題とどう違うんですか'}, {'top3_id': '0601', 'top3_ans': '回帰問題ってなんですか'}, {'top4_id': '0601', 'top4_ans': '回帰問題とはなんですか'}, {'top5_id': '0616', 'top5_ans': 'カーネル回帰とはなんですか'}]}, {'id': '0901', 'q': 'ディープニューラルネットワークとは何ですか', 'search': [{'top1_id': '0810', 'top1_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}, {'top2_id': '0717', 'top2_ans': 'グリッドサーチとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'サポートベクトルマシンとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'SVMとは何ですか'}, {'top5_id': '0801', 'top5_ans': 'ニューラルネットワークとは何ですか'}]}, {'id': '0204', 'q': '次元削減って何ですか', 'search': [{'top1_id': '0204', 'top1_ans': '次元削減とはなんですか'}, {'top2_id': '0204', 'top2_ans': 'なぜ次元を削減するんですか'}, {'top3_id': '0204', 'top3_ans': 'なぜ前処理で次元削減を行うのですか'}, {'top4_id': '0204', 'top4_ans': '特徴ベクトルの次元数が増えるとどんな問題がありますか'}, {'top5_id': '0115', 'top5_ans': 'パターンマイニングって何ですか'}]}, {'id': '0105', 'q': 'パターン認識とはどういうものですか', 'search': [{'top1_id': '0105', 'top1_ans': 'パターン認識とはなんですか'}, {'top2_id': '0105', 'top2_ans': 'パターン認識ってなんですか'}, {'top3_id': '0104', 'top3_ans': '深層学習と機械学習の違いはなんですか'}, {'top4_id': '0103', 'top4_ans': '機械学習で用いられるビッグデータとは何ですか'}, {'top5_id': '0102', 'top5_ans': '機械学習と人工知能の意味の差はありますか'}]}, {'id': '1007', 'q': 'ランダムフォレストとは何ですか', 'search': [{'top1_id': '1007', 'top1_ans': 'ランダムフォレストとはなんですか'}, {'top2_id': '0717', 'top2_ans': 'グリッドサーチとは何ですか'}, {'top3_id': '0701', 'top3_ans': 'サポートベクトルマシンとは何ですか'}, {'top4_id': '0701', 'top4_ans': 'SVMとは何ですか'}, {'top5_id': '0917', 'top5_ans': 'LSTMとは何ですか'}]}, {'id': '0109', 'q': '教師なし学習とはなんですか', 'search': [{'top1_id': '0109', 'top1_ans': '教師なし学習とはどういう学習法ですか'}, {'top2_id': '0109', 'top2_ans': '教師あり学習とはなんですか'}, {'top3_id': '0109', 'top3_ans': '教師なし学習とはどういうものですか'}, {'top4_id': '0109', 'top4_ans': '教師なし学習はどういうものですか'}, {'top5_id': '0104', 'top5_ans': '深層学習と機械学習の違いはなんですか'}]}, {'id': '0416', 'q': 'ここでいう目的変数って何ですか', 'search': [{'top1_id': '0416', 'top1_ans': '目的変数ってなんですか'}, {'top2_id': '0311', 'top2_ans': '分類能力が高いとはどういうことですか'}, {'top3_id': '0404', 'top3_ans': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか'}, {'top4_id': '0402', 'top4_ans': '統計的識別ってどうやってやるんですか'}, {'top5_id': '0114', 'top5_ans': 'クラスタリングはどのようなことに使われますか'}]}, {'id': '0710', 'q': '特徴空間の次元数$d$が大きい場合はどうなりますか', 'search': [{'top1_id': '0509', 'top1_ans': '学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか'}, {'top2_id': '0710', 'top2_ans': '特徴次元が多いとどうなるのか'}, {'top3_id': '0509', 'top3_ans': '学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか'}, {'top4_id': '0204', 'top4_ans': '特徴ベクトルの次元数が増えるとどんな問題がありますか'}, {'top5_id': '0710', 'top5_ans': '特徴ベクトルの次元が増えるとどうなるのか'}]}, {'id': '0801', 'q': 'ニューラルとはどういう意味か', 'search': [{'top1_id': '0406', 'top1_ans': '尤度とはどういう意味ですか'}, {'top2_id': '0606', 'top2_ans': 'Ridgeってどういう意味ですか'}, {'top3_id': '0607', 'top3_ans': 'lassoってどういう意味ですか'}, {'top4_id': '0110', 'top4_ans': '識別と回帰の意味の違いは何ですか'}, {'top5_id': '0102', 'top5_ans': '人工知能と機械学習は同じ意味ですか'}]}, {'id': '0811', 'q': '勾配消失問題を解決する手法はあるのか', 'search': [{'top1_id': '0811', 'top1_ans': 'どのようにして勾配消失問題を解決しますか'}, {'top2_id': '0810', 'top2_ans': '勾配消失問題とは何か'}, {'top3_id': '0810', 'top3_ans': '勾配消失問題とは何ですか'}, {'top4_id': '0810', 'top4_ans': '勾配消失問題とはどのような問題ですか'}, {'top5_id': '0810', 'top5_ans': '勾配消失問題とはなんですか'}]}, {'id': '1106', 'q': 'Ward法とはなんですか', 'search': [{'top1_id': '1106', 'top1_ans': 'Ward法とは何ですか'}, {'top2_id': '1106', 'top2_ans': 'Ward法はどういうものですか'}, {'top3_id': '0208', 'top3_ans': 'k-NN法とはなんですか'}, {'top4_id': '1106', 'top4_ans': '重心法とはなんですか'}, {'top5_id': '1106', 'top5_ans': '単連結法とはなんですか'}]}, {'id': '0701', 'q': '超平面とは何ですか', 'search': [{'top1_id': '0701', 'top1_ans': '学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか'}, {'top2_id': '0510', 'top2_ans': '識別面とはなんですか'}, {'top3_id': '0510', 'top3_ans': '識別面ってなんですか'}, {'top4_id': '0502', 'top4_ans': '線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに'}, {'top5_id': '0701', 'top5_ans': 'サポートベクトルマシンとは何ですか'}]}, {'id': '0610', 'q': 'トレードオフってなんですか', 'search': [{'top1_id': '0114', 'top1_ans': 'クラスタリングってなんですか'}, {'top2_id': '0505', 'top2_ans': 'パーセプトロンってなんですか'}, {'top3_id': '0602', 'top3_ans': 'ターゲットってなんですか'}, {'top4_id': '0412', 'top4_ans': 'ベイジアンネットワークってなんですか'}, {'top5_id': '0502', 'top5_ans': 'ニューラルネットワークってなんですか'}]}, {'id': '1409', 'q': '自己学習の問題点は何ですか', 'search': [{'top1_id': '1409', 'top1_ans': '自己学習の問題点は何がありますか'}, {'top2_id': '1407', 'top2_ans': '自己学習とは何ですか'}, {'top3_id': '1407', 'top3_ans': '自己学習の狙いは何ですか'}, {'top4_id': '1407', 'top4_ans': '自己学習とはなんですか'}, {'top5_id': '0906', 'top5_ans': '階層が多いニューラルネットワークの学習の問題点は？'}]}, {'id': '1205', 'q': 'a priori な原理の対偶とは何ですか', 'search': [{'top1_id': '1205', 'top1_ans': 'a prioriな原理とは何ですか'}, {'top2_id': '1110', 'top2_ans': 'モデルの対数尤度とは何ですか'}, {'top3_id': '0810', 'top3_ans': 'ディープニューラルネットワークの性能が向上しない原因はなんですか'}, {'top4_id': '0507', 'top4_ans': 'パーセプトロンの収束定理ってなんですか'}, {'top5_id': '0407', 'top5_ans': '何故尤度の対数をとって計算するんですか'}]}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "pickle_dump(tutoring_search_top_5, './tutoring_search2_top_5.pickle')\n",
    "print(pickle_load('./tutoring_search2_top_5.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#\n",
    "#                       パッセージ直接検索\n",
    "#\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b6122\\Anaconda3\\envs\\ginza\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 index                                                       0\n",
      "id_title                                                 0101\n",
      "context     我々人間は，日々五感を通して得られる情報から，対象を分類する能力や，事象の背後にある規則性を...\n",
      "Name: 0, dtype: object\n",
      "1 index                                                       1\n",
      "id_title                                                 0102\n",
      "context     近年，日常生活やビジネスにおけるさまざまな場面で人工知能 (aritificial inte...\n",
      "Name: 1, dtype: object\n",
      "2 index                                                       2\n",
      "id_title                                                 0103\n",
      "context     機械学習の出番は，簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用...\n",
      "Name: 2, dtype: object\n",
      "3 index                                                       3\n",
      "id_title                                                 0104\n",
      "context     データから規則や知見を得る機械学習技術のなかでも，特に深層学習 (deep learning...\n",
      "Name: 3, dtype: object\n",
      "4 index                                                       4\n",
      "id_title                                                 0105\n",
      "context     ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を...\n",
      "Name: 4, dtype: object\n",
      "5 index                                                       8\n",
      "id_title                                                 0109\n",
      "context     ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざ...\n",
      "Name: 5, dtype: object\n",
      "6 index                                                       9\n",
      "id_title                                                 0110\n",
      "context     教師あり学習では，正解の付いた学習データを用います．このデータを訓練例とよぶこともあります．...\n",
      "Name: 6, dtype: object\n",
      "7 index                                                      10\n",
      "id_title                                                 0111\n",
      "context     識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字...\n",
      "Name: 7, dtype: object\n",
      "8 index                                                      11\n",
      "id_title                                                 0112\n",
      "context     回帰は入力から予測される妥当な出力値を求める問題です．典型的な回帰問題には，消費電力の予測，...\n",
      "Name: 8, dtype: object\n",
      "9 index                                                      12\n",
      "id_title                                                 0113\n",
      "context     教師なし学習では，学習に用いられるデータに正解情報が付いていません．入力ベクトル$\\bm{x...\n",
      "Name: 9, dtype: object\n",
      "10 index                                                      13\n",
      "id_title                                                 0114\n",
      "context     モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさ...\n",
      "Name: 10, dtype: object\n",
      "11 index                                                      14\n",
      "id_title                                                 0115\n",
      "context     パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見す...\n",
      "Name: 11, dtype: object\n",
      "12 index                                                      15\n",
      "id_title                                                 0116\n",
      "context     ここでは，これまでに説明した教師あり学習／教師なし学習に当てはまらない問題について説明します...\n",
      "Name: 12, dtype: object\n",
      "13 index                                                      16\n",
      "id_title                                                 0117\n",
      "context     これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，ある...\n",
      "Name: 13, dtype: object\n",
      "14 index                                                      17\n",
      "id_title                                                 0118\n",
      "context     問題の性質によっては，間接的に正解が与えられる場合があります．図1.12のような迷路を抜ける...\n",
      "Name: 14, dtype: object\n",
      "15 index                                                      18\n",
      "id_title                                                 0119\n",
      "context     この章では，近年注目されてきている人工知能・機械学習・深層学習の関係を説明し，その中心的な技...\n",
      "Name: 15, dtype: object\n",
      "16 index                                                      19\n",
      "id_title                                                 0201\n",
      "context     本章では，機械学習の基本的な手順を学びましょう．まず，それぞれのステップで理解しておくべき内...\n",
      "Name: 16, dtype: object\n",
      "17 index                                                      20\n",
      "id_title                                                 0202\n",
      "context     Wekaは，機械学習を含むデータマイニング一般のアルゴリズムを実装したJavaのライブラリと...\n",
      "Name: 17, dtype: object\n",
      "18 index                                                      21\n",
      "id_title                                                 0203\n",
      "context     機械学習の第一段階はデータ収集です．購買記録からのパターンマイニングなどのように使用するデー...\n",
      "Name: 18, dtype: object\n",
      "19 index                                                      22\n",
      "id_title                                                 0204\n",
      "context     ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．...\n",
      "Name: 19, dtype: object\n",
      "20 index                                                      23\n",
      "id_title                                                 0205\n",
      "context     ここでは，特徴数削減の手法として，主成分分析 (principal component an...\n",
      "Name: 20, dtype: object\n",
      "21 index                                                      24\n",
      "id_title                                                 0206\n",
      "context     それでは学習です，とゆきたいところですが，その前に学習結果の評価基準を設定します．ここで扱っ...\n",
      "Name: 21, dtype: object\n",
      "22 index                                                      26\n",
      "id_title                                                 0208\n",
      "context     さて，いよいよ学習です．ここでは学習アルゴリズムとして，入力されたデータに近い学習データを近...\n",
      "Name: 22, dtype: object\n",
      "23 index                                                      27\n",
      "id_title                                                 0209\n",
      "context     最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラ...\n",
      "Name: 23, dtype: object\n",
      "24 index                                                      28\n",
      "id_title                                                 0210\n",
      "context     精度と再現率は一般にトレードオフの関係にあり，識別器によっては，パラメータ設定でその値を調整...\n",
      "Name: 24, dtype: object\n",
      "25 index                                                      29\n",
      "id_title                                                 0211\n",
      "context     精度あるいは再現率のどちらかを重視する場合に，閾値を変えたときの精度と再現率の関係を見ること...\n",
      "Name: 25, dtype: object\n",
      "26 index                                                      30\n",
      "id_title                                                 0212\n",
      "context     プログラミング言語Pythonはオブジェクト指向スクリプト言語です．コンパイルが不要なので，...\n",
      "Name: 26, dtype: object\n",
      "27 index                                                      31\n",
      "id_title                                                 0213\n",
      "context     本節では，scikit-learnパッケージを使って機械学習の手順をコーディングします．典型...\n",
      "Name: 27, dtype: object\n",
      "28 index                                                      32\n",
      "id_title                                                 0301\n",
      "context     この章では，各次元がカテゴリカルデータである特徴ベクトルと，その正解クラスの情報からなる学習...\n",
      "Name: 28, dtype: object\n",
      "29 index                                                      33\n",
      "id_title                                                 0302\n",
      "context     第3章から第5章では，正解情報の付いた学習データを用いる教師あり学習の設定で，識別をおこなう...\n",
      "Name: 29, dtype: object\n",
      "30 index                                                      34\n",
      "id_title                                                 0303\n",
      "context     具体的に学習データを見ながら考えてゆきましょう．表3.1に示すデータは，Weka 付属の c...\n",
      "Name: 30, dtype: object\n",
      "31 index                                                      35\n",
      "id_title                                                 0304\n",
      "context     機械学習において与えられるデータは，個々の事例です．その個々の事例から，あるクラスについて共...\n",
      "Name: 31, dtype: object\n",
      "32 index                                                      36\n",
      "id_title                                                 0305\n",
      "context     概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる...\n",
      "Name: 32, dtype: object\n",
      "33 index                                                      37\n",
      "id_title                                                 0306\n",
      "context     候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いか...\n",
      "Name: 33, dtype: object\n",
      "34 index                                                      38\n",
      "id_title                                                 0307\n",
      "context     それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはあ...\n",
      "Name: 34, dtype: object\n",
      "35 index                                                      39\n",
      "id_title                                                 0308\n",
      "context     決定木の説明には，「二十の扉」という遊びがよく用いられます．「二十の扉」は，出題者が思い浮か...\n",
      "Name: 35, dtype: object\n",
      "36 index                                                    40\n",
      "id_title                                               0309\n",
      "context     このような決定木を作成するもっとも基本的な手順がID3アルゴリズム（アルゴリズム3.1）です．\n",
      "Name: 36, dtype: object\n",
      "37 index                                                      42\n",
      "id_title                                                 0311\n",
      "context     ID3アルゴリズムの中で，詳しい説明のない「特徴集合A中で最も分類能力の高い特徴」を決定する...\n",
      "Name: 37, dtype: object\n",
      "38 index                                                      43\n",
      "id_title                                                 0312\n",
      "context     情報獲得量は，ある特徴を用いた分類後のエントロピーの減少量とします．特徴$a$の可能な値$V...\n",
      "Name: 38, dtype: object\n",
      "39 index                                                      44\n",
      "id_title                                                 0313\n",
      "context     ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学...\n",
      "Name: 39, dtype: object\n",
      "40 index                                                      45\n",
      "id_title                                                 0314\n",
      "context     リーフの要素数は，学習データの量や性質によって左右され，事前に決めるのが難しいので，枝刈りの...\n",
      "Name: 40, dtype: object\n",
      "41 index                                                      46\n",
      "id_title                                                 0315\n",
      "context     ID3アルゴリズムで用いた情報獲得量は，値の種類が多い特徴ほど大きな値になる傾向があります．...\n",
      "Name: 41, dtype: object\n",
      "42 index                                                      47\n",
      "id_title                                                 0316\n",
      "context     ここでは，特徴ベクトルのなかで数値を値とする特徴がある場合の決定木学習について説明します．基...\n",
      "Name: 42, dtype: object\n",
      "43 index                                                      48\n",
      "id_title                                                 0401\n",
      "context     この章では前章に引き続き，教師あり学習における識別問題で，特徴ベクトルの要素がすべてカテゴリ...\n",
      "Name: 43, dtype: object\n",
      "44 index                                                      49\n",
      "id_title                                                 0402\n",
      "context     第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たと...\n",
      "Name: 44, dtype: object\n",
      "45 index                                                      51\n",
      "id_title                                                 0404\n",
      "context     突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nomina...\n",
      "Name: 45, dtype: object\n",
      "46 index                                                      52\n",
      "id_title                                                 0405\n",
      "context     式(4.4)右辺第1項の条件付き確率$P(\\bm{x} \\vert \\omega_i)$を{...\n",
      "Name: 46, dtype: object\n",
      "47 index                                                      53\n",
      "id_title                                                 0406\n",
      "context     しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる...\n",
      "Name: 47, dtype: object\n",
      "48 index                                                      54\n",
      "id_title                                                 0407\n",
      "context     こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています...\n",
      "Name: 48, dtype: object\n",
      "49 index                                                      55\n",
      "id_title                                                 0408\n",
      "context     ここで，特徴ベクトルが1次元で，値として2値$x \\in \\{0,1\\}$を取り，その出現が...\n",
      "Name: 49, dtype: object\n",
      "50 index                                                      56\n",
      "id_title                                                 0409\n",
      "context     ここで，対数尤度$\\mathcal{L}(D)$を最大にするパラメータ$\\hat{\\thet...\n",
      "Name: 50, dtype: object\n",
      "51 index                                                      57\n",
      "id_title                                                 0410\n",
      "context     それでは，この最尤推定法を使って，式(4.4)の尤度を具体的に求める方法をみてゆきましょう．...\n",
      "Name: 51, dtype: object\n",
      "52 index                                                      58\n",
      "id_title                                                 0411\n",
      "context     しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データ...\n",
      "Name: 52, dtype: object\n",
      "53 index                                                      59\n",
      "id_title                                                 0412\n",
      "context     ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的に...\n",
      "Name: 53, dtype: object\n",
      "54 index                                                      60\n",
      "id_title                                                 0413\n",
      "context     上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアン...\n",
      "Name: 54, dtype: object\n",
      "55 index                                                      61\n",
      "id_title                                                 0414\n",
      "context     二つめの独立性のパターンは，Tail-to-tail connectionで，二つのノードが...\n",
      "Name: 55, dtype: object\n",
      "56 index                                                      62\n",
      "id_title                                                 0415\n",
      "context     最後は，子ノードを共有するHead-to-head connectionパターンです．このパ...\n",
      "Name: 56, dtype: object\n",
      "57 index                                                      63\n",
      "id_title                                                 0416\n",
      "context     ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワー...\n",
      "Name: 57, dtype: object\n",
      "58 index                                                      64\n",
      "id_title                                                 0417\n",
      "context     最後に，ベイジアンネットワークの学習について説明します．ベイジアンネットワークにおいて学習す...\n",
      "Name: 58, dtype: object\n",
      "59 index                                                      65\n",
      "id_title                                                 0501\n",
      "context     この章では，前章で学んだ統計モデルによる識別法で，数値を要素とする特徴ベクトルを識別する問題...\n",
      "Name: 59, dtype: object\n",
      "60 index                                                      66\n",
      "id_title                                                 0502\n",
      "context     識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるク...\n",
      "Name: 60, dtype: object\n",
      "61 index                                                      67\n",
      "id_title                                                 0503\n",
      "context     第4章では，カテゴリ特徴に対する統計的識別手法を説明してきました．その基本的な考え方は，数値...\n",
      "Name: 61, dtype: object\n",
      "62 index                                                      68\n",
      "id_title                                                 0504\n",
      "context     ここで，式(4.2)に基づいて得られた事後確率の計算式を，記号を変えてもう一度見直してみます...\n",
      "Name: 62, dtype: object\n",
      "63 index                                                      69\n",
      "id_title                                                 0505\n",
      "context     $P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよ...\n",
      "Name: 63, dtype: object\n",
      "64 index                                                      70\n",
      "id_title                                                 0506\n",
      "context     まず，最も基本的な識別関数法である誤り訂正学習から説明を始めます．1943年に，McCull...\n",
      "Name: 64, dtype: object\n",
      "65 index                                                      71\n",
      "id_title                                                 0507\n",
      "context     ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\b...\n",
      "Name: 65, dtype: object\n",
      "66 index                                                      72\n",
      "id_title                                                 0508\n",
      "context     前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実の...\n",
      "Name: 66, dtype: object\n",
      "67 index                                                      73\n",
      "id_title                                                 0509\n",
      "context     求める関数を線形であると仮定すると，式(5.5)のように表現できます．この式の係数$\\bm{...\n",
      "Name: 67, dtype: object\n",
      "68 index                                                      74\n",
      "id_title                                                 0510\n",
      "context     次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．第4章で...\n",
      "Name: 68, dtype: object\n",
      "69 index                                                      75\n",
      "id_title                                                 0511\n",
      "context     ただし，このままでは，$g(\\bm{x})$は$\\bm{x}$の値次第で，極端に大きな（ある...\n",
      "Name: 69, dtype: object\n",
      "70 index                                                      76\n",
      "id_title                                                 0512\n",
      "context     ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}...\n",
      "Name: 70, dtype: object\n",
      "71 index                                                      77\n",
      "id_title                                                 0513\n",
      "context     そして，誤差$E(\\bm{w})$の勾配方向の計算は以下のようになります．$x_{ij}$は...\n",
      "Name: 71, dtype: object\n",
      "72 index                                                      78\n",
      "id_title                                                 0514\n",
      "context     ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．ま...\n",
      "Name: 72, dtype: object\n",
      "73 index                                                      79\n",
      "id_title                                                 0601\n",
      "context     本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決...\n",
      "Name: 73, dtype: object\n",
      "74 index                                                      80\n",
      "id_title                                                 0602\n",
      "context     関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの...\n",
      "Name: 74, dtype: object\n",
      "75 index                                                      81\n",
      "id_title                                                 0603\n",
      "context     まず，最も単純な，入力も出力もスカラーである場合の回帰問題（図6.2）を考えましょう．この学...\n",
      "Name: 75, dtype: object\n",
      "76 index                                                      82\n",
      "id_title                                                 0604\n",
      "context     誤差の二乗和は以下のようになります．ただし，$\\bm{X}$は1列目のすべての要素が1，2列...\n",
      "Name: 76, dtype: object\n",
      "77 index                                                      83\n",
      "id_title                                                 0605\n",
      "context     ここでは回帰モデルの評価について考えます．教師付き学習においては，未知データに対する誤差が問...\n",
      "Name: 77, dtype: object\n",
      "78 index                                                      84\n",
      "id_title                                                 0606\n",
      "context     次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するよ...\n",
      "Name: 78, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 index                                                      85\n",
      "id_title                                                 0607\n",
      "context     また，パラメータ$\\bm{w}$の絶対値を正則化項とするものをLasso回帰とよびます．一般...\n",
      "Name: 79, dtype: object\n",
      "80 index                                                      86\n",
      "id_title                                                 0608\n",
      "context     Ridge回帰とLasso回帰における正則化の振る舞いの違いを図6.4に示します．Ridge...\n",
      "Name: 80, dtype: object\n",
      "81 index                                                      87\n",
      "id_title                                                 0609\n",
      "context     前節で説明した最小二乗法は，回帰式を高次方程式に置き換えてもそのまま適用できます．一般に，特...\n",
      "Name: 81, dtype: object\n",
      "82 index                                                      88\n",
      "id_title                                                 0610\n",
      "context     ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散...\n",
      "Name: 82, dtype: object\n",
      "83 index                                                      89\n",
      "id_title                                                 0611\n",
      "context     回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから...\n",
      "Name: 83, dtype: object\n",
      "84 index                                                      90\n",
      "id_title                                                 0612\n",
      "context     CART(classification and regression tree)は，木の構造...\n",
      "Name: 84, dtype: object\n",
      "85 index                                                      91\n",
      "id_title                                                 0613\n",
      "context     このCARTを回帰問題に用いる時は，分類基準としてデータの散らばり$SS$の減り方$\\Del...\n",
      "Name: 85, dtype: object\n",
      "86 index                                                      92\n",
      "id_title                                                 0614\n",
      "context     モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数で...\n",
      "Name: 86, dtype: object\n",
      "87 index                                                      94\n",
      "id_title                                                 0616\n",
      "context     次に考えられる手法としては，区分線形よりももっとなめらかな非線形関数を用いて回帰式を得られな...\n",
      "Name: 87, dtype: object\n",
      "88 index                                                      95\n",
      "id_title                                                 0701\n",
      "context     第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．...\n",
      "Name: 88, dtype: object\n",
      "89 index                                                      96\n",
      "id_title                                                 0702\n",
      "context     まず，学習データが線形識別可能な状況で，マージンが最大となる識別面を求める方法を考えてゆきま...\n",
      "Name: 89, dtype: object\n",
      "90 index                                                      97\n",
      "id_title                                                 0703\n",
      "context     識別面としてすべての学習データを識別できるという式(7.5)の条件を加えます．$y_i=1~...\n",
      "Name: 90, dtype: object\n",
      "91 index                                                      98\n",
      "id_title                                                 0704\n",
      "context     ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．ラ...\n",
      "Name: 91, dtype: object\n",
      "92 index                                                      99\n",
      "id_title                                                 0705\n",
      "context     最小値では$L$の勾配が0になるはずなので，以下の式が成り立ちます．これを式(7.9)に代入...\n",
      "Name: 92, dtype: object\n",
      "93 index                                                     100\n",
      "id_title                                                 0706\n",
      "context     また，$w_0$は，$\\bm{x}_{+}, \\bm{x}_{-}$をそれぞれ正例，負例に属...\n",
      "Name: 93, dtype: object\n",
      "94 index                                                     101\n",
      "id_title                                                 0707\n",
      "context     次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが...\n",
      "Name: 94, dtype: object\n",
      "95 index                                                     102\n",
      "id_title                                                 0708\n",
      "context     式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数...\n",
      "Name: 95, dtype: object\n",
      "96 index                                                     103\n",
      "id_title                                                 0709\n",
      "context     ここで$C$は制約を満たさないデータをどの程度の重みで組み込むかを決める定数で，$C$が大き...\n",
      "Name: 96, dtype: object\n",
      "97 index                                                     104\n",
      "id_title                                                 0710\n",
      "context     一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分...\n",
      "Name: 97, dtype: object\n",
      "98 index                                                     105\n",
      "id_title                                                 0711\n",
      "context     ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるあ...\n",
      "Name: 98, dtype: object\n",
      "99 index                                                     106\n",
      "id_title                                                 0712\n",
      "context     さて，カーネル関数が正定値関数という条件を満たすときには，このような非線形変換$\\phi$が...\n",
      "Name: 99, dtype: object\n",
      "100 index                                                     107\n",
      "id_title                                                 0713\n",
      "context     ここで，簡単なカーネルについてその非線形変換$\\phi$を求めてみましょう．特徴ベクトルを2...\n",
      "Name: 100, dtype: object\n",
      "101 index                                                     109\n",
      "id_title                                                 0715\n",
      "context     そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用す...\n",
      "Name: 101, dtype: object\n",
      "102 index                                                     110\n",
      "id_title                                                 0716\n",
      "context     ここまで，数値特徴を対象にSVMを説明してきましたが，SVMは一見カテゴリ特徴の問題に見える...\n",
      "Name: 102, dtype: object\n",
      "103 index                                                     111\n",
      "id_title                                                 0717\n",
      "context     Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせにつ...\n",
      "Name: 103, dtype: object\n",
      "104 index                                                     112\n",
      "id_title                                                 0801\n",
      "context     この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経...\n",
      "Name: 104, dtype: object\n",
      "105 index                                                     113\n",
      "id_title                                                 0802\n",
      "context     前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を...\n",
      "Name: 105, dtype: object\n",
      "106 index                                                     114\n",
      "id_title                                                 0803\n",
      "context     前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非...\n",
      "Name: 106, dtype: object\n",
      "107 index                                                     116\n",
      "id_title                                                 0805\n",
      "context     ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました...\n",
      "Name: 107, dtype: object\n",
      "108 index                                                     117\n",
      "id_title                                                 0806\n",
      "context     ニューラルネットワークでも，出力と正解情報の二乗誤差$(y_i - o_i)^2$が最小にな...\n",
      "Name: 108, dtype: object\n",
      "109 index                                         118\n",
      "id_title                                     0807\n",
      "context     誤差の二乗和$E(\\bm{w})$の勾配方向の計算は以下のようになります．\n",
      "Name: 109, dtype: object\n",
      "110 index                                                     119\n",
      "id_title                                                 0808\n",
      "context     ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシ...\n",
      "Name: 110, dtype: object\n",
      "111 index                                                     121\n",
      "id_title                                                 0810\n",
      "context     人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもって...\n",
      "Name: 111, dtype: object\n",
      "112 index                                                     122\n",
      "id_title                                                 0811\n",
      "context     勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイ...\n",
      "Name: 112, dtype: object\n",
      "113 index                                                     123\n",
      "id_title                                                 0901\n",
      "context     深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの...\n",
      "Name: 113, dtype: object\n",
      "114 index                                                     124\n",
      "id_title                                                 0902\n",
      "context     これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声...\n",
      "Name: 114, dtype: object\n",
      "115 index                                                     125\n",
      "id_title                                                 0903\n",
      "context     深層学習に用いられるニューラルネットワークは，問題に応じてさまざまな形に特化してゆきました．...\n",
      "Name: 115, dtype: object\n",
      "116 index                                                     128\n",
      "id_title                                                 0906\n",
      "context     多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理...\n",
      "Name: 116, dtype: object\n",
      "117 index                                                     129\n",
      "id_title                                                 0907\n",
      "context     この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆...\n",
      "Name: 117, dtype: object\n",
      "118 index                                                     130\n",
      "id_title                                                 0908\n",
      "context     深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノード...\n",
      "Name: 118, dtype: object\n",
      "119 index                                                     133\n",
      "id_title                                                 0911\n",
      "context     ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより...\n",
      "Name: 119, dtype: object\n",
      "120 index                                                     134\n",
      "id_title                                                 0912\n",
      "context     タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み...\n",
      "Name: 120, dtype: object\n",
      "121 index                                                     136\n",
      "id_title                                                 0914\n",
      "context     畳み込み層の処理は，画像にフィルタをかける処理に相当します（図9.9）．最初の畳み込み層は入...\n",
      "Name: 121, dtype: object\n",
      "122 index                                                     138\n",
      "id_title                                                 0916\n",
      "context     もうひとつのタスクに特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで...\n",
      "Name: 122, dtype: object\n",
      "123 index                                                     139\n",
      "id_title                                                 0917\n",
      "context     そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしま...\n",
      "Name: 123, dtype: object\n",
      "124 index                                                     141\n",
      "id_title                                                 0919\n",
      "context     これらのゲートの開閉は入力情報を元に判定され，現在の入力が自分に関係があるものか，自分は出力...\n",
      "Name: 124, dtype: object\n",
      "125 index                                                     142\n",
      "id_title                                                 1001\n",
      "context     アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器より...\n",
      "Name: 125, dtype: object\n",
      "126 index                                                     143\n",
      "id_title                                                 1002\n",
      "context     具体的な数値で考えるために，識別器の個数$L=11$，誤り率$\\epsilon=0.2$とし...\n",
      "Name: 126, dtype: object\n",
      "127 index                                                     144\n",
      "id_title                                                 1003\n",
      "context     ただし，これは理論上のことで，現実にはこんなにはうまくゆきません．どこが一番現実と合っていな...\n",
      "Name: 127, dtype: object\n",
      "128 index                                                     145\n",
      "id_title                                                 1004\n",
      "context     異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意...\n",
      "Name: 128, dtype: object\n",
      "129 index                                                     147\n",
      "id_title                                                 1006\n",
      "context     復元抽出によって作成されたデータ集合が，もとのデータ集合とどれぐらい異なるのかを試算してみま...\n",
      "Name: 129, dtype: object\n",
      "130 index                                                     148\n",
      "id_title                                                 1007\n",
      "context     バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，復元抽出...\n",
      "Name: 130, dtype: object\n",
      "131 index                                                     149\n",
      "id_title                                                 1008\n",
      "context     ランダムフォレストの学習手順は，基本的にバギングアルゴリズムと同様，学習データから復元抽出し...\n",
      "Name: 131, dtype: object\n",
      "132 index                                                     150\n",
      "id_title                                                 1009\n",
      "context     通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸...\n",
      "Name: 132, dtype: object\n",
      "133 index                                                     151\n",
      "id_title                                                 1010\n",
      "context     バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたり...\n",
      "Name: 133, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 index                                                     153\n",
      "id_title                                                 1012\n",
      "context     作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定し...\n",
      "Name: 134, dtype: object\n",
      "135 index                                                     154\n",
      "id_title                                                 1013\n",
      "context     このようなブースティングアルゴリズムの代表的なものにAdaBoostがあります．AdaBoo...\n",
      "Name: 135, dtype: object\n",
      "136 index                                                     155\n",
      "id_title                                                 1014\n",
      "context     前章で，Adaboostは，特定の損失関数を最小化していることと同じであることを説明しました...\n",
      "Name: 136, dtype: object\n",
      "137 index                                                     156\n",
      "id_title                                                 1015\n",
      "context     式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10....\n",
      "Name: 137, dtype: object\n",
      "138 index                                                     157\n",
      "id_title                                                 1101\n",
      "context     ここからは，教師なし学習の問題に取り組みます．この章では，特徴ベクトルの要素が数値である場合...\n",
      "Name: 138, dtype: object\n",
      "139 index                                                     158\n",
      "id_title                                                 1102\n",
      "context     教師なし学習とは，正解情報が付けられていないデータを対象に行う学習です．データの集合を，以下...\n",
      "Name: 139, dtype: object\n",
      "140 index                                                     159\n",
      "id_title                                                 1103\n",
      "context     与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」とい...\n",
      "Name: 140, dtype: object\n",
      "141 index                                                     160\n",
      "id_title                                                 1104\n",
      "context     階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作...\n",
      "Name: 141, dtype: object\n",
      "142 index                                                     161\n",
      "id_title                                                 1105\n",
      "context     階層的クラスタリングの手順を，アルゴリズム11.1に示します．ここで，simはクラスタ間の類...\n",
      "Name: 142, dtype: object\n",
      "143 index                                                     162\n",
      "id_title                                                 1106\n",
      "context     ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するか...\n",
      "Name: 143, dtype: object\n",
      "144 index                                                     163\n",
      "id_title                                                 1107\n",
      "context     階層的クラスタリングはボトムアップ的にデータをまとめるので，全体的な視点からみると，いびつな...\n",
      "Name: 144, dtype: object\n",
      "145 index                                                     164\n",
      "id_title                                                 1108\n",
      "context     分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，...\n",
      "Name: 145, dtype: object\n",
      "146 index                                                     165\n",
      "id_title                                                 1109\n",
      "context     ここで，データ分割のよさを評価する関数を，各データと所属するクラスタの中心ベクトルとの距離の...\n",
      "Name: 146, dtype: object\n",
      "147 index                                                     166\n",
      "id_title                                                 1110\n",
      "context     k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題...\n",
      "Name: 147, dtype: object\n",
      "148 index                                                     167\n",
      "id_title                                                 1111\n",
      "context     教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\b...\n",
      "Name: 148, dtype: object\n",
      "149 index                                                     168\n",
      "id_title                                                 1112\n",
      "context     ここで紹介する局所異常因子 (LOF: Local Outlier Factor)の考え方は...\n",
      "Name: 149, dtype: object\n",
      "150 index                                                     169\n",
      "id_title                                                 1113\n",
      "context     まず，データの個数に応じて$k$を適当な値に定めます．その$k$を用いて，あるデータ$\\bm...\n",
      "Name: 150, dtype: object\n",
      "151 index                                                     170\n",
      "id_title                                                 1114\n",
      "context     ここでは，ここまでに説明してきたクラスタリングの結果を用いて，新たなデータが観測されたときに...\n",
      "Name: 151, dtype: object\n",
      "152 index                                                     171\n",
      "id_title                                                 1115\n",
      "context     このような設定で，k-meansアルゴリズを一般化すると，以下のように考えることができます．...\n",
      "Name: 152, dtype: object\n",
      "153 index                                                     172\n",
      "id_title                                                 1116\n",
      "context     このように，ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデー...\n",
      "Name: 153, dtype: object\n",
      "154 index                                                     173\n",
      "id_title                                                 1201\n",
      "context     パターンマイニングはデータマイニングともよばれ，ビッグデータ活用の一つとして注目を集めている...\n",
      "Name: 154, dtype: object\n",
      "155 index                                                     174\n",
      "id_title                                                 1202\n",
      "context     また，この章で扱うカテゴリ特徴は，あるユーザがある商品を「購入した」／「購入しなかった」のよ...\n",
      "Name: 155, dtype: object\n",
      "156 index                                                     175\n",
      "id_title                                                 1203\n",
      "context     パターンマイニングで扱うデータは，一般的には\\ruby{疎}{まばら}なデータです．典型的な...\n",
      "Name: 156, dtype: object\n",
      "157 index                                                     176\n",
      "id_title                                                 1204\n",
      "context     ここでやっていることは，項目集合を作って数えれば終わりなので，あまり難しいことをしているよう...\n",
      "Name: 157, dtype: object\n",
      "158 index                                                     177\n",
      "id_title                                                 1205\n",
      "context     ここでもう一度，表12.1の小規模データに戻って，項目集合を絞り込む方法を考えましょう．今度...\n",
      "Name: 158, dtype: object\n",
      "159 index                                                     178\n",
      "id_title                                                 1206\n",
      "context     これは，命題論理でいうところの \"AならばB\" という形をしています．命題論理では，\"Aなら...\n",
      "Name: 159, dtype: object\n",
      "160 index                                                     179\n",
      "id_title                                                 1207\n",
      "context     図で表すと，図12.3のようになります．この a priori な原理の対偶を用いて，小さな...\n",
      "Name: 160, dtype: object\n",
      "161 index                                                     180\n",
      "id_title                                                 1208\n",
      "context     アルゴリズム12.1 に，Aprioriアルゴリズムの手順を示します．ここで，$F_k$は要...\n",
      "Name: 161, dtype: object\n",
      "162 index                                                     181\n",
      "id_title                                                 1209\n",
      "context     この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の...\n",
      "Name: 162, dtype: object\n",
      "163 index                                                     183\n",
      "id_title                                                 1211\n",
      "context     このような規則を作るために，まずAprioriアルゴリズムで頻出項目を抽出します．次に，その...\n",
      "Name: 163, dtype: object\n",
      "164 index                                                     184\n",
      "id_title                                                 1212\n",
      "context     ここでも a priori原理に基づき，評価値の高い規則を絞り込むことを試みます．以下，評価...\n",
      "Name: 164, dtype: object\n",
      "165 index                                                     186\n",
      "id_title                                                 1214\n",
      "context     前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げら...\n",
      "Name: 165, dtype: object\n",
      "166 index                                                     187\n",
      "id_title                                                 1215\n",
      "context     そして，このデータからFP-木 (Frequent Pattern Tree) を作成します...\n",
      "Name: 166, dtype: object\n",
      "167 index                                                     188\n",
      "id_title                                                 1216\n",
      "context     パターンマイニングは作成したFP-木を対象に行います（図12.8下）．たとえば，頻出項目抽出...\n",
      "Name: 167, dtype: object\n",
      "168 index                                                     189\n",
      "id_title                                                 1217\n",
      "context     前節までで扱ったトランザクションデータは，それぞれのデータが1件分の売り上げを表していました...\n",
      "Name: 168, dtype: object\n",
      "169 index                                                     191\n",
      "id_title                                                 1219\n",
      "context     協調フィルタリングの前提は，どの個人がどの商品を購入したかが記録されているデータがあることで...\n",
      "Name: 169, dtype: object\n",
      "170 index                                                     192\n",
      "id_title                                                 1220\n",
      "context     Matrix Factorizationは，まばらなデータを低次元行列の積に分解する方法の一...\n",
      "Name: 170, dtype: object\n",
      "171 index                                                     193\n",
      "id_title                                                 1301\n",
      "context     この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．...\n",
      "Name: 171, dtype: object\n",
      "172 index                                                     194\n",
      "id_title                                                 1302\n",
      "context     最初の問題設定として，ラベル特徴の系列を入力として，それと同じ長さのラベル系列を出力する識別...\n",
      "Name: 172, dtype: object\n",
      "173 index                                                     195\n",
      "id_title                                                 1303\n",
      "context     前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付け...\n",
      "Name: 173, dtype: object\n",
      "174 index                                                     196\n",
      "id_title                                                 1304\n",
      "context     そこで，利用できる素性を図13.3に示す組合せに限定します．つまり，出力系列で参照できる情報...\n",
      "Name: 174, dtype: object\n",
      "175 index                                                     197\n",
      "id_title                                                 1305\n",
      "context     前後の入力や一つ前の出力など，役に立ちそうな特徴を利用し，かつ系列としての確からしさを評価し...\n",
      "Name: 175, dtype: object\n",
      "176 index                                                     198\n",
      "id_title                                                 1306\n",
      "context     そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右...\n",
      "Name: 176, dtype: object\n",
      "177 index                                                     201\n",
      "id_title                                                 1309\n",
      "context     次に，「入力の系列長に関わらず出力の系列長が1である問題」を扱います．出力が一つになったので...\n",
      "Name: 177, dtype: object\n",
      "178 index                                                     202\n",
      "id_title                                                 1310\n",
      "context     この与えられた系列を$\\bm{x}$として，クラス$y$（ただし，$y= B (初心者) o...\n",
      "Name: 178, dtype: object\n",
      "179 index                                                     203\n",
      "id_title                                                 1311\n",
      "context     形式的に定義すると，HMMは以下の要素と確率で定義されます．\\begin{itemize}\\...\n",
      "Name: 179, dtype: object\n",
      "180 index                                                     205\n",
      "id_title                                                 1313\n",
      "context     この連結されたHMMを用いて，入力系列に対して最も確率が高くなる遷移系列をビタビアルゴリズム...\n",
      "Name: 180, dtype: object\n",
      "181 index                                                     206\n",
      "id_title                                                 1401\n",
      "context     第14章と第15章は，教師あり学習と教師なし学習のどちらでもない学習手法について説明します（...\n",
      "Name: 181, dtype: object\n",
      "182 index                                                     207\n",
      "id_title                                                 1402\n",
      "context     上で定義したように，本章で扱う半教師あり学習は教師あり／なしの混在型データに対する識別学習で...\n",
      "Name: 182, dtype: object\n",
      "183 index                                                     208\n",
      "id_title                                                 1403\n",
      "context     このようなことを考慮すると，入力が数値のベクトルである場合，半教師あり学習が可能なデータは以...\n",
      "Name: 183, dtype: object\n",
      "184 index                                                     209\n",
      "id_title                                                 1404\n",
      "context     カテゴリ特徴の場合は，数値特徴の場合のような一般化は難しいのですが，カテゴリ特徴で大量に学習...\n",
      "Name: 184, dtype: object\n",
      "185 index                                                     210\n",
      "id_title                                                 1405\n",
      "context     しかし，通常はそう簡単にはゆきません．図14.3の例で示したような商品の評価を行う文書にして...\n",
      "Name: 185, dtype: object\n",
      "186 index                                                     211\n",
      "id_title                                                 1406\n",
      "context     ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成し...\n",
      "Name: 186, dtype: object\n",
      "187 index                                                     212\n",
      "id_title                                                 1407\n",
      "context     自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデ...\n",
      "Name: 187, dtype: object\n",
      "188 index                                                     213\n",
      "id_title                                                 1408\n",
      "context     自己学習は図14.2左の図のような，半教師あり学習に適したデータの場合はよいのですが，低密度...\n",
      "Name: 188, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 index                                                     214\n",
      "id_title                                                 1409\n",
      "context     自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そ...\n",
      "Name: 189, dtype: object\n",
      "190 index                                                     215\n",
      "id_title                                                 1410\n",
      "context     共訓練の特徴は，学習初期の誤りに強いということが挙げられます．欠点としては，それぞれが識別器...\n",
      "Name: 190, dtype: object\n",
      "191 index                                                     216\n",
      "id_title                                                 1411\n",
      "context     ラベル特徴の教師あり／なしの混合データに対する半教師あり学習のように，特徴的なラベルが同じク...\n",
      "Name: 191, dtype: object\n",
      "192 index                                                     217\n",
      "id_title                                                 1412\n",
      "context     ラベル伝搬法の考え方は，特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築...\n",
      "Name: 192, dtype: object\n",
      "193 index                                                     218\n",
      "id_title                                                 1413\n",
      "context     最小化の手順は，以下の通りです．\\begin{enumerate}\\item データ間の類似...\n",
      "Name: 193, dtype: object\n",
      "194 index                                                     219\n",
      "id_title                                                 1414\n",
      "context     \\item ラベル付きノードからラベルなしノードにラベルを伝播させる操作を繰り返し，隣接する...\n",
      "Name: 194, dtype: object\n",
      "195 index                                                     220\n",
      "id_title                                                 1501\n",
      "context     この章では，強化学習について説明します．強化学習は，教師信号ではないがそれに準ずる情報が，一...\n",
      "Name: 195, dtype: object\n",
      "196 index                                                     221\n",
      "id_title                                                 1502\n",
      "context     強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」...\n",
      "Name: 196, dtype: object\n",
      "197 index                                                     222\n",
      "id_title                                                 1503\n",
      "context     このような設定で，最も単純な例から始めましょう．対象とするものはK-armed bandit...\n",
      "Name: 197, dtype: object\n",
      "198 index                                                     224\n",
      "id_title                                                 1505\n",
      "context     次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが移動するとい...\n",
      "Name: 198, dtype: object\n",
      "199 index                                                     225\n",
      "id_title                                                 1506\n",
      "context     マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得して...\n",
      "Name: 199, dtype: object\n",
      "200 index                                                     226\n",
      "id_title                                                 1507\n",
      "context     最適政策$\\pi^*$に従ったときの累積報酬の期待値$V^{\\pi^*} (s_t)$は，見...\n",
      "Name: 200, dtype: object\n",
      "201 index                                                     227\n",
      "id_title                                                 1508\n",
      "context     式(15.4)は，無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの...\n",
      "Name: 201, dtype: object\n",
      "202 index                                                     228\n",
      "id_title                                                 1509\n",
      "context     Q値を推定する方法は，モデルの関する知識の前提によって大きく2つに分類されます．環境をモデル...\n",
      "Name: 202, dtype: object\n",
      "203 index                                                     229\n",
      "id_title                                                 1510\n",
      "context     環境モデルが未知の場合，TD(Temporal Difference)学習と呼ばれる方法を使...\n",
      "Name: 203, dtype: object\n",
      "204 index                                                     230\n",
      "id_title                                                 1511\n",
      "context     まず，報酬と遷移は，未知ではあるが決定的に定まる，という状況でのTD学習を考えます．このよう...\n",
      "Name: 204, dtype: object\n",
      "205 index                                     231\n",
      "id_title                                 1512\n",
      "context     このベルマン方程式を用いて，以下のアルゴリズムでQ値が求まります．\n",
      "Name: 205, dtype: object\n",
      "206 index                                                     232\n",
      "id_title                                                 1513\n",
      "context     次に報酬と遷移が非決定的なTD学習を考えます．この場合，報酬$r$が確率的であるので，決定性...\n",
      "Name: 206, dtype: object\n",
      "207 index                                                     233\n",
      "id_title                                                 1514\n",
      "context     MDP設定下での強化学習では，エージェントは行為後の次状態を環境から受け取るという仮定を置い...\n",
      "Name: 207, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_context = pd.read_csv('context.csv',dtype=str)\n",
    "# anyでNaNが一つでもある行を削除する\n",
    "df_context = df_context.dropna(how='any')\n",
    "# id が飛び番号になるので振り直しする\n",
    "df_context = df_context.reset_index()\n",
    "\n",
    "\n",
    "df_context_copy = df_context.copy()\n",
    "df_context_copy\n",
    "\n",
    "import re\n",
    "\n",
    "# %表記を消して、改行も消して文字を詰める\n",
    "\n",
    "for i, col in enumerate(df_context['context']):\n",
    "#     print(i)\n",
    "#     print(\"------------------------------------------------------------------\")\n",
    "#     print(col)\n",
    "    col_tmp = re.sub(r'\\\\%', \"-TMP-\", col) \n",
    "    col_tmp = re.sub(r'%.*\\n', \"\", col_tmp) \n",
    "    # col_tmp = re.sub(r' ', \"\", col_tmp) \n",
    "    col_tmp = re.sub(r'-TMP-', \"\\%\", col_tmp) \n",
    "#     print(\"---------------------------変換後-----------------------------------\")\n",
    "    col_tmp = re.sub(r'\\n', \"\", col_tmp)\n",
    "#     print(col_tmp)\n",
    "    df_context_copy['context'][i] = col_tmp\n",
    "    \n",
    "    \n",
    "for i, item in df_context_copy.iterrows():\n",
    "    print(i, item)\n",
    "    es.index(index=\"context\", \n",
    "             doc_type=\"context\", \n",
    "             body={ \n",
    "                 \"context\": item['context'],\n",
    "                 \"id\": item['id_title']\n",
    "             })\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------devの質問---------------\n",
      "0302 クラスって何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1302 系列ラベリングでの問題は何がありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0710 二次元から三次元の変換・写像で気をつけることはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0707 線形分離可能でない場合はどうすれば良いですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1407 自己学習とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0313 過学習とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0901 深層学習とは何か\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0404 事前確率とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0810 勾配消失問題とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0117 半教師あり学習はどんなときに使われますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0307 決定木ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0417 ベイジアンネットワークにおいて学習するべき項目はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0912 畳み込みニューラルネットワークとはどんなものか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0404 入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1007 ランダムフォレストとはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0412 ベイジアンネットワークはどのような仮定を表現したものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0715 カーネルトリックとは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0510 識別面とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 階層が多いニューラルネットワークの学習の問題点は？\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1106 完全連結法とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1015 損失関数にはどのようなものがありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1214 Aprioriアルゴリズムの問題点は何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0717 パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0304 概念学習とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0614 モデル木は回帰木とどう違いますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1207 Aprioriアルゴリズムとはどのようなアルゴリズムですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0505 識別モデルとはどういうものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1220 推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1509 モデルベースの手法とはどのような場合ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1116 EM アルゴリズムとは、どのようなアルゴリズムですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1403 多様体仮定とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0808 1つ目の式の右辺の第1項は何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1506 マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1009 通常の決定木とランダムフォレストの違いはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0504 生成モデルアプローチが有効なのはどのようなときですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1301 系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0802 中間層の別名は何か\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1209 リフト値ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0502 統計モデルによるアプローチはどのようなときに有効ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0901 深層学習とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1406 半教師ある学習の基本的な進め方はどういったものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0801 ニューラルネットワークとはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0505 パーセプトロンとは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1012 バギングでは、個々のデータに対してどのように重みを設定しますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1510 ε-greedy法とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0908 深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0502 ニューラルネットワークってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0402 事後確率が最大となるクラスを識別結果とする方法を何と言いますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0902 深層学習と他の手法との大きな違いは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0917 LSTMと通常のユニットの違いは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1215 FP 木とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0902 深層学習におけるこれまでの識別問題との差はなにか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0701 サポートベクトルマシンとは何か\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0104 深層学習が得意な問題はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0808 1つ目の式の右辺の第2項は何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0102 人工知能の定義はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0610 バイアスってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0413 ベイジアンネットワークの利点はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0406 尤度とはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0111 識別の代表的な手法には何がありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0708 スラック変数はなにをするものか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1409 共訓練ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0802 誤り訂正学習は特徴空間上では何に相当しますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0701 サポートベクトルマシンってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0711 カーネル関数とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0715 カーネル関数を定めて識別面を得る方法はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0306 概念学習が失敗するのはどういう理由ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1408 自己学習の性質にはどのようなものがありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0313 過学習とはどのような状態ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0502 数値特徴はカテゴリ特徴とどう違うんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0911 ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0701 マージンとは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0713 高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0715 カーネル関数が定まれば何が得られますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 誤差逆伝搬法は多階層構造でも利用できますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 多階層ニューラルネットワークとは何か\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0911 なぜドロップアウトによって過学習が回避できるのですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0906 特徴抽出を学習するには何が必要か\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1310 HMMとは何の略称ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0109 教師なし学習とはどういうものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0614 モデル木ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0512 ロジスティック識別器ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1207 a prioriアルゴリズムとは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0811 ReLu関数の良さは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0701 マージンの定義はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1303 系列ラベリングの典型的な問題は何かありますか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1214 FP-Growthアルゴリズムとはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0803 ノードを階層的に組むとどのような識別面ができるか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1407 自己学習の狙いは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1001 アンサンブル学習とは何ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0105 パターン認識ってなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1306 CRFは何の略ですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1104 階層的クラスタリングとはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0701 SVMの正式名はなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0115 パターンマイニングとはなんですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1111 外れ値とはどういうものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "1503 強化学習で、報酬が決定的な場合の学習はどのようなものですか\n",
      "------------trainの検索結果------------\n",
      "------------devの質問---------------\n",
      "0901 深層学習の定義はなにか\n",
      "------------trainの検索結果------------\n",
      "------結果-------\n",
      "60\n",
      "40\n",
      "精度： 0.6\n"
     ]
    }
   ],
   "source": [
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "\n",
    "search_top_5 = []  # { 'id':'0101', \n",
    "                    #    'q':'多階層ニューラルネットワークとは何か', \n",
    "                    #    'ruizido' : [\n",
    "                    #       { 'top1_id': '0102, 'top1_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #       { 'top2_id': '0102, 'top2_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #          ...\n",
    "                    #     ]\n",
    "\n",
    "with open('qa_dev1.json', 'r',encoding=\"utf-8\") as f:\n",
    "    qa_dev = json.load(f)\n",
    "    \n",
    "qa_dev = qa_dev['data']\n",
    "\n",
    "for idx in range(len(qa_dev)):\n",
    "    \n",
    "# sudachiを使うver    \n",
    "#     body = {\n",
    "#         \"query\" : {\n",
    "#             \"bool\": {\n",
    "#                 \"must\": [\n",
    "#                     {\n",
    "#                       \"query_string\": {\n",
    "#                         \"analyzer\": \"sudachi_analyzer\",\n",
    "#                         \"query\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question']\n",
    "#                       }\n",
    "#                     }\n",
    "#                 ]            \n",
    "#             }\n",
    "#         },\n",
    "#         \"highlight\": {\n",
    "#             \"fields\": {\n",
    "#                 \"itemCaption\": {}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#  sudachiを使わないver\n",
    "    body = {\n",
    "        \"query\" : {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                ]            \n",
    "            }\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"itemCaption\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    body['query']['bool']['must'].append(\n",
    "        { \n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"context\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question'] } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"------------devの質問---------------\")\n",
    "    print(qa_dev[idx]['title'], qa_dev[idx]['paragraphs'][0]['qas'][0]['question'])\n",
    "    print(\"------------trainの検索結果------------\")\n",
    "    result = es.search(index='context', body=body, size=5) # ここをかえる！！！！\n",
    "    result_num = result['hits']['total']['value']\n",
    "    get_qa_train = result['hits']['hits']\n",
    "#     print(get_qa_train[0]['_source']['id'],get_qa_train[0]['_source']['question'])\n",
    "#     print(get_qa_train[1]['_source']['id'],get_qa_train[1]['_source']['question'])\n",
    "#     print(get_qa_train[2]['_source']['id'],get_qa_train[2]['_source']['question'])\n",
    "#     print(get_qa_train)\n",
    "    \n",
    "    # ruizido_top_5に格納する辞書型を作る\n",
    "    mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_qa_train[0]['_source']['id']},\n",
    "                        { 'top2_id':  get_qa_train[1]['_source']['id']},\n",
    "                        { 'top3_id':  get_qa_train[2]['_source']['id']},\n",
    "                        { 'top4_id':  get_qa_train[3]['_source']['id']},\n",
    "                        { 'top5_id':  get_qa_train[4]['_source']['id']}\n",
    "          ]\n",
    "          }\n",
    "    search_top_5.append(mydict)\n",
    "    \n",
    "    if qa_dev[idx]['title'] == get_qa_train[0]['_source']['id']:\n",
    "        true_count += 1\n",
    "    else:\n",
    "        false_count += 1\n",
    "\n",
    "\n",
    "print(\"------結果-------\")\n",
    "print(true_count)\n",
    "print(false_count)\n",
    "print(\"精度：\", true_count / (true_count + false_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '0302', 'q': 'クラスって何ですか', 'search': [{'top1_id': '0114'}, {'top2_id': '0406'}, {'top3_id': '0404'}, {'top4_id': '0402'}, {'top5_id': '1310'}]}, {'id': '1302', 'q': '系列ラベリングでの問題は何がありますか', 'search': [{'top1_id': '1302'}, {'top2_id': '1301'}, {'top3_id': '1303'}, {'top4_id': '0916'}, {'top5_id': '1309'}]}, {'id': '0710', 'q': '二次元から三次元の変換・写像で気をつけることはなんですか', 'search': [{'top1_id': '0710'}, {'top2_id': '1403'}, {'top3_id': '0112'}, {'top4_id': '0205'}, {'top5_id': '0908'}]}, {'id': '0906', 'q': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか', 'search': [{'top1_id': '0906'}, {'top2_id': '0802'}, {'top3_id': '0803'}, {'top4_id': '0810'}, {'top5_id': '0903'}]}, {'id': '0707', 'q': '線形分離可能でない場合はどうすれば良いですか', 'search': [{'top1_id': '0710'}, {'top2_id': '0508'}, {'top3_id': '0507'}, {'top4_id': '0707'}, {'top5_id': '0706'}]}, {'id': '1407', 'q': '自己学習とは何ですか', 'search': [{'top1_id': '1407'}, {'top2_id': '1408'}, {'top3_id': '1410'}, {'top4_id': '1409'}, {'top5_id': '0908'}]}, {'id': '0313', 'q': '過学習とはなんですか', 'search': [{'top1_id': '1009'}, {'top2_id': '0313'}, {'top3_id': '0601'}, {'top4_id': '0911'}, {'top5_id': '1514'}]}, {'id': '0901', 'q': '深層学習とは何か', 'search': [{'top1_id': '0907'}, {'top2_id': '0901'}, {'top3_id': '0104'}, {'top4_id': '0902'}, {'top5_id': '0810'}]}, {'id': '0404', 'q': '事前確率とはなんですか', 'search': [{'top1_id': '0404'}, {'top2_id': '0406'}, {'top3_id': '1114'}, {'top4_id': '0402'}, {'top5_id': '0405'}]}, {'id': '0810', 'q': '勾配消失問題とは何ですか', 'search': [{'top1_id': '0811'}, {'top2_id': '0919'}, {'top3_id': '0810'}, {'top4_id': '0917'}, {'top5_id': '1015'}]}, {'id': '0117', 'q': '半教師あり学習はどんなときに使われますか', 'search': [{'top1_id': '1405'}, {'top2_id': '1406'}, {'top3_id': '1401'}, {'top4_id': '1402'}, {'top5_id': '0206'}]}, {'id': '0307', 'q': '決定木ってなんですか', 'search': [{'top1_id': '0307'}, {'top2_id': '0308'}, {'top3_id': '1008'}, {'top4_id': '0611'}, {'top5_id': '0316'}]}, {'id': '0417', 'q': 'ベイジアンネットワークにおいて学習するべき項目はなんですか', 'search': [{'top1_id': '0417'}, {'top2_id': '0412'}, {'top3_id': '1207'}, {'top4_id': '1204'}, {'top5_id': '1205'}]}, {'id': '0912', 'q': '畳み込みニューラルネットワークとはどんなものか', 'search': [{'top1_id': '0914'}, {'top2_id': '0912'}, {'top3_id': '0903'}, {'top4_id': '0906'}, {'top5_id': '0709'}]}, {'id': '0404', 'q': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか', 'search': [{'top1_id': '0404'}, {'top2_id': '0114'}, {'top3_id': '0402'}, {'top4_id': '0415'}, {'top5_id': '1514'}]}, {'id': '1007', 'q': 'ランダムフォレストとはなんですか', 'search': [{'top1_id': '1008'}, {'top2_id': '1007'}, {'top3_id': '1009'}, {'top4_id': '1010'}, {'top5_id': '0906'}]}, {'id': '0412', 'q': 'ベイジアンネットワークはどのような仮定を表現したものですか', 'search': [{'top1_id': '0412'}, {'top2_id': '0417'}, {'top3_id': '0413'}, {'top4_id': '0305'}, {'top5_id': '0313'}]}, {'id': '0715', 'q': 'カーネルトリックとは何ですか', 'search': [{'top1_id': '0715'}, {'top2_id': '0114'}, {'top3_id': '0406'}, {'top4_id': '0902'}, {'top5_id': '0101'}]}, {'id': '0510', 'q': '識別面とはなんですか', 'search': [{'top1_id': '0702'}, {'top2_id': '0510'}, {'top3_id': '0502'}, {'top4_id': '0707'}, {'top5_id': '0803'}]}, {'id': '0906', 'q': '階層が多いニューラルネットワークの学習の問題点は？', 'search': [{'top1_id': '0906'}, {'top2_id': '0810'}, {'top3_id': '0903'}, {'top4_id': '0104'}, {'top5_id': '0901'}]}, {'id': '1106', 'q': '完全連結法とは何ですか', 'search': [{'top1_id': '1106'}, {'top2_id': '0114'}, {'top3_id': '1413'}, {'top4_id': '0313'}, {'top5_id': '1313'}]}, {'id': '1015', 'q': '損失関数にはどのようなものがありますか', 'search': [{'top1_id': '1015'}, {'top2_id': '1014'}, {'top3_id': '0811'}, {'top4_id': '0205'}, {'top5_id': '0810'}]}, {'id': '1214', 'q': 'Aprioriアルゴリズムの問題点は何ですか', 'search': [{'top1_id': '1214'}, {'top2_id': '1201'}, {'top3_id': '1208'}, {'top4_id': '1207'}, {'top5_id': '0114'}]}, {'id': '0717', 'q': 'パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに', 'search': [{'top1_id': '0717'}, {'top2_id': '0208'}, {'top3_id': '1305'}, {'top4_id': '0606'}, {'top5_id': '0111'}]}, {'id': '0304', 'q': '概念学習とはなんですか', 'search': [{'top1_id': '0306'}, {'top2_id': '0305'}, {'top3_id': '0303'}, {'top4_id': '0301'}, {'top5_id': '0307'}]}, {'id': '0614', 'q': 'モデル木は回帰木とどう違いますか', 'search': [{'top1_id': '0614'}, {'top2_id': '0611'}, {'top3_id': '0112'}, {'top4_id': '1215'}, {'top5_id': '0610'}]}, {'id': '1207', 'q': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか', 'search': [{'top1_id': '1208'}, {'top2_id': '1207'}, {'top3_id': '1214'}, {'top4_id': '1211'}, {'top5_id': '1201'}]}, {'id': '0505', 'q': '識別モデルとはどういうものですか', 'search': [{'top1_id': '0505'}, {'top2_id': '0510'}, {'top3_id': '1306'}, {'top4_id': '0506'}, {'top5_id': '0502'}]}, {'id': '1220', 'q': '推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか', 'search': [{'top1_id': '1220'}, {'top2_id': '0102'}, {'top3_id': '1217'}, {'top4_id': '1201'}, {'top5_id': '0205'}]}, {'id': '1509', 'q': 'モデルベースの手法とはどのような場合ですか', 'search': [{'top1_id': '1509'}, {'top2_id': '0208'}, {'top3_id': '0602'}, {'top4_id': '0118'}, {'top5_id': '0104'}]}, {'id': '1116', 'q': 'EM アルゴリズムとは、どのようなアルゴリズムですか', 'search': [{'top1_id': '1116'}, {'top2_id': '0114'}, {'top3_id': '1311'}, {'top4_id': '1406'}, {'top5_id': '1411'}]}, {'id': '1403', 'q': '多様体仮定とは何ですか', 'search': [{'top1_id': '1403'}, {'top2_id': '0406'}, {'top3_id': '0410'}, {'top4_id': '0503'}, {'top5_id': '1502'}]}, {'id': '0906', 'q': '多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか', 'search': [{'top1_id': '0906'}, {'top2_id': '0104'}, {'top3_id': '0901'}, {'top4_id': '0914'}, {'top5_id': '0802'}]}, {'id': '0808', 'q': '1つ目の式の右辺の第1項は何ですか', 'search': [{'top1_id': '0405'}, {'top2_id': '0402'}, {'top3_id': '0702'}, {'top4_id': '0409'}, {'top5_id': '1306'}]}, {'id': '1506', 'q': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか', 'search': [{'top1_id': '1506'}, {'top2_id': '1505'}, {'top3_id': '1514'}, {'top4_id': '1502'}, {'top5_id': '1209'}]}, {'id': '1009', 'q': '通常の決定木とランダムフォレストの違いはなんですか', 'search': [{'top1_id': '1009'}, {'top2_id': '1008'}, {'top3_id': '0307'}, {'top4_id': '0917'}, {'top5_id': '0316'}]}, {'id': '0504', 'q': '生成モデルアプローチが有効なのはどのようなときですか', 'search': [{'top1_id': '0504'}, {'top2_id': '1310'}, {'top3_id': '0205'}, {'top4_id': '0406'}, {'top5_id': '1211'}]}, {'id': '1301', 'q': '系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか', 'search': [{'top1_id': '1301'}, {'top2_id': '1302'}, {'top3_id': '1309'}, {'top4_id': '1311'}, {'top5_id': '1313'}]}, {'id': '0802', 'q': '中間層の別名は何か', 'search': [{'top1_id': '0805'}, {'top2_id': '0916'}, {'top3_id': '0802'}, {'top4_id': '0917'}, {'top5_id': '0810'}]}, {'id': '1209', 'q': 'リフト値ってなんですか', 'search': [{'top1_id': '1209'}, {'top2_id': '0413'}, {'top3_id': '0906'}, {'top4_id': '0307'}, {'top5_id': '1503'}]}, {'id': '0502', 'q': '統計モデルによるアプローチはどのようなときに有効ですか', 'search': [{'top1_id': '0502'}, {'top2_id': '0505'}, {'top3_id': '0616'}, {'top4_id': '0503'}, {'top5_id': '0811'}]}, {'id': '0901', 'q': '深層学習とはなんですか', 'search': [{'top1_id': '0104'}, {'top2_id': '0810'}, {'top3_id': '0901'}, {'top4_id': '0907'}, {'top5_id': '0906'}]}, {'id': '1406', 'q': '半教師ある学習の基本的な進め方はどういったものですか', 'search': [{'top1_id': '1406'}, {'top2_id': '1402'}, {'top3_id': '0109'}, {'top4_id': '0118'}, {'top5_id': '1401'}]}, {'id': '0801', 'q': 'ニューラルネットワークとはなんですか', 'search': [{'top1_id': '0906'}, {'top2_id': '0810'}, {'top3_id': '0903'}, {'top4_id': '0912'}, {'top5_id': '0901'}]}, {'id': '0505', 'q': 'パーセプトロンとは何ですか', 'search': [{'top1_id': '0505'}, {'top2_id': '0506'}, {'top3_id': '0802'}, {'top4_id': '0507'}, {'top5_id': '0114'}]}, {'id': '1012', 'q': 'バギングでは、個々のデータに対してどのように重みを設定しますか', 'search': [{'top1_id': '1012'}, {'top2_id': '1007'}, {'top3_id': '0118'}, {'top4_id': '1001'}, {'top5_id': '0803'}]}, {'id': '1510', 'q': 'ε-greedy法とは何ですか', 'search': [{'top1_id': '1510'}, {'top2_id': '0114'}, {'top3_id': '0907'}, {'top4_id': '0514'}, {'top5_id': '0604'}]}, {'id': '0908', 'q': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに', 'search': [{'top1_id': '0908'}, {'top2_id': '0907'}, {'top3_id': '0104'}, {'top4_id': '0906'}, {'top5_id': '0205'}]}, {'id': '0502', 'q': 'ニューラルネットワークってなんですか', 'search': [{'top1_id': '0906'}, {'top2_id': '0810'}, {'top3_id': '0911'}, {'top4_id': '0903'}, {'top5_id': '0914'}]}, {'id': '0402', 'q': '事後確率が最大となるクラスを識別結果とする方法を何と言いますか', 'search': [{'top1_id': '0402'}, {'top2_id': '1114'}, {'top3_id': '0405'}, {'top4_id': '0504'}, {'top5_id': '0510'}]}, {'id': '0902', 'q': '深層学習と他の手法との大きな違いは何ですか', 'search': [{'top1_id': '0104'}, {'top2_id': '0901'}, {'top3_id': '0907'}, {'top4_id': '0810'}, {'top5_id': '1502'}]}, {'id': '0917', 'q': 'LSTMと通常のユニットの違いは何ですか', 'search': [{'top1_id': '0917'}, {'top2_id': '0914'}, {'top3_id': '0603'}, {'top4_id': '0114'}, {'top5_id': '1113'}]}, {'id': '1215', 'q': 'FP 木とは何ですか', 'search': [{'top1_id': '1215'}, {'top2_id': '1216'}, {'top3_id': '0308'}, {'top4_id': '0115'}, {'top5_id': '1214'}]}, {'id': '0902', 'q': '深層学習におけるこれまでの識別問題との差はなにか', 'search': [{'top1_id': '0810'}, {'top2_id': '0906'}, {'top3_id': '0902'}, {'top4_id': '0104'}, {'top5_id': '0907'}]}, {'id': '0701', 'q': 'サポートベクトルマシンとは何か', 'search': [{'top1_id': '0701'}, {'top2_id': '0505'}, {'top3_id': '0114'}, {'top4_id': '0406'}, {'top5_id': '0902'}]}, {'id': '0104', 'q': '深層学習が得意な問題はなんですか', 'search': [{'top1_id': '0104'}, {'top2_id': '0810'}, {'top3_id': '0907'}, {'top4_id': '0911'}, {'top5_id': '0906'}]}, {'id': '0808', 'q': '1つ目の式の右辺の第2項は何ですか', 'search': [{'top1_id': '0405'}, {'top2_id': '0402'}, {'top3_id': '0702'}, {'top4_id': '0713'}, {'top5_id': '0808'}]}, {'id': '0102', 'q': '人工知能の定義はなんですか', 'search': [{'top1_id': '0102'}, {'top2_id': '0119'}, {'top3_id': '0105'}, {'top4_id': '1113'}, {'top5_id': '0701'}]}, {'id': '0610', 'q': 'バイアスってなんですか', 'search': [{'top1_id': '0610'}, {'top2_id': '0307'}, {'top3_id': '0313'}, {'top4_id': '0306'}, {'top5_id': '0305'}]}, {'id': '0413', 'q': 'ベイジアンネットワークの利点はなんですか', 'search': [{'top1_id': '0413'}, {'top2_id': '0412'}, {'top3_id': '0811'}, {'top4_id': '1306'}, {'top5_id': '0417'}]}, {'id': '0406', 'q': '尤度とはなんですか', 'search': [{'top1_id': '0411'}, {'top2_id': '0503'}, {'top3_id': '0406'}, {'top4_id': '0410'}, {'top5_id': '1110'}]}, {'id': '0111', 'q': '識別の代表的な手法には何がありますか', 'search': [{'top1_id': '0505'}, {'top2_id': '0117'}, {'top3_id': '0115'}, {'top4_id': '0402'}, {'top5_id': '0502'}]}, {'id': '0708', 'q': 'スラック変数はなにをするものか', 'search': [{'top1_id': '0708'}, {'top2_id': '0717'}, {'top3_id': '0416'}, {'top4_id': '0412'}, {'top5_id': '0602'}]}, {'id': '1409', 'q': '共訓練ってなんですか', 'search': [{'top1_id': '1409'}, {'top2_id': '1410'}, {'top3_id': '1411'}, {'top4_id': '1309'}, {'top5_id': '0414'}]}, {'id': '0802', 'q': '誤り訂正学習は特徴空間上では何に相当しますか', 'search': [{'top1_id': '0802'}, {'top2_id': '0506'}, {'top3_id': '0710'}, {'top4_id': '0502'}, {'top5_id': '0205'}]}, {'id': '0701', 'q': 'サポートベクトルマシンってなんですか', 'search': [{'top1_id': '0701'}, {'top2_id': '0505'}, {'top3_id': '0502'}, {'top4_id': '0111'}, {'top5_id': '1003'}]}, {'id': '0711', 'q': 'カーネル関数とは何ですか', 'search': [{'top1_id': '0712'}, {'top2_id': '0711'}, {'top3_id': '0715'}, {'top4_id': '0616'}, {'top5_id': '0602'}]}, {'id': '0715', 'q': 'カーネル関数を定めて識別面を得る方法はなんですか', 'search': [{'top1_id': '0715'}, {'top2_id': '0510'}, {'top3_id': '0602'}, {'top4_id': '0616'}, {'top5_id': '0710'}]}, {'id': '0306', 'q': '概念学習が失敗するのはどういう理由ですか', 'search': [{'top1_id': '0306'}, {'top2_id': '0610'}, {'top3_id': '0305'}, {'top4_id': '0301'}, {'top5_id': '0303'}]}, {'id': '1408', 'q': '自己学習の性質にはどのようなものがありますか', 'search': [{'top1_id': '1408'}, {'top2_id': '1407'}, {'top3_id': '1410'}, {'top4_id': '1411'}, {'top5_id': '0908'}]}, {'id': '0313', 'q': '過学習とはどのような状態ですか', 'search': [{'top1_id': '1505'}, {'top2_id': '1514'}, {'top3_id': '1506'}, {'top4_id': '1502'}, {'top5_id': '1503'}]}, {'id': '0502', 'q': '数値特徴はカテゴリ特徴とどう違うんですか', 'search': [{'top1_id': '0316'}, {'top2_id': '0503'}, {'top3_id': '0203'}, {'top4_id': '0502'}, {'top5_id': '0602'}]}, {'id': '0911', 'q': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという', 'search': [{'top1_id': '0911'}, {'top2_id': '0811'}, {'top3_id': '0914'}, {'top4_id': '1008'}, {'top5_id': '0810'}]}, {'id': '0701', 'q': 'マージンとは何ですか', 'search': [{'top1_id': '0706'}, {'top2_id': '0702'}, {'top3_id': '0701'}, {'top4_id': '0703'}, {'top5_id': '0708'}]}, {'id': '0713', 'q': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか', 'search': [{'top1_id': '0710'}, {'top2_id': '0502'}, {'top3_id': '0701'}, {'top4_id': '0713'}, {'top5_id': '0715'}]}, {'id': '0715', 'q': 'カーネル関数が定まれば何が得られますか', 'search': [{'top1_id': '0715'}, {'top2_id': '0712'}, {'top3_id': '0616'}, {'top4_id': '0602'}, {'top5_id': '0711'}]}, {'id': '0906', 'q': '誤差逆伝搬法は多階層構造でも利用できますか', 'search': [{'top1_id': '0810'}, {'top2_id': '0906'}, {'top3_id': '0917'}, {'top4_id': '0805'}, {'top5_id': '0907'}]}, {'id': '0906', 'q': '多階層ニューラルネットワークとは何か', 'search': [{'top1_id': '0906'}, {'top2_id': '0810'}, {'top3_id': '0903'}, {'top4_id': '0901'}, {'top5_id': '0914'}]}, {'id': '0911', 'q': 'なぜドロップアウトによって過学習が回避できるのですか', 'search': [{'top1_id': '0911'}, {'top2_id': '0417'}, {'top3_id': '0313'}, {'top4_id': '0601'}, {'top5_id': '0919'}]}, {'id': '0906', 'q': '特徴抽出を学習するには何が必要か', 'search': [{'top1_id': '0902'}, {'top2_id': '0906'}, {'top3_id': '0907'}, {'top4_id': '0203'}, {'top5_id': '0204'}]}, {'id': '1310', 'q': 'HMMとは何の略称ですか', 'search': [{'top1_id': '1510'}, {'top2_id': '1310'}, {'top3_id': '1311'}, {'top4_id': '1313'}, {'top5_id': '1405'}]}, {'id': '0109', 'q': '教師なし学習とはどういうものですか', 'search': [{'top1_id': '1402'}, {'top2_id': '1209'}, {'top3_id': '0109'}, {'top4_id': '1401'}, {'top5_id': '1406'}]}, {'id': '0614', 'q': 'モデル木ってなんですか', 'search': [{'top1_id': '0614'}, {'top2_id': '0307'}, {'top3_id': '0313'}, {'top4_id': '1215'}, {'top5_id': '0308'}]}, {'id': '0512', 'q': 'ロジスティック識別器ってなんですか', 'search': [{'top1_id': '0805'}, {'top2_id': '0803'}, {'top3_id': '1003'}, {'top4_id': '1012'}, {'top5_id': '1007'}]}, {'id': '1207', 'q': 'a prioriアルゴリズムとは何ですか', 'search': [{'top1_id': '1207'}, {'top2_id': '1205'}, {'top3_id': '1212'}, {'top4_id': '1503'}, {'top5_id': '0916'}]}, {'id': '0811', 'q': 'ReLu関数の良さは何ですか', 'search': [{'top1_id': '0811'}, {'top2_id': '0508'}, {'top3_id': '0710'}, {'top4_id': '1506'}, {'top5_id': '1502'}]}, {'id': '0701', 'q': 'マージンの定義はなんですか', 'search': [{'top1_id': '0702'}, {'top2_id': '0701'}, {'top3_id': '0706'}, {'top4_id': '0102'}, {'top5_id': '0508'}]}, {'id': '1303', 'q': '系列ラベリングの典型的な問題は何かありますか', 'search': [{'top1_id': '1301'}, {'top2_id': '1303'}, {'top3_id': '1302'}, {'top4_id': '1305'}, {'top5_id': '0716'}]}, {'id': '1214', 'q': 'FP-Growthアルゴリズムとはなんですか', 'search': [{'top1_id': '1201'}, {'top2_id': '1214'}, {'top3_id': '0115'}, {'top4_id': '1215'}, {'top5_id': '1216'}]}, {'id': '0803', 'q': 'ノードを階層的に組むとどのような識別面ができるか', 'search': [{'top1_id': '0803'}, {'top2_id': '0805'}, {'top3_id': '0802'}, {'top4_id': '0506'}, {'top5_id': '0906'}]}, {'id': '1407', 'q': '自己学習の狙いは何ですか', 'search': [{'top1_id': '1407'}, {'top2_id': '1408'}, {'top3_id': '1410'}, {'top4_id': '1409'}, {'top5_id': '0908'}]}, {'id': '1001', 'q': 'アンサンブル学習とは何ですか', 'search': [{'top1_id': '1001'}, {'top2_id': '1003'}, {'top3_id': '0406'}, {'top4_id': '0902'}, {'top5_id': '0101'}]}, {'id': '0105', 'q': 'パターン認識ってなんですか', 'search': [{'top1_id': '0501'}, {'top2_id': '1301'}, {'top3_id': '0505'}, {'top4_id': '0105'}, {'top5_id': '0213'}]}, {'id': '1306', 'q': 'CRFは何の略ですか', 'search': [{'top1_id': '1306'}, {'top2_id': '1405'}, {'top3_id': '1510'}, {'top4_id': '0114'}, {'top5_id': '0406'}]}, {'id': '1104', 'q': '階層的クラスタリングとはなんですか', 'search': [{'top1_id': '1105'}, {'top2_id': '1107'}, {'top3_id': '1104'}, {'top4_id': '0114'}, {'top5_id': '0906'}]}, {'id': '0701', 'q': 'SVMの正式名はなんですか', 'search': [{'top1_id': '0203'}, {'top2_id': '0616'}, {'top3_id': '0209'}, {'top4_id': '0716'}, {'top5_id': '0715'}]}, {'id': '0115', 'q': 'パターンマイニングとはなんですか', 'search': [{'top1_id': '1201'}, {'top2_id': '0115'}, {'top3_id': '1214'}, {'top4_id': '0113'}, {'top5_id': '1202'}]}, {'id': '1111', 'q': '外れ値とはどういうものですか', 'search': [{'top1_id': '1112'}, {'top2_id': '1111'}, {'top3_id': '1015'}, {'top4_id': '1406'}, {'top5_id': '0906'}]}, {'id': '1503', 'q': '強化学習で、報酬が決定的な場合の学習はどのようなものですか', 'search': [{'top1_id': '1502'}, {'top2_id': '0118'}, {'top3_id': '1513'}, {'top4_id': '1503'}, {'top5_id': '1505'}]}, {'id': '0901', 'q': '深層学習の定義はなにか', 'search': [{'top1_id': '0901'}, {'top2_id': '0104'}, {'top3_id': '0810'}, {'top4_id': '0907'}, {'top5_id': '0903'}]}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "pickle_dump(search_top_5, './passage_top_5.pickle')\n",
    "print(pickle_load('./passage_top_5.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#\n",
    "#                                     以上フラットな検索\n",
    "#\n",
    "#                    以下ではチュータリングシステムにおける検索精度を測る\n",
    "#\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------結果-------\n",
      "80\n",
      "20\n",
      "精度： 0.8\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "\n",
    "\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "\n",
    "\n",
    "tutoring_search_top_5 = []  # { 'id':'0101', \n",
    "                    #    'q':'多階層ニューラルネットワークとは何か', \n",
    "                    #    'ruizido' : [\n",
    "                    #       { 'top1_id': '0102, 'top1_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #       { 'top2_id': '0102, 'top2_ans': '多層パーセプトロンあるいはニューラルネットワーク'},\n",
    "                    #          ...\n",
    "                    #     ]\n",
    "\n",
    "for idx in range(len(qa_dev)):\n",
    "    \n",
    "# sudachiを使うver    \n",
    "#     body = {\n",
    "#         \"query\" : {\n",
    "#             \"bool\": {\n",
    "#                 \"must\": [\n",
    "#                     {\n",
    "#                       \"query_string\": {\n",
    "#                         \"analyzer\": \"sudachi_analyzer\",\n",
    "#                         \"query\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question']\n",
    "#                       }\n",
    "#                     }\n",
    "#                 ]            \n",
    "#             }\n",
    "#         },\n",
    "#         \"highlight\": {\n",
    "#             \"fields\": {\n",
    "#                 \"itemCaption\": {}\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#  sudachiを使わないver\n",
    "    body = {\n",
    "        \"query\" : {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                ]            \n",
    "            }\n",
    "        },\n",
    "        \"highlight\": {\n",
    "            \"fields\": {\n",
    "                \"itemCaption\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    body['query']['bool']['must'].append(\n",
    "        { \n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    { \"match\": { \"context\": qa_dev[idx]['paragraphs'][0]['qas'][0]['question'] } }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "#     print(\"------------devの質問---------------\")\n",
    "#     print(qa_dev[idx]['title'], qa_dev[idx]['paragraphs'][0]['qas'][0]['question'])\n",
    "#     print(\"------------trainの検索結果------------\")\n",
    "    result = es.search(index='context', body=body, size=50)\n",
    "    result_num = result['hits']['total']['value']\n",
    "    get_qa_train = result['hits']['hits']\n",
    "    \n",
    "    get_id_and_ans = []\n",
    "    \n",
    "    for one_get_qa_train in get_qa_train:\n",
    "        \n",
    "        if int(qa_dev[idx]['title']) >= int(one_get_qa_train['_source']['id']):\n",
    "            \n",
    "            get_id_and_ans.append([one_get_qa_train['_source']['id']])\n",
    "            \n",
    "        if len(get_id_and_ans) == 5:\n",
    "            break\n",
    "    \n",
    "\n",
    "    if len(get_id_and_ans) == 1:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  None },\n",
    "                        { 'top2_id':  None, 'top2_ans':  None },\n",
    "                        { 'top3_id':  None, 'top3_ans':  None },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 2:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans': None },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans': None },\n",
    "                        { 'top3_id':  None, 'top3_ans':  None },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 3:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  None },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  None },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans':  None },\n",
    "                        { 'top4_id':  None, 'top4_ans':  None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    if len(get_id_and_ans) == 4:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans': None },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans': None },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans': None },\n",
    "                        { 'top4_id':  get_id_and_ans[3][0], 'top4_ans': None },\n",
    "                        { 'top5_id':  None, 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "        \n",
    "    if len(get_id_and_ans) == 5:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  get_id_and_ans[0][0], 'top1_ans':  None },\n",
    "                        { 'top2_id':  get_id_and_ans[1][0], 'top2_ans':  None },\n",
    "                        { 'top3_id':  get_id_and_ans[2][0], 'top3_ans':  None },\n",
    "                        { 'top4_id':  get_id_and_ans[3][0], 'top4_ans':  None },\n",
    "                        { 'top5_id':  get_id_and_ans[4][0], 'top5_ans':  None }\n",
    "          ]\n",
    "        }\n",
    "    \n",
    "    # ruizido_top_5に格納する辞書型を作る\n",
    "#     mydict = { \n",
    "#           'id': qa_dev[idx]['title'],\n",
    "#           'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "#           'search' : [\n",
    "#                         { 'top1_id':  get_qa_train[0]['_source']['id'], 'top1_ans':  get_qa_train[0]['_source']['question'] },\n",
    "#                         { 'top2_id':  get_qa_train[1]['_source']['id'], 'top2_ans':  get_qa_train[1]['_source']['question'] },\n",
    "#                         { 'top3_id':  get_qa_train[2]['_source']['id'], 'top3_ans':  get_qa_train[2]['_source']['question'] },\n",
    "#                         { 'top4_id':  get_qa_train[3]['_source']['id'], 'top4_ans':  get_qa_train[3]['_source']['question'] },\n",
    "#                         { 'top5_id':  get_qa_train[4]['_source']['id'], 'top5_ans':  get_qa_train[4]['_source']['question'] }\n",
    "#           ]\n",
    "#           }\n",
    "\n",
    "\n",
    "    if len(get_id_and_ans) == 0:\n",
    "        mydict = { \n",
    "          'id': qa_dev[idx]['title'],\n",
    "          'q' : qa_dev[idx]['paragraphs'][0]['qas'][0]['question'],\n",
    "          'search' : [\n",
    "                        { 'top1_id':  None },\n",
    "                        { 'top2_id':  None },\n",
    "                        { 'top3_id':  None },\n",
    "                        { 'top4_id':  None },\n",
    "                        { 'top5_id':  None }\n",
    "          ]\n",
    "        }\n",
    "        \n",
    "    tutoring_search_top_5.append(mydict)\n",
    "    \n",
    "    if len(get_id_and_ans) == 0:\n",
    "        false_count += 1\n",
    "        print(qa_dev[idx]['paragraphs'][0]['qas'][0]['question'])\n",
    "        print( int(qa_dev[idx]['title']) >= int(one_get_qa_train['_source']['id']))\n",
    "        continue\n",
    "    \n",
    "    if qa_dev[idx]['title'] == get_id_and_ans[0][0]:\n",
    "        true_count += 1\n",
    "    else:\n",
    "        false_count += 1\n",
    "\n",
    "\n",
    "print(\"------結果-------\")\n",
    "print(true_count)\n",
    "print(false_count)\n",
    "print(\"精度：\", true_count / (true_count + false_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '0302', 'q': 'クラスって何ですか', 'search': [{'top1_id': '0114', 'top1_ans': None}, {'top2_id': '0117', 'top2_ans': None}, {'top3_id': '0111', 'top3_ans': None}, {'top4_id': '0211', 'top4_ans': None}, {'top5_id': '0101', 'top5_ans': None}]}, {'id': '1302', 'q': '系列ラベリングでの問題は何がありますか', 'search': [{'top1_id': '1302', 'top1_ans': None}, {'top2_id': '1301', 'top2_ans': None}, {'top3_id': '0916', 'top3_ans': None}, {'top4_id': '0906', 'top4_ans': None}, {'top5_id': '0604', 'top5_ans': None}]}, {'id': '0710', 'q': '二次元から三次元の変換・写像で気をつけることはなんですか', 'search': [{'top1_id': '0710', 'top1_ans': None}, {'top2_id': '0112', 'top2_ans': None}, {'top3_id': '0205', 'top3_ans': None}, {'top4_id': '0502', 'top4_ans': None}, {'top5_id': '0213', 'top5_ans': None}]}, {'id': '0906', 'q': '多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか', 'search': [{'top1_id': '0906', 'top1_ans': None}, {'top2_id': '0802', 'top2_ans': None}, {'top3_id': '0803', 'top3_ans': None}, {'top4_id': '0810', 'top4_ans': None}, {'top5_id': '0903', 'top5_ans': None}]}, {'id': '0707', 'q': '線形分離可能でない場合はどうすれば良いですか', 'search': [{'top1_id': '0508', 'top1_ans': None}, {'top2_id': '0507', 'top2_ans': None}, {'top3_id': '0707', 'top3_ans': None}, {'top4_id': '0706', 'top4_ans': None}, {'top5_id': '0506', 'top5_ans': None}]}, {'id': '1407', 'q': '自己学習とは何ですか', 'search': [{'top1_id': '1407', 'top1_ans': None}, {'top2_id': '0908', 'top2_ans': None}, {'top3_id': '1404', 'top3_ans': None}, {'top4_id': '1310', 'top4_ans': None}, {'top5_id': '0916', 'top5_ans': None}]}, {'id': '0313', 'q': '過学習とはなんですか', 'search': [{'top1_id': '0313', 'top1_ans': None}, {'top2_id': '0102', 'top2_ans': None}, {'top3_id': '0208', 'top3_ans': None}, {'top4_id': '0104', 'top4_ans': None}, {'top5_id': '0105', 'top5_ans': None}]}, {'id': '0901', 'q': '深層学習とは何か', 'search': [{'top1_id': '0901', 'top1_ans': None}, {'top2_id': '0104', 'top2_ans': None}, {'top3_id': '0810', 'top3_ans': None}, {'top4_id': '0119', 'top4_ans': None}, {'top5_id': '0505', 'top5_ans': None}]}, {'id': '0404', 'q': '事前確率とはなんですか', 'search': [{'top1_id': '0404', 'top1_ans': None}, {'top2_id': '0402', 'top2_ans': None}, {'top3_id': '0206', 'top3_ans': None}, {'top4_id': '0209', 'top4_ans': None}, {'top5_id': '0314', 'top5_ans': None}]}, {'id': '0810', 'q': '勾配消失問題とは何ですか', 'search': [{'top1_id': '0810', 'top1_ans': None}, {'top2_id': '0514', 'top2_ans': None}, {'top3_id': '0512', 'top3_ans': None}, {'top4_id': '0604', 'top4_ans': None}, {'top5_id': '0513', 'top5_ans': None}]}, {'id': '0117', 'q': '半教師あり学習はどんなときに使われますか', 'search': [{'top1_id': '0117', 'top1_ans': None}, {'top2_id': '0116', 'top2_ans': None}, {'top3_id': '0109', 'top3_ans': None}, {'top4_id': '0110', 'top4_ans': None}, {'top5_id': '0113', 'top5_ans': None}]}, {'id': '0307', 'q': '決定木ってなんですか', 'search': [{'top1_id': '0307', 'top1_ans': None}, {'top2_id': '0112', 'top2_ans': None}, {'top3_id': '0111', 'top3_ans': None}, {'top4_id': '0208', 'top4_ans': None}, {'top5_id': '0209', 'top5_ans': None}]}, {'id': '0417', 'q': 'ベイジアンネットワークにおいて学習するべき項目はなんですか', 'search': [{'top1_id': '0417', 'top1_ans': None}, {'top2_id': '0412', 'top2_ans': None}, {'top3_id': '0416', 'top3_ans': None}, {'top4_id': '0113', 'top4_ans': None}, {'top5_id': '0413', 'top5_ans': None}]}, {'id': '0912', 'q': '畳み込みニューラルネットワークとはどんなものか', 'search': [{'top1_id': '0912', 'top1_ans': None}, {'top2_id': '0903', 'top2_ans': None}, {'top3_id': '0906', 'top3_ans': None}, {'top4_id': '0709', 'top4_ans': None}, {'top5_id': '0810', 'top5_ans': None}]}, {'id': '0404', 'q': '入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか', 'search': [{'top1_id': '0404', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0402', 'top3_ans': None}, {'top4_id': '0109', 'top4_ans': None}, {'top5_id': '0105', 'top5_ans': None}]}, {'id': '1007', 'q': 'ランダムフォレストとはなんですか', 'search': [{'top1_id': '1007', 'top1_ans': None}, {'top2_id': '0906', 'top2_ans': None}, {'top3_id': '1003', 'top3_ans': None}, {'top4_id': '0810', 'top4_ans': None}, {'top5_id': '0307', 'top5_ans': None}]}, {'id': '0412', 'q': 'ベイジアンネットワークはどのような仮定を表現したものですか', 'search': [{'top1_id': '0412', 'top1_ans': None}, {'top2_id': '0305', 'top2_ans': None}, {'top3_id': '0313', 'top3_ans': None}, {'top4_id': '0406', 'top4_ans': None}, {'top5_id': '0307', 'top5_ans': None}]}, {'id': '0715', 'q': 'カーネルトリックとは何ですか', 'search': [{'top1_id': '0715', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0101', 'top4_ans': None}, {'top5_id': '0604', 'top5_ans': None}]}, {'id': '0510', 'q': '識別面とはなんですか', 'search': [{'top1_id': '0510', 'top1_ans': None}, {'top2_id': '0502', 'top2_ans': None}, {'top3_id': '0506', 'top3_ans': None}, {'top4_id': '0111', 'top4_ans': None}, {'top5_id': '0507', 'top5_ans': None}]}, {'id': '0906', 'q': '階層が多いニューラルネットワークの学習の問題点は？', 'search': [{'top1_id': '0906', 'top1_ans': None}, {'top2_id': '0810', 'top2_ans': None}, {'top3_id': '0903', 'top3_ans': None}, {'top4_id': '0104', 'top4_ans': None}, {'top5_id': '0901', 'top5_ans': None}]}, {'id': '1106', 'q': '完全連結法とは何ですか', 'search': [{'top1_id': '1106', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0313', 'top3_ans': None}, {'top4_id': '0402', 'top4_ans': None}, {'top5_id': '0514', 'top5_ans': None}]}, {'id': '1015', 'q': '損失関数にはどのようなものがありますか', 'search': [{'top1_id': '1015', 'top1_ans': None}, {'top2_id': '1014', 'top2_ans': None}, {'top3_id': '0811', 'top3_ans': None}, {'top4_id': '0205', 'top4_ans': None}, {'top5_id': '0810', 'top5_ans': None}]}, {'id': '1214', 'q': 'Aprioriアルゴリズムの問題点は何ですか', 'search': [{'top1_id': '1214', 'top1_ans': None}, {'top2_id': '1201', 'top2_ans': None}, {'top3_id': '1208', 'top3_ans': None}, {'top4_id': '1207', 'top4_ans': None}, {'top5_id': '0114', 'top5_ans': None}]}, {'id': '0717', 'q': 'パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに', 'search': [{'top1_id': '0717', 'top1_ans': None}, {'top2_id': '0208', 'top2_ans': None}, {'top3_id': '0606', 'top3_ans': None}, {'top4_id': '0111', 'top4_ans': None}, {'top5_id': '0506', 'top5_ans': None}]}, {'id': '0304', 'q': '概念学習とはなんですか', 'search': [{'top1_id': '0303', 'top1_ans': None}, {'top2_id': '0301', 'top2_ans': None}, {'top3_id': '0302', 'top3_ans': None}, {'top4_id': '0304', 'top4_ans': None}, {'top5_id': '0209', 'top5_ans': None}]}, {'id': '0614', 'q': 'モデル木は回帰木とどう違いますか', 'search': [{'top1_id': '0614', 'top1_ans': None}, {'top2_id': '0611', 'top2_ans': None}, {'top3_id': '0112', 'top3_ans': None}, {'top4_id': '0610', 'top4_ans': None}, {'top5_id': '0613', 'top5_ans': None}]}, {'id': '1207', 'q': 'Aprioriアルゴリズムとはどのようなアルゴリズムですか', 'search': [{'top1_id': '1207', 'top1_ans': None}, {'top2_id': '1201', 'top2_ans': None}, {'top3_id': '0115', 'top3_ans': None}, {'top4_id': '1007', 'top4_ans': None}, {'top5_id': '0306', 'top5_ans': None}]}, {'id': '0505', 'q': '識別モデルとはどういうものですか', 'search': [{'top1_id': '0505', 'top1_ans': None}, {'top2_id': '0502', 'top2_ans': None}, {'top3_id': '0105', 'top3_ans': None}, {'top4_id': '0302', 'top4_ans': None}, {'top5_id': '0503', 'top5_ans': None}]}, {'id': '1220', 'q': '推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか', 'search': [{'top1_id': '1220', 'top1_ans': None}, {'top2_id': '0102', 'top2_ans': None}, {'top3_id': '1217', 'top3_ans': None}, {'top4_id': '1201', 'top4_ans': None}, {'top5_id': '0205', 'top5_ans': None}]}, {'id': '1509', 'q': 'モデルベースの手法とはどのような場合ですか', 'search': [{'top1_id': '1509', 'top1_ans': None}, {'top2_id': '0208', 'top2_ans': None}, {'top3_id': '0602', 'top3_ans': None}, {'top4_id': '0118', 'top4_ans': None}, {'top5_id': '0104', 'top5_ans': None}]}, {'id': '1116', 'q': 'EM アルゴリズムとは、どのようなアルゴリズムですか', 'search': [{'top1_id': '1116', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '1007', 'top3_ans': None}, {'top4_id': '0306', 'top4_ans': None}, {'top5_id': '1110', 'top5_ans': None}]}, {'id': '1403', 'q': '多様体仮定とは何ですか', 'search': [{'top1_id': '1403', 'top1_ans': None}, {'top2_id': '0406', 'top2_ans': None}, {'top3_id': '0410', 'top3_ans': None}, {'top4_id': '0503', 'top4_ans': None}, {'top5_id': '0316', 'top5_ans': None}]}, {'id': '0906', 'q': '多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか', 'search': [{'top1_id': '0906', 'top1_ans': None}, {'top2_id': '0104', 'top2_ans': None}, {'top3_id': '0901', 'top3_ans': None}, {'top4_id': '0802', 'top4_ans': None}, {'top5_id': '0810', 'top5_ans': None}]}, {'id': '0808', 'q': '1つ目の式の右辺の第1項は何ですか', 'search': [{'top1_id': '0405', 'top1_ans': None}, {'top2_id': '0402', 'top2_ans': None}, {'top3_id': '0702', 'top3_ans': None}, {'top4_id': '0409', 'top4_ans': None}, {'top5_id': '0713', 'top5_ans': None}]}, {'id': '1506', 'q': 'マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか', 'search': [{'top1_id': '1506', 'top1_ans': None}, {'top2_id': '1505', 'top2_ans': None}, {'top3_id': '1502', 'top3_ans': None}, {'top4_id': '1209', 'top4_ans': None}, {'top5_id': '0605', 'top5_ans': None}]}, {'id': '1009', 'q': '通常の決定木とランダムフォレストの違いはなんですか', 'search': [{'top1_id': '1009', 'top1_ans': None}, {'top2_id': '1008', 'top2_ans': None}, {'top3_id': '0307', 'top3_ans': None}, {'top4_id': '0917', 'top4_ans': None}, {'top5_id': '0316', 'top5_ans': None}]}, {'id': '0504', 'q': '生成モデルアプローチが有効なのはどのようなときですか', 'search': [{'top1_id': '0504', 'top1_ans': None}, {'top2_id': '0205', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0314', 'top4_ans': None}, {'top5_id': '0114', 'top5_ans': None}]}, {'id': '1301', 'q': '系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか', 'search': [{'top1_id': '1301', 'top1_ans': None}, {'top2_id': '0916', 'top2_ans': None}, {'top3_id': '0906', 'top3_ans': None}, {'top4_id': '0110', 'top4_ans': None}, {'top5_id': '0604', 'top5_ans': None}]}, {'id': '0802', 'q': '中間層の別名は何か', 'search': [{'top1_id': '0802', 'top1_ans': None}, {'top2_id': '0105', 'top2_ans': None}, {'top3_id': '0117', 'top3_ans': None}, {'top4_id': '0203', 'top4_ans': None}, {'top5_id': '0109', 'top5_ans': None}]}, {'id': '1209', 'q': 'リフト値ってなんですか', 'search': [{'top1_id': '1209', 'top1_ans': None}, {'top2_id': '0413', 'top2_ans': None}, {'top3_id': '0906', 'top3_ans': None}, {'top4_id': '0307', 'top4_ans': None}, {'top5_id': '1109', 'top5_ans': None}]}, {'id': '0502', 'q': '統計モデルによるアプローチはどのようなときに有効ですか', 'search': [{'top1_id': '0502', 'top1_ans': None}, {'top2_id': '0205', 'top2_ans': None}, {'top3_id': '0302', 'top3_ans': None}, {'top4_id': '0501', 'top4_ans': None}, {'top5_id': '0402', 'top5_ans': None}]}, {'id': '0901', 'q': '深層学習とはなんですか', 'search': [{'top1_id': '0104', 'top1_ans': None}, {'top2_id': '0810', 'top2_ans': None}, {'top3_id': '0901', 'top3_ans': None}, {'top4_id': '0119', 'top4_ans': None}, {'top5_id': '0805', 'top5_ans': None}]}, {'id': '1406', 'q': '半教師ある学習の基本的な進め方はどういったものですか', 'search': [{'top1_id': '1406', 'top1_ans': None}, {'top2_id': '1402', 'top2_ans': None}, {'top3_id': '0109', 'top3_ans': None}, {'top4_id': '0118', 'top4_ans': None}, {'top5_id': '1401', 'top5_ans': None}]}, {'id': '0801', 'q': 'ニューラルネットワークとはなんですか', 'search': [{'top1_id': '0104', 'top1_ans': None}, {'top2_id': '0801', 'top2_ans': None}, {'top3_id': '0502', 'top3_ans': None}, {'top4_id': '0111', 'top4_ans': None}, {'top5_id': '0505', 'top5_ans': None}]}, {'id': '0505', 'q': 'パーセプトロンとは何ですか', 'search': [{'top1_id': '0505', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0101', 'top4_ans': None}, {'top5_id': '0308', 'top5_ans': None}]}, {'id': '1012', 'q': 'バギングでは、個々のデータに対してどのように重みを設定しますか', 'search': [{'top1_id': '1012', 'top1_ans': None}, {'top2_id': '1007', 'top2_ans': None}, {'top3_id': '0118', 'top3_ans': None}, {'top4_id': '1001', 'top4_ans': None}, {'top5_id': '0803', 'top5_ans': None}]}, {'id': '1510', 'q': 'ε-greedy法とは何ですか', 'search': [{'top1_id': '1510', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0907', 'top3_ans': None}, {'top4_id': '0514', 'top4_ans': None}, {'top5_id': '0604', 'top5_ans': None}]}, {'id': '0908', 'q': '深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに', 'search': [{'top1_id': '0908', 'top1_ans': None}, {'top2_id': '0907', 'top2_ans': None}, {'top3_id': '0104', 'top3_ans': None}, {'top4_id': '0906', 'top4_ans': None}, {'top5_id': '0205', 'top5_ans': None}]}, {'id': '0502', 'q': 'ニューラルネットワークってなんですか', 'search': [{'top1_id': '0104', 'top1_ans': None}, {'top2_id': '0502', 'top2_ans': None}, {'top3_id': '0111', 'top3_ans': None}, {'top4_id': '0413', 'top4_ans': None}, {'top5_id': '0307', 'top5_ans': None}]}, {'id': '0402', 'q': '事後確率が最大となるクラスを識別結果とする方法を何と言いますか', 'search': [{'top1_id': '0402', 'top1_ans': None}, {'top2_id': '0209', 'top2_ans': None}, {'top3_id': '0208', 'top3_ans': None}, {'top4_id': '0117', 'top4_ans': None}, {'top5_id': '0111', 'top5_ans': None}]}, {'id': '0902', 'q': '深層学習と他の手法との大きな違いは何ですか', 'search': [{'top1_id': '0104', 'top1_ans': None}, {'top2_id': '0901', 'top2_ans': None}, {'top3_id': '0810', 'top3_ans': None}, {'top4_id': '0902', 'top4_ans': None}, {'top5_id': '0505', 'top5_ans': None}]}, {'id': '0917', 'q': 'LSTMと通常のユニットの違いは何ですか', 'search': [{'top1_id': '0917', 'top1_ans': None}, {'top2_id': '0914', 'top2_ans': None}, {'top3_id': '0603', 'top3_ans': None}, {'top4_id': '0114', 'top4_ans': None}, {'top5_id': '0916', 'top5_ans': None}]}, {'id': '1215', 'q': 'FP 木とは何ですか', 'search': [{'top1_id': '1215', 'top1_ans': None}, {'top2_id': '0308', 'top2_ans': None}, {'top3_id': '0115', 'top3_ans': None}, {'top4_id': '1214', 'top4_ans': None}, {'top5_id': '0307', 'top5_ans': None}]}, {'id': '0902', 'q': '深層学習におけるこれまでの識別問題との差はなにか', 'search': [{'top1_id': '0810', 'top1_ans': None}, {'top2_id': '0902', 'top2_ans': None}, {'top3_id': '0104', 'top3_ans': None}, {'top4_id': '0805', 'top4_ans': None}, {'top5_id': '0111', 'top5_ans': None}]}, {'id': '0701', 'q': 'サポートベクトルマシンとは何か', 'search': [{'top1_id': '0701', 'top1_ans': None}, {'top2_id': '0505', 'top2_ans': None}, {'top3_id': '0114', 'top3_ans': None}, {'top4_id': '0406', 'top4_ans': None}, {'top5_id': '0101', 'top5_ans': None}]}, {'id': '0104', 'q': '深層学習が得意な問題はなんですか', 'search': [{'top1_id': '0104', 'top1_ans': None}, {'top2_id': None, 'top2_ans': None}, {'top3_id': None, 'top3_ans': None}, {'top4_id': None, 'top4_ans': None}, {'top5_id': None, 'top5_ans': None}]}, {'id': '0808', 'q': '1つ目の式の右辺の第2項は何ですか', 'search': [{'top1_id': '0405', 'top1_ans': None}, {'top2_id': '0402', 'top2_ans': None}, {'top3_id': '0702', 'top3_ans': None}, {'top4_id': '0713', 'top4_ans': None}, {'top5_id': '0808', 'top5_ans': None}]}, {'id': '0102', 'q': '人工知能の定義はなんですか', 'search': [{'top1_id': '0102', 'top1_ans': None}, {'top2_id': '0101', 'top2_ans': None}, {'top3_id': None, 'top3_ans': None}, {'top4_id': None, 'top4_ans': None}, {'top5_id': None, 'top5_ans': None}]}, {'id': '0610', 'q': 'バイアスってなんですか', 'search': [{'top1_id': '0610', 'top1_ans': None}, {'top2_id': '0307', 'top2_ans': None}, {'top3_id': '0313', 'top3_ans': None}, {'top4_id': '0306', 'top4_ans': None}, {'top5_id': '0305', 'top5_ans': None}]}, {'id': '0413', 'q': 'ベイジアンネットワークの利点はなんですか', 'search': [{'top1_id': '0413', 'top1_ans': None}, {'top2_id': '0412', 'top2_ans': None}, {'top3_id': '0102', 'top3_ans': None}, {'top4_id': '0213', 'top4_ans': None}, {'top5_id': '0211', 'top5_ans': None}]}, {'id': '0406', 'q': '尤度とはなんですか', 'search': [{'top1_id': '0406', 'top1_ans': None}, {'top2_id': '0405', 'top2_ans': None}, {'top3_id': '0210', 'top3_ans': None}, {'top4_id': '0402', 'top4_ans': None}, {'top5_id': '0114', 'top5_ans': None}]}, {'id': '0111', 'q': '識別の代表的な手法には何がありますか', 'search': [{'top1_id': '0111', 'top1_ans': None}, {'top2_id': '0102', 'top2_ans': None}, {'top3_id': '0105', 'top3_ans': None}, {'top4_id': '0109', 'top4_ans': None}, {'top5_id': None, 'top5_ans': None}]}, {'id': '0708', 'q': 'スラック変数はなにをするものか', 'search': [{'top1_id': '0708', 'top1_ans': None}, {'top2_id': '0416', 'top2_ans': None}, {'top3_id': '0412', 'top3_ans': None}, {'top4_id': '0602', 'top4_ans': None}, {'top5_id': '0606', 'top5_ans': None}]}, {'id': '1409', 'q': '共訓練ってなんですか', 'search': [{'top1_id': '1409', 'top1_ans': None}, {'top2_id': '1309', 'top2_ans': None}, {'top3_id': '0414', 'top3_ans': None}, {'top4_id': '1007', 'top4_ans': None}, {'top5_id': '0110', 'top5_ans': None}]}, {'id': '0802', 'q': '誤り訂正学習は特徴空間上では何に相当しますか', 'search': [{'top1_id': '0802', 'top1_ans': None}, {'top2_id': '0506', 'top2_ans': None}, {'top3_id': '0710', 'top3_ans': None}, {'top4_id': '0502', 'top4_ans': None}, {'top5_id': '0205', 'top5_ans': None}]}, {'id': '0701', 'q': 'サポートベクトルマシンってなんですか', 'search': [{'top1_id': '0701', 'top1_ans': None}, {'top2_id': '0505', 'top2_ans': None}, {'top3_id': '0502', 'top3_ans': None}, {'top4_id': '0111', 'top4_ans': None}, {'top5_id': '0413', 'top5_ans': None}]}, {'id': '0711', 'q': 'カーネル関数とは何ですか', 'search': [{'top1_id': '0711', 'top1_ans': None}, {'top2_id': '0616', 'top2_ans': None}, {'top3_id': '0602', 'top3_ans': None}, {'top4_id': '0114', 'top4_ans': None}, {'top5_id': '0605', 'top5_ans': None}]}, {'id': '0715', 'q': 'カーネル関数を定めて識別面を得る方法はなんですか', 'search': [{'top1_id': '0715', 'top1_ans': None}, {'top2_id': '0510', 'top2_ans': None}, {'top3_id': '0602', 'top3_ans': None}, {'top4_id': '0616', 'top4_ans': None}, {'top5_id': '0710', 'top5_ans': None}]}, {'id': '0306', 'q': '概念学習が失敗するのはどういう理由ですか', 'search': [{'top1_id': '0306', 'top1_ans': None}, {'top2_id': '0305', 'top2_ans': None}, {'top3_id': '0301', 'top3_ans': None}, {'top4_id': '0303', 'top4_ans': None}, {'top5_id': '0302', 'top5_ans': None}]}, {'id': '1408', 'q': '自己学習の性質にはどのようなものがありますか', 'search': [{'top1_id': '1408', 'top1_ans': None}, {'top2_id': '1407', 'top2_ans': None}, {'top3_id': '0908', 'top3_ans': None}, {'top4_id': '1404', 'top4_ans': None}, {'top5_id': '0606', 'top5_ans': None}]}, {'id': '0313', 'q': '過学習とはどのような状態ですか', 'search': [{'top1_id': '0118', 'top1_ans': None}, {'top2_id': '0313', 'top2_ans': None}, {'top3_id': '0102', 'top3_ans': None}, {'top4_id': '0104', 'top4_ans': None}, {'top5_id': '0209', 'top5_ans': None}]}, {'id': '0502', 'q': '数値特徴はカテゴリ特徴とどう違うんですか', 'search': [{'top1_id': '0316', 'top1_ans': None}, {'top2_id': '0203', 'top2_ans': None}, {'top3_id': '0502', 'top3_ans': None}, {'top4_id': '0501', 'top4_ans': None}, {'top5_id': '0110', 'top5_ans': None}]}, {'id': '0911', 'q': 'ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという', 'search': [{'top1_id': '0911', 'top1_ans': None}, {'top2_id': '0811', 'top2_ans': None}, {'top3_id': '0810', 'top3_ans': None}, {'top4_id': '0801', 'top4_ans': None}, {'top5_id': '0514', 'top5_ans': None}]}, {'id': '0701', 'q': 'マージンとは何ですか', 'search': [{'top1_id': '0701', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0101', 'top4_ans': None}, {'top5_id': '0604', 'top5_ans': None}]}, {'id': '0713', 'q': '高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか', 'search': [{'top1_id': '0710', 'top1_ans': None}, {'top2_id': '0502', 'top2_ans': None}, {'top3_id': '0701', 'top3_ans': None}, {'top4_id': '0713', 'top4_ans': None}, {'top5_id': '0510', 'top5_ans': None}]}, {'id': '0715', 'q': 'カーネル関数が定まれば何が得られますか', 'search': [{'top1_id': '0715', 'top1_ans': None}, {'top2_id': '0712', 'top2_ans': None}, {'top3_id': '0616', 'top3_ans': None}, {'top4_id': '0602', 'top4_ans': None}, {'top5_id': '0711', 'top5_ans': None}]}, {'id': '0906', 'q': '誤差逆伝搬法は多階層構造でも利用できますか', 'search': [{'top1_id': '0810', 'top1_ans': None}, {'top2_id': '0906', 'top2_ans': None}, {'top3_id': '0805', 'top3_ans': None}, {'top4_id': '0505', 'top4_ans': None}, {'top5_id': '0802', 'top5_ans': None}]}, {'id': '0906', 'q': '多階層ニューラルネットワークとは何か', 'search': [{'top1_id': '0906', 'top1_ans': None}, {'top2_id': '0810', 'top2_ans': None}, {'top3_id': '0903', 'top3_ans': None}, {'top4_id': '0901', 'top4_ans': None}, {'top5_id': '0802', 'top5_ans': None}]}, {'id': '0911', 'q': 'なぜドロップアウトによって過学習が回避できるのですか', 'search': [{'top1_id': '0911', 'top1_ans': None}, {'top2_id': '0417', 'top2_ans': None}, {'top3_id': '0313', 'top3_ans': None}, {'top4_id': '0601', 'top4_ans': None}, {'top5_id': '0616', 'top5_ans': None}]}, {'id': '0906', 'q': '特徴抽出を学習するには何が必要か', 'search': [{'top1_id': '0902', 'top1_ans': None}, {'top2_id': '0906', 'top2_ans': None}, {'top3_id': '0203', 'top3_ans': None}, {'top4_id': '0204', 'top4_ans': None}, {'top5_id': '0104', 'top5_ans': None}]}, {'id': '1310', 'q': 'HMMとは何の略称ですか', 'search': [{'top1_id': '1310', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0902', 'top4_ans': None}, {'top5_id': '0101', 'top5_ans': None}]}, {'id': '0109', 'q': '教師なし学習とはどういうものですか', 'search': [{'top1_id': '0109', 'top1_ans': None}, {'top2_id': '0105', 'top2_ans': None}, {'top3_id': '0104', 'top3_ans': None}, {'top4_id': None, 'top4_ans': None}, {'top5_id': None, 'top5_ans': None}]}, {'id': '0614', 'q': 'モデル木ってなんですか', 'search': [{'top1_id': '0614', 'top1_ans': None}, {'top2_id': '0307', 'top2_ans': None}, {'top3_id': '0313', 'top3_ans': None}, {'top4_id': '0308', 'top4_ans': None}, {'top5_id': '0112', 'top5_ans': None}]}, {'id': '0512', 'q': 'ロジスティック識別器ってなんですか', 'search': [{'top1_id': '0111', 'top1_ans': None}, {'top2_id': '0512', 'top2_ans': None}, {'top3_id': '0209', 'top3_ans': None}, {'top4_id': '0210', 'top4_ans': None}, {'top5_id': '0211', 'top5_ans': None}]}, {'id': '1207', 'q': 'a prioriアルゴリズムとは何ですか', 'search': [{'top1_id': '1207', 'top1_ans': None}, {'top2_id': '1205', 'top2_ans': None}, {'top3_id': '0916', 'top3_ans': None}, {'top4_id': '0312', 'top4_ans': None}, {'top5_id': '0114', 'top5_ans': None}]}, {'id': '0811', 'q': 'ReLu関数の良さは何ですか', 'search': [{'top1_id': '0811', 'top1_ans': None}, {'top2_id': '0508', 'top2_ans': None}, {'top3_id': '0710', 'top3_ans': None}, {'top4_id': '0114', 'top4_ans': None}, {'top5_id': '0404', 'top5_ans': None}]}, {'id': '0701', 'q': 'マージンの定義はなんですか', 'search': [{'top1_id': '0701', 'top1_ans': None}, {'top2_id': '0102', 'top2_ans': None}, {'top3_id': '0508', 'top3_ans': None}, {'top4_id': '0209', 'top4_ans': None}, {'top5_id': '0311', 'top5_ans': None}]}, {'id': '1303', 'q': '系列ラベリングの典型的な問題は何かありますか', 'search': [{'top1_id': '1301', 'top1_ans': None}, {'top2_id': '1303', 'top2_ans': None}, {'top3_id': '1302', 'top3_ans': None}, {'top4_id': '0716', 'top4_ans': None}, {'top5_id': '0916', 'top5_ans': None}]}, {'id': '1214', 'q': 'FP-Growthアルゴリズムとはなんですか', 'search': [{'top1_id': '1201', 'top1_ans': None}, {'top2_id': '1214', 'top2_ans': None}, {'top3_id': '0115', 'top3_ans': None}, {'top4_id': '0507', 'top4_ans': None}, {'top5_id': '1007', 'top5_ans': None}]}, {'id': '0803', 'q': 'ノードを階層的に組むとどのような識別面ができるか', 'search': [{'top1_id': '0803', 'top1_ans': None}, {'top2_id': '0802', 'top2_ans': None}, {'top3_id': '0506', 'top3_ans': None}, {'top4_id': '0104', 'top4_ans': None}, {'top5_id': '0111', 'top5_ans': None}]}, {'id': '1407', 'q': '自己学習の狙いは何ですか', 'search': [{'top1_id': '1407', 'top1_ans': None}, {'top2_id': '0908', 'top2_ans': None}, {'top3_id': '1404', 'top3_ans': None}, {'top4_id': '1310', 'top4_ans': None}, {'top5_id': '0916', 'top5_ans': None}]}, {'id': '1001', 'q': 'アンサンブル学習とは何ですか', 'search': [{'top1_id': '1001', 'top1_ans': None}, {'top2_id': '0406', 'top2_ans': None}, {'top3_id': '0902', 'top3_ans': None}, {'top4_id': '0101', 'top4_ans': None}, {'top5_id': '0907', 'top5_ans': None}]}, {'id': '0105', 'q': 'パターン認識ってなんですか', 'search': [{'top1_id': '0105', 'top1_ans': None}, {'top2_id': '0103', 'top2_ans': None}, {'top3_id': '0102', 'top3_ans': None}, {'top4_id': None, 'top4_ans': None}, {'top5_id': None, 'top5_ans': None}]}, {'id': '1306', 'q': 'CRFは何の略ですか', 'search': [{'top1_id': '1306', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0406', 'top3_ans': None}, {'top4_id': '0902', 'top4_ans': None}, {'top5_id': '0101', 'top5_ans': None}]}, {'id': '1104', 'q': '階層的クラスタリングとはなんですか', 'search': [{'top1_id': '1104', 'top1_ans': None}, {'top2_id': '0114', 'top2_ans': None}, {'top3_id': '0906', 'top3_ans': None}, {'top4_id': '1103', 'top4_ans': None}, {'top5_id': '0810', 'top5_ans': None}]}, {'id': '0701', 'q': 'SVMの正式名はなんですか', 'search': [{'top1_id': '0203', 'top1_ans': None}, {'top2_id': '0616', 'top2_ans': None}, {'top3_id': '0209', 'top3_ans': None}, {'top4_id': '0606', 'top4_ans': None}, {'top5_id': '0607', 'top5_ans': None}]}, {'id': '0115', 'q': 'パターンマイニングとはなんですか', 'search': [{'top1_id': '0115', 'top1_ans': None}, {'top2_id': '0113', 'top2_ans': None}, {'top3_id': '0109', 'top3_ans': None}, {'top4_id': '0104', 'top4_ans': None}, {'top5_id': '0111', 'top5_ans': None}]}, {'id': '1111', 'q': '外れ値とはどういうものですか', 'search': [{'top1_id': '1111', 'top1_ans': None}, {'top2_id': '1015', 'top2_ans': None}, {'top3_id': '0906', 'top3_ans': None}, {'top4_id': '0206', 'top4_ans': None}, {'top5_id': '0112', 'top5_ans': None}]}, {'id': '1503', 'q': '強化学習で、報酬が決定的な場合の学習はどのようなものですか', 'search': [{'top1_id': '1502', 'top1_ans': None}, {'top2_id': '0118', 'top2_ans': None}, {'top3_id': '1503', 'top3_ans': None}, {'top4_id': '0109', 'top4_ans': None}, {'top5_id': '1501', 'top5_ans': None}]}, {'id': '0901', 'q': '深層学習の定義はなにか', 'search': [{'top1_id': '0901', 'top1_ans': None}, {'top2_id': '0104', 'top2_ans': None}, {'top3_id': '0810', 'top3_ans': None}, {'top4_id': '0119', 'top4_ans': None}, {'top5_id': '0505', 'top5_ans': None}]}]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(obj,f)\n",
    "\n",
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "pickle_dump(tutoring_search_top_5, './tutoring_passage_top_5.pickle')\n",
    "print(pickle_load('./tutoring_passage_top_5.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
