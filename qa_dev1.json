{
    "data": [
        {
            "paragraphs": [
                {
                    "context": "第3章から第5章では，正解情報の付いた学習データを用いる教師あり学習の設定で，識別をおこなうモデルを学習する方法について説明します．まず第3章と第4章は，カテゴリカルデータからなる特徴ベクトルを入力として，それをクラス分けする（すなわち属するクラスラベルを出力する）識別器を作る方法について学びます（図3.1）．識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報$y_i$のペアからなります．ここでの設定は，特徴ベクトル$\\bm{x}_i$の各次元および正解情報$y_i$がいずれもカテゴリです．特にカテゴリ形式の正解情報のことをクラスとよびます．このカテゴリ特徴に対する「教師あり・識別」問題に対して，いかに納得のゆく概念モデルを獲得するか，という点に重点を置いたものが，この章で説明する概念学習です．一方，特徴が与えられたときに，それがあるクラスに属する確率を計算するモデルの獲得を目的とするものが，第4章で説明する統計的手法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 267,
                                    "text": "カテゴリ形式の正解情報のこと"
                                }
                            ],
                            "id": "8b8ffa89-741b-4df5-9244-3b6215996763",
                            "question": "クラスって何ですか"
                        }
                    ]
                }
            ],
            "title": "0302"
        },
        {
            "paragraphs": [
                {
                    "context": "最初の問題設定として，ラベル特徴の系列を入力として，それと同じ長さのラベル系列を出力する識別問題を扱います．では，このような系列ラベリング問題を，機械学習によって解決する識別器の構成を考えてゆきましょう．単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点があります．学習データは，入力系列と出力ラベルのペアとして与えられますが，形態素解析や固有表現抽出の例でみたように，出力系列には並びによる依存関係があるので，個々の識別問題として扱うのは不適当だということです（問題点1）．それでは出力もまとめてしまって，出力系列を一つのクラスとするということも考えられますが，通常，そのクラス数は膨大な数になってしまいます．たとえば，品詞が10種類で，20単語からなる文にラベル付けする問題では$10^{20}$種類の出力が可能になり，これらを個別のクラスとして扱うのはほとんど不可能です（問題点2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点"
                                }
                            ],
                            "id": "f01f7942-7033-4284-9683-6b2dd64c7b44",
                            "question": "系列ラベリングでの問題は何がありますか"
                        }
                    ]
                }
            ],
            "title": "1302"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 395,
                                    "text": "もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ"
                                }
                            ],
                            "id": "f0c7be8c-79c2-490c-a38a-37dabfa154f7",
                            "question": "二次元から三次元の変換・写像で気をつけることはなんですか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 191,
                                    "text": "隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので"
                                }
                            ],
                            "id": "b0d035fa-089a-415b-bbb9-857fe5c12bea",
                            "question": "多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ"
                                }
                            ],
                            "id": "a3ddd75b-f74e-4d3c-a5d1-31c38496aa03",
                            "question": "線形分離可能でない場合はどうすれば良いですか"
                        }
                    ]
                }
            ],
            "title": "0707"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです(図14.5)自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 21,
                                    "text": "最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの"
                                }
                            ],
                            "id": "d8f74f56-7279-42da-9fdd-2723f4efbbfb",
                            "question": "自己学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1407"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象"
                                }
                            ],
                            "id": "076792d3-9ef4-4263-852e-c32bf30f7168",
                            "question": "過学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "27f59888-6db8-4924-bd5d-b09508f0896c",
                            "question": "深層学習とは何か"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，事前確率 (prior probability) とよびます．クラス$\\omega_i$の事前確率は$P(\\omega_i) ~~ (i=1,\\dots,c)$ （ただし$c$はクラス数）と表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 303,
                                    "text": "入力を観測する前にもっているそれぞれのクラスの起こりやすさ"
                                }
                            ],
                            "id": "3bdcf6d0-df79-4b4a-8123-7831309f5356",
                            "question": "事前確率とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0404"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく"
                                }
                            ],
                            "id": "3e0b3cd9-0289-4a09-a576-ce64fa7768c3",
                            "question": "勾配消失問題とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，あるいはまったく与えられていないかのいずれかでした．その中間的な設定として，学習データの一部にだけ正解が与えられている場合が考えられます．学習データに正解を与えるのは人間なので，正解付きのデータを作成するにはコスト（費用・時間）がかかります．一方，正解なしのデータならば，容易にかつ大量に入手可能であるという状況があります．たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります．このような状況で，正解付きデータから得られた識別器の性能を，正解なしデータを使って向上させる問題を半教師あり学習 (semi-supervised learning) といいます．半教師あり学習は主として識別問題に対して用いられます．半教師あり学習の代表的な手法のアイディアを図1.11に示します．図1.11左のように，全データの中で正解の付加されたデータを丸・バツで表し，正解のないデータを三角形で表します．最初は丸・バツが付いたデータだけから識別器を作り，たとえば，その中間あたりに境界直線を引いたものとします．これに従って三角形のデータを分類しますが，境界線近辺のデータはあまり信用せず，境界線から大きく離れたものを確信度が高いとみなして正解を付与します．今度は，これらの新しく正解を付与されたデータも加えて，再度識別境界を計算します．これを，新しい正解付きデータが増えなくなるまで繰り返します．この学習法は，識別するべきクラスがうまくまとまっているようなデータや，識別結果によって有効な特徴が増えてゆくような，やや特殊なデータに対して適用するときにうまくゆきます．この手法を第14章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 82,
                                    "text": "学習データの一部にだけ正解が与えられている場合"
                                }
                            ],
                            "id": "2ca00f38-3bdc-45a6-a581-fa6c2e361960",
                            "question": "半教師あり学習はどんなときに使われますか"
                        }
                    ]
                }
            ],
            "title": "0117"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはありません．仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します．決定木とは，データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現です．正例のリーフに到るノードの分岐の値をAND条件で結合し，それらをさらにOR条件で結合することで等価な論理式に変換できますが，木構造のままの方が，人間の目から見て学習結果がわかりやすいので，こちらの表現が好まれます．コンタクトレンズデータ (contact-lens.arff)（表3.1）から作成した決定木の例を図3.3に示します．図3.3の木では特徴tear-prod-rate（涙量）が最初の質問で，この値がreduced（減少）であると，コンタクトレンズは勧められない，という結論になります．この値がnormal（正常）であれば，次の特徴astigmatism（乱視）を調べる，という手順になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 171,
                                    "text": "データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現"
                                }
                            ],
                            "id": "a0224946-0fda-4d00-bf05-035ec02a1175",
                            "question": "決定木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0307"
        },
        {
            "paragraphs": [
                {
                    "context": "最後に，ベイジアンネットワークの学習について説明します．ベイジアンネットワークにおいて学習するべき項目は，ネットワークの構造とアークの条件付き確率表です．まず，ネットワークの構造が得られているものとして，アークの条件付き確率表を得る方法について説明します．学習データにすべての変数の値が含まれる場合は，単純ベイズ法と同様な数え上げによって確率値を決めることができます．ここでも，ゼロ頻度問題を回避するために，データカウント数の初期値を一定値にしておくなどの工夫が必要になります．一方，学習データに値が観測されない変数がある場合は，適当な初期値を設定して，第5章で説明する最急勾配法により学習することになります．また，ベイジアンネットワークの構造の学習は，そのネットワークによって計算される式(4.7)の対数尤度が大きくなるように，アークを探索的に追加してゆく方法が考えられます．その基本的な方法がK2アルゴリズムで，概要は以下のようになります．ここで$Node$は特徴集合とクラスからなるノード全体の集合を表します．一般に，複雑なネットワークのほうが対数尤度は大きくなるので，このアルゴリズムは簡単に過学習に陥りやすいといわれています．過学習への対処法としては，親ノードの数をあらかじめ制限する方法が提案されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 53,
                                    "text": "ネットワークの構造とアークの条件付き確率"
                                }
                            ],
                            "id": "40184451-ac00-4b90-8339-d78176473bbb",
                            "question": "ベイジアンネットワークにおいて学習するべき項目はなんですか"
                        }
                    ]
                }
            ],
            "title": "0417"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです"
                                }
                            ],
                            "id": "7caac203-f96a-421d-9eb9-d20a4dc2f8f6",
                            "question": "畳み込みニューラルネットワークとはどんなものか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，事前確率 (prior probability) とよびます．クラス$\\omega_i$の事前確率は$P(\\omega_i) ~~ (i=1,\\dots,c)$ （ただし$c$はクラス数）と表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 334,
                                    "text": "事前確率"
                                }
                            ],
                            "id": "862c535b-b80e-491f-b8f9-73fd9f4f1c23",
                            "question": "入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0404"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，復元抽出によって作られた個々のデータ集合は，もとの学習データ集合と約$2/3$のデータを共有しているので，とくに，元のデータのまとまりがよい場合，それほど極端に異なった識別器にはなりません．ここで説明するランダムフォレスト(RandomForest)は，識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 169,
                                    "text": "識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法"
                                }
                            ],
                            "id": "5d51a0fe-e36d-40bb-adc3-f05420dd9fe1",
                            "question": "ランダムフォレストとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1007"
        },
        {
            "paragraphs": [
                {
                    "context": "ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けないというものです．数式で表すと，確率変数の値$\\{z_1,\\dots,z_n\\}$の結合確率は，以下のように計算されます．ただし$\\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，この計算を順に行うことで，ネットワーク全体の確率計算が行えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 103,
                                    "text": "「特徴の部分集合が，あるクラスのもとで独立である」と仮定"
                                }
                            ],
                            "id": "9b8f9931-345c-4277-9695-fcf131303e3d",
                            "question": "ベイジアンネットワークはどのような仮定を表現したものですか"
                        }
                    ]
                }
            ],
            "title": "0412"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "a15f0e57-c977-497b-9308-f5c05a46b28a",
                            "question": "カーネルトリックとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．第4章で説明したように，データから直接に頻度を数えて事後確率$P(\\omega_i|\\bm{x})$を求めることはできません．そこで，識別モデルでは，この事後確率を特徴値の組み合わせから求めるようにモデルを作ります．つまり，特徴ベクトル$\\bm{x$}が与えられたときに，その$\\bm{x}$の値を用いて，何らかの方法で，出力$y$の確率分布を計算するメカニズムをモデル化します．いま，2値分類問題における特徴ベクトル$\\bm{x}=(x_1, \\dots, x_d)^T$に対して，各特徴の重み付き和$w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d$を考え，正例に関しては正の値，負例に関しては負の値を出力するように重みを調整することを考えます．ただし，これでは原点$\\bm{x} = \\bm{0}$に対して判定ができないので，定数$w_0$をパラメータとして加え，改めて$g(\\bm{x})=w_0 + w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d = w_0 + \\bm{w} \\cdot \\bm{x}$と定義します．ここで，$g(\\bm{x})=0$とおいたものは，式の形から$d$次元空間上の平面を表しています．もし，この平面が上記のようにふるまうように調整ができたとすると，この平面上にある点は，どちらのクラスとも判別がつかず，平面の正の側（$g(\\bm{x})>0$となる側）の空間に正例，平面の負の側（$g(\\bm{x})<0$となる側）の空間に負例の空間ができるはずです．このように，特徴空間上でクラスを分割する面を識別面とよびます．また，それぞれの点の判定の確からしさは，識別面からの距離に反映されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 722,
                                    "text": "特徴空間上でクラスを分割する面"
                                }
                            ],
                            "id": "adff495f-1c9f-44ab-ac89-7b4639250030",
                            "question": "識別面とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0510"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 487,
                                    "text": "誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない"
                                }
                            ],
                            "id": "80eec3c8-c684-4a88-985e-7c3178edc2d9",
                            "question": "階層が多いニューラルネットワークの学習の問題点は？"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 170,
                                    "text": "最も遠い事例対の距離を類似度とする"
                                }
                            ],
                            "id": "8ff01e73-f72c-41b2-8b7c-97298be3bb0e",
                            "question": "完全連結法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法が，勾配ブースティングです．損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 80,
                                    "text": "損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます"
                                }
                            ],
                            "id": "14cbf303-1104-488c-aa0e-4c13a54f4fb6",
                            "question": "損失関数にはどのようなものがありますか"
                        }
                    ]
                }
            ],
            "title": "1015"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "計算量が膨大であること"
                                }
                            ],
                            "id": "916ef9bc-078e-42d1-8260-67ee67c20492",
                            "question": "Aprioriアルゴリズムの問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "Grid search"
                                }
                            ],
                            "id": "04dcd7db-e5a4-4565-be76-b356c445d66d",
                            "question": "パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "機械学習において与えられるデータは，個々の事例です．その個々の事例から，あるクラスについて共通点を見つけることが，概念学習です．共通点は，特徴の値の組み合わせによって表現されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 28,
                                    "text": "個々の事例から，あるクラスについて共通点を見つけること"
                                }
                            ],
                            "id": "0cc51a7c-b3be-4199-b73f-38b1f5a32e9d",
                            "question": "概念学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0304"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った"
                                }
                            ],
                            "id": "4f6478bb-ac7f-4ba8-a3b4-b1eea50dac79",
                            "question": "モデル木は回帰木とどう違いますか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "図で表すと，図12.3のようになります．この a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法が，Aprioriアルゴリズムです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法"
                                }
                            ],
                            "id": "04cc0f4b-75be-43a8-b2af-7e068ed29461",
                            "question": "Aprioriアルゴリズムとはどのようなアルゴリズムですか"
                        }
                    ]
                }
            ],
            "title": "1207"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチ"
                                }
                            ],
                            "id": "50f702b3-9749-4bc3-8bb0-5e64bc83eb83",
                            "question": "識別モデルとはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "Matrix Factorizationは，まばらなデータを低次元行列の積に分解する方法の一つです．一般に，行列分解にはSVD (Singular Value Decomposition) とよばれる方法がありますが，推薦システムにこの方法を適用しても，うまくいかないことが多いといわれています．その理由として，購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するからです．購入したものを1，しなかったものを0として行列で表現した購入データをそのまま推薦に利用すると，1が「好き」，0が「きらい」に対応するものとして扱ってしまいます．そのようなことを避けるために，値の入っているところのみで最適化をおこなう手法が，Matrix Factorizationです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 156,
                                    "text": "購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから"
                                }
                            ],
                            "id": "1214f637-1492-4e13-9da9-efbd087d81f6",
                            "question": "推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか"
                        }
                    ]
                }
            ],
            "title": "1220"
        },
        {
            "paragraphs": [
                {
                    "context": "Q値を推定する方法は，モデルの関する知識の前提によって大きく2つに分類されます．環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合，Q値は，動的計画法の考え方を用いて求めることができます．この方法をモデルベースの手法 と呼びます．一方，環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合，試行錯誤を通じて環境と相互作用をした結果を使って学習を行います．この方法をモデルフリーの手法 と呼びます．本節ではモデルベースの手法を，次節ではモデルフリーの手法を説明します．モデルベースの手法では，状態遷移確率$P(s_{t+1}|s_t, a_t)$と，報酬の確率分布$p(r_{t+1}|s_t, a_t)$が与えられているものとします．その前提で，アルゴリズム15.1に示すValue iterationアルゴリズムを実行すると，状態価値関数$V(s)$の最適値を求めることができ，それぞれの状態でQ値を最大とする行為が求まりますので，これが最適政策ということになります．アルゴリズム15.1 中の報酬の期待値$E(r|s,a)$は報酬の確率分布$p(r_{t+1}|s_t, a_t)$から求めます．このアルゴリズムは，迷路中で報酬がもらえる状態（ゴール）が1つだけある場合，まずそのゴール状態の1つ手前での最適行為が得られ，次にその1つ手前，さらにその1つ手前と，繰り返しを回る毎に正しい最適値が得られている状態がゴールを中心に広がってゆくイメージをしていただけると，わかりやすいと思います．また，モデルベースの手法には，Value iterationアルゴリズムの他にも，適当な政策を初期値として，そのもとでの状態価値関数$V(s)$を計算し，各状態で現在の知識から得られる最適行為を選び直すことを繰り返すPolicy iterationアルゴリズムもあります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 40,
                                    "text": "環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合"
                                }
                            ],
                            "id": "908d3b41-6952-4ea5-9832-f3c5c9c8e62b",
                            "question": "モデルベースの手法とはどのような場合ですか"
                        }
                    ]
                }
            ],
            "title": "1509"
        },
        {
            "paragraphs": [
                {
                    "context": "このように，ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズムをEM アルゴリズム(Expectation-Maximization)とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム"
                                }
                            ],
                            "id": "bbcf6fcc-781e-498e-a432-0ee5365953a1",
                            "question": "EM アルゴリズムとは、どのようなアルゴリズムですか"
                        }
                    ]
                }
            ],
            "title": "1116"
        },
        {
            "paragraphs": [
                {
                    "context": "このようなことを考慮すると，入力が数値のベクトルである場合，半教師あり学習が可能なデータは以下の仮定を満たしていることが必要になります．\\begin{itemize}\\item 半教師あり平滑性仮定\\\\　もし二つの入力 $\\bm{x}_1$ と $\\bm{x}_2$ が高密度領域　　で近ければ，出力 $y_1$ と $y_2$ も関連している\\item クラスタ仮定\\\\　もし入力が同じクラスタに属するなら，それらは同じクラスになりやすい\\item 低密度分離\\\\　識別境界は低密度領域にある\\item 多様体仮定\\\\　高次元のデータは，低次元の多様体上に写像できる\\end{itemize}最後の仮定は，多次元でも「次元の呪い」にかかっていない，ということです．第7章で行った，多次元空間への写像の逆が成り立っているということになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 304,
                                    "text": "多次元でも「次元の呪い」にかかっていない，ということ"
                                }
                            ],
                            "id": "affad3de-167a-4120-90df-6f9d33061aef",
                            "question": "多様体仮定とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1403"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 37,
                                    "text": "入力に近い側の処理で，特徴抽出をおこなおうとするものです．"
                                }
                            ],
                            "id": "5710b1bf-14ac-4bf2-b2e1-4c60d15aa61d",
                            "question": "多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシグモイド関数の微分です．第2項は入力の重み付き和の微分です．従って，出力層の重みの更新式は以下のようになります．通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．また，中間層は，この修正量を重み付きで足し合わせます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 45,
                                    "text": "シグモイド関数の微分"
                                }
                            ],
                            "id": "292132fd-584f-4e9a-9653-57a139d48532",
                            "question": "1つ目の式の右辺の第1項は何ですか"
                        }
                    ]
                }
            ],
            "title": "0808"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "その政策に従って行動したときの累積報酬の期待値で評価します"
                                }
                            ],
                            "id": "c5ce4536-b0be-4c60-bcc0-d9c377349cf5",
                            "question": "マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．"
                                }
                            ],
                            "id": "c61097c4-e7b3-4ccb-a752-e17d126fcdf1",
                            "question": "通常の決定木とランダムフォレストの違いはなんですか"
                        }
                    ]
                }
            ],
            "title": "1009"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，式(4.2)に基づいて得られた事後確率の計算式を，記号を変えてもう一度見直してみます．式(5.3)の分子は，生成モデルとよばれる考え方で解釈することができます．まず，あるクラス$\\omega_i$が確率$P(\\omega_i)$で選ばれ，そのクラスから特徴ベクトル$\\bm{x}$が確率$p(\\bm{x} \\vert \\omega_i)$に基づいて生成されたという考え方です．これは式(5.4)の分子である特徴ベクトルとクラスの同時確率$p(\\omega_i, \\bm{x})$を求めていることになります．この生成モデルアプローチは，（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合には有効です．しかし，そうでない場合は，推定するべきパラメータは，$P(\\omega_i|\\bm{x})$を直接推定するよりも増えてしまいます．同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなるのが一般的です．つまり，生成モデルアプローチは，本来解くべき問題を，あえて難しい問題にしてしまっているのではないかという疑問が出てくるわけです．この問題への対処法として，次節では，$P(\\omega_i|\\bm{x})$を直接推定する方法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 271,
                                    "text": "（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合"
                                }
                            ],
                            "id": "8092cffd-c8a9-4180-97b2-8049fa454b00",
                            "question": "生成モデルアプローチが有効なのはどのようなときですか"
                        }
                    ]
                }
            ],
            "title": "0504"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 365,
                                    "text": "形態素解析処理が典型的な問題"
                                }
                            ],
                            "id": "4ab0ecda-a39a-4cc4-8503-78ee3dcff3d8",
                            "question": "系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 444,
                                    "text": "隠れ層"
                                }
                            ],
                            "id": "35ca0a27-549f-4d43-83c8-4822cae92420",
                            "question": "中間層の別名は何か"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 452,
                                    "text": "規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比"
                                }
                            ],
                            "id": "d3f4cef1-db5e-4461-9970-72dab3cc8a2a",
                            "question": "リフト値ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 403,
                                    "text": "境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合"
                                }
                            ],
                            "id": "d03de029-3b59-400d-bdd5-4388393fcff3",
                            "question": "統計モデルによるアプローチはどのようなときに有効ですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "05b77148-b043-481b-a758-456ba94ba674",
                            "question": "深層学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく，というのが半教師あり学習の基本的な進め方です（この章の最後に紹介するYATSIアルゴリズムは例外です）．識別器を作成するアルゴリズムはこれまで紹介してきたものを問題に応じて用いればよいのですが，信用できる出力をする教師なしデータを次回の識別器作成に取り込むためには，ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切です．一方，繰り返しアルゴリズムに関して，単純に終了のための閾値チェックをするだけなのか，識別器のパラメータを繰り返しの度に変化させるか，識別器で使う特徴に制限をかけるか，など様々な設定が可能です．以下では，繰り返しアルゴリズムの違いによって生じる，様々な半教師あり学習手法について説明してゆきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく"
                                }
                            ],
                            "id": "99171d2a-2499-4ab2-8f3e-f25aef85e59f",
                            "question": "半教師ある学習の基本的な進め方はどういったものですか"
                        }
                    ]
                }
            ],
            "title": "1406"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "9fb9d75a-2f16-4610-b83e-3e3c21f8561a",
                            "question": "ニューラルネットワークとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 271,
                                    "text": "生物の神経細胞の仕組みをモデル化したもの"
                                }
                            ],
                            "id": "5876384e-c6d5-4013-9e89-f53a1af2c044",
                            "question": "パーセプトロンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 55,
                                    "text": "すべてのデータの重みは平等"
                                }
                            ],
                            "id": "09b0f4dc-20ba-47dd-bdc5-2471b9da9387",
                            "question": "バギングでは、個々のデータに対してどのように重みを設定しますか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "環境モデルが未知の場合，TD(Temporal Difference)学習と呼ばれる方法を使います．環境の探索が必要なので，探索戦略としてε-greedy法を使います．ε-greedy法は確率$1-\\epsilon$で最適な行為，確率$\\epsilon$でそれ以外の行為を実行する探索手法の総称で，実際にはQ値を確率に変換したものを基準に行為を選択します．ただし，探索の初期はいろいろな行為を試し，落ち着いてくると最適な行為を多く選ぶようにするように，温度の概念を導入します．温度を$T$として，式(15.8)で表される確率に従って行為を選びます．$T$をアニーリング(焼き鈍し)における温度と呼び，高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "確率$1-\\epsilon$で最適な行為，確率$\\epsilon$でそれ以外の行為を実行する探索手法の総称"
                                }
                            ],
                            "id": "a2523aa2-8dc7-439e-a309-56f50f4928b0",
                            "question": "ε-greedy法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1510"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "AutoencoderとRestricted Bolzmann Machine(RBM)"
                                }
                            ],
                            "id": "822979d1-6cdb-476a-8b70-36c1d4ca59ff",
                            "question": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 809,
                                    "text": "非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法"
                                }
                            ],
                            "id": "530fa458-ac12-425a-bbdb-366ab2fde357",
                            "question": "ニューラルネットワークってなんですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 531,
                                    "text": "最大事後確率則"
                                }
                            ],
                            "id": "d99edff3-2c89-4140-aba1-05093a280c25",
                            "question": "事後確率が最大となるクラスを識別結果とする方法を何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとする"
                                }
                            ],
                            "id": "415fe5df-a68b-440a-bb6f-3c5abfcaa5d2",
                            "question": "深層学習と他の手法との大きな違いは何ですか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 287,
                                    "text": "内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．"
                                }
                            ],
                            "id": "aeaae3dc-fa9c-40b0-9de1-413f061211a0",
                            "question": "LSTMと通常のユニットの違いは何ですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，このデータからFP-木 (Frequent Pattern Tree) を作成します．FP-木は，最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．挿入アルゴリズムは以下のようになります．具体的な FP-木の作成手順は図12.7のようになります．まず，$\\{z,r\\}$がnull($\\emptyset$)を根とするFP-木に挿入されます．次に，$\\{z,x,y,s,t\\}$の挿入です．まず$z$はFP-木にあるのでカウントを1増やして2として，この$z:2$をルートとするFP-木に対して，残りの$\\{x,y,s,t\\}$を挿入します．次の$x$はFP-木にないので，新たにノードを作って$z:2$につなぎます．そして，この新しい$x:1$をルートとするFP-木に対して残りの要素を挿入する作業を再帰的に繰り返します．できあがったFP-木に対して，特徴を見出しとするヘッダテーブルを作成し，その頻度を記録しておくとともにFP-木に出現する同じ要素をリンクで結んでおきます（図12.8上）．特定の特徴は，自分より頻度の高い特徴の出現の有無に応じて，複数の枝に分かれて出現します．このリンクをたどって集めた出現数は，全体のトランザクション集合での出現数に一致します．% figure 12.8",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 53,
                                    "text": "最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．"
                                }
                            ],
                            "id": "41a1a7b3-dd1c-4945-978d-5a056b1c86fb",
                            "question": "FP 木とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1215"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとするものです"
                                }
                            ],
                            "id": "6b130dd6-9904-499d-ba4a-309a722cef17",
                            "question": "深層学習におけるこれまでの識別問題との差はなにか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法"
                                }
                            ],
                            "id": "d440d87b-f8c3-42c8-becb-2f253ab00fb3",
                            "question": "サポートベクトルマシンとは何か"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "データから規則や知見を得る機械学習技術のなかでも，特に深層学習 (deep learning)は，高い性能を実現する方法として近年注目を集めています．深層学習は，一般に隠れ層を多くもつニューラルネットワーク（図1.3）によって実装されています．深層学習が他の機械学習手法と異なるのは，深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです．近年の深層学習の流行を見ると，他の機械学習技術はもう不要に見えるかもしれません．しかし，深層学習がその強さを発揮しているのは，音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題であるという傾向があります．さまざまな問題に対して機械学習アルゴリズムの性能を競うサイトでは，深層学習と並んで勾配ブースティングなどの手法が上位を占めることがあります．また一方で，性能は多少低くてもよいので判定結果に至るプロセスがわかりやすい手法や，運用後のチューニングが容易な手法が好まれる場合もあり，さまざまな状況でさまざまな問題に取り組むためには，深層学習だけではなく機械学習手法全般に関して理解しておくことが必要であるといえます．本書では機械学習全般に関して，設定した問題に対する基本的な手法の概要と，フリーソフトを用いた例題の解法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 270,
                                    "text": "音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題"
                                }
                            ],
                            "id": "f9d80c4f-fbe1-4c76-84b7-211efec646bf",
                            "question": "深層学習が得意な問題はなんですか"
                        }
                    ]
                }
            ],
            "title": "0104"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシグモイド関数の微分です．第2項は入力の重み付き和の微分です．従って，出力層の重みの更新式は以下のようになります．通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．また，中間層は，この修正量を重み付きで足し合わせます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 62,
                                    "text": "入力の重み付き和の微分"
                                }
                            ],
                            "id": "b51d3ae6-d6a8-4f5d-83bc-714ae4f82a01",
                            "question": "1つ目の式の右辺の第2項は何ですか"
                        }
                    ]
                }
            ],
            "title": "0808"
        },
        {
            "paragraphs": [
                {
                    "context": "近年，日常生活やビジネスにおけるさまざまな場面で人工知能 (aritificial intelligence)を活用した製品やサービスの開発が注目されています．人工知能は，人と対話を行うアプリやロボット・自動運転・病気の診断の補助・高度な生産システムなどの中心的技術として位置づけられています．人工知能はさまざまな立場から異なった定義がされていますが，本書では人工知能を「現在，人が行っている知的な判断を代わりに行う技術」と定義します．このように定義すると，探索・知識表現・推論などの技術とともに，データから規則性を導く機械学習 (machine learning)も「人が行っている知的な判断を代わりに行う」技術を実現するための，ひとつの方法ということになります（図1.1）．この定義のもとでは，「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます．日常生活で我々が便利だと感じている技術の大半は，人間が作成したプログラムで動いています．また，機械可読な web 情報源の構築を目指した LOD (linked open data) の取り組みや，システムが結論を出した過程をわかりやすく人に説明するための推論・プランニングの技術は，直接的には機械学習とは関係がなくとも，知的なシステムを作成するために重要な人工知能の要素技術です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "現在，人が行っている知的な判断を代わりに行う技術"
                                }
                            ],
                            "id": "392f7677-a0a4-4232-b7d4-e1ff407f941d",
                            "question": "人工知能の定義はなんですか"
                        }
                    ]
                }
            ],
            "title": "0102"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "真のモデルとの距離"
                                }
                            ],
                            "id": "5ea1775b-8288-4440-a15f-f5c9fde5ca88",
                            "question": "バイアスってなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワークの利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの確率計算の例を示します．最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)がHead-to-tail connectionでつながっている例を示します．これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．何も情報がない状態での「芝生が濡れている」確率は以下のようになります．まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って「芝生が濡れている」確率$P(W)$を求めます．ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，以下のようになります．つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していないことになります．なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，その日が「曇っている」確率は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 57,
                                    "text": "変数間の独立性を表現できること"
                                }
                            ],
                            "id": "12742949-0b45-4b49-84f8-62af0922415e",
                            "question": "ベイジアンネットワークの利点はなんですか"
                        }
                    ]
                }
            ],
            "title": "0413"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる特徴の尤もらしさを表す尤度はわかりません．そこで，この事前確率や尤度を計算する確率モデルを仮定し，そのパラメータを学習データに最も合うように調整することを考えます．それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します．学習データ全体$D$が生成される確率$P(D)$は，個々の事例$\\{\\bm{x}_1,\\dots,\\bm{x}_N\\}$の独立性、すなわち i.i.d. を仮定すると，式(4.5)のように，個々の事例が生成される確率の積で求めることができます．$P$は，データの生成確率を何らかのパラメータに基づいて計算するモデルです．ある程度複雑なモデルでは，パラメータが複数あることが一般的なので，これらのパラメータをまとめて$\\bm{\\theta}$と表記して明示すると，式(4.5)は式(4.6)のように書けます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 37,
                                    "text": "各クラスから生じる特徴の尤もらしさを表す"
                                }
                            ],
                            "id": "58eb3d7e-6b4d-4705-9087-a3eadcdeb16c",
                            "question": "尤度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0406"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1276,
                                    "text": "決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど"
                                }
                            ],
                            "id": "d7a78084-aaa7-40aa-9f5c-aff3cc670dda",
                            "question": "識別の代表的な手法には何がありますか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "制約を弱める変数"
                                }
                            ],
                            "id": "85d624f6-388e-4a01-a68e-6598d194dd4c",
                            "question": "スラック変数はなにをするものか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 49,
                                    "text": "判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法"
                                }
                            ],
                            "id": "d7bd25d6-ce8c-43fc-a6a3-f9fd7f331d9a",
                            "question": "共訓練ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "特徴空間上では線形識別面を設定すること"
                                }
                            ],
                            "id": "f08ec29d-037b-479f-b226-fa6cfa8a7324",
                            "question": "誤り訂正学習は特徴空間上では何に相当しますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法"
                                }
                            ],
                            "id": "c7c524a6-cd05-4665-b2f8-18a62e768651",
                            "question": "サポートベクトルマシンってなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 4,
                                    "text": "もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$"
                                }
                            ],
                            "id": "4dcaf937-a6c0-48ad-b4b6-8d5dc75e097e",
                            "question": "カーネル関数とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 252,
                                    "text": "カーネルトリック"
                                }
                            ],
                            "id": "8b1fff20-7ae5-4fa6-baa1-0a119b222a6f",
                            "question": "カーネル関数を定めて識別面を得る方法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 204,
                                    "text": "仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと"
                                }
                            ],
                            "id": "833e91ee-9faf-4557-ac08-1fb9f17287f9",
                            "question": "概念学習が失敗するのはどういう理由ですか"
                        }
                    ]
                }
            ],
            "title": "0306"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習は図14.2左の図のような，半教師あり学習に適したデータの場合はよいのですが，低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質"
                                }
                            ],
                            "id": "c2b66b89-b9b2-4f40-9faf-87aeb1570920",
                            "question": "自己学習の性質にはどのようなものがありますか"
                        }
                    ]
                }
            ],
            "title": "1408"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象"
                                }
                            ],
                            "id": "02fb411d-051d-4ec6-a2eb-b43ec8bffafd",
                            "question": "過学習とはどのような状態ですか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 75,
                                    "text": "特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点"
                                }
                            ],
                            "id": "473a9329-fa5b-4164-b15f-1bdd38948838",
                            "question": "数値特徴はカテゴリ特徴とどう違うんですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 81,
                                    "text": "ドロップアウト"
                                }
                            ],
                            "id": "8875c061-99e6-42d6-80ff-3ff6b5d43005",
                            "question": "ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "0725b5ad-0902-4b22-bf2f-8ab0b034e64f",
                            "question": "マージンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，簡単なカーネルについてその非線形変換$\\phi$を求めてみましょう．特徴ベクトルを2次元として多項式カーネル(p=2)を展開します．したがって，$\\bm{x}=(x_1, x_2)$のとき，$\\phi(\\bm{x})=(x_1^2, x_2^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1) )$となります．この変換の第3項に注目してください．特徴の積の項が加わっています．積をとるということは，2つの特徴が同時に現れるときに大きな値になります．すなわち，共起の情報が加わったことになります．このような，非線形変換で線形分離可能な高次元にデータを飛ばしてしまい，マージン最大化基準で信頼できる識別面を求めるというSVMの方法は非常に強力で，文書分類やバイオインフォマティックスなど様々な分野で利用されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "文書分類やバイオインフォマティックスなど"
                                }
                            ],
                            "id": "81a35dc5-8146-4bd5-9b33-29b7fa443bdf",
                            "question": "高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか"
                        }
                    ]
                }
            ],
            "title": "0713"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 206,
                                    "text": "識別面"
                                }
                            ],
                            "id": "7d6b533a-cbb8-4e67-997a-019b9bfdeac6",
                            "question": "カーネル関数が定まれば何が得られますか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 356,
                                    "text": "多階層構造でもそのまま適用できます"
                                }
                            ],
                            "id": "0e86242e-e1ed-4e25-bff5-02758a78a5b2",
                            "question": "誤差逆伝搬法は多階層構造でも利用できますか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 24,
                                    "text": "ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの"
                                }
                            ],
                            "id": "d91b2929-f75b-4dfe-b031-20f237303535",
                            "question": "多階層ニューラルネットワークとは何か"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 428,
                                    "text": "学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります"
                                }
                            ],
                            "id": "014ea192-1c88-41f9-9c88-7580eb0f652a",
                            "question": "なぜドロップアウトによって過学習が回避できるのですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 287,
                                    "text": "十分多くの層を持つニューラルネットワーク"
                                }
                            ],
                            "id": "8e2e1d62-8b2b-4dc8-8fa9-020a3a59e779",
                            "question": "特徴抽出を学習するには何が必要か"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "この与えられた系列を$\\bm{x}$として，クラス$y$（ただし，$y= B (初心者) or S (熟練者)$）の事後確率$P(y|\\bm{x})$を何らかのモデルを使って計算することを考えます．ここで式(13.4)のような$\\bm{x}, y$の同時確率を考える生成モデルアプローチをとるのが\\textbf{HMM}(Hidden Marcov Model: 隠れマルコフモデル)の考え方です．HMMは，式(13.4)の$P(\\bm{x}|y)$の値を与える確率的非決定性オートマトンの一種です．各状態であるシンボルをある確率で出力し，ある確率で他の状態(あるいは自分自身)に遷移します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 161,
                                    "text": "Hidden Marcov Model: 隠れマルコフモデル"
                                }
                            ],
                            "id": "9e4c36e4-783b-4c09-aa8d-2672d917562b",
                            "question": "HMMとは何の略称ですか"
                        }
                    ]
                }
            ],
            "title": "1310"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 388,
                                    "text": "正解が付いていない場合の学習"
                                }
                            ],
                            "id": "a2fe8d70-3fab-4594-9a2e-bc4d7dfae4d9",
                            "question": "教師なし学習とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った方法"
                                }
                            ],
                            "id": "7569d255-3876-46ce-8f14-92fabc8ec7e4",
                            "question": "モデル木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル"
                                }
                            ],
                            "id": "f5b4ff09-355b-452e-b771-635bbab660d0",
                            "question": "ロジスティック識別器ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "図で表すと，図12.3のようになります．この a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法が，Aprioriアルゴリズムです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法"
                                }
                            ],
                            "id": "5c6bef46-3385-4977-80c2-8adb83ab964a",
                            "question": "a prioriアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1207"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える"
                                }
                            ],
                            "id": "149b6814-a230-44f3-b833-356e153d1de9",
                            "question": "ReLu関数の良さは何ですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "3f2146fc-2271-4a4d-8bc3-22ff54491869",
                            "question": "マージンの定義はなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 173,
                                    "text": "地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出"
                                }
                            ],
                            "id": "c6621c1f-2c0c-4e6e-a894-0689247b5c20",
                            "question": "系列ラベリングの典型的な問題は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 150,
                                    "text": "高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法"
                                }
                            ],
                            "id": "81930c27-414c-4724-87bd-ff530bdec0c5",
                            "question": "FP-Growthアルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 18,
                                    "text": "非線形識別面"
                                }
                            ],
                            "id": "f842d40b-2411-427d-ad16-dc247892cc6a",
                            "question": "ノードを階層的に組むとどのような識別面ができるか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです(図14.5)自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 171,
                                    "text": "繰り返しによって学習データが増加し，より信頼性の高い識別器ができること"
                                }
                            ],
                            "id": "4d0fccb7-bea8-4463-987f-c0b95d7a23ee",
                            "question": "自己学習の狙いは何ですか"
                        }
                    ]
                }
            ],
            "title": "1407"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法"
                                }
                            ],
                            "id": "1ae601f5-d616-40be-86f2-601e7b306ee6",
                            "question": "アンサンブル学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術"
                                }
                            ],
                            "id": "7dc8535c-6e63-4070-b74a-1763e6f2993a",
                            "question": "パターン認識ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "条件付き確率場（Conditional Random Field: CRF）"
                                }
                            ],
                            "id": "9939cb5f-3ea7-4bd9-89f8-6528fcdc5ab6",
                            "question": "CRFは何の略ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．図11.4のように，データまたはクラスタを融合する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作でクラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの"
                                }
                            ],
                            "id": "d379e198-d7d3-483d-af36-0cbc0a4dda10",
                            "question": "階層的クラスタリングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1104"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 98,
                                    "text": "サポートベクトルマシン"
                                }
                            ],
                            "id": "050566d9-e662-40da-8185-fda762df8b8c",
                            "question": "SVMの正式名はなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法です．スーパーマーケットなどで同時に購入される商品の組み合わせを発見するバスケット分析が代表的な応用例です．図1.10にパターンマイニングの考え方を示します．パターンマイニングの敵は膨大な計算量です．まさに，大量のデータの中から，貴重な知見をマイニング（＝発掘）する作業です．図1.10に示した例では，発見された規則の条件部も結論部も要素数が一つなので，すべての商品の組み合わせに対してその出現頻度を計算することは，それほど膨大な計算量にはみえません．しかし，一般的なパターンマイニングでは，条件部・結論部のいずれも要素の集合となります．それらのあらゆる組み合わせに対して，マイニングの対象となる大きなデータ集合から出現数を数えあげなければならないので，単純な方法では気の遠くなるような計算量になってしまいます．そこで効率よく頻出パターンを見つけ出す手法が必要になります．パターンマイニングの代表的な手法としては Apriori アルゴリズムやその高速化版である FP-Growth があります．これらを第12章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法"
                                }
                            ],
                            "id": "2be9ca76-a1ef-4ed0-aeb0-92870d36a070",
                            "question": "パターンマイニングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0115"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです．ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．外れ値は，学習データに含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れたデータや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に除去しておくのが望ましいデータです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 121,
                                    "text": "学習データに含まれるデータの中で，ほかと大きく異なるデータ"
                                }
                            ],
                            "id": "d1b8f631-cce4-42be-8e8b-d1c95a0c0b04",
                            "question": "外れ値とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "1111"
        },
        {
            "paragraphs": [
                {
                    "context": "このような設定で，最も単純な例から始めましょう．対象とするものはK-armed banditと呼ばれる，$K$本のアームを持つスロットマシンです（図15.3）．$K$本のアームのうち，どのアームを引くかによって賞金が変わるものとします．これは，1状態，$K$種の行為，即時報酬の問題となります．学習結果は，このスロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）になります．もし，報酬が決定的であれば，学習は非常に簡単です．全ての行為を順に試みて最も報酬の高い行為を選べばよいのです．あまりにも単純ですが，今後のことを考えて学習過程を定式化しておきましょう．行為$a$の価値を$Q(a)$と定義し，学習過程によって正しい$Q(a)$の値(以後Q値といいます)が得られれば，Q値を最大とする行為が学習の結果になります．最初は，行為$a$を行ってどれだけの報酬が得られるのかわからないので，全ての$a$について$Q(a)$の値を0に初期化します．次に，可能な$a$を順番に行って($K$本のアームを順番に引いて)，そのときの報酬$r_a$を得ます．そして，各$a$について$Q(a)= r_a$として，Q値がいちばん高い$a$が求める行為になります．一方，報酬が非決定的な場合は，こんなに簡単にはゆきません．各行為$a$に対応する報酬$r$は，非決定的ですがまったくでたらめではなく，確率分布$p(r|a)$に従うと仮定します．つまり，決定的ではないが，確率的であると仮定します．ただし，この確率分布は未知だとします．そのような状況では，各アームを1回だけ引くのではなく，何度も引いて，平均的に多くの報酬が得られるアームを選ぶことになります．何回も試行することで，確率分布$p(r|a)$を推定するわけです．何度も試行して学習を行うので，定式化に時刻$t$を持ち込みます．扱いやすいように，$t$は離散的であるとして，時刻$t$で一回試行，時刻$t+1$で次の試行と続けてゆくと考えます．この場合，行為$a$の価値の時刻$t$における見積りを$Q_t(a)$とします．このQ値を時刻$t$以前での，行為$a$による報酬の平均値に一致させることを目指します．そうすると，その行為が平均的にどれぐらいうまくゆくか，ということがわかります．ただし，単純に平均値を求めるためには，それまでの行為$a$の試行回数を記憶しておかなければなりませんし，Q値はずっと変動し続けます．そこで，式(15.1)のように，時刻$t$の行為$a$による試行の報酬$r_{t+1}$と，現在のQ値との差を変動幅とし，学習係数$\\eta$をかけてQ値の更新を行います．学習率$\\eta$は最初は1以下の適当な値に設定し，時刻$t$の増加に従って減少するようにしておけば，試行を重ねてゆけばQ値が収束します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 236,
                                    "text": "全ての行為を順に試みて最も報酬の高い行為を選べばよい"
                                }
                            ],
                            "id": "ff518bbd-bac5-4572-8222-94d894f08b87",
                            "question": "強化学習で、報酬が決定的な場合の学習はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1503"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "ef8daba0-6d9f-4f34-9225-df132ef2097f",
                            "question": "深層学習の定義はなにか"
                        }
                    ]
                }
            ],
            "title": "0901"
        }
    ],
    "version": "1.0"
}