{
    "data": [
        {
            "paragraphs": [
                {
                    "context": "ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けないというものです．数式で表すと，確率変数の値$\\{z_1,\\dots,z_n\\}$の結合確率は，以下のように計算されます．ただし$\\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，この計算を順に行うことで，ネットワーク全体の確率計算が行えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 103,
                                    "text": "「特徴の部分集合が，あるクラスのもとで独立である」と仮定"
                                }
                            ],
                            "id": "982e8bf0-0ced-4c67-8c2d-223033867dba",
                            "question": "ベイジアンネットワークはどのような仮定を表現したものですか"
                        }
                    ]
                }
            ],
            "title": "0412"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア"
                                }
                            ],
                            "id": "4c35ed0f-7287-4869-bbb4-cbe70d65395f",
                            "question": "事前学習法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 459,
                                    "text": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法"
                                }
                            ],
                            "id": "c63294b4-39f8-41dc-91a0-161542a68b70",
                            "question": "誤差逆伝播法とはどういう手法ですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 398,
                                    "text": "識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを"
                                }
                            ],
                            "id": "3a045b56-d0c0-42fd-a0e2-2e92d318a118",
                            "question": "二乗誤差とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 92,
                                    "text": "関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法"
                                }
                            ],
                            "id": "34d38740-ed86-433c-8659-c44efcbef6d6",
                            "question": "識別関数法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチ"
                                }
                            ],
                            "id": "ac7fa1c5-12bf-47ea-8485-1c23e7895ebd",
                            "question": "識別モデルとはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，簡単なカーネルについてその非線形変換$\\phi$を求めてみましょう．特徴ベクトルを2次元として多項式カーネル(p=2)を展開します．したがって，$\\bm{x}=(x_1, x_2)$のとき，$\\phi(\\bm{x})=(x_1^2, x_2^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1) )$となります．この変換の第3項に注目してください．特徴の積の項が加わっています．積をとるということは，2つの特徴が同時に現れるときに大きな値になります．すなわち，共起の情報が加わったことになります．このような，非線形変換で線形分離可能な高次元にデータを飛ばしてしまい，マージン最大化基準で信頼できる識別面を求めるというSVMの方法は非常に強力で，文書分類やバイオインフォマティックスなど様々な分野で利用されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "文書分類やバイオインフォマティックスなど様々な分野で利用されています"
                                }
                            ],
                            "id": "c2f0fd4d-a27c-4880-8d67-b63f016dde5e",
                            "question": "高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか"
                        }
                    ]
                }
            ],
            "title": "0713"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 115,
                                    "text": "重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．"
                                }
                            ],
                            "id": "8696fdff-a1f5-45a4-98e6-27a6e5877e65",
                            "question": "なぜパラメータを0に近づけるんですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 465,
                                    "text": "二乗誤差を最小にするように識別関数を調整する方法"
                                }
                            ],
                            "id": "11eaf1a8-4186-441c-a602-97997e234640",
                            "question": "最小二乗法って何ですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習では，学習に用いられるデータに正解情報が付いていません．入力ベクトル$\\bm{x}_i$の次元数に関しては，教師あり学習の場合と同様に，$d$次元の固定長ベクトルで，各要素は数値あるいはカテゴリのいずれかの値をとると考えておきます．教師なし学習は，入力データに潜む規則性を学習することを目的とします．ここで着目すべき規則性としては，2通り考えられます．一つめは，入力データ全体を支配する規則性で，これを学習によって推定するの問題がモデル推定 (model estimation)です．もう一つは，入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです．これを発見する問題がパターンマイニング (pattern mining) です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 130,
                                    "text": "入力データに潜む規則性を学習すること"
                                }
                            ],
                            "id": "1cc1b74d-b3fb-46bd-b6cd-f4b62abc9ae9",
                            "question": "教師なし学習では何を学習しますか"
                        }
                    ]
                }
            ],
            "title": "0113"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 191,
                                    "text": "さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した"
                                }
                            ],
                            "id": "c154780e-8b82-4b6b-a2bc-5eea1d19c9a7",
                            "question": "BICとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 40,
                                    "text": "学習結果の散らばり具合"
                                }
                            ],
                            "id": "d786a826-249e-4c10-bccc-de784c27c8cb",
                            "question": "分散とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア"
                                }
                            ],
                            "id": "9536b7a5-fa4c-4063-a842-7ceb53780943",
                            "question": "事前学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "dfd22d5f-b359-49eb-88b9-6fe669221d75",
                            "question": "カーネルトリックってどういうもの"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです"
                                }
                            ],
                            "id": "ef1f7f60-9439-4f57-a085-2c292fa845cb",
                            "question": "なぜ重みを更新するのですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 150,
                                    "text": "高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法"
                                }
                            ],
                            "id": "b9d8737d-6336-400d-b04d-8b2fed322e87",
                            "question": "FP-Growthアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 252,
                                    "text": "カーネルトリック"
                                }
                            ],
                            "id": "d748fd99-7184-4a49-a29d-812b5cf34ac7",
                            "question": "複雑な非線形変換を求めるという操作を避ける方法をなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 10,
                                    "text": "自分が出した誤りを指摘してくれる他人がいない"
                                }
                            ],
                            "id": "85a6049a-1b5f-4ac3-bbc9-d1ff67fe7485",
                            "question": "自己学習の問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "機械学習において与えられるデータは，個々の事例です．その個々の事例から，あるクラスについて共通点を見つけることが，概念学習です．共通点は，特徴の値の組み合わせによって表現されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 28,
                                    "text": "個々の事例から，あるクラスについて共通点を見つけることが，概念学習です"
                                }
                            ],
                            "id": "d4375f81-2e59-41e4-9c1f-9c05bf065858",
                            "question": "概念学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0304"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，通常はそう簡単にはゆきません．図14.3の例で示したような商品の評価を行う文書にしても，褒める言葉やけなす言葉は様々なバリエーションがあります．顔文字を使ったり，略語を使ったりもするでしょう．そのような場合，正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段が考えられます（図14.4）．つまり，ラベル特徴の場合，教師ありデータと教師なしデータにラベル値のオーバーラップが全く見られないデータでは，半教師あり学習は役に立ちませんが，教師なしデータの一部とでも適当なオーバーラップがあれば，その一部の教師なしデータが他の教師なしデータを徐々に巻き込んでゆく可能性があります．通常，自然言語で書かれたデータはこの後者の仮定を満たすことが多いので，半教師あり学習は文書分類問題によく適用されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 108,
                                    "text": "正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段"
                                }
                            ],
                            "id": "50882dc6-34e5-4c9c-8f11-aec16832c5a1",
                            "question": "特徴の伝搬とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1405"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 75,
                                    "text": "特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点"
                                }
                            ],
                            "id": "402687aa-b1c7-495a-af60-c0c67a297ff5",
                            "question": "数値特徴はカテゴリ特徴とどう違うんですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "最後に，ベイジアンネットワークの学習について説明します．ベイジアンネットワークにおいて学習するべき項目は，ネットワークの構造とアークの条件付き確率表です．まず，ネットワークの構造が得られているものとして，アークの条件付き確率表を得る方法について説明します．学習データにすべての変数の値が含まれる場合は，単純ベイズ法と同様な数え上げによって確率値を決めることができます．ここでも，ゼロ頻度問題を回避するために，データカウント数の初期値を一定値にしておくなどの工夫が必要になります．一方，学習データに値が観測されない変数がある場合は，適当な初期値を設定して，第5章で説明する最急勾配法により学習することになります．また，ベイジアンネットワークの構造の学習は，そのネットワークによって計算される式(4.7)の対数尤度が大きくなるように，アークを探索的に追加してゆく方法が考えられます．その基本的な方法がK2アルゴリズムで，概要は以下のようになります．ここで$Node$は特徴集合とクラスからなるノード全体の集合を表します．一般に，複雑なネットワークのほうが対数尤度は大きくなるので，このアルゴリズムは簡単に過学習に陥りやすいといわれています．過学習への対処法としては，親ノードの数をあらかじめ制限する方法が提案されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 53,
                                    "text": "ネットワークの構造とアークの条件付き確率"
                                }
                            ],
                            "id": "14468291-0050-4024-a843-539c2a00f2f0",
                            "question": "ベイジアンネットワークにおいて学習するべき項目はなんですか"
                        }
                    ]
                }
            ],
            "title": "0417"
        },
        {
            "paragraphs": [
                {
                    "context": "このように，ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズムをEM アルゴリズム(Expectation-Maximization)とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行う"
                                }
                            ],
                            "id": "8af7e688-0c4c-45b9-8a4e-05e014b2d7cf",
                            "question": "EM アルゴリズムとは、どのようなアルゴリズムですか"
                        }
                    ]
                }
            ],
            "title": "1116"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，前章で学んだ統計モデルによる識別法で，数値を要素とする特徴ベクトルを識別する問題に取り組みます．数値を要素とする特徴ベクトルに対する識別問題は，一般にはパターン認識とよばれます．第3章と第4章では，カテゴリ特徴に対する識別問題を扱いました．続いて本章では，数値を要素とする特徴ベクトル$\\bm{x}$に対する識別問題を扱います（図5.1）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 54,
                                    "text": "数値を要素とする特徴ベクトルに対する識別問題は"
                                }
                            ],
                            "id": "f5c6bf60-e7f5-417e-9153-62a50f5b286a",
                            "question": "パターン認識ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0501"
        },
        {
            "paragraphs": [
                {
                    "context": "近年，日常生活やビジネスにおけるさまざまな場面で人工知能 (aritificial intelligence)を活用した製品やサービスの開発が注目されています．人工知能は，人と対話を行うアプリやロボット・自動運転・病気の診断の補助・高度な生産システムなどの中心的技術として位置づけられています．人工知能はさまざまな立場から異なった定義がされていますが，本書では人工知能を「現在，人が行っている知的な判断を代わりに行う技術」と定義します．このように定義すると，探索・知識表現・推論などの技術とともに，データから規則性を導く機械学習 (machine learning)も「人が行っている知的な判断を代わりに行う」技術を実現するための，ひとつの方法ということになります（図1.1）．この定義のもとでは，「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます．日常生活で我々が便利だと感じている技術の大半は，人間が作成したプログラムで動いています．また，機械可読な web 情報源の構築を目指した LOD (linked open data) の取り組みや，システムが結論を出した過程をわかりやすく人に説明するための推論・プランニングの技術は，直接的には機械学習とは関係がなくとも，知的なシステムを作成するために重要な人工知能の要素技術です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 349,
                                    "text": "「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます"
                                }
                            ],
                            "id": "ffb34b6f-568b-4d1e-8817-899a144f4a38",
                            "question": "人工知能と機械学習は同じ意味ですか"
                        }
                    ]
                }
            ],
            "title": "0102"
        },
        {
            "paragraphs": [
                {
                    "context": "Matrix Factorizationは，まばらなデータを低次元行列の積に分解する方法の一つです．一般に，行列分解にはSVD (Singular Value Decomposition) とよばれる方法がありますが，推薦システムにこの方法を適用しても，うまくいかないことが多いといわれています．その理由として，購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するからです．購入したものを1，しなかったものを0として行列で表現した購入データをそのまま推薦に利用すると，1が「好き」，0が「きらい」に対応するものとして扱ってしまいます．そのようなことを避けるために，値の入っているところのみで最適化をおこなう手法が，Matrix Factorizationです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 22,
                                    "text": "まばらなデータを低次元行列の積に分解する方法の一つ"
                                }
                            ],
                            "id": "23368f18-f35e-4959-a2ea-2ae774bcbc9b",
                            "question": "Matrix Factorizationとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1220"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 404,
                                    "text": "最適政策$\\pi^*$を獲得すること"
                                }
                            ],
                            "id": "d0136f26-6ce0-4f40-93fd-6a1719476bc5",
                            "question": "マルコフ決定過程における学習の目標は何ですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 4,
                                    "text": "もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$"
                                }
                            ],
                            "id": "cf35946a-7798-4a81-9275-9cd2e0da3d1e",
                            "question": "カーネル関数とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法です．スーパーマーケットなどで同時に購入される商品の組み合わせを発見するバスケット分析が代表的な応用例です．図1.10にパターンマイニングの考え方を示します．パターンマイニングの敵は膨大な計算量です．まさに，大量のデータの中から，貴重な知見をマイニング（＝発掘）する作業です．図1.10に示した例では，発見された規則の条件部も結論部も要素数が一つなので，すべての商品の組み合わせに対してその出現頻度を計算することは，それほど膨大な計算量にはみえません．しかし，一般的なパターンマイニングでは，条件部・結論部のいずれも要素の集合となります．それらのあらゆる組み合わせに対して，マイニングの対象となる大きなデータ集合から出現数を数えあげなければならないので，単純な方法では気の遠くなるような計算量になってしまいます．そこで効率よく頻出パターンを見つけ出す手法が必要になります．パターンマイニングの代表的な手法としては Apriori アルゴリズムやその高速化版である FP-Growth があります．これらを第12章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "パターンマイニング"
                                }
                            ],
                            "id": "502b8ef0-a47f-4a7c-9f7b-f2081f31d651",
                            "question": "データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0115"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 54,
                                    "text": "個々の要素の間に i.i.d. の関係が成立しないもの"
                                }
                            ],
                            "id": "7ecffbae-d8df-4e21-8737-f02b4562b884",
                            "question": "系列データとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "制約を弱める変数"
                                }
                            ],
                            "id": "5517fde7-4c9a-4a67-87f2-e84a04357c3b",
                            "question": "スラック変数とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法が，勾配ブースティングです．損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 80,
                                    "text": "損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます"
                                }
                            ],
                            "id": "d33eef41-92f0-4d5b-837f-8a479c48b5e1",
                            "question": "損失関数にはどのようなものがありますか"
                        }
                    ]
                }
            ],
            "title": "1015"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる特徴の尤もらしさを表す尤度はわかりません．そこで，この事前確率や尤度を計算する確率モデルを仮定し，そのパラメータを学習データに最も合うように調整することを考えます．それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します．学習データ全体$D$が生成される確率$P(D)$は，個々の事例$\\{\\bm{x}_1,\\dots,\\bm{x}_N\\}$の独立性、すなわち i.i.d. を仮定すると，式(4.5)のように，個々の事例が生成される確率の積で求めることができます．$P$は，データの生成確率を何らかのパラメータに基づいて計算するモデルです．ある程度複雑なモデルでは，パラメータが複数あることが一般的なので，これらのパラメータをまとめて$\\bm{\\theta}$と表記して明示すると，式(4.5)は式(4.6)のように書けます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 37,
                                    "text": "各クラスから生じる特徴の尤もらしさを表す"
                                }
                            ],
                            "id": "57b27f01-2507-4d41-bdde-1dab4476a8fe",
                            "question": "尤度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0406"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "85e5bf73-e7a9-459c-902d-cbbbe5cfbffa",
                            "question": "k-平均法とは、どのようなアルゴリズムなのですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 688,
                                    "text": "学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．"
                                }
                            ],
                            "id": "a9e78cab-f445-4647-aba8-3535053060fe",
                            "question": "マージンが広いとどうなるのか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの"
                                }
                            ],
                            "id": "09589d1b-3726-48d3-8e67-be9e33b44289",
                            "question": "畳み込みネットワークとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 495,
                                    "text": "入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない"
                                }
                            ],
                            "id": "612d8ff6-6a2e-416f-b845-ac5e5f1f8b30",
                            "question": "多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 267,
                                    "text": "素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質"
                                }
                            ],
                            "id": "0bb8bcd9-fbc0-4fd2-8b59-176999609a74",
                            "question": "条件付き確率場の学習にはどのような性質を利用しますか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "機械学習の出番は，簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるときです．このような大量のデータは，ビッグデータ (big data)とよばれます．ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです．この大量・多様なデータから規則性を抽出したり，データを分類するモデルを獲得することで，購買記録からのお勧め商品提示のようなおなじみの機能に加えて，不審者の行動パターンの検出や，インフルエンザの流行の予想など，これまでになかったサービスや機能を実現することもできます（図1.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき"
                                }
                            ],
                            "id": "c9773049-646e-4720-b4f7-db8de18982aa",
                            "question": "機械学習はどんな時に利用されますか"
                        }
                    ]
                }
            ],
            "title": "0103"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "3c767c49-e968-4026-9a8e-299390ceab68",
                            "question": "ニューラルネットワークとはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意する，ということです．ここまでの教師あり学習の説明からもわかるように，学習データが異なれば，たいていその学習結果も異なります．バギング(Bagging)はこのアイディアに基づいて，学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです（図10.3}）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 136,
                                    "text": "学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです"
                                }
                            ],
                            "id": "76ca4f55-2795-41dc-a763-3758e516bead",
                            "question": "バギングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1004"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが移動するという状況です．ゴールGに着けば，報酬が得られます．単純なケースでは報酬は決定的（ゴールに着けば必ずもらえる）で，部屋の移動にあたる状態遷移も決定的（必ず意図した部屋に移動できる）です．問題を一般化して，報酬や遷移が確率的である場合も想定できます．これらが確率的になる原因として，例えばロボットのゴールを探知するセンサーがノイズで誤動作をしたり，路面状況でスリップが生じるなどの不確定な要因で行為が成功しない状況が考えられます．これらは，非決定的であるとはいえ，学習中に状況が変化してしまうとどうしようもないので，この非決定性が確率的であるとし，確率分布は学習期間中を通じて一定であるとします．このような問題は，以下のようなマルコフ決定過程として定式化することができます．\\begin{itemize}\\item 時刻$t$における状態$s_t \\in S$\\item 時刻$t$における行為$a_t \\in A(s_t)$\\item 報酬 $r_{t+1} \\in R$，確率分布$p(r_{t+1}|s_t, a_t)$\\item 次状態$s_{t+1} \\in S$，確率分布$P(s_{t+1}|s_t, a_t)$\\end{itemize}マルコフ決定過程は，「マルコフ性」を持つ確率過程における意思決定問題で，「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質です．ここでは，報酬と次状態への遷移の確率が現在の状態と行為のみに依存しているという定式化になっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 605,
                                    "text": "「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質"
                                }
                            ],
                            "id": "ec0139fd-9bd4-4a9b-b0d4-ab1dede5b33b",
                            "question": "マルコフ決定過程の性質はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1505"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，復元抽出によって作られた個々のデータ集合は，もとの学習データ集合と約$2/3$のデータを共有しているので，とくに，元のデータのまとまりがよい場合，それほど極端に異なった識別器にはなりません．ここで説明するランダムフォレスト(RandomForest)は，識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 169,
                                    "text": "識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法"
                                }
                            ],
                            "id": "cba9408f-e551-448a-a687-6a6a00298025",
                            "question": "ランダムフォレストとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1007"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 37,
                                    "text": "入力に近い側の処理"
                                }
                            ],
                            "id": "45d9f423-23ff-49c2-aeef-38619cae922f",
                            "question": "多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 307,
                                    "text": "入力ゲート・出力ゲート・忘却ゲート"
                                }
                            ],
                            "id": "0d29d744-5cfd-4e8c-a5fd-4485fc54fd1f",
                            "question": "LSTMセルが持つ3つのゲートは何ですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる"
                                }
                            ],
                            "id": "d60c26b9-2035-43d7-8537-a88c4f104ce5",
                            "question": "ゼロ頻度問題とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 503,
                                    "text": "誤差逆伝播法"
                                }
                            ],
                            "id": "67b489f0-e49d-406c-97b4-44fd520f0213",
                            "question": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 98,
                                    "text": "サポートベクトルマシン"
                                }
                            ],
                            "id": "df6e56e7-5977-478a-bbd0-d9fbd72e51d5",
                            "question": "学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 314,
                                    "text": "ニューラルネットワーク"
                                }
                            ],
                            "id": "6a5b6d1c-10fa-4a55-bf0f-554bc1f33f57",
                            "question": "パーセプトロンを多層に重ねたものはなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法が，勾配ブースティングです．損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 89,
                                    "text": "二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）"
                                }
                            ],
                            "id": "a5bf0fe4-9624-4bdd-9a3e-46d117bb8909",
                            "question": "勾配ブースティングで、どのような損失関数が使われますか"
                        }
                    ]
                }
            ],
            "title": "1015"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 368,
                                    "text": "規則の条件部が起こったときに結論部が起こる割合"
                                }
                            ],
                            "id": "ef6c558d-048b-4e66-9c64-ac53f6572cdc",
                            "question": "確信度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "第4章では，カテゴリ特徴に対する統計的識別手法を説明してきました．その基本的な考え方は，数値特徴に対しても適用することができます．数値特徴の場合のナイーブベイズ識別の結果を求める式は，式(5.1)のようになります．カテゴリ特徴の場合の式(4.12)とほとんど同じですが，尤度が離散事象に対する確率分布$P(x_j|\\omega_i)$ではなく，数値に対する確率密度関数$p(x_j|\\omega_i)$になっています．事前確率$P(\\omega_i)$に関しては，カテゴリ特徴のときと同様に，学習データ中のクラス$\\omega_i$に属するデータを数える最尤推定で求めればよいので，とくに問題はありません．しかし，尤度$p(x_j \\vert \\omega_i)$に関しては，求めるものが「クラス$\\omega_i$のデータの属性$a_j$が値$x_j$となる確率」で，$x_j$が連続値なので，頻度を数えるという方法を用いることはできません．そこで，数値特徴に対しては，尤度を計算する確率密度関数に適切な統計モデルをあてはめ，そのモデルのパラメータを学習データから推定するという方法を取ります．数値データに対する統計的モデル化は，それだけで一冊の本になるぐらい奥が深い問題です．本書では，教師なし学習におけるモデル推定のところで少し詳しく説明するので，ここでは，最も単純な方法で考えます．様々な数値データに対して多く用いられる統計モデルが正規分布です．正規分布は図5.4に示すような釣り鐘型をした分布で，身長・体重の分布や，多人数が受けるテストの点数の分布などがよく当てはまります．1次元データの正規分布は，式(5.2)のようになります．ここで，$\\mu$は正規分布の平均値，$\\sigma$は標準偏差です．この二つを正規分布のパラメータとよび，パラメータの値が決まると，$p(x)$の関数形が決まります．このような仮定をおいたときの学習は，正規分布の平均値と標準偏差を学習データから推定するという問題になります．これは，カテゴリカルデータの頻度による推定と同様の考え方で，学習データの平均値をモデルの平均値，学習データの標準偏差をモデルの標準偏差とすることで，最尤推定になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 595,
                                    "text": "様々な数値データに対して多く用いられる統計モデル"
                                }
                            ],
                            "id": "b7db7d9a-1101-46d8-9014-71bebebc3a0c",
                            "question": "正規分布ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0503"
        },
        {
            "paragraphs": [
                {
                    "context": "上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワークの利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの確率計算の例を示します．最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)がHead-to-tail connectionでつながっている例を示します．これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．何も情報がない状態での「芝生が濡れている」確率は以下のようになります．まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って「芝生が濡れている」確率$P(W)$を求めます．ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，以下のようになります．つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していないことになります．なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，その日が「曇っている」確率は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 57,
                                    "text": "変数間の独立性を表現できること"
                                }
                            ],
                            "id": "81be6a4b-c8f5-4097-b545-37f879dbce73",
                            "question": "ベイジアンネットワークの利点とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0413"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 395,
                                    "text": "もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです"
                                }
                            ],
                            "id": "6a3a90aa-a3bc-458b-a252-a92aee38621d",
                            "question": "二次元から三次元の変換・写像で気をつけることはなんですか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "次に考えられる手法としては，区分線形よりももっとなめらかな非線形関数を用いて回帰式を得られないか，ということになるのですが，一般に非線形式ではデータにフィットしすぎてしまうため，過学習が問題になります．そこで，SVMのところでも説明したカーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチが有効になります．この手法については，文献\\cite{akaho08}に詳しく書かれています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 62,
                                    "text": "一般に非線形式ではデータにフィットしすぎてしまうため"
                                }
                            ],
                            "id": "295a9a1c-7362-4fda-bdb7-0461ce4e5333",
                            "question": "なぜ回帰問題にカーネル法を使うんですか"
                        }
                    ]
                }
            ],
            "title": "0616"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる特徴の尤もらしさを表す尤度はわかりません．そこで，この事前確率や尤度を計算する確率モデルを仮定し，そのパラメータを学習データに最も合うように調整することを考えます．それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します．学習データ全体$D$が生成される確率$P(D)$は，個々の事例$\\{\\bm{x}_1,\\dots,\\bm{x}_N\\}$の独立性、すなわち i.i.d. を仮定すると，式(4.5)のように，個々の事例が生成される確率の積で求めることができます．$P$は，データの生成確率を何らかのパラメータに基づいて計算するモデルです．ある程度複雑なモデルでは，パラメータが複数あることが一般的なので，これらのパラメータをまとめて$\\bm{\\theta}$と表記して明示すると，式(4.5)は式(4.6)のように書けます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 128,
                                    "text": "それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します"
                                }
                            ],
                            "id": "a2d1647d-3625-4498-9274-6f73d2b7d15d",
                            "question": "i.i.d. (independent and identically distributed) とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0406"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習に用いられるニューラルネットワークは，問題に応じてさまざまな形に特化してゆきました．本章では，その特化した構造を，多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワークに分類して説明をします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク"
                                }
                            ],
                            "id": "a205be5a-ed92-4ac8-a499-a6bc885e547e",
                            "question": "深層学習に用いられるニューラルネットワークはどのような種類がありますか"
                        }
                    ]
                }
            ],
            "title": "0903"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 412,
                                    "text": "入力層・出力層の数に応じた適当な数"
                                }
                            ],
                            "id": "203e50b6-6378-4f5d-a9a8-47ea45886878",
                            "question": "中間層はいくつ用意しますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "この与えられた系列を$\\bm{x}$として，クラス$y$（ただし，$y= B (初心者) or S (熟練者)$）の事後確率$P(y|\\bm{x})$を何らかのモデルを使って計算することを考えます．ここで式(13.4)のような$\\bm{x}, y$の同時確率を考える生成モデルアプローチをとるのが\\textbf{HMM}(Hidden Marcov Model: 隠れマルコフモデル)の考え方です．HMMは，式(13.4)の$P(\\bm{x}|y)$の値を与える確率的非決定性オートマトンの一種です．各状態であるシンボルをある確率で出力し，ある確率で他の状態(あるいは自分自身)に遷移します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 161,
                                    "text": "Hidden Marcov Model: 隠れマルコフモデル"
                                }
                            ],
                            "id": "f68cc457-8609-4e76-8d56-b39e90f6ac40",
                            "question": "HMMとは何の略称ですか"
                        }
                    ]
                }
            ],
            "title": "1310"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 252,
                                    "text": "カーネルトリック"
                                }
                            ],
                            "id": "a2734c61-292a-461f-bb4e-4b33ea4c82a3",
                            "question": "カーネル関数を定めて識別面を得る方法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 271,
                                    "text": "生物の神経細胞の仕組みをモデル化したもの"
                                }
                            ],
                            "id": "96e1ed2b-e34d-4bbd-ab72-d9027bbec649",
                            "question": "パーセプトロンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術"
                                }
                            ],
                            "id": "f54a60c8-9f90-41f8-85e2-3cc84b2b0368",
                            "question": "パターン認識とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "式(15.4)は，無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したものです．この再帰方程式をベルマン方程式 (Bellman equation)といいます．状態遷移確率を明示的にすると，ベルマン方程式は以下のように書き換えられます．さらに，式(15.5)を，Q値を用いて書き換えると，以下のようになります．求めるべき最適政策は，Q値を用いて，以下のように表現できます．後は，どのようにしてQ値を推定するか，という問題になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したものです"
                                }
                            ],
                            "id": "987f6c32-bc3a-47a0-bcd3-63353609e9a1",
                            "question": "ベルマン方程式とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1508"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア"
                                }
                            ],
                            "id": "e407400d-b9fd-4000-8db8-f43dab4d0081",
                            "question": "事前学習法の考え方は何ですか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチ"
                                }
                            ],
                            "id": "7edf5270-1cea-4738-a5e4-ae48981a1419",
                            "question": "識別モデルってなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 340,
                                    "text": "特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ"
                                }
                            ],
                            "id": "f9d1b8a8-9af7-422f-8013-78563291c4ba",
                            "question": "深層学習のポイントは何"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングはデータマイニングともよばれ，ビッグデータ活用の一つとして注目を集めているものです．パターンマイニングの応用例としては，ネットショッピングサイトなどでの「おすすめ商品」の提示や，データからの連想規則（あるいは相関規則）の抽出による新たな知見の獲得などが試みられています．この章では，まず，パターンマイニングの基本的な手法であるApriori（アプリオリ）アルゴリズムとその高速化版であるFP-Growthについて説明します．次に，問題設定を推薦システムに絞り，協調フィルタリングとMatrix Factorizationについて概説します．この章で扱う問題は，カテゴリからなる特徴ベクトルに対して，正解が付与されていない状況（すなわち「教師なし」の状況）で，そのデータに潜んでいる有用なパターンを見つけてくる，というものです（図12.1）．一般にパターンマイニングとよばれます．パターンマイニングの基本技術は頻出項目抽出です．これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．この頻出項目から，連想規則抽出をおこなうことができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 426,
                                    "text": "データ集合中に一定頻度以上で現れるパターンを抽出する手法"
                                }
                            ],
                            "id": "02e46970-5234-42eb-8c8d-76e0f490cb36",
                            "question": "頻出項目抽出とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1201"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 654,
                                    "text": "最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です"
                                }
                            ],
                            "id": "74bde735-e10b-40d8-8963-7e38ab5b7557",
                            "question": "最急勾配法ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 452,
                                    "text": "規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比"
                                }
                            ],
                            "id": "2e62d214-729e-4dcd-a60b-4167347a2ada",
                            "question": "リフト値とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 24,
                                    "text": "ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの"
                                }
                            ],
                            "id": "b3301ba5-6e07-4f17-8424-c144390161d8",
                            "question": "多階層ニューラルネットワークとは何か"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "4c14e30c-8120-44f7-b240-c6811c69e961",
                            "question": "サポートベクトルマシンとは何か"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "データから規則や知見を得る機械学習技術のなかでも，特に深層学習 (deep learning)は，高い性能を実現する方法として近年注目を集めています．深層学習は，一般に隠れ層を多くもつニューラルネットワーク（図1.3）によって実装されています．深層学習が他の機械学習手法と異なるのは，深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです．近年の深層学習の流行を見ると，他の機械学習技術はもう不要に見えるかもしれません．しかし，深層学習がその強さを発揮しているのは，音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題であるという傾向があります．さまざまな問題に対して機械学習アルゴリズムの性能を競うサイトでは，深層学習と並んで勾配ブースティングなどの手法が上位を占めることがあります．また一方で，性能は多少低くてもよいので判定結果に至るプロセスがわかりやすい手法や，運用後のチューニングが容易な手法が好まれる場合もあり，さまざまな状況でさまざまな問題に取り組むためには，深層学習だけではなく機械学習手法全般に関して理解しておくことが必要であるといえます．本書では機械学習全般に関して，設定した問題に対する基本的な手法の概要と，フリーソフトを用いた例題の解法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 142,
                                    "text": "深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです"
                                }
                            ],
                            "id": "e4bfed02-79d4-4f75-b457-25a79304a208",
                            "question": "深層学習が他の機械学習手法と異なる点はなんですか"
                        }
                    ]
                }
            ],
            "title": "0104"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから"
                                }
                            ],
                            "id": "3b0bfca4-d599-4c66-b3f8-6d50b91e0511",
                            "question": "なぜノードを階層的に組むと非線形識別面が実現できるのですか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 288,
                                    "text": "片方を減らせば片方が増える"
                                }
                            ],
                            "id": "3f6f7100-6156-4f81-9842-09b637b28788",
                            "question": "トレードオフってなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワークの利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの確率計算の例を示します．最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)がHead-to-tail connectionでつながっている例を示します．これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．何も情報がない状態での「芝生が濡れている」確率は以下のようになります．まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って「芝生が濡れている」確率$P(W)$を求めます．ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，以下のようになります．つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していないことになります．なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，その日が「曇っている」確率は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 57,
                                    "text": "変数間の独立性を表現できること"
                                }
                            ],
                            "id": "11f05560-cb18-45f3-aa0a-d4a60cf2d45e",
                            "question": "ベイジアンネットワークの利点はなんですか"
                        }
                    ]
                }
            ],
            "title": "0413"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，対数尤度$\\mathcal{L}(D)$を最大にするパラメータ$\\hat{\\theta}$は，$d \\mathcal{L}(D) / d\\theta = 0$の解として式(4.10)のように求めることができます．式(4.10)右辺の分子は値1をとる事例数，分母は全事例数です．このように，推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 148,
                                    "text": "推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます"
                                }
                            ],
                            "id": "fb23d005-5c87-4862-86c6-38863625133c",
                            "question": "最尤推定法ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0409"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 601,
                                    "text": "各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します"
                                }
                            ],
                            "id": "eb340dc5-faa9-4097-9cf6-d0a9d1776e3d",
                            "question": "各識別器の結果は、どうするのですか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "aeb6b131-1900-4c1e-a655-0abcab250836",
                            "question": "ディープニューラルネットワークとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，強化学習について説明します．強化学習は，教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので，中間的学習として位置付けています（図15.1）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので"
                                }
                            ],
                            "id": "ce781ce8-8b19-4e1e-a56f-7937bfc19c10",
                            "question": "強化学習はなぜ中間的学習なのですか"
                        }
                    ]
                }
            ],
            "title": "1501"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "41e819e7-ff2b-42a8-9400-9972b8f1b41a",
                            "question": "サポートベクトルマシンってなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 219,
                                    "text": "もとの空間におけるデータ間の距離関係を保存"
                                }
                            ],
                            "id": "afb5250e-05b7-4f78-b55e-fd3db73d4538",
                            "question": "サポートベクトルマシンで高次元に非線形変換する際の条件はなに"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります"
                                }
                            ],
                            "id": "76e61141-490b-4690-85c6-e2c4770c0990",
                            "question": "特徴空間の次元数$d$が大きい場合はどうなりますか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 204,
                                    "text": "仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと"
                                }
                            ],
                            "id": "2c563c39-6cbe-4d33-ae6d-63732eda496f",
                            "question": "概念学習が失敗するのはどういう理由ですか"
                        }
                    ]
                }
            ],
            "title": "0306"
        },
        {
            "paragraphs": [
                {
                    "context": "ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく，というのが半教師あり学習の基本的な進め方です（この章の最後に紹介するYATSIアルゴリズムは例外です）．識別器を作成するアルゴリズムはこれまで紹介してきたものを問題に応じて用いればよいのですが，信用できる出力をする教師なしデータを次回の識別器作成に取り込むためには，ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切です．一方，繰り返しアルゴリズムに関して，単純に終了のための閾値チェックをするだけなのか，識別器のパラメータを繰り返しの度に変化させるか，識別器で使う特徴に制限をかけるか，など様々な設定が可能です．以下では，繰り返しアルゴリズムの違いによって生じる，様々な半教師あり学習手法について説明してゆきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく"
                                }
                            ],
                            "id": "46e6be0b-4e8f-44e2-932c-a9b30db20fdf",
                            "question": "半教師ある学習の基本的な進め方はどういったものですか"
                        }
                    ]
                }
            ],
            "title": "1406"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムで用いた情報獲得量は，値の種類が多い特徴ほど大きな値になる傾向があります．一般に，その性質は悪いものではないのですが，値の種類が極端に多い場合には問題があります．例えば表3.3の特徴として，日付(date)があったとし，その値が全てのデータで異なっているとします．この場合，dateによって分割した集合は要素数が1となって，そのエントロピーは0となりますので，$\\mbox{Gain}(D, date)$の値は最大値である$E(D)$になって，この特徴が決定木のルートに選ばれることになります．こうして出来た決定木ではテスト例は分類できません．そこで，分割の程度を式(3.3)によって評価し，分割が少ない方が有利になるように式(3.4)で定義された獲得率を用いて特徴を選択することもあります．また，学習データの性質や学習の目的によって，データの乱雑さを評価する基準も変化することがあります．データの乱雑さを不純度(impurity)と定義すると，先述のエントロピー以外にいくつかの可能性を考えることができます．式(3.5)で計算されるGini Inpurityは，分割後のデータの分散を表します．この性質は回帰木の作成で用いますので，そこで再度，解説します．また，Gini Inpurityの平方根を取って，最大値がGini Inpurityと同じ0.5になるように係数を補正したものをRootGiniImpurityとして，式(3.6)で定義します．いずれも，分割前後の値の差によって選ぶ特徴を決めるのですが，獲得率やジニ不純度は，正例・負例の数に偏りがあると，多数派の性能の影響が大きくなってしまいます．一方，RootGiniImpurityは，分割前のGini Inpurityと，分割後の重み付きGini Inpurityの比を計算していることになり，正例・負例の数に偏りがあっても，分割基準としては影響を受けないようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 491,
                                    "text": "分割後のデータの分散"
                                }
                            ],
                            "id": "3891948b-bd62-45ea-b131-e137161d8551",
                            "question": "Gini Inpurityは何を表しますか"
                        }
                    ]
                }
            ],
            "title": "0315"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，式(4.2)に基づいて得られた事後確率の計算式を，記号を変えてもう一度見直してみます．式(5.3)の分子は，生成モデルとよばれる考え方で解釈することができます．まず，あるクラス$\\omega_i$が確率$P(\\omega_i)$で選ばれ，そのクラスから特徴ベクトル$\\bm{x}$が確率$p(\\bm{x} \\vert \\omega_i)$に基づいて生成されたという考え方です．これは式(5.4)の分子である特徴ベクトルとクラスの同時確率$p(\\omega_i, \\bm{x})$を求めていることになります．この生成モデルアプローチは，（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合には有効です．しかし，そうでない場合は，推定するべきパラメータは，$P(\\omega_i|\\bm{x})$を直接推定するよりも増えてしまいます．同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなるのが一般的です．つまり，生成モデルアプローチは，本来解くべき問題を，あえて難しい問題にしてしまっているのではないかという疑問が出てくるわけです．この問題への対処法として，次節では，$P(\\omega_i|\\bm{x})$を直接推定する方法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 394,
                                    "text": "同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる"
                                }
                            ],
                            "id": "8251802e-f72a-4462-a825-a2285aca74f6",
                            "question": "なぜ生成モデルアプローチは問題を難しくしているといえるのですか"
                        }
                    ]
                }
            ],
            "title": "0504"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 487,
                                    "text": "誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない"
                                }
                            ],
                            "id": "171a30ad-0c64-44d7-839c-22519ca85ea8",
                            "question": "階層が多いニューラルネットワークの学習の問題点は？"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 67,
                                    "text": "線形回帰式"
                                }
                            ],
                            "id": "505d26ff-aebe-4b1c-9de1-7c358f1c49d6",
                            "question": "モデル木ではリーフの値をどうしますか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "Grid search"
                                }
                            ],
                            "id": "179544d1-00ab-4da8-86bd-21d595308bca",
                            "question": "パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 96,
                                    "text": "グリッド"
                                }
                            ],
                            "id": "c7e892f0-59bd-4f52-a724-6fc55e933835",
                            "question": "複数のパラメータを組合わせる空間を何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "23aaac20-2c3f-43f2-adf1-3461df1d5817",
                            "question": "深層学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．"
                                }
                            ],
                            "id": "85ebe6d2-ef3b-46ec-b9d3-b65d0d4cfa30",
                            "question": "誤差逆伝播法による多階層ネットワークの学習は何故難しいのか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "もうひとつのタスクに特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつリカレントニューラルネットワーク（図9.10(a)）があります．リカレントニューラルネットワーは時系列信号や自然言語などの系列パターンを扱うことができます．このリカレントニューラルネットワークへの入力は，特徴ベクトルの系列$\\bm{x}_1,\\bm{x}_2,\\dots, \\bm{x}_T $という形式になります．たとえば，動画像を入力して異常検知を行ったり，ベクトル化された単語系列を入力して品詞列を出力するようなタスクが具体的に考えられます．これらに共通していることは，単純に各時点の入力からだけでは出力を決めることが難しく，それまでの入力系列の情報が何らかの役に立つという点です．リカレントニューラルネットワークの中間層は，入力層からの情報に加えて，一つ前の中間層の活性化状態を入力とします．この振舞いを時間方向に展開したものが，図9.10(b)です．時刻$t$における出力は，時刻$t-1$以前のすべての入力を元に計算されるので，これが深い構造をもっていることがわかります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 109,
                                    "text": "時系列信号や自然言語などの系列パターンを扱うことができます．"
                                }
                            ],
                            "id": "7258e121-d8b3-4160-86fd-e45a437c7ff8",
                            "question": "リカレントネットワークの特徴は何ですか"
                        }
                    ]
                }
            ],
            "title": "0916"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとする"
                                }
                            ],
                            "id": "917d61df-0726-459d-9898-04a4b0a171c3",
                            "question": "深層学習と他の識別問題との違いはなんですか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "前後の入力や一つ前の出力など，役に立ちそうな特徴を利用し，かつ系列としての確からしさを評価しながら探索的に出力を求める手法として，識別モデルの一つである対数線型モデルを応用する方法を考えます．対数線型モデルに基づくと，入力$\\bm{x}$が与えられたときの出力$\\bm{y}$の条件付き確率は，式(13.1)のように表現できます．ここで$\\phi(\\bm{x}, \\bm{y})$は素性ベクトルで，各次元は$\\bm{x}, \\bm{y}$から定められる様々な素性，$\\bm{w}$はそれらの素性の重みからなる重みベクトル，$Z_{\\bm{x},\\bm{w}}$は$Z_{\\bm{x},\\bm{w}}=\\sum_{\\bm{y}} exp(\\bm{w}\\cdot\\phi(\\bm{x}, \\bm{y}))$で定義される定数で，$\\sum_{\\bm{y}}P(\\bm{y}|\\bm{x})=1$を保証するためのものです．そして，式(13.1)の条件付き確率を用いると，出力$\\bm{y}^*$は式(13.2)の最大化問題を解くことによって求まります．この段階では，素性関数としては前後の入力や出力を自由に組み合わせることができるので，系列としての情報を反映したものを設定することができ，上記の問題点1は解決したように見えます．しかし，式(13.2)ではすべての可能な$\\bm{y}$についての値を計算する必要があるので，まだ問題点2が解決していません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "前後の入力や一つ前の出力など，役に立ちそうな特徴を利用し，かつ系列としての確からしさを評価しながら探索的に出力を求める手法"
                                }
                            ],
                            "id": "2124db60-a8e5-4bc3-9904-a17e859b979f",
                            "question": "識別モデルの一つである対数線型モデルを応用する方法とは具体的にどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1305"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習とは，正解情報が付けられていないデータを対象に行う学習です．データの集合を，以下のように定義します．この章では，この特徴ベクトルの要素がすべて数値である場合を考えます．要素がすべて数値であるということは，特徴ベクトルを$d$次元空間上の点として考えることができます．そうすると，モデル推定は，特徴空間上にあるデータのまとまりを見つける問題ということになります．データがまとまっている，ということは，共通の性質をもつようにみえる，ということなので，図11.2のように，このような個々のデータを生じさせた共通の性質を持つクラスを見つけることが目標になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "正解情報が付けられていないデータを対象に行う学習"
                                }
                            ],
                            "id": "70a33bcb-9d4e-43a3-9c88-54a8bc1d715e",
                            "question": "教師なし学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1102"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする"
                                }
                            ],
                            "id": "5dcdf14c-9593-48c7-94e2-edbb22a7781c",
                            "question": "機械学習では何ができますか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 307,
                                    "text": "ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法"
                                }
                            ],
                            "id": "4387b802-4436-4172-b951-2c618847f26d",
                            "question": "確率的最急勾配法ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで紹介する局所異常因子 (LOF: Local Outlier Factor)の考え方は，単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．ただし，この「近く」という概念は，データの散らばり具合によって異なるので，一定の閾値をあらかじめ定めておくことはできません．そこで，それぞれのデータにとっての「周辺」を，$k$番目までに近いデータがある範囲と定義し，周辺にあるデータまでの距離の平均を，「周辺密度」として定義します（図11.13）．そして，あるデータの周辺密度が，近くの$k$個のデータの周辺密度の平均と比べて極端に低いときに，そのデータを外れ値とみなします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 47,
                                    "text": "単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．"
                                }
                            ],
                            "id": "22c59709-e55b-44bc-bb5f-057125e0a19a",
                            "question": "局所異常因子とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1112"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 295,
                                    "text": "入力データ集合から適切なまとまりを作ることでクラスを推定する手法"
                                }
                            ],
                            "id": "c77fe4c0-5b38-4e18-aa00-817d3d893012",
                            "question": "クラスタリングってなんですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 219,
                                    "text": "もとの空間におけるデータ間の距離関係を保存"
                                }
                            ],
                            "id": "25383226-61ba-4e85-bbb5-f75b0c764643",
                            "question": "特徴空間を高次元に変換する際に重要な条件はなんですか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 613,
                                    "text": "動画像の分類や音声で入力された単語の識別などの問題が考えられます"
                                }
                            ],
                            "id": "243f96ac-2e0b-42e8-b11e-f801b7651978",
                            "question": "系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです"
                                }
                            ],
                            "id": "a4f0fb39-de3a-41ed-a168-3db3046b56b4",
                            "question": "畳み込みニューラルネットワークとはどんなものか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 406,
                                    "text": "勾配消失問題"
                                }
                            ],
                            "id": "ee8684bc-ebcf-4f47-ac9a-6016394d50dd",
                            "question": "多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 49,
                                    "text": "判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法"
                                }
                            ],
                            "id": "a7c30bfb-5d93-49b6-8b0c-2d53fef1e9e8",
                            "question": "共訓練ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 689,
                                    "text": "条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます"
                                }
                            ],
                            "id": "ac98d6ae-a61e-4808-a512-0751a17f8013",
                            "question": "条件付き確率ってどうやって求めるんですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 809,
                                    "text": "非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法"
                                }
                            ],
                            "id": "40ab4288-d0e3-4aa0-9657-f1c111f5cda4",
                            "question": "ニューラルネットワークってなんですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，「入力の系列長に関わらず出力の系列長が1である問題」を扱います．出力が一つになったので，ラベルの膨大な組合せを扱う前節の設定よりもやさしく感じるかもしれません．しかし，この問題の難しさは，学習の際に観測されない隠れ変数の値を用いなければならない，という点にあります．まず，簡単な例題を考えてみましょう．PCで文書作成を行っているユーザのキー入力・マウス操作をシンボルで表して，その系列で初心者と熟練者を識別する問題を考えます．入力はキー入力・マウス操作を抽象化したもので，10以上の連続通常キー入力k，エラーキー（DeleteキーやBack spaceキー）入力e，ファイル保存や文字修飾などのGUI操作をgとします．ここで知見として，初心者はキー入力kとGUI入力gを頻繁に繰り返し，かつ時間が経過するにつれてエラーeが増える傾向にあるとします．また，熟練者は最初にキー入力を重点的に，後からGUI入力をまとめて行う，という傾向があるとします．初心者Bさんの操作記録は以下のようなものでした．\\begin{quote}\\tt k e k g k e k g g k g k k e g e e k e e e g e\\end{quote}一方，熟練者Sさんの操作記録は以下のようなものでした．\\begin{quote}\\tt k k e k g k k k e k g k g g g e g k g\\end{quote}そして，問題として以下のような系列が観測されたとき，この人は初心者か，熟練者かを識別するという状況を考えます．\\begin{quote}\\tt k g e k g k k g e k g e k e e k e g e k\\end{quote}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 97,
                                    "text": "学習の際に観測されない隠れ変数の値を用いなければならない，という点"
                                }
                            ],
                            "id": "d50a3710-f1db-4db1-8ce8-6ff57c25a292",
                            "question": "入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか"
                        }
                    ]
                }
            ],
            "title": "1309"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく"
                                }
                            ],
                            "id": "d4294682-5ce1-4516-961c-897d546f34a1",
                            "question": "勾配消失問題とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "制約を弱める変数"
                                }
                            ],
                            "id": "96a92f7a-09b2-4573-a45e-8afa3fcef4a5",
                            "question": "スラック変数はなにをするものか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法です．スーパーマーケットなどで同時に購入される商品の組み合わせを発見するバスケット分析が代表的な応用例です．図1.10にパターンマイニングの考え方を示します．パターンマイニングの敵は膨大な計算量です．まさに，大量のデータの中から，貴重な知見をマイニング（＝発掘）する作業です．図1.10に示した例では，発見された規則の条件部も結論部も要素数が一つなので，すべての商品の組み合わせに対してその出現頻度を計算することは，それほど膨大な計算量にはみえません．しかし，一般的なパターンマイニングでは，条件部・結論部のいずれも要素の集合となります．それらのあらゆる組み合わせに対して，マイニングの対象となる大きなデータ集合から出現数を数えあげなければならないので，単純な方法では気の遠くなるような計算量になってしまいます．そこで効率よく頻出パターンを見つけ出す手法が必要になります．パターンマイニングの代表的な手法としては Apriori アルゴリズムやその高速化版である FP-Growth があります．これらを第12章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法"
                                }
                            ],
                            "id": "5a3c1929-6c49-4865-a260-4f2d17658207",
                            "question": "パターンマイニングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0115"
        },
        {
            "paragraphs": [
                {
                    "context": "機械学習において与えられるデータは，個々の事例です．その個々の事例から，あるクラスについて共通点を見つけることが，概念学習です．共通点は，特徴の値の組み合わせによって表現されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 28,
                                    "text": "個々の事例から，あるクラスについて共通点を見つけることが，概念学習です"
                                }
                            ],
                            "id": "ff412f4f-176e-4b9e-9f33-3685facd1696",
                            "question": "概念学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0304"
        },
        {
            "paragraphs": [
                {
                    "context": "ラベル伝搬法の考え方は，特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築するというものです．近くのノードは同じクラスになりやすいという仮定で，正解なしデータの予測を行います．評価関数は式(14.1)に示すもので，この評価関数の最小化をおこないます．$f_i$は$i$番目のノードの予測値，$y_i$は$i$番目のノードの正解ラベル{ -1, 0, 1}，$w_{ij}$は$i$番目のノードと$j$番目のノードの結合の有無を表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する"
                                }
                            ],
                            "id": "9231a856-dac4-4b17-aaa5-c66eee55c3ae",
                            "question": "ラベル伝搬法の考え方はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1412"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 287,
                                    "text": "十分多くの層"
                                }
                            ],
                            "id": "02d2b931-a100-431d-8bf1-0b995efc924d",
                            "question": "特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか"
                        }
                    ]
                }
            ],
            "title": "0906"
        }
    ],
    "version": "1.0"
}