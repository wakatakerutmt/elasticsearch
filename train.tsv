Quality	#1 ID	#2 ID	#1 String	#2 String
1	0102	0102	人工知能と機械学習は同じ意味ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0102	1001	人工知能と機械学習は同じ意味ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0102	1110	人工知能と機械学習は同じ意味ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0102	1108	人工知能と機械学習は同じ意味ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0102	0505	人工知能と機械学習は同じ意味ですか	生物の神経細胞の仕組みをモデル化したもの
0	0102	1412	人工知能と機械学習は同じ意味ですか	近くのノードは同じクラスになりやすいという仮定
0	0102	0805	人工知能と機械学習は同じ意味ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0102	1403	人工知能と機械学習は同じ意味ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0102	0416	人工知能と機械学習は同じ意味ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0102	0405	人工知能と機械学習は同じ意味ですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0102	0917	人工知能と機械学習は同じ意味ですか	LSTM
0	0102	1411	人工知能と機械学習は同じ意味ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0102	0712	人工知能と機械学習は同じ意味ですか	カーネル関数が正定値関数という条件を満たすとき
0	0102	0505	人工知能と機械学習は同じ意味ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0102	0105	人工知能と機械学習は同じ意味ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0102	0514	人工知能と機械学習は同じ意味ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0102	1503	人工知能と機械学習は同じ意味ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0102	0509	人工知能と機械学習は同じ意味ですか	確率的最急勾配法
0	0102	0306	人工知能と機械学習は同じ意味ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0102	0507	人工知能と機械学習は同じ意味ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0102	0912	人工知能と機械学習は同じ意味ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0102	0701	人工知能と機械学習は同じ意味ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0102	1303	人工知能と機械学習は同じ意味ですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0102	0713	人工知能と機械学習は同じ意味ですか	文書分類やバイオインフォマティックスなど
0	0102	0114	人工知能と機械学習は同じ意味ですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
1	0115	0115	パターンマイニングって何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0115	1209	パターンマイニングって何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0115	1501	パターンマイニングって何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0115	0610	パターンマイニングって何ですか	学習結果の散らばり具合
0	0115	0311	パターンマイニングって何ですか	集合の乱雑さ
0	0115	0411	パターンマイニングって何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0115	0602	パターンマイニングって何ですか	正解情報$y$が数値であるということ
0	0115	1508	パターンマイニングって何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0115	0916	パターンマイニングって何ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0115	0810	パターンマイニングって何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0115	0209	パターンマイニングって何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0115	0601	パターンマイニングって何ですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0115	1509	パターンマイニングって何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0115	0606	パターンマイニングって何ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0115	0111	パターンマイニングって何ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0115	0701	パターンマイニングって何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0115	1012	パターンマイニングって何ですか	すべてのデータの重みは平等
0	0115	0717	パターンマイニングって何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0115	0901	パターンマイニングって何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0115	0109	パターンマイニングって何ですか	学習データに正解が付いている場合の学習
0	0115	0912	パターンマイニングって何ですか	画像認識
0	0115	0508	パターンマイニングって何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0115	1411	パターンマイニングって何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0115	0211	パターンマイニングって何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0115	0404	パターンマイニングって何ですか	事前確率
1	0109	0109	教師あり学習とはなんですか	学習データに正解が付いている場合の学習
0	0109	1219	教師あり学習とはなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0109	1310	教師あり学習とはなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	0109	0103	教師あり学習とはなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0109	0115	教師あり学習とはなんですか	パターンマイニング
0	0109	0402	教師あり学習とはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0109	0610	教師あり学習とはなんですか	真のモデルとの距離
0	0109	0901	教師あり学習とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0109	0801	教師あり学習とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0109	1106	教師あり学習とはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0109	0701	教師あり学習とはなんですか	サポートベクトルマシン
0	0109	0908	教師あり学習とはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0109	1407	教師あり学習とはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0109	0715	教師あり学習とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0109	0803	教師あり学習とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0109	0801	教師あり学習とはなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0109	0607	教師あり学習とはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0109	1502	教師あり学習とはなんですか	将棋や囲碁などを行うプログラム
0	0109	0903	教師あり学習とはなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0109	0616	教師あり学習とはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0109	0717	教師あり学習とはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0109	0510	教師あり学習とはなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0109	0701	教師あり学習とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0109	0115	教師あり学習とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0109	0907	教師あり学習とはなんですか	事前学習法
1	0110	0110	識別と回帰の違いはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0110	0601	識別と回帰の違いはなんですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0110	0211	識別と回帰の違いはなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0110	0411	識別と回帰の違いはなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0110	0509	識別と回帰の違いはなんですか	確率的最急勾配法
0	0110	0111	識別と回帰の違いはなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0110	0405	識別と回帰の違いはなんですか	尤度と事前確率の積を最大とするクラス
0	0110	1001	識別と回帰の違いはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0110	1104	識別と回帰の違いはなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0110	0717	識別と回帰の違いはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0110	0810	識別と回帰の違いはなんですか	勾配消失問題
0	0110	1510	識別と回帰の違いはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0110	1205	識別と回帰の違いはなんですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0110	0715	識別と回帰の違いはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0110	0810	識別と回帰の違いはなんですか	誤差が小さくなって消失してしまう
0	0110	0402	識別と回帰の違いはなんですか	最大事後確率則
0	0110	1301	識別と回帰の違いはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0110	0906	識別と回帰の違いはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0110	0805	識別と回帰の違いはなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0110	0803	識別と回帰の違いはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0110	0105	識別と回帰の違いはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0110	0805	識別と回帰の違いはなんですか	誤差逆伝播法
0	0110	0402	識別と回帰の違いはなんですか	統計的識別手法
0	0110	1304	識別と回帰の違いはなんですか	入力と対応させる素性
0	0110	1207	識別と回帰の違いはなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
1	0113	0113	教師なし学習の目的はなんですか	入力データに潜む規則性を学習すること
0	0113	0514	教師なし学習の目的はなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0113	0602	教師なし学習の目的はなんですか	正解情報$y$が数値であるということ
0	0113	1303	教師なし学習の目的はなんですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0113	0701	教師なし学習の目的はなんですか	線形で識別できないデータに対応するため
0	0113	0601	教師なし学習の目的はなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0113	0114	教師なし学習の目的はなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0113	0313	教師なし学習の目的はなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0113	0912	教師なし学習の目的はなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0113	0611	教師なし学習の目的はなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0113	0509	教師なし学習の目的はなんですか	確率的最急勾配法
0	0113	0109	教師なし学習の目的はなんですか	正解が付いていない場合の学習
0	0113	0614	教師なし学習の目的はなんですか	回帰木と線形回帰の双方のよいところを取った
0	0113	0606	教師なし学習の目的はなんですか	山の尾根という意味
0	0113	0311	教師なし学習の目的はなんですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0113	0209	教師なし学習の目的はなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0113	1301	教師なし学習の目的はなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0113	0917	教師なし学習の目的はなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0113	0701	教師なし学習の目的はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0113	1301	教師なし学習の目的はなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0113	1502	教師なし学習の目的はなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0113	0801	教師なし学習の目的はなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0113	0103	教師なし学習の目的はなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0113	1309	教師なし学習の目的はなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0113	1508	教師なし学習の目的はなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
1	0114	0114	モデル推定とはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0114	0810	モデル推定とはなんですか	勾配消失問題
0	0114	0610	モデル推定とはなんですか	真のモデルとの距離
0	0114	0209	モデル推定とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0114	0514	モデル推定とはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0114	1214	モデル推定とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0114	0908	モデル推定とはなんですか	ユークリッド距離
0	0114	0906	モデル推定とはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0114	0313	モデル推定とはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0114	1215	モデル推定とはなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0114	0708	モデル推定とはなんですか	制約を弱める変数
0	0114	0209	モデル推定とはなんですか	正例がどれだけ正しく判定されているかという指標
0	0114	0803	モデル推定とはなんですか	非線形識別面
0	0114	1502	モデル推定とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0114	0606	モデル推定とはなんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0114	1411	モデル推定とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0114	0614	モデル推定とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0114	1506	モデル推定とはなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0114	0111	モデル推定とはなんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0114	0417	モデル推定とはなんですか	ネットワークの構造とアークの条件付き確率
0	0114	0906	モデル推定とはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0114	1412	モデル推定とはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0114	1412	モデル推定とはなんですか	近くのノードは同じクラスになりやすいという仮定
0	0114	0510	モデル推定とはなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0114	0810	モデル推定とはなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
1	0114	0114	クラスタリングとはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0114	1403	クラスタリングとはなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0114	0204	クラスタリングとはなんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0114	0410	クラスタリングとはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0114	0801	クラスタリングとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0114	0601	クラスタリングとはなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0114	0209	クラスタリングとはなんですか	正例がどれだけ正しく判定されているかという指標
0	0114	1104	クラスタリングとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0114	1108	クラスタリングとはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0114	0304	クラスタリングとはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0114	0514	クラスタリングとはなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0114	0311	クラスタリングとはなんですか	集合の乱雑さ
0	0114	0810	クラスタリングとはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0114	0606	クラスタリングとはなんですか	入力が少し変化したときに，出力も少し変化する
0	0114	0805	クラスタリングとはなんですか	誤差逆伝播法
0	0114	0810	クラスタリングとはなんですか	誤差が小さくなって消失してしまう
0	0114	0715	クラスタリングとはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0114	1301	クラスタリングとはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0114	0901	クラスタリングとはなんですか	深層学習に用いるニューラルネットワーク
0	0114	0503	クラスタリングとはなんですか	様々な数値データに対して多く用いられる統計モデル
0	0114	0703	クラスタリングとはなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0114	0906	クラスタリングとはなんですか	多階層構造でもそのまま適用できます
0	0114	1010	クラスタリングとはなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0114	1301	クラスタリングとはなんですか	形態素解析処理が典型的な問題
0	0114	0802	クラスタリングとはなんですか	識別対象のクラス数
1	0114	0114	クラスタリングはどのようなことに使われますか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0114	0506	クラスタリングはどのようなことに使われますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0114	0810	クラスタリングはどのようなことに使われますか	勾配消失問題
0	0114	0906	クラスタリングはどのようなことに使われますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0114	1309	クラスタリングはどのようなことに使われますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0114	1506	クラスタリングはどのようなことに使われますか	各状態でどの行為を取ればよいのかという意思決定規則
0	0114	1106	クラスタリングはどのようなことに使われますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0114	0902	クラスタリングはどのようなことに使われますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0114	0701	クラスタリングはどのようなことに使われますか	線形で識別できないデータに対応するため
0	0114	1509	クラスタリングはどのようなことに使われますか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0114	0907	クラスタリングはどのようなことに使われますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0114	1114	クラスタリングはどのようなことに使われますか	クラスタリング結果のデータ数の分布から
0	0114	0901	クラスタリングはどのようなことに使われますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0114	0510	クラスタリングはどのようなことに使われますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0114	0409	クラスタリングはどのようなことに使われますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0114	0912	クラスタリングはどのようなことに使われますか	畳み込みニューラルネットワーク
0	0114	0908	クラスタリングはどのようなことに使われますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0114	0906	クラスタリングはどのようなことに使われますか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0114	0406	クラスタリングはどのようなことに使われますか	各クラスから生じる特徴の尤もらしさを表す
0	0114	1303	クラスタリングはどのようなことに使われますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0114	0105	クラスタリングはどのようなことに使われますか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0114	0713	クラスタリングはどのようなことに使われますか	文書分類やバイオインフォマティックスなど
0	0114	1409	クラスタリングはどのようなことに使われますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0114	1219	クラスタリングはどのようなことに使われますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0114	0508	クラスタリングはどのようなことに使われますか	二乗誤差を最小にするように識別関数を調整する方法
1	0115	0115	パターンマイニングとはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0115	0113	パターンマイニングとはなんですか	入力データに潜む規則性
0	0115	1112	パターンマイニングとはなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0115	1110	パターンマイニングとはなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0115	1209	パターンマイニングとはなんですか	この値が高いほど，得られる情報の多い規則であること
0	0115	0109	パターンマイニングとはなんですか	正解が付いていない場合の学習
0	0115	0509	パターンマイニングとはなんですか	確率的最急勾配法
0	0115	1406	パターンマイニングとはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0115	0614	パターンマイニングとはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0115	1104	パターンマイニングとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0115	0313	パターンマイニングとはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0115	1410	パターンマイニングとはなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0115	0112	パターンマイニングとはなんですか	線形回帰，回帰木，モデル木など
0	0115	1506	パターンマイニングとはなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0115	1015	パターンマイニングとはなんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0115	0206	パターンマイニングとはなんですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0115	0402	パターンマイニングとはなんですか	入力を観測した後で計算される確率
0	0115	0508	パターンマイニングとはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0115	0901	パターンマイニングとはなんですか	深層学習に用いるニューラルネットワーク
0	0115	1106	パターンマイニングとはなんですか	クラスタの重心間の距離を類似度とする
0	0115	0406	パターンマイニングとはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0115	0504	パターンマイニングとはなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0115	0508	パターンマイニングとはなんですか	最小二乗法
0	0115	1301	パターンマイニングとはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0115	0810	パターンマイニングとはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
1	0116	0116	半教師あり学習とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0116	1012	半教師あり学習とはなんですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0116	0810	半教師あり学習とはなんですか	勾配消失問題
0	0116	0614	半教師あり学習とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0116	0514	半教師あり学習とはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0116	0611	半教師あり学習とはなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0116	0111	半教師あり学習とはなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0116	1301	半教師あり学習とはなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0116	1306	半教師あり学習とはなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0116	1209	半教師あり学習とはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0116	0901	半教師あり学習とはなんですか	Deep Neural Network (DNN) 
0	0116	0313	半教師あり学習とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0116	0505	半教師あり学習とはなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0116	1406	半教師あり学習とはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0116	1103	半教師あり学習とはなんですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0116	0811	半教師あり学習とはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0116	1001	半教師あり学習とはなんですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0116	0614	半教師あり学習とはなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0116	0715	半教師あり学習とはなんですか	識別面
0	0116	0908	半教師あり学習とはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0116	1012	半教師あり学習とはなんですか	各データに重みを付け，そのもとで識別器を作成します
0	0116	0901	半教師あり学習とはなんですか	表現学習
0	0116	1112	半教師あり学習とはなんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0116	0616	半教師あり学習とはなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0116	0605	半教師あり学習とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
1	0117	0117	半教師あり学習はどんなときに使われますか	学習データの一部にだけ正解が与えられている場合
0	0117	0702	半教師あり学習はどんなときに使われますか	識別面は平面を仮定する
0	0117	0805	半教師あり学習はどんなときに使われますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0117	0409	半教師あり学習はどんなときに使われますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0117	1402	半教師あり学習はどんなときに使われますか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0117	1506	半教師あり学習はどんなときに使われますか	政策
0	0117	0901	半教師あり学習はどんなときに使われますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0117	0105	半教師あり学習はどんなときに使われますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0117	0917	半教師あり学習はどんなときに使われますか	入力ゲート・出力ゲート・忘却ゲート
0	0117	0701	半教師あり学習はどんなときに使われますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0117	0802	半教師あり学習はどんなときに使われますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0117	0513	半教師あり学習はどんなときに使われますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0117	1214	半教師あり学習はどんなときに使われますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0117	0715	半教師あり学習はどんなときに使われますか	複雑な非線形変換を求めるという操作を避ける方法
0	0117	0810	半教師あり学習はどんなときに使われますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0117	1015	半教師あり学習はどんなときに使われますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0117	0906	半教師あり学習はどんなときに使われますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0117	0611	半教師あり学習はどんなときに使われますか	回帰
0	0117	1103	半教師あり学習はどんなときに使われますか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0117	0701	半教師あり学習はどんなときに使われますか	識別境界線と最も近いデータとの距離
0	0117	1111	半教師あり学習はどんなときに使われますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0117	0708	半教師あり学習はどんなときに使われますか	制約を満たさない程度を表すので，小さい方が望ましい
0	0117	0710	半教師あり学習はどんなときに使われますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0117	0811	半教師あり学習はどんなときに使われますか	ユニットの活性化関数を工夫する方法があります
0	0117	0612	半教師あり学習はどんなときに使われますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
1	0204	0204	次元削減とはなんですか	特徴ベクトルの次元数を減らすこと
0	0204	1209	次元削減とはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0204	0802	次元削減とはなんですか	識別対象のクラス数
0	0204	0614	次元削減とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0204	1203	次元削減とはなんですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0204	0805	次元削減とはなんですか	誤差逆伝播法
0	0204	0506	次元削減とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0204	0704	次元削減とはなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0204	1219	次元削減とはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0204	0810	次元削減とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0204	0802	次元削減とはなんですか	特徴空間上では線形識別面を設定すること
0	0204	0209	次元削減とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0204	0109	次元削減とはなんですか	正解が付いていない場合の学習
0	0204	1103	次元削減とはなんですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0204	0508	次元削減とはなんですか	最小二乗法
0	0204	0708	次元削減とはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0204	0902	次元削減とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0204	0907	次元削減とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0204	1104	次元削減とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0204	1410	次元削減とはなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0204	0606	次元削減とはなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0204	0901	次元削減とはなんですか	表現学習
0	0204	0906	次元削減とはなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0204	0313	次元削減とはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0204	0811	次元削減とはなんですか	ユニットの活性化関数を工夫する方法があります
1	0204	0204	なぜ次元を削減するんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0204	0901	なぜ次元を削減するんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0204	1407	なぜ次元を削減するんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0204	0715	なぜ次元を削減するんですか	カーネルトリック
0	0204	1409	なぜ次元を削減するんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0204	0803	なぜ次元を削減するんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0204	1509	なぜ次元を削減するんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0204	1010	なぜ次元を削減するんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0204	0313	なぜ次元を削減するんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0204	0502	なぜ次元を削減するんですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0204	0514	なぜ次元を削減するんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0204	0917	なぜ次元を削減するんですか	入力ゲート・出力ゲート・忘却ゲート
0	0204	0313	なぜ次元を削減するんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0204	1106	なぜ次元を削減するんですか	最も近い事例対の距離を類似度とする
0	0204	1313	なぜ次元を削減するんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0204	1509	なぜ次元を削減するんですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0204	1001	なぜ次元を削減するんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0204	0405	なぜ次元を削減するんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0204	0907	なぜ次元を削減するんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0204	0311	なぜ次元を削減するんですか	集合の乱雑さ
0	0204	0502	なぜ次元を削減するんですか	SVM
0	0204	0508	なぜ次元を削減するんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0204	0805	なぜ次元を削減するんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0204	0305	なぜ次元を削減するんですか	仮説に対して課す制約
0	0204	1505	なぜ次元を削減するんですか	「マルコフ性」を持つ確率過程における意思決定問題
1	0205	0205	主成分分析とはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0205	1214	主成分分析とはなんですか	計算量が膨大であること
0	0205	0810	主成分分析とはなんですか	誤差が小さくなって消失してしまう
0	0205	1104	主成分分析とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0205	0305	主成分分析とはなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0205	0808	主成分分析とはなんですか	シグモイド関数の微分
0	0205	0911	主成分分析とはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0205	0506	主成分分析とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0205	0912	主成分分析とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0205	0416	主成分分析とはなんですか	値が真となる確率を知りたいノードが表す変数
0	0205	1506	主成分分析とはなんですか	後に得られる報酬ほど割り引いて計算するための係数
0	0205	0901	主成分分析とはなんですか	深層学習に用いるニューラルネットワーク
0	0205	0810	主成分分析とはなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0205	0612	主成分分析とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0205	1114	主成分分析とはなんですか	クラスタリング結果のデータ数の分布
0	0205	0810	主成分分析とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0205	0114	主成分分析とはなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0205	1110	主成分分析とはなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0205	0112	主成分分析とはなんですか	線形回帰，回帰木，モデル木など
0	0205	0906	主成分分析とはなんですか	多階層構造でもそのまま適用できます
0	0205	0102	主成分分析とはなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0205	0708	主成分分析とはなんですか	制約を弱める変数
0	0205	0406	主成分分析とはなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0205	1004	主成分分析とはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0205	1004	主成分分析とはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
1	0206	0206	分割学習法の際学習データが足りない場合どうしたらいいですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0206	0407	分割学習法の際学習データが足りない場合どうしたらいいですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0206	0115	分割学習法の際学習データが足りない場合どうしたらいいですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0206	1114	分割学習法の際学習データが足りない場合どうしたらいいですか	クラスタリング結果のデータ数の分布
0	0206	0316	分割学習法の際学習データが足りない場合どうしたらいいですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0206	0109	分割学習法の際学習データが足りない場合どうしたらいいですか	正解が付いていない場合の学習
0	0206	0610	分割学習法の際学習データが足りない場合どうしたらいいですか	真のモデルとの距離
0	0206	0701	分割学習法の際学習データが足りない場合どうしたらいいですか	サポートベクトルマシン
0	0206	0701	分割学習法の際学習データが足りない場合どうしたらいいですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0206	1503	分割学習法の際学習データが足りない場合どうしたらいいですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0206	0708	分割学習法の際学習データが足りない場合どうしたらいいですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0206	0607	分割学習法の際学習データが足りない場合どうしたらいいですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0206	0514	分割学習法の際学習データが足りない場合どうしたらいいですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0206	0114	分割学習法の際学習データが足りない場合どうしたらいいですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0206	0808	分割学習法の際学習データが足りない場合どうしたらいいですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0206	0311	分割学習法の際学習データが足りない場合どうしたらいいですか	集合の乱雑さ
0	0206	0402	分割学習法の際学習データが足りない場合どうしたらいいですか	最大事後確率則
0	0206	0701	分割学習法の際学習データが足りない場合どうしたらいいですか	識別境界線と最も近いデータとの距離
0	0206	0514	分割学習法の際学習データが足りない場合どうしたらいいですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0206	0505	分割学習法の際学習データが足りない場合どうしたらいいですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0206	1114	分割学習法の際学習データが足りない場合どうしたらいいですか	クラスタリング結果のデータ数の分布から
0	0206	0115	分割学習法の際学習データが足りない場合どうしたらいいですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0206	0701	分割学習法の際学習データが足りない場合どうしたらいいですか	識別境界線と最も近いデータとの距離
0	0206	0116	分割学習法の際学習データが足りない場合どうしたらいいですか	学習データが教師あり／教師なしの混在となっているもの
0	0206	1205	分割学習法の際学習データが足りない場合どうしたらいいですか	ある項目集合が頻出ならば，その部分集合も頻出である
1	0208	0208	k-NN法とはなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0208	0701	k-NN法とはなんですか	識別境界線と最も近いデータとの距離
0	0208	0715	k-NN法とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0208	0115	k-NN法とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0208	0810	k-NN法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0208	0906	k-NN法とはなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0208	0407	k-NN法とはなんですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0208	0710	k-NN法とはなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0208	0103	k-NN法とはなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0208	1309	k-NN法とはなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0208	0103	k-NN法とはなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0208	1502	k-NN法とはなんですか	将棋や囲碁などを行うプログラム
0	0208	0701	k-NN法とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0208	0402	k-NN法とはなんですか	事後確率
0	0208	0412	k-NN法とはなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0208	1205	k-NN法とはなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0208	0907	k-NN法とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0208	0701	k-NN法とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0208	0715	k-NN法とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0208	0509	k-NN法とはなんですか	確率的最急勾配法
0	0208	0911	k-NN法とはなんですか	ドロップアウト
0	0208	1304	k-NN法とはなんですか	入力と対応させる素性
0	0208	0612	k-NN法とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0208	1209	k-NN法とはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0208	0701	k-NN法とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0209	0209	精度とはなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0209	0901	精度とはなんですか	表現学習
0	0209	0111	精度とはなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0209	0811	精度とはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0209	1209	精度とはなんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0209	0407	精度とはなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0209	0105	精度とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0209	1404	精度とはなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0209	0505	精度とはなんですか	ニューラルネットワーク
0	0209	0602	精度とはなんですか	正解情報$y$が数値であるということ
0	0209	0502	精度とはなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0209	1408	精度とはなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0209	0405	精度とはなんですか	尤度と事前確率の積を最大とするクラス
0	0209	1407	精度とはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0209	0610	精度とはなんですか	真のモデルとの距離
0	0209	0514	精度とはなんですか	ランダムに学習データを一つ
0	0209	0605	精度とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0209	0810	精度とはなんですか	2006 年頃に考案された事前学習法
0	0209	1510	精度とはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0209	0614	精度とはなんですか	回帰木と線形回帰の双方のよいところを取った
0	0209	0701	精度とはなんですか	線形で識別できないデータに対応するため
0	0209	0402	精度とはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0209	0302	精度とはなんですか	カテゴリ形式の正解情報のこと
0	0209	0601	精度とはなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0209	0708	精度とはなんですか	制約を弱める変数
1	0209	0209	F値とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0209	0113	F値とはなんですか	入力データに潜む規則性を学習すること
0	0209	0115	F値とはなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0209	0606	F値とはなんですか	入力が少し変化したときに，出力も少し変化する
0	0209	1409	F値とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0209	0803	F値とはなんですか	非線形識別面
0	0209	1313	F値とはなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0209	0412	F値とはなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0209	0114	F値とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0209	0109	F値とはなんですか	学習データに正解が付いている場合の学習
0	0209	0404	F値とはなんですか	事前確率
0	0209	1409	F値とはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0209	0313	F値とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0209	1009	F値とはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0209	0416	F値とはなんですか	値が真となる確率を知りたいノードが表す変数
0	0209	0505	F値とはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0209	0206	F値とはなんですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0209	1207	F値とはなんですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0209	0811	F値とはなんですか	ユニットの活性化関数を工夫する方法があります
0	0209	0205	F値とはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0209	1501	F値とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0209	0707	F値とはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0209	0111	F値とはなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0209	1205	F値とはなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0209	0502	F値とはなんですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
1	0211	0211	ROC曲線とはなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0211	0502	ROC曲線とはなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0211	0506	ROC曲線とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0211	0911	ROC曲線とはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0211	0605	ROC曲線とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0211	0602	ROC曲線とはなんですか	正解情報$y$が数値であるということ
0	0211	0906	ROC曲線とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0211	1412	ROC曲線とはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0211	0402	ROC曲線とはなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0211	0614	ROC曲線とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0211	0411	ROC曲線とはなんですか	確率のm推定という考え方を用います
0	0211	1406	ROC曲線とはなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0211	1506	ROC曲線とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0211	0412	ROC曲線とはなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0211	0511	ROC曲線とはなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0211	0315	ROC曲線とはなんですか	分割後のデータの分散
0	0211	0710	ROC曲線とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0211	0805	ROC曲線とはなんですか	誤差逆伝播法
0	0211	0802	ROC曲線とはなんですか	特徴ベクトルの次元数
0	0211	0402	ROC曲線とはなんですか	統計的識別手法
0	0211	0808	ROC曲線とはなんですか	シグモイド関数の微分
0	0211	1306	ROC曲線とはなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0211	0205	ROC曲線とはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0211	0906	ROC曲線とはなんですか	多階層構造でもそのまま適用できます
0	0211	0510	ROC曲線とはなんですか	特徴空間上でクラスを分割する面
1	0304	0304	概念学習とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0304	1502	概念学習とはなんですか	将棋や囲碁などを行うプログラム
0	0304	0611	概念学習とはなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0304	0209	概念学習とはなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0304	0413	概念学習とはなんですか	変数間の独立性を表現できること
0	0304	0911	概念学習とはなんですか	ランダムに一定割合のユニットを消して学習を行う
0	0304	0701	概念学習とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0304	0315	概念学習とはなんですか	分割後のデータの分散
0	0304	1411	概念学習とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0304	1009	概念学習とはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0304	0715	概念学習とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0304	0906	概念学習とはなんですか	入力に近い側の処理
0	0304	0803	概念学習とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0304	0503	概念学習とはなんですか	様々な数値データに対して多く用いられる統計モデル
0	0304	0707	概念学習とはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0304	0105	概念学習とはなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0304	0802	概念学習とはなんですか	識別対象のクラス数
0	0304	0704	概念学習とはなんですか	以下の関数$L$の最小値を求めるという問題
0	0304	1004	概念学習とはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0304	0912	概念学習とはなんですか	畳み込みニューラルネットワーク
0	0304	0810	概念学習とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0304	0115	概念学習とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0304	1104	概念学習とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0304	1407	概念学習とはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0304	0110	概念学習とはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
1	0305	0305	FIND-Sアルゴリズムとはなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0305	0810	FIND-Sアルゴリズムとはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0305	0402	FIND-Sアルゴリズムとはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0305	0209	FIND-Sアルゴリズムとはなんですか	正例がどれだけ正しく判定されているかという指標
0	0305	0105	FIND-Sアルゴリズムとはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0305	1304	FIND-Sアルゴリズムとはなんですか	入力と対応させる素性
0	0305	0313	FIND-Sアルゴリズムとはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0305	1103	FIND-Sアルゴリズムとはなんですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0305	0114	FIND-Sアルゴリズムとはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0305	1106	FIND-Sアルゴリズムとはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0305	0906	FIND-Sアルゴリズムとはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0305	0505	FIND-Sアルゴリズムとはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0305	0811	FIND-Sアルゴリズムとはなんですか	半分の領域で勾配が1になるので
0	0305	0514	FIND-Sアルゴリズムとはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0305	0802	FIND-Sアルゴリズムとはなんですか	隠れ層
0	0305	0606	FIND-Sアルゴリズムとはなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0305	0902	FIND-Sアルゴリズムとはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0305	0505	FIND-Sアルゴリズムとはなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0305	0715	FIND-Sアルゴリズムとはなんですか	カーネルトリック
0	0305	0614	FIND-Sアルゴリズムとはなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0305	0810	FIND-Sアルゴリズムとはなんですか	2006 年頃に考案された事前学習法
0	0305	0801	FIND-Sアルゴリズムとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0305	0907	FIND-Sアルゴリズムとはなんですか	事前学習法
0	0305	0803	FIND-Sアルゴリズムとはなんですか	非線形識別面
0	0305	0902	FIND-Sアルゴリズムとはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
1	0306	0306	概念学習が失敗するのはどういう理由ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0306	0117	概念学習が失敗するのはどういう理由ですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0306	0405	概念学習が失敗するのはどういう理由ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0306	1310	概念学習が失敗するのはどういう理由ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0306	1507	概念学習が失敗するのはどういう理由ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0306	0810	概念学習が失敗するのはどういう理由ですか	勾配消失問題
0	0306	1012	概念学習が失敗するのはどういう理由ですか	すべてのデータの重みは平等
0	0306	1219	概念学習が失敗するのはどういう理由ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0306	0505	概念学習が失敗するのはどういう理由ですか	ニューラルネットワーク
0	0306	0508	概念学習が失敗するのはどういう理由ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0306	1505	概念学習が失敗するのはどういう理由ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0306	0606	概念学習が失敗するのはどういう理由ですか	入力が少し変化したときに，出力も少し変化する
0	0306	1306	概念学習が失敗するのはどういう理由ですか	条件付き確率場（Conditional Random Field: CRF）
0	0306	0402	概念学習が失敗するのはどういう理由ですか	入力を観測した後で計算される確率
0	0306	0206	概念学習が失敗するのはどういう理由ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0306	1407	概念学習が失敗するのはどういう理由ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0306	1004	概念学習が失敗するのはどういう理由ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0306	0614	概念学習が失敗するのはどういう理由ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0306	0610	概念学習が失敗するのはどういう理由ですか	真のモデルとの距離
0	0306	0803	概念学習が失敗するのはどういう理由ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0306	1306	概念学習が失敗するのはどういう理由ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0306	0805	概念学習が失敗するのはどういう理由ですか	誤差逆伝播法
0	0306	0715	概念学習が失敗するのはどういう理由ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0306	0508	概念学習が失敗するのはどういう理由ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0306	0503	概念学習が失敗するのはどういう理由ですか	様々な数値データに対して多く用いられる統計モデル
1	0307	0307	決定木とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0307	0410	決定木とはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0307	0601	決定木とはなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0307	0704	決定木とはなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0307	0405	決定木とはなんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0307	0204	決定木とはなんですか	特徴ベクトルの次元数を減らすこと
0	0307	0104	決定木とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0307	1510	決定木とはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0307	0610	決定木とはなんですか	学習結果の散らばり具合
0	0307	0505	決定木とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0307	0508	決定木とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0307	1215	決定木とはなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0307	0114	決定木とはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0307	1411	決定木とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0307	1407	決定木とはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0307	0802	決定木とはなんですか	特徴空間上では線形識別面を設定すること
0	0307	0801	決定木とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0307	1502	決定木とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0307	0713	決定木とはなんですか	文書分類やバイオインフォマティックスなど
0	0307	0912	決定木とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0307	0908	決定木とはなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0307	0701	決定木とはなんですか	識別境界線と最も近いデータとの距離
0	0307	0911	決定木とはなんですか	学習時の自由度を意図的に下げていること
0	0307	0610	決定木とはなんですか	トレードオフの関係
0	0307	0614	決定木とはなんですか	回帰木と線形回帰の双方のよいところを取った
1	0311	0311	分類能力が高いとはどういうことですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0311	0614	分類能力が高いとはどういうことですか	回帰木と線形回帰の双方のよいところを取った方法
0	0311	0810	分類能力が高いとはどういうことですか	勾配消失問題
0	0311	0402	分類能力が高いとはどういうことですか	入力を観測した後で計算される確率
0	0311	1104	分類能力が高いとはどういうことですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0311	1310	分類能力が高いとはどういうことですか	Hidden Marcov Model: 隠れマルコフモデル
0	0311	0715	分類能力が高いとはどういうことですか	識別面
0	0311	1108	分類能力が高いとはどういうことですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0311	1411	分類能力が高いとはどういうことですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0311	0416	分類能力が高いとはどういうことですか	値が真となる確率を知りたいノードが表す変数
0	0311	0109	分類能力が高いとはどういうことですか	学習データに正解が付いている場合の学習
0	0311	0514	分類能力が高いとはどういうことですか	ランダムに学習データを一つ
0	0311	0810	分類能力が高いとはどういうことですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0311	0917	分類能力が高いとはどういうことですか	入力ゲート・出力ゲート・忘却ゲート
0	0311	1510	分類能力が高いとはどういうことですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0311	0801	分類能力が高いとはどういうことですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0311	0114	分類能力が高いとはどういうことですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0311	0111	分類能力が高いとはどういうことですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0311	1506	分類能力が高いとはどういうことですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0311	0316	分類能力が高いとはどういうことですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0311	1404	分類能力が高いとはどういうことですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0311	0903	分類能力が高いとはどういうことですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0311	1406	分類能力が高いとはどういうことですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0311	1303	分類能力が高いとはどういうことですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0311	0614	分類能力が高いとはどういうことですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
1	0311	0311	エントロピーとはなんですか	集合の乱雑さ
0	0311	0611	エントロピーとはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0311	1214	エントロピーとはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0311	0102	エントロピーとはなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0311	0611	エントロピーとはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0311	1116	エントロピーとはなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0311	0606	エントロピーとはなんですか	山の尾根という意味
0	0311	0205	エントロピーとはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0311	0313	エントロピーとはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0311	0717	エントロピーとはなんですか	連続値
0	0311	1214	エントロピーとはなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0311	0416	エントロピーとはなんですか	値が真となる確率を知りたいノードが表す変数
0	0311	1505	エントロピーとはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0311	0612	エントロピーとはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0311	0611	エントロピーとはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0311	0610	エントロピーとはなんですか	学習結果の散らばり具合
0	0311	0606	エントロピーとはなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0311	1407	エントロピーとはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0311	0612	エントロピーとはなんですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0311	0105	エントロピーとはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0311	0701	エントロピーとはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0311	1306	エントロピーとはなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0311	0602	エントロピーとはなんですか	正解情報$y$が数値であるということ
0	0311	0803	エントロピーとはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0311	0811	エントロピーとはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
1	0313	0313	過学習とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0313	1301	過学習とはなんですか	連続音声認識
0	0313	0701	過学習とはなんですか	サポートベクトルマシン
0	0313	1509	過学習とはなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0313	0508	過学習とはなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0313	0805	過学習とはなんですか	誤差逆伝播法
0	0313	0502	過学習とはなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0313	1505	過学習とはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0313	0511	過学習とはなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0313	0805	過学習とはなんですか	誤差逆伝播法
0	0313	0211	過学習とはなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0313	0701	過学習とはなんですか	線形で識別できないデータに対応するため
0	0313	1201	過学習とはなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0313	0811	過学習とはなんですか	ユニットの活性化関数を工夫する方法
0	0313	1412	過学習とはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0313	1207	過学習とはなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0313	0805	過学習とはなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0313	0409	過学習とはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0313	1302	過学習とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0313	1001	過学習とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0313	1010	過学習とはなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0313	0717	過学習とはなんですか	連続値
0	0313	1306	過学習とはなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0313	0901	過学習とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0313	1116	過学習とはなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
1	0313	0313	決定木ではどうやって過学習を回避しますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0313	1412	決定木ではどうやって過学習を回避しますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0313	0409	決定木ではどうやって過学習を回避しますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0313	0606	決定木ではどうやって過学習を回避しますか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0313	0710	決定木ではどうやって過学習を回避しますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0313	0115	決定木ではどうやって過学習を回避しますか	パターンマイニング
0	0313	0610	決定木ではどうやって過学習を回避しますか	学習結果の散らばり具合
0	0313	0614	決定木ではどうやって過学習を回避しますか	モデル木
0	0313	0306	決定木ではどうやって過学習を回避しますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0313	1108	決定木ではどうやって過学習を回避しますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0313	0115	決定木ではどうやって過学習を回避しますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0313	1412	決定木ではどうやって過学習を回避しますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0313	0907	決定木ではどうやって過学習を回避しますか	事前学習法
0	0313	0402	決定木ではどうやって過学習を回避しますか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0313	0701	決定木ではどうやって過学習を回避しますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0313	0209	決定木ではどうやって過学習を回避しますか	正例がどれだけ正しく判定されているかという指標
0	0313	0901	決定木ではどうやって過学習を回避しますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0313	0810	決定木ではどうやって過学習を回避しますか	2006 年頃に考案された事前学習法
0	0313	1505	決定木ではどうやって過学習を回避しますか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0313	1012	決定木ではどうやって過学習を回避しますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0313	0111	決定木ではどうやって過学習を回避しますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0313	1111	決定木ではどうやって過学習を回避しますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0313	0802	決定木ではどうやって過学習を回避しますか	入力層・出力層の数に応じた適当な数
0	0313	0810	決定木ではどうやって過学習を回避しますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0313	0805	決定木ではどうやって過学習を回避しますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
1	0316	0316	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0316	0509	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	確率的最急勾配法
0	0316	0917	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0316	1010	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0316	0313	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0316	0304	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	個々の事例から，あるクラスについて共通点を見つけること
0	0316	1106	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0316	0307	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0316	0810	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0316	0510	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	特徴空間上でクラスを分割する面
0	0316	0402	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	最大事後確率則
0	0316	0802	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	識別対象のクラス数
0	0316	0508	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	二乗誤差を最小にするように識別関数を調整する方法
0	0316	0506	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0316	1301	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0316	0507	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	パーセプトロンの収束定理
0	0316	0708	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	制約を弱める変数
0	0316	1106	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	クラスタの重心間の距離を類似度とする
0	0316	1506	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	後に得られる報酬ほど割り引いて計算するための係数
0	0316	0912	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0316	0510	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	特徴空間上でクラスを分割する面
0	0316	0717	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0316	0708	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	制約を弱める変数
0	0316	0712	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	カーネル関数が正定値関数という条件を満たすとき
0	0316	0708	数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか	制約を弱める変数
1	0315	0315	ジニ不純度とはなんですか	分割後のデータの分散
0	0315	1106	ジニ不純度とはなんですか	最も近い事例対の距離を類似度とする
0	0315	1406	ジニ不純度とはなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0315	1001	ジニ不純度とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0315	0707	ジニ不純度とはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0315	1404	ジニ不純度とはなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0315	0506	ジニ不純度とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0315	0507	ジニ不純度とはなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0315	0803	ジニ不純度とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0315	0810	ジニ不純度とはなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0315	0701	ジニ不純度とはなんですか	識別境界線と最も近いデータとの距離
0	0315	0416	ジニ不純度とはなんですか	アークを無向とみなした結合を考えたとき
0	0315	1106	ジニ不純度とはなんですか	最も遠い事例対の距離を類似度とする
0	0315	1404	ジニ不純度とはなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0315	0206	ジニ不純度とはなんですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0315	0610	ジニ不純度とはなんですか	片方を減らせば片方が増える
0	0315	0306	ジニ不純度とはなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0315	0810	ジニ不純度とはなんですか	誤差が小さくなって消失してしまう
0	0315	1114	ジニ不純度とはなんですか	クラスタリング結果のデータ数の分布から
0	0315	0802	ジニ不純度とはなんですか	入力層・出力層の数に応じた適当な数
0	0315	0906	ジニ不純度とはなんですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0315	0508	ジニ不純度とはなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0315	0406	ジニ不純度とはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0315	0612	ジニ不純度とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0315	0702	ジニ不純度とはなんですか	識別面は平面を仮定する
1	0404	0404	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	事前確率
0	0404	0701	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0404	0505	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0404	1106	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	最も遠い事例対の距離を類似度とする
0	0404	1108	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0404	0102	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	現在，人が行っている知的な判断を代わりに行う技術
0	0404	1304	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	出力系列を参照する素性
0	0404	0611	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0404	0717	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0404	0805	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0404	0612	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0404	0512	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0404	0906	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	多階層構造でもそのまま適用できます
0	0404	0507	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0404	0906	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0404	0701	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0404	1214	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0404	1104	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0404	0911	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0404	1303	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0404	1106	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	最も近い事例対の距離を類似度とする
0	0404	0801	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	このモデルは生物の神経細胞のモデルであると考えられています
0	0404	0906	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0404	0808	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	シグモイド関数の微分
0	0404	0811	入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか	ユニットの活性化関数を工夫する方法があります
1	0701	0701	サポートベクトルマシンとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0701	0902	サポートベクトルマシンとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0701	1219	サポートベクトルマシンとは何ですか	どの個人がどの商品を購入したかが記録されているデータ
0	0701	0811	サポートベクトルマシンとは何ですか	ユニットの活性化関数を工夫する方法
0	0701	0611	サポートベクトルマシンとは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0701	0117	サポートベクトルマシンとは何ですか	学習データの一部にだけ正解が与えられている場合
0	0701	0508	サポートベクトルマシンとは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	0701	0413	サポートベクトルマシンとは何ですか	変数間の独立性を表現できること
0	0701	0708	サポートベクトルマシンとは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0701	0610	サポートベクトルマシンとは何ですか	トレードオフの関係
0	0701	0116	サポートベクトルマシンとは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	0701	0707	サポートベクトルマシンとは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0701	0205	サポートベクトルマシンとは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0701	0503	サポートベクトルマシンとは何ですか	様々な数値データに対して多く用いられる統計モデル
0	0701	0805	サポートベクトルマシンとは何ですか	誤差逆伝播法
0	0701	0405	サポートベクトルマシンとは何ですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0701	0917	サポートベクトルマシンとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0701	0908	サポートベクトルマシンとは何ですか	ユークリッド距離
0	0701	0315	サポートベクトルマシンとは何ですか	分割後のデータの分散
0	0701	1219	サポートベクトルマシンとは何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0701	1203	サポートベクトルマシンとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0701	0412	サポートベクトルマシンとは何ですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0701	0109	サポートベクトルマシンとは何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0701	1405	サポートベクトルマシンとは何ですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0701	0614	サポートベクトルマシンとは何ですか	モデル木
1	0402	0402	入力を観測した後で計算される確率は何と言いますか	事後確率
0	0402	0912	入力を観測した後で計算される確率は何と言いますか	畳み込みニューラルネットワーク
0	0402	0802	入力を観測した後で計算される確率は何と言いますか	特徴空間上では線形識別面を設定すること
0	0402	0708	入力を観測した後で計算される確率は何と言いますか	制約を弱める変数
0	0402	0405	入力を観測した後で計算される確率は何と言いますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0402	0413	入力を観測した後で計算される確率は何と言いますか	変数間の独立性を表現できること
0	0402	1310	入力を観測した後で計算される確率は何と言いますか	Hidden Marcov Model: 隠れマルコフモデル
0	0402	0103	入力を観測した後で計算される確率は何と言いますか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0402	0803	入力を観測した後で計算される確率は何と言いますか	重みパラメータに対しては線形で，入力を非線形変換する
0	0402	0710	入力を観測した後で計算される確率は何と言いますか	もとの空間におけるデータ間の距離関係を保存
0	0402	1306	入力を観測した後で計算される確率は何と言いますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0402	0114	入力を観測した後で計算される確率は何と言いますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0402	0111	入力を観測した後で計算される確率は何と言いますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0402	0917	入力を観測した後で計算される確率は何と言いますか	入力ゲート・出力ゲート・忘却ゲート
0	0402	0906	入力を観測した後で計算される確率は何と言いますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0402	0507	入力を観測した後で計算される確率は何と言いますか	パーセプトロンの収束定理
0	0402	0406	入力を観測した後で計算される確率は何と言いますか	各クラスから生じる特徴の尤もらしさを表す
0	0402	0209	入力を観測した後で計算される確率は何と言いますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0402	1409	入力を観測した後で計算される確率は何と言いますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0402	0204	入力を観測した後で計算される確率は何と言いますか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0402	0701	入力を観測した後で計算される確率は何と言いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0402	0505	入力を観測した後で計算される確率は何と言いますか	パーセプトロン
0	0402	1406	入力を観測した後で計算される確率は何と言いますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0402	0803	入力を観測した後で計算される確率は何と言いますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0402	0411	入力を観測した後で計算される確率は何と言いますか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
1	0405	0405	事後確率が最大となるクラスは、何を求めることで得られますか	尤度と事前確率の積を最大とするクラス
0	0405	1306	事後確率が最大となるクラスは、何を求めることで得られますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0405	1306	事後確率が最大となるクラスは、何を求めることで得られますか	条件付き確率場（Conditional Random Field: CRF）
0	0405	0805	事後確率が最大となるクラスは、何を求めることで得られますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0405	0802	事後確率が最大となるクラスは、何を求めることで得られますか	特徴空間上では線形識別面を設定すること
0	0405	0407	事後確率が最大となるクラスは、何を求めることで得られますか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0405	0611	事後確率が最大となるクラスは、何を求めることで得られますか	識別における決定木の考え方を回帰問題に適用する方法
0	0405	0916	事後確率が最大となるクラスは、何を求めることで得られますか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0405	1004	事後確率が最大となるクラスは、何を求めることで得られますか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0405	1404	事後確率が最大となるクラスは、何を求めることで得られますか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0405	0114	事後確率が最大となるクラスは、何を求めることで得られますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0405	1301	事後確率が最大となるクラスは、何を求めることで得られますか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0405	0717	事後確率が最大となるクラスは、何を求めることで得られますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0405	1302	事後確率が最大となるクラスは、何を求めることで得られますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0405	1302	事後確率が最大となるクラスは、何を求めることで得られますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0405	0304	事後確率が最大となるクラスは、何を求めることで得られますか	個々の事例から，あるクラスについて共通点を見つけること
0	0405	0808	事後確率が最大となるクラスは、何を求めることで得られますか	シグモイド関数の微分
0	0405	0404	事後確率が最大となるクラスは、何を求めることで得られますか	事前確率
0	0405	0313	事後確率が最大となるクラスは、何を求めることで得られますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0405	0502	事後確率が最大となるクラスは、何を求めることで得られますか	SVM
0	0405	0614	事後確率が最大となるクラスは、何を求めることで得られますか	回帰木と線形回帰の双方のよいところを取った方法
0	0405	0115	事後確率が最大となるクラスは、何を求めることで得られますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0405	0906	事後確率が最大となるクラスは、何を求めることで得られますか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0405	0510	事後確率が最大となるクラスは、何を求めることで得られますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0405	1215	事後確率が最大となるクラスは、何を求めることで得られますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
1	0412	0412	ベイジアンネットワークはどのような仮定を表現したものですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0412	0802	ベイジアンネットワークはどのような仮定を表現したものですか	識別対象のクラス数
0	0412	0404	ベイジアンネットワークはどのような仮定を表現したものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0412	1508	ベイジアンネットワークはどのような仮定を表現したものですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0412	0306	ベイジアンネットワークはどのような仮定を表現したものですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0412	0410	ベイジアンネットワークはどのような仮定を表現したものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0412	0405	ベイジアンネットワークはどのような仮定を表現したものですか	尤度と事前確率の積を最大とするクラス
0	0412	0916	ベイジアンネットワークはどのような仮定を表現したものですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0412	0114	ベイジアンネットワークはどのような仮定を表現したものですか	階層的クラスタリングや k-means 法
0	0412	0908	ベイジアンネットワークはどのような仮定を表現したものですか	シグモイド関数
0	0412	1403	ベイジアンネットワークはどのような仮定を表現したものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0412	0802	ベイジアンネットワークはどのような仮定を表現したものですか	特徴空間上では線形識別面を設定すること
0	0412	0514	ベイジアンネットワークはどのような仮定を表現したものですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0412	0701	ベイジアンネットワークはどのような仮定を表現したものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0412	1111	ベイジアンネットワークはどのような仮定を表現したものですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0412	0917	ベイジアンネットワークはどのような仮定を表現したものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0412	0505	ベイジアンネットワークはどのような仮定を表現したものですか	ニューラルネットワーク
0	0412	1012	ベイジアンネットワークはどのような仮定を表現したものですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0412	0912	ベイジアンネットワークはどのような仮定を表現したものですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0412	0109	ベイジアンネットワークはどのような仮定を表現したものですか	正解が付いていない場合の学習
0	0412	1114	ベイジアンネットワークはどのような仮定を表現したものですか	クラスタリング結果のデータ数の分布から
0	0412	0901	ベイジアンネットワークはどのような仮定を表現したものですか	表現学習
0	0412	1506	ベイジアンネットワークはどのような仮定を表現したものですか	政策
0	0412	1010	ベイジアンネットワークはどのような仮定を表現したものですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0412	0912	ベイジアンネットワークはどのような仮定を表現したものですか	畳み込みニューラルネットワーク
1	0413	0413	ベイジアンネットワークの利点はなんですか	変数間の独立性を表現できること
0	0413	1112	ベイジアンネットワークの利点はなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0413	0908	ベイジアンネットワークの利点はなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0413	0211	ベイジアンネットワークの利点はなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0413	1010	ベイジアンネットワークの利点はなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0413	0917	ベイジアンネットワークの利点はなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0413	1209	ベイジアンネットワークの利点はなんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0413	1302	ベイジアンネットワークの利点はなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0413	0510	ベイジアンネットワークの利点はなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0413	0114	ベイジアンネットワークの利点はなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0413	0103	ベイジアンネットワークの利点はなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0413	0402	ベイジアンネットワークの利点はなんですか	入力を観測した後で計算される確率
0	0413	1404	ベイジアンネットワークの利点はなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0413	0114	ベイジアンネットワークの利点はなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0413	1411	ベイジアンネットワークの利点はなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0413	0502	ベイジアンネットワークの利点はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0413	1203	ベイジアンネットワークの利点はなんですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0413	0912	ベイジアンネットワークの利点はなんですか	画像認識
0	0413	1503	ベイジアンネットワークの利点はなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0413	0901	ベイジアンネットワークの利点はなんですか	深層学習に用いるニューラルネットワーク
0	0413	1506	ベイジアンネットワークの利点はなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0413	1303	ベイジアンネットワークの利点はなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0413	1304	ベイジアンネットワークの利点はなんですか	出力系列を参照する素性
0	0413	0715	ベイジアンネットワークの利点はなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0413	1009	ベイジアンネットワークの利点はなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
1	0417	0417	ベイジアンネットワークにおいて学習するべき項目はなんですか	ネットワークの構造とアークの条件付き確率
0	0417	0614	ベイジアンネットワークにおいて学習するべき項目はなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0417	0206	ベイジアンネットワークにおいて学習するべき項目はなんですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0417	0405	ベイジアンネットワークにおいて学習するべき項目はなんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0417	0306	ベイジアンネットワークにおいて学習するべき項目はなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0417	1301	ベイジアンネットワークにおいて学習するべき項目はなんですか	連続音声認識
0	0417	1505	ベイジアンネットワークにおいて学習するべき項目はなんですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0417	0111	ベイジアンネットワークにおいて学習するべき項目はなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0417	0710	ベイジアンネットワークにおいて学習するべき項目はなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0417	0109	ベイジアンネットワークにおいて学習するべき項目はなんですか	正解が付いていない場合の学習
0	0417	0113	ベイジアンネットワークにおいて学習するべき項目はなんですか	入力データに潜む規則性
0	0417	1201	ベイジアンネットワークにおいて学習するべき項目はなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0417	0117	ベイジアンネットワークにおいて学習するべき項目はなんですか	学習データの一部にだけ正解が与えられている場合
0	0417	0102	ベイジアンネットワークにおいて学習するべき項目はなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0417	1108	ベイジアンネットワークにおいて学習するべき項目はなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0417	0102	ベイジアンネットワークにおいて学習するべき項目はなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0417	1412	ベイジアンネットワークにおいて学習するべき項目はなんですか	近くのノードは同じクラスになりやすいという仮定
0	0417	0507	ベイジアンネットワークにおいて学習するべき項目はなんですか	全ての誤りがなくなることが学習の終了条件なので
0	0417	0306	ベイジアンネットワークにおいて学習するべき項目はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0417	0204	ベイジアンネットワークにおいて学習するべき項目はなんですか	特徴ベクトルの次元数を減らすこと
0	0417	0205	ベイジアンネットワークにおいて学習するべき項目はなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0417	0105	ベイジアンネットワークにおいて学習するべき項目はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0417	1010	ベイジアンネットワークにおいて学習するべき項目はなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0417	0602	ベイジアンネットワークにおいて学習するべき項目はなんですか	正解情報$y$が数値であるということ
0	0417	1219	ベイジアンネットワークにおいて学習するべき項目はなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
1	0402	0402	最も確率が高いクラスを出力とする手法はなに	統計的識別手法
0	0402	0109	最も確率が高いクラスを出力とする手法はなに	正解が付いていない場合の学習
0	0402	0506	最も確率が高いクラスを出力とする手法はなに	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0402	0502	最も確率が高いクラスを出力とする手法はなに	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0402	0911	最も確率が高いクラスを出力とする手法はなに	ドロップアウト
0	0402	0612	最も確率が高いクラスを出力とする手法はなに	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0402	0707	最も確率が高いクラスを出力とする手法はなに	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0402	0803	最も確率が高いクラスを出力とする手法はなに	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0402	0903	最も確率が高いクラスを出力とする手法はなに	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0402	0810	最も確率が高いクラスを出力とする手法はなに	誤差が小さくなって消失してしまう
0	0402	1009	最も確率が高いクラスを出力とする手法はなに	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0402	0105	最も確率が高いクラスを出力とする手法はなに	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0402	1010	最も確率が高いクラスを出力とする手法はなに	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0402	0311	最も確率が高いクラスを出力とする手法はなに	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0402	0510	最も確率が高いクラスを出力とする手法はなに	特徴空間上でクラスを分割する面
0	0402	0404	最も確率が高いクラスを出力とする手法はなに	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0402	0810	最も確率が高いクラスを出力とする手法はなに	勾配消失問題
0	0402	0606	最も確率が高いクラスを出力とする手法はなに	山の尾根という意味
0	0402	0907	最も確率が高いクラスを出力とする手法はなに	事前学習法
0	0402	0611	最も確率が高いクラスを出力とする手法はなに	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0402	1402	最も確率が高いクラスを出力とする手法はなに	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0402	0902	最も確率が高いクラスを出力とする手法はなに	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0402	1207	最も確率が高いクラスを出力とする手法はなに	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0402	1116	最も確率が高いクラスを出力とする手法はなに	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0402	0116	最も確率が高いクラスを出力とする手法はなに	学習データが教師あり／教師なしの混在となっているもの
1	0502	0502	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	SVM
0	0502	1110	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0502	0509	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	確率的最急勾配法
0	0502	0711	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0502	0701	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	学習データからのマージンが最大となる識別境界線
0	0502	0901	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0502	0504	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0502	1106	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	クラスタの重心間の距離を類似度とする
0	0502	0908	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0502	0610	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	学習結果の散らばり具合
0	0502	0404	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0502	1409	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	自分が出した誤りを指摘してくれる他人がいない
0	0502	1108	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0502	0901	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	深層学習に用いるニューラルネットワーク
0	0502	1004	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0502	0711	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0502	0607	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0502	0811	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0502	0109	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	正解が付いていない場合の学習
0	0502	1110	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0502	0614	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	モデル木
0	0502	0404	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0502	1410	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	学習初期の誤りに強いということ
0	0502	0607	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	「投げ縄」という意味
0	0502	0917	線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに	入力ゲート・出力ゲート・忘却ゲート
1	0505	0505	最も古典的な識別関数法の手法はなんですか	パーセプトロン
0	0505	0413	最も古典的な識別関数法の手法はなんですか	変数間の独立性を表現できること
0	0505	0903	最も古典的な識別関数法の手法はなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0505	0105	最も古典的な識別関数法の手法はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0505	1008	最も古典的な識別関数法の手法はなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0505	1201	最も古典的な識別関数法の手法はなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0505	1103	最も古典的な識別関数法の手法はなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0505	0703	最も古典的な識別関数法の手法はなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0505	0205	最も古典的な識別関数法の手法はなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0505	1209	最も古典的な識別関数法の手法はなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0505	1410	最も古典的な識別関数法の手法はなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0505	0512	最も古典的な識別関数法の手法はなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0505	0105	最も古典的な識別関数法の手法はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0505	1220	最も古典的な識別関数法の手法はなんですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0505	0805	最も古典的な識別関数法の手法はなんですか	誤差逆伝播法
0	0505	1106	最も古典的な識別関数法の手法はなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0505	0514	最も古典的な識別関数法の手法はなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0505	1313	最も古典的な識別関数法の手法はなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0505	1503	最も古典的な識別関数法の手法はなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0505	0704	最も古典的な識別関数法の手法はなんですか	以下の関数$L$の最小値を求めるという問題
0	0505	0701	最も古典的な識別関数法の手法はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0505	0715	最も古典的な識別関数法の手法はなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0505	1302	最も古典的な識別関数法の手法はなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0505	0102	最も古典的な識別関数法の手法はなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0505	0810	最も古典的な識別関数法の手法はなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
1	0505	0505	パーセプトロンとは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	0505	1007	パーセプトロンとは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0505	0703	パーセプトロンとは何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0505	0901	パーセプトロンとは何ですか	Deep Neural Network (DNN) 
0	0505	0715	パーセプトロンとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0505	0313	パーセプトロンとは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0505	0906	パーセプトロンとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0505	1410	パーセプトロンとは何ですか	学習初期の誤りに強いということ
0	0505	0810	パーセプトロンとは何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0505	1506	パーセプトロンとは何ですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0505	1207	パーセプトロンとは何ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0505	1207	パーセプトロンとは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0505	0901	パーセプトロンとは何ですか	音声認識・画像認識・自然言語処理など
0	0505	0711	パーセプトロンとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0505	0906	パーセプトロンとは何ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0505	1313	パーセプトロンとは何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0505	0504	パーセプトロンとは何ですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0505	0111	パーセプトロンとは何ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0505	0612	パーセプトロンとは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0505	1506	パーセプトロンとは何ですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0505	0105	パーセプトロンとは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0505	0906	パーセプトロンとは何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0505	0614	パーセプトロンとは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0505	0810	パーセプトロンとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0505	0802	パーセプトロンとは何ですか	特徴空間上では線形識別面を設定すること
1	0505	0505	パーセプトロンを多層に重ねたものはなんですか	ニューラルネットワーク
0	0505	0113	パーセプトロンを多層に重ねたものはなんですか	入力データに潜む規則性を学習すること
0	0505	1409	パーセプトロンを多層に重ねたものはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0505	0610	パーセプトロンを多層に重ねたものはなんですか	真のモデルとの距離
0	0505	1503	パーセプトロンを多層に重ねたものはなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0505	1214	パーセプトロンを多層に重ねたものはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0505	0902	パーセプトロンを多層に重ねたものはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0505	0404	パーセプトロンを多層に重ねたものはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0505	0701	パーセプトロンを多層に重ねたものはなんですか	識別境界線と最も近いデータとの距離
0	0505	0416	パーセプトロンを多層に重ねたものはなんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0505	1015	パーセプトロンを多層に重ねたものはなんですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0505	1404	パーセプトロンを多層に重ねたものはなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0505	0409	パーセプトロンを多層に重ねたものはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0505	0116	パーセプトロンを多層に重ねたものはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0505	0110	パーセプトロンを多層に重ねたものはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0505	0512	パーセプトロンを多層に重ねたものはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0505	0114	パーセプトロンを多層に重ねたものはなんですか	階層的クラスタリングや k-means 法
0	0505	0404	パーセプトロンを多層に重ねたものはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0505	1112	パーセプトロンを多層に重ねたものはなんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0505	0402	パーセプトロンを多層に重ねたものはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0505	1510	パーセプトロンを多層に重ねたものはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0505	0512	パーセプトロンを多層に重ねたものはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0505	0901	パーセプトロンを多層に重ねたものはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0505	1012	パーセプトロンを多層に重ねたものはなんですか	すべてのデータの重みは平等
0	0505	0402	パーセプトロンを多層に重ねたものはなんですか	最大事後確率則
1	0507	0507	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	パーセプトロンの収束定理
0	0507	0512	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0507	1302	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0507	1301	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0507	1103	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0507	0610	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	学習結果の散らばり具合
0	0507	1507	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0507	0508	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0507	0701	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0507	0701	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	識別境界線と最も近いデータとの距離
0	0507	0404	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0507	1219	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0507	0416	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	値が真となる確率を知りたいノードが表す変数
0	0507	0103	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0507	0610	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	真のモデルとの距離
0	0507	1010	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0507	0911	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0507	0610	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	学習結果の散らばり具合
0	0507	0811	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	ユニットの活性化関数を工夫する方法
0	0507	0105	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0507	1408	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0507	0902	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0507	0801	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0507	0715	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	カーネルトリック
0	0507	0105	学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
1	0508	0508	二乗誤差とは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0508	1404	二乗誤差とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0508	0810	二乗誤差とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0508	1012	二乗誤差とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0508	0701	二乗誤差とは何ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0508	0504	二乗誤差とは何ですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0508	0402	二乗誤差とは何ですか	事後確率
0	0508	1412	二乗誤差とは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0508	0802	二乗誤差とは何ですか	多層パーセプトロンあるいはニューラルネットワーク
0	0508	0606	二乗誤差とは何ですか	入力が少し変化したときに，出力も少し変化する
0	0508	1501	二乗誤差とは何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0508	1112	二乗誤差とは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0508	0801	二乗誤差とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0508	1306	二乗誤差とは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0508	0906	二乗誤差とは何ですか	入力に近い側の処理
0	0508	0507	二乗誤差とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0508	0505	二乗誤差とは何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0508	0311	二乗誤差とは何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0508	1407	二乗誤差とは何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0508	0114	二乗誤差とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0508	0702	二乗誤差とは何ですか	識別面は平面を仮定する
0	0508	0606	二乗誤差とは何ですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0508	1112	二乗誤差とは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0508	1214	二乗誤差とは何ですか	計算量が膨大であること
0	0508	1106	二乗誤差とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
1	0508	0508	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	最小二乗法
0	0508	0906	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	入力に近い側の処理
0	0508	1110	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0508	1108	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0508	0802	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0508	0411	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	確率のm推定という考え方を用います
0	0508	0504	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0508	0801	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0508	1306	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0508	0901	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	表現学習
0	0508	1412	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0508	1406	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0508	0103	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0508	1409	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	自分が出した誤りを指摘してくれる他人がいない
0	0508	0206	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0508	0702	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	識別面は平面を仮定する
0	0508	0717	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	連続値
0	0508	0114	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0508	0311	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0508	0801	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0508	0513	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0508	0808	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	シグモイド関数の微分
0	0508	1510	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0508	0811	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	引数が負のときは0，0以上のときはその値を出力
0	0508	1403	二乗誤差を最小にするように識別関数を調整する方法を何と言いますか	多次元でも「次元の呪い」にかかっていない，ということ
1	0509	0509	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	確率的最急勾配法
0	0509	0701	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	識別境界線と最も近いデータとの距離
0	0509	0917	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	LSTMセル
0	0509	1505	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0509	0105	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0509	0717	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	連続値
0	0509	1502	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	将棋や囲碁などを行うプログラム
0	0509	0701	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0509	1303	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0509	1303	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0509	1306	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	条件付き確率場（Conditional Random Field: CRF）
0	0509	1214	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0509	0802	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	多層パーセプトロンあるいはニューラルネットワーク
0	0509	0410	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0509	0901	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0509	1510	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0509	0504	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0509	0402	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	事後確率
0	0509	0502	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0509	0206	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0509	1104	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0509	1506	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	その政策に従って行動したときの累積報酬の期待値で評価
0	0509	1310	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	Hidden Marcov Model: 隠れマルコフモデル
0	0509	1015	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0509	0808	学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか	入力の重み付き和の微分
1	0701	0701	サポートベクトルマシンってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0701	0710	サポートベクトルマシンってなんですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0701	0903	サポートベクトルマシンってなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0701	1012	サポートベクトルマシンってなんですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0701	0715	サポートベクトルマシンってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0701	0512	サポートベクトルマシンってなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0701	1506	サポートベクトルマシンってなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0701	1309	サポートベクトルマシンってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0701	0116	サポートベクトルマシンってなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0701	1302	サポートベクトルマシンってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0701	0902	サポートベクトルマシンってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0701	0508	サポートベクトルマシンってなんですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0701	0917	サポートベクトルマシンってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0701	1112	サポートベクトルマシンってなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0701	0901	サポートベクトルマシンってなんですか	深層学習に用いるニューラルネットワーク
0	0701	1503	サポートベクトルマシンってなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0701	0105	サポートベクトルマシンってなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0701	0801	サポートベクトルマシンってなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0701	1008	サポートベクトルマシンってなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0701	0109	サポートベクトルマシンってなんですか	学習データに正解が付いている場合の学習
0	0701	1410	サポートベクトルマシンってなんですか	学習初期の誤りに強いということ
0	0701	1111	サポートベクトルマシンってなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0701	0509	サポートベクトルマシンってなんですか	確率的最急勾配法
0	0701	0907	サポートベクトルマシンってなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0701	0307	サポートベクトルマシンってなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
1	0708	0708	スラック変数ってなんですか	制約を弱める変数
0	0708	0902	スラック変数ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0708	0506	スラック変数ってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0708	1009	スラック変数ってなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0708	0908	スラック変数ってなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0708	1301	スラック変数ってなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0708	1502	スラック変数ってなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0708	0111	スラック変数ってなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0708	1106	スラック変数ってなんですか	最も近い事例対の距離を類似度とする
0	0708	1306	スラック変数ってなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0708	0901	スラック変数ってなんですか	表現学習
0	0708	0402	スラック変数ってなんですか	事後確率
0	0708	0412	スラック変数ってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0708	0701	スラック変数ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0708	0906	スラック変数ってなんですか	多階層構造でもそのまま適用できます
0	0708	0506	スラック変数ってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0708	0808	スラック変数ってなんですか	シグモイド関数の微分
0	0708	1012	スラック変数ってなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0708	0411	スラック変数ってなんですか	確率のm推定という考え方を用います
0	0708	1215	スラック変数ってなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0708	1501	スラック変数ってなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0708	0409	スラック変数ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0708	0110	スラック変数ってなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0708	0916	スラック変数ってなんですか	リカレントニューラルネットワーク
0	0708	0104	スラック変数ってなんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
1	1004	1004	バギングはどういう手法ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1004	0115	バギングはどういう手法ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1004	0508	バギングはどういう手法ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1004	1313	バギングはどういう手法ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1004	0307	バギングはどういう手法ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	1004	1009	バギングはどういう手法ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	1004	0508	バギングはどういう手法ですか	最小二乗法
0	1004	0514	バギングはどういう手法ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1004	1116	バギングはどういう手法ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1004	1508	バギングはどういう手法ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1004	0514	バギングはどういう手法ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1004	0902	バギングはどういう手法ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1004	0612	バギングはどういう手法ですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	1004	0204	バギングはどういう手法ですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	1004	0306	バギングはどういう手法ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1004	0711	バギングはどういう手法ですか	カーネル関数
0	1004	0802	バギングはどういう手法ですか	入力層・出力層の数に応じた適当な数
0	1004	0601	バギングはどういう手法ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1004	0901	バギングはどういう手法ですか	表現学習
0	1004	1219	バギングはどういう手法ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1004	0811	バギングはどういう手法ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1004	0102	バギングはどういう手法ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1004	0902	バギングはどういう手法ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1004	0701	バギングはどういう手法ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1004	0803	バギングはどういう手法ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
1	1001	1001	アンサンブル学習とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1001	0505	アンサンブル学習とはなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1001	0701	アンサンブル学習とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1001	0413	アンサンブル学習とはなんですか	変数間の独立性を表現できること
0	1001	1501	アンサンブル学習とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1001	1214	アンサンブル学習とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1001	0406	アンサンブル学習とはなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1001	1302	アンサンブル学習とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1001	1219	アンサンブル学習とはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1001	0802	アンサンブル学習とはなんですか	入力層・出力層の数に応じた適当な数
0	1001	1507	アンサンブル学習とはなんですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1001	0911	アンサンブル学習とはなんですか	ランダムに一定割合のユニットを消して学習を行う
0	1001	1214	アンサンブル学習とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1001	0514	アンサンブル学習とはなんですか	ランダムに学習データを一つ
0	1001	0411	アンサンブル学習とはなんですか	確率のm推定という考え方を用います
0	1001	1506	アンサンブル学習とはなんですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1001	1410	アンサンブル学習とはなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1001	0911	アンサンブル学習とはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1001	1303	アンサンブル学習とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1001	1506	アンサンブル学習とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1001	0712	アンサンブル学習とはなんですか	カーネル関数が正定値関数という条件を満たすとき
0	1001	1106	アンサンブル学習とはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1001	1407	アンサンブル学習とはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1001	0805	アンサンブル学習とはなんですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	1001	0110	アンサンブル学習とはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
1	0701	0701	サポートベクトルマシンとは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0701	0708	サポートベクトルマシンとは何か	制約を弱める変数
0	0701	0205	サポートベクトルマシンとは何か	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0701	1303	サポートベクトルマシンとは何か	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0701	0405	サポートベクトルマシンとは何か	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0701	0917	サポートベクトルマシンとは何か	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0701	1001	サポートベクトルマシンとは何か	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0701	0903	サポートベクトルマシンとは何か	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0701	0204	サポートベクトルマシンとは何か	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0701	0810	サポートベクトルマシンとは何か	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0701	0703	サポートベクトルマシンとは何か	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0701	0610	サポートベクトルマシンとは何か	真のモデルとの距離
0	0701	1304	サポートベクトルマシンとは何か	出力系列を参照する素性
0	0701	1214	サポートベクトルマシンとは何か	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0701	1010	サポートベクトルマシンとは何か	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0701	0614	サポートベクトルマシンとは何か	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0701	0610	サポートベクトルマシンとは何か	学習結果の散らばり具合
0	0701	1505	サポートベクトルマシンとは何か	「マルコフ性」を持つ確率過程における意思決定問題
0	0701	1510	サポートベクトルマシンとは何か	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0701	1111	サポートベクトルマシンとは何か	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0701	0208	サポートベクトルマシンとは何か	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0701	0906	サポートベクトルマシンとは何か	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0701	0416	サポートベクトルマシンとは何か	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0701	1012	サポートベクトルマシンとは何か	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0701	1507	サポートベクトルマシンとは何か	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
1	1007	1007	ランダムフォレストとはなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1007	0115	ランダムフォレストとはなんですか	パターンマイニング
0	1007	0708	ランダムフォレストとはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1007	0916	ランダムフォレストとはなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1007	1303	ランダムフォレストとはなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1007	0901	ランダムフォレストとはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1007	0614	ランダムフォレストとはなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	1007	0610	ランダムフォレストとはなんですか	学習結果の散らばり具合
0	1007	1302	ランダムフォレストとはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1007	0209	ランダムフォレストとはなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1007	1209	ランダムフォレストとはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	1007	1510	ランダムフォレストとはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1007	0616	ランダムフォレストとはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1007	0305	ランダムフォレストとはなんですか	仮説に対して課す制約
0	1007	0512	ランダムフォレストとはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1007	0105	ランダムフォレストとはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1007	1114	ランダムフォレストとはなんですか	クラスタリング結果のデータ数の分布から
0	1007	0606	ランダムフォレストとはなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1007	0710	ランダムフォレストとはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1007	0803	ランダムフォレストとはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1007	0406	ランダムフォレストとはなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1007	0907	ランダムフォレストとはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1007	1402	ランダムフォレストとはなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1007	0802	ランダムフォレストとはなんですか	隠れ層
0	1007	0510	ランダムフォレストとはなんですか	特徴空間上でクラスを分割する面
1	0704	0704	マージンを最大とする識別面の計算方法はどうするの	ラグランジュの未定乗数法
0	0704	0315	マージンを最大とする識別面の計算方法はどうするの	分割後のデータの分散
0	0704	0610	マージンを最大とする識別面の計算方法はどうするの	片方を減らせば片方が増える
0	0704	1220	マージンを最大とする識別面の計算方法はどうするの	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0704	0209	マージンを最大とする識別面の計算方法はどうするの	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0704	0315	マージンを最大とする識別面の計算方法はどうするの	分割後のデータの分散
0	0704	1111	マージンを最大とする識別面の計算方法はどうするの	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0704	0508	マージンを最大とする識別面の計算方法はどうするの	二乗誤差を最小にするように識別関数を調整する方法
0	0704	0208	マージンを最大とする識別面の計算方法はどうするの	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0704	0717	マージンを最大とする識別面の計算方法はどうするの	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0704	0605	マージンを最大とする識別面の計算方法はどうするの	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0704	1106	マージンを最大とする識別面の計算方法はどうするの	最も遠い事例対の距離を類似度とする
0	0704	0105	マージンを最大とする識別面の計算方法はどうするの	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0704	0801	マージンを最大とする識別面の計算方法はどうするの	このモデルは生物の神経細胞のモデルであると考えられています
0	0704	0109	マージンを最大とする識別面の計算方法はどうするの	正解が付いていない場合の学習
0	0704	1012	マージンを最大とする識別面の計算方法はどうするの	すべてのデータの重みは平等
0	0704	1406	マージンを最大とする識別面の計算方法はどうするの	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0704	0112	マージンを最大とする識別面の計算方法はどうするの	線形回帰，回帰木，モデル木など
0	0704	0114	マージンを最大とする識別面の計算方法はどうするの	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0704	1505	マージンを最大とする識別面の計算方法はどうするの	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0704	0802	マージンを最大とする識別面の計算方法はどうするの	隠れ層
0	0704	0917	マージンを最大とする識別面の計算方法はどうするの	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0704	1506	マージンを最大とする識別面の計算方法はどうするの	各状態でどの行為を取ればよいのかという意思決定規則
0	0704	0614	マージンを最大とする識別面の計算方法はどうするの	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0704	0504	マージンを最大とする識別面の計算方法はどうするの	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
1	1008	1008	ランダムフォレストの学習手順は	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1008	1215	ランダムフォレストの学習手順は	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	1008	0912	ランダムフォレストの学習手順は	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1008	1111	ランダムフォレストの学習手順は	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1008	0708	ランダムフォレストの学習手順は	制約を満たさない程度を表すので，小さい方が望ましい
0	1008	0109	ランダムフォレストの学習手順は	正解が付いていない場合の学習
0	1008	0710	ランダムフォレストの学習手順は	もとの空間におけるデータ間の距離関係を保存
0	1008	0508	ランダムフォレストの学習手順は	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1008	1410	ランダムフォレストの学習手順は	学習初期の誤りに強いということ
0	1008	1004	ランダムフォレストの学習手順は	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1008	1007	ランダムフォレストの学習手順は	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1008	0702	ランダムフォレストの学習手順は	識別面は平面を仮定する
0	1008	0708	ランダムフォレストの学習手順は	制約を弱める変数
0	1008	0507	ランダムフォレストの学習手順は	全ての誤りがなくなることが学習の終了条件なので
0	1008	0302	ランダムフォレストの学習手順は	カテゴリ形式の正解情報のこと
0	1008	0616	ランダムフォレストの学習手順は	一般に非線形式ではデータにフィットしすぎてしまうため
0	1008	0901	ランダムフォレストの学習手順は	深層学習に用いるニューラルネットワーク
0	1008	0412	ランダムフォレストの学習手順は	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1008	1009	ランダムフォレストの学習手順は	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	1008	0508	ランダムフォレストの学習手順は	二乗誤差を最小にするように識別関数を調整する方法
0	1008	0305	ランダムフォレストの学習手順は	仮説に対して課す制約
0	1008	1214	ランダムフォレストの学習手順は	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1008	1402	ランダムフォレストの学習手順は	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	1008	0906	ランダムフォレストの学習手順は	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1008	1407	ランダムフォレストの学習手順は	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
1	0715	0715	カーネルトリックってどういうもの	複雑な非線形変換を求めるという操作を避ける方法
0	0715	0505	カーネルトリックってどういうもの	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0715	0105	カーネルトリックってどういうもの	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0715	0211	カーネルトリックってどういうもの	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0715	0717	カーネルトリックってどういうもの	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0715	0916	カーネルトリックってどういうもの	リカレントニューラルネットワーク
0	0715	1412	カーネルトリックってどういうもの	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0715	1015	カーネルトリックってどういうもの	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0715	1009	カーネルトリックってどういうもの	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0715	0512	カーネルトリックってどういうもの	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0715	1301	カーネルトリックってどういうもの	動画像の分類や音声で入力された単語の識別などの問題
0	0715	0514	カーネルトリックってどういうもの	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0715	0901	カーネルトリックってどういうもの	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0715	0811	カーネルトリックってどういうもの	誤差が消失しません
0	0715	0402	カーネルトリックってどういうもの	事後確率が最大となるクラスを識別結果とする方法
0	0715	0707	カーネルトリックってどういうもの	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0715	1205	カーネルトリックってどういうもの	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0715	1007	カーネルトリックってどういうもの	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0715	1302	カーネルトリックってどういうもの	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0715	1012	カーネルトリックってどういうもの	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0715	0805	カーネルトリックってどういうもの	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0715	1405	カーネルトリックってどういうもの	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0715	0606	カーネルトリックってどういうもの	山の尾根という意味
0	0715	1502	カーネルトリックってどういうもの	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0715	0514	カーネルトリックってどういうもの	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
1	0717	0717	グリッドサーチとは何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0717	0405	グリッドサーチとは何ですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0717	0307	グリッドサーチとは何ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0717	1412	グリッドサーチとは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0717	0902	グリッドサーチとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0717	0715	グリッドサーチとは何ですか	カーネルトリック
0	0717	0405	グリッドサーチとは何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0717	0114	グリッドサーチとは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0717	0307	グリッドサーチとは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0717	0805	グリッドサーチとは何ですか	誤差逆伝播法
0	0717	0109	グリッドサーチとは何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0717	0407	グリッドサーチとは何ですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0717	1004	グリッドサーチとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0717	0808	グリッドサーチとは何ですか	入力の重み付き和の微分
0	0717	0113	グリッドサーチとは何ですか	入力データに潜む規則性
0	0717	0612	グリッドサーチとは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0717	0502	グリッドサーチとは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0717	1012	グリッドサーチとは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0717	0402	グリッドサーチとは何ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0717	1406	グリッドサーチとは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0717	1301	グリッドサーチとは何ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0717	0306	グリッドサーチとは何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0717	0110	グリッドサーチとは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0717	0803	グリッドサーチとは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0717	1503	グリッドサーチとは何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
1	0801	0801	ニューラルネットワークってどういうものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0801	0708	ニューラルネットワークってどういうものですか	制約を弱める変数
0	0801	1111	ニューラルネットワークってどういうものですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0801	0701	ニューラルネットワークってどういうものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0801	0502	ニューラルネットワークってどういうものですか	SVM
0	0801	1508	ニューラルネットワークってどういうものですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0801	0104	ニューラルネットワークってどういうものですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0801	0701	ニューラルネットワークってどういうものですか	マージン
0	0801	0906	ニューラルネットワークってどういうものですか	十分多くの層
0	0801	0906	ニューラルネットワークってどういうものですか	十分多くの層を持つニューラルネットワーク
0	0801	0802	ニューラルネットワークってどういうものですか	入力層・出力層の数に応じた適当な数
0	0801	0311	ニューラルネットワークってどういうものですか	集合の乱雑さ
0	0801	0702	ニューラルネットワークってどういうものですか	識別面は平面を仮定する
0	0801	0901	ニューラルネットワークってどういうものですか	深層学習に用いるニューラルネットワーク
0	0801	0510	ニューラルネットワークってどういうものですか	特徴空間上でクラスを分割する面
0	0801	1220	ニューラルネットワークってどういうものですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0801	0611	ニューラルネットワークってどういうものですか	識別における決定木の考え方を回帰問題に適用する方法
0	0801	0417	ニューラルネットワークってどういうものですか	ネットワークの構造とアークの条件付き確率
0	0801	0416	ニューラルネットワークってどういうものですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0801	0513	ニューラルネットワークってどういうものですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0801	0911	ニューラルネットワークってどういうものですか	学習時の自由度を意図的に下げていること
0	0801	0502	ニューラルネットワークってどういうものですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0801	0204	ニューラルネットワークってどういうものですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0801	0612	ニューラルネットワークってどういうものですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0801	1004	ニューラルネットワークってどういうものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
1	0802	0802	フィードフォワード型モデルの出力層のクラスの数はいくつ	識別対象のクラス数
0	0802	0209	フィードフォワード型モデルの出力層のクラスの数はいくつ	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0802	1301	フィードフォワード型モデルの出力層のクラスの数はいくつ	連続音声認識
0	0802	0307	フィードフォワード型モデルの出力層のクラスの数はいくつ	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0802	0105	フィードフォワード型モデルの出力層のクラスの数はいくつ	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0802	0505	フィードフォワード型モデルの出力層のクラスの数はいくつ	生物の神経細胞の仕組みをモデル化したもの
0	0802	1111	フィードフォワード型モデルの出力層のクラスの数はいくつ	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0802	1506	フィードフォワード型モデルの出力層のクラスの数はいくつ	その政策に従って行動したときの累積報酬の期待値で評価します
0	0802	1203	フィードフォワード型モデルの出力層のクラスの数はいくつ	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0802	1112	フィードフォワード型モデルの出力層のクラスの数はいくつ	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0802	0701	フィードフォワード型モデルの出力層のクラスの数はいくつ	識別境界線と最も近いデータとの距離
0	0802	0402	フィードフォワード型モデルの出力層のクラスの数はいくつ	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0802	0405	フィードフォワード型モデルの出力層のクラスの数はいくつ	尤度と事前確率の積を最大とするクラス
0	0802	0508	フィードフォワード型モデルの出力層のクラスの数はいくつ	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0802	0811	フィードフォワード型モデルの出力層のクラスの数はいくつ	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0802	0717	フィードフォワード型モデルの出力層のクラスの数はいくつ	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0802	1205	フィードフォワード型モデルの出力層のクラスの数はいくつ	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0802	0606	フィードフォワード型モデルの出力層のクラスの数はいくつ	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0802	0304	フィードフォワード型モデルの出力層のクラスの数はいくつ	個々の事例から，あるクラスについて共通点を見つけること
0	0802	1209	フィードフォワード型モデルの出力層のクラスの数はいくつ	規則の条件部が起こったときに結論部が起こる割合
0	0802	1303	フィードフォワード型モデルの出力層のクラスの数はいくつ	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0802	0504	フィードフォワード型モデルの出力層のクラスの数はいくつ	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0802	1219	フィードフォワード型モデルの出力層のクラスの数はいくつ	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0802	0503	フィードフォワード型モデルの出力層のクラスの数はいくつ	様々な数値データに対して多く用いられる統計モデル
0	0802	1301	フィードフォワード型モデルの出力層のクラスの数はいくつ	形態素解析処理が典型的な問題
1	0803	0803	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0803	0906	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0803	0906	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	多階層構造でもそのまま適用できます
0	0803	0901	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	深層学習に用いるニューラルネットワーク
0	0803	0612	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0803	1205	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0803	0906	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0803	1201	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0803	0307	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0803	0902	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0803	0810	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	誤差が小さくなって消失してしまう
0	0803	1001	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0803	1219	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0803	0105	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0803	1406	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0803	1012	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0803	1510	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0803	0416	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	値が真となる確率を知りたいノードが表す変数
0	0803	0115	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0803	0510	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0803	0708	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	制約を弱める変数
0	0803	0115	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	パターンマイニング
0	0803	1220	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0803	0504	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0803	0810	基底関数ベクトルによる非線形識別面はどういう風に実現されているのか	重みの修正量が層を戻るにつれて小さくなってゆく
1	0109	0109	教師なし学習はどういうものですか	正解が付いていない場合の学習
0	0109	0911	教師なし学習はどういうものですか	学習時の自由度を意図的に下げていること
0	0109	0209	教師なし学習はどういうものですか	正例がどれだけ正しく判定されているかという指標
0	0109	0916	教師なし学習はどういうものですか	リカレントニューラルネットワーク
0	0109	0114	教師なし学習はどういうものですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0109	0901	教師なし学習はどういうものですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0109	1301	教師なし学習はどういうものですか	形態素解析処理が典型的な問題
0	0109	1219	教師なし学習はどういうものですか	どの個人がどの商品を購入したかが記録されているデータ
0	0109	1505	教師なし学習はどういうものですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0109	1403	教師なし学習はどういうものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0109	0402	教師なし学習はどういうものですか	事後確率が最大となるクラスを識別結果とする方法
0	0109	0703	教師なし学習はどういうものですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0109	0114	教師なし学習はどういうものですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0109	1502	教師なし学習はどういうものですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0109	1209	教師なし学習はどういうものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0109	0710	教師なし学習はどういうものですか	もとの空間におけるデータ間の距離関係を保存
0	0109	0510	教師なし学習はどういうものですか	特徴空間上でクラスを分割する面
0	0109	1004	教師なし学習はどういうものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0109	0410	教師なし学習はどういうものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0109	0402	教師なし学習はどういうものですか	事後確率が最大となるクラスを識別結果とする方法
0	0109	0416	教師なし学習はどういうものですか	値が真となる確率を知りたいノードが表す変数
0	0109	0701	教師なし学習はどういうものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0109	0417	教師なし学習はどういうものですか	ネットワークの構造とアークの条件付き確率表
0	0109	1506	教師なし学習はどういうものですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0109	0111	教師なし学習はどういうものですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
1	0805	0805	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	誤差逆伝播法
0	0805	0901	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	深層学習に用いるニューラルネットワーク
0	0805	0906	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0805	0717	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	Grid search
0	0805	0607	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	「投げ縄」という意味
0	0805	0416	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	値が真となる確率を知りたいノードが表す変数
0	0805	0306	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0805	1303	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0805	0602	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	数値型の正解情報のこと
0	0805	0912	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0805	0407	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0805	0602	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	正解情報$y$が数値であるということ
0	0805	0302	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	カテゴリ形式の正解情報のこと
0	0805	1409	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0805	1106	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0805	0611	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0805	1502	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	将棋や囲碁などを行うプログラム
0	0805	0902	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0805	0901	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0805	1506	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	その政策に従って行動したときの累積報酬の期待値で評価します
0	0805	0908	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	ユークリッド距離
0	0805	1412	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	近くのノードは同じクラスになりやすいという仮定
0	0805	0906	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0805	1402	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0805	1409	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法は何か	自分が出した誤りを指摘してくれる他人がいない
1	0810	0810	勾配消失問題とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0810	0901	勾配消失問題とはなんですか	深層学習に用いるニューラルネットワーク
0	0810	0607	勾配消失問題とはなんですか	Lasso回帰
0	0810	1412	勾配消失問題とはなんですか	近くのノードは同じクラスになりやすいという仮定
0	0810	0701	勾配消失問題とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0810	0717	勾配消失問題とはなんですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0810	0512	勾配消失問題とはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0810	0110	勾配消失問題とはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0810	0508	勾配消失問題とはなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0810	0605	勾配消失問題とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0810	1214	勾配消失問題とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0810	0111	勾配消失問題とはなんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0810	1303	勾配消失問題とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0810	1015	勾配消失問題とはなんですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0810	0906	勾配消失問題とはなんですか	十分多くの層を持つニューラルネットワーク
0	0810	1104	勾配消失問題とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0810	0109	勾配消失問題とはなんですか	正解が付いていない場合の学習
0	0810	0413	勾配消失問題とはなんですか	変数間の独立性を表現できること
0	0810	0711	勾配消失問題とはなんですか	カーネル関数
0	0810	1205	勾配消失問題とはなんですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0810	1411	勾配消失問題とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0810	0109	勾配消失問題とはなんですか	正解が付いていない場合の学習
0	0810	1402	勾配消失問題とはなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0810	1001	勾配消失問題とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0810	0402	勾配消失問題とはなんですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
1	1104	1104	階層的クラスタリングとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1104	0901	階層的クラスタリングとはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1104	0614	階層的クラスタリングとはなんですか	線形回帰式
0	1104	1408	階層的クラスタリングとはなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1104	0205	階層的クラスタリングとはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1104	0802	階層的クラスタリングとはなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1104	0302	階層的クラスタリングとはなんですか	カテゴリ形式の正解情報のこと
0	1104	0611	階層的クラスタリングとはなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1104	1110	階層的クラスタリングとはなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1104	0710	階層的クラスタリングとはなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1104	0701	階層的クラスタリングとはなんですか	線形で識別できないデータに対応するため
0	1104	0409	階層的クラスタリングとはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1104	1106	階層的クラスタリングとはなんですか	最も遠い事例対の距離を類似度とする
0	1104	1301	階層的クラスタリングとはなんですか	連続音声認識
0	1104	0805	階層的クラスタリングとはなんですか	誤差逆伝播法
0	1104	1106	階層的クラスタリングとはなんですか	クラスタの重心間の距離を類似度とする
0	1104	0612	階層的クラスタリングとはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1104	0306	階層的クラスタリングとはなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	1104	1219	階層的クラスタリングとはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1104	0801	階層的クラスタリングとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1104	0610	階層的クラスタリングとはなんですか	学習結果の散らばり具合
0	1104	0614	階層的クラスタリングとはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1104	1502	階層的クラスタリングとはなんですか	将棋や囲碁などを行うプログラム
0	1104	0911	階層的クラスタリングとはなんですか	ドロップアウト
0	1104	0404	階層的クラスタリングとはなんですか	事前確率
1	0114	0114	クラスタリングとはどういうものですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0114	0702	クラスタリングとはどういうものですか	識別面は平面を仮定する
0	0114	0512	クラスタリングとはどういうものですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0114	0111	クラスタリングとはどういうものですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0114	0917	クラスタリングとはどういうものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0114	1404	クラスタリングとはどういうものですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0114	0810	クラスタリングとはどういうものですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0114	0805	クラスタリングとはどういうものですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0114	0802	クラスタリングとはどういうものですか	入力層・出力層の数に応じた適当な数
0	0114	1001	クラスタリングとはどういうものですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0114	0614	クラスタリングとはどういうものですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0114	1502	クラスタリングとはどういうものですか	将棋や囲碁などを行うプログラム
0	0114	0311	クラスタリングとはどういうものですか	集合の乱雑さ
0	0114	0507	クラスタリングとはどういうものですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0114	0104	クラスタリングとはどういうものですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0114	0511	クラスタリングとはどういうものですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0114	0902	クラスタリングとはどういうものですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0114	0304	クラスタリングとはどういうものですか	個々の事例から，あるクラスについて共通点を見つけること
0	0114	1306	クラスタリングとはどういうものですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0114	0109	クラスタリングとはどういうものですか	正解が付いていない場合の学習
0	0114	0911	クラスタリングとはどういうものですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0114	0717	クラスタリングとはどういうものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0114	0110	クラスタリングとはどういうものですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0114	0907	クラスタリングとはどういうものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0114	1209	クラスタリングとはどういうものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
1	0811	0811	ReLu関数の良さは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0811	0810	ReLu関数の良さは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0811	0616	ReLu関数の良さは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0811	0508	ReLu関数の良さは何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0811	1506	ReLu関数の良さは何ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0811	0117	ReLu関数の良さは何ですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0811	0606	ReLu関数の良さは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0811	1303	ReLu関数の良さは何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0811	0512	ReLu関数の良さは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0811	0514	ReLu関数の良さは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0811	0104	ReLu関数の良さは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0811	0204	ReLu関数の良さは何ですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0811	0417	ReLu関数の良さは何ですか	ネットワークの構造とアークの条件付き確率
0	0811	0311	ReLu関数の良さは何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0811	0605	ReLu関数の良さは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0811	0703	ReLu関数の良さは何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0811	0510	ReLu関数の良さは何ですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0811	0114	ReLu関数の良さは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0811	0906	ReLu関数の良さは何ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0811	0614	ReLu関数の良さは何ですか	線形回帰式
0	0811	0906	ReLu関数の良さは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0811	1505	ReLu関数の良さは何ですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0811	0805	ReLu関数の良さは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0811	0505	ReLu関数の良さは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	0811	1508	ReLu関数の良さは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
1	0811	0811	勾配が消失しない関数ってなんだったっけ	ReLu
0	0811	0606	勾配が消失しない関数ってなんだったっけ	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0811	0916	勾配が消失しない関数ってなんだったっけ	リカレントニューラルネットワーク
0	0811	0602	勾配が消失しない関数ってなんだったっけ	数値型の正解情報のこと
0	0811	0102	勾配が消失しない関数ってなんだったっけ	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0811	0802	勾配が消失しない関数ってなんだったっけ	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0811	0105	勾配が消失しない関数ってなんだったっけ	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0811	0805	勾配が消失しない関数ってなんだったっけ	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0811	1310	勾配が消失しない関数ってなんだったっけ	Hidden Marcov Model: 隠れマルコフモデル
0	0811	1301	勾配が消失しない関数ってなんだったっけ	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0811	0704	勾配が消失しない関数ってなんだったっけ	ラグランジュの未定乗数法
0	0811	1301	勾配が消失しない関数ってなんだったっけ	個々の要素の間に i.i.d. の関係が成立しないもの
0	0811	1306	勾配が消失しない関数ってなんだったっけ	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0811	1219	勾配が消失しない関数ってなんだったっけ	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0811	1001	勾配が消失しない関数ってなんだったっけ	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0811	1009	勾配が消失しない関数ってなんだったっけ	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0811	0611	勾配が消失しない関数ってなんだったっけ	識別における決定木の考え方を回帰問題に適用する方法
0	0811	0508	勾配が消失しない関数ってなんだったっけ	最小二乗法
0	0811	0505	勾配が消失しない関数ってなんだったっけ	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0811	1409	勾配が消失しない関数ってなんだったっけ	自分が出した誤りを指摘してくれる他人がいない
0	0811	0402	勾配が消失しない関数ってなんだったっけ	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0811	1008	勾配が消失しない関数ってなんだったっけ	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0811	0610	勾配が消失しない関数ってなんだったっけ	学習結果の散らばり具合
0	0811	0711	勾配が消失しない関数ってなんだったっけ	カーネル関数
0	0811	1110	勾配が消失しない関数ってなんだったっけ	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
1	1106	1106	単連結法とはなんですか	最も近い事例対の距離を類似度とする
0	1106	0209	単連結法とはなんですか	正例がどれだけ正しく判定されているかという指標
0	1106	1503	単連結法とはなんですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	1106	1012	単連結法とはなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1106	1303	単連結法とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1106	0901	単連結法とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1106	0911	単連結法とはなんですか	ドロップアウト
0	1106	1111	単連結法とはなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1106	0810	単連結法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1106	1301	単連結法とはなんですか	連続音声認識
0	1106	0701	単連結法とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1106	0117	単連結法とはなんですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	1106	0701	単連結法とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1106	0204	単連結法とはなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	1106	1201	単連結法とはなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1106	1012	単連結法とはなんですか	すべてのデータの重みは平等
0	1106	0602	単連結法とはなんですか	正解情報$y$が数値であるということ
0	1106	0504	単連結法とはなんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1106	1104	単連結法とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1106	0110	単連結法とはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1106	1303	単連結法とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	1106	0602	単連結法とはなんですか	数値型の正解情報のこと
0	1106	1111	単連結法とはなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1106	0404	単連結法とはなんですか	事前確率
0	1106	0508	単連結法とはなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
1	1106	1106	完全連結法とはなんですか	最も遠い事例対の距離を類似度とする
0	1106	1505	完全連結法とはなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1106	0606	完全連結法とはなんですか	山の尾根という意味
0	1106	0505	完全連結法とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1106	0906	完全連結法とはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1106	0405	完全連結法とはなんですか	尤度と事前確率の積を最大とするクラス
0	1106	1407	完全連結法とはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1106	0717	完全連結法とはなんですか	Grid search
0	1106	0503	完全連結法とはなんですか	様々な数値データに対して多く用いられる統計モデル
0	1106	0115	完全連結法とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1106	0409	完全連結法とはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1106	0717	完全連結法とはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1106	0710	完全連結法とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1106	0505	完全連結法とはなんですか	ニューラルネットワーク
0	1106	0114	完全連結法とはなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1106	1306	完全連結法とはなんですか	条件付き確率場（Conditional Random Field: CRF）
0	1106	1506	完全連結法とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1106	1407	完全連結法とはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1106	1214	完全連結法とはなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1106	1501	完全連結法とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1106	0614	完全連結法とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	1106	0411	完全連結法とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1106	0908	完全連結法とはなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1106	0811	完全連結法とはなんですか	ユニットの活性化関数を工夫する方法があります
0	1106	0701	完全連結法とはなんですか	サポートベクトルマシン
1	1106	1106	重心法とはなんですか	クラスタの重心間の距離を類似度とする
0	1106	0505	重心法とはなんですか	パーセプトロン
0	1106	0114	重心法とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1106	1220	重心法とはなんですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1106	1209	重心法とはなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1106	0304	重心法とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	1106	1303	重心法とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1106	0112	重心法とはなんですか	線形回帰，回帰木，モデル木など
0	1106	0801	重心法とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1106	0505	重心法とはなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1106	1310	重心法とはなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	1106	0701	重心法とはなんですか	識別境界線と最も近いデータとの距離
0	1106	1510	重心法とはなんですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	1106	1402	重心法とはなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1106	0313	重心法とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1106	0612	重心法とはなんですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	1106	0810	重心法とはなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1106	0311	重心法とはなんですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1106	1004	重心法とはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1106	1508	重心法とはなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1106	0811	重心法とはなんですか	ユニットの活性化関数を工夫する方法
0	1106	0711	重心法とはなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1106	1301	重心法とはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1106	0810	重心法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1106	0715	重心法とはなんですか	識別面
1	1110	1110	k-means法の問題点はなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	1110	0209	k-means法の問題点はなんですか	正例がどれだけ正しく判定されているかという指標
0	1110	0701	k-means法の問題点はなんですか	マージン
0	1110	0606	k-means法の問題点はなんですか	入力が少し変化したときに，出力も少し変化する
0	1110	0112	k-means法の問題点はなんですか	線形回帰，回帰木，モデル木など
0	1110	0115	k-means法の問題点はなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1110	0505	k-means法の問題点はなんですか	ニューラルネットワーク
0	1110	0502	k-means法の問題点はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1110	0313	k-means法の問題点はなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1110	0510	k-means法の問題点はなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1110	0114	k-means法の問題点はなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1110	0711	k-means法の問題点はなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1110	0701	k-means法の問題点はなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	1110	1007	k-means法の問題点はなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1110	0717	k-means法の問題点はなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1110	1505	k-means法の問題点はなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1110	0206	k-means法の問題点はなんですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1110	0405	k-means法の問題点はなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	1110	1502	k-means法の問題点はなんですか	将棋や囲碁などを行うプログラム
0	1110	0411	k-means法の問題点はなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1110	0105	k-means法の問題点はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1110	0307	k-means法の問題点はなんですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	1110	0911	k-means法の問題点はなんですか	ランダムに一定割合のユニットを消して学習を行う
0	1110	0506	k-means法の問題点はなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1110	0507	k-means法の問題点はなんですか	全ての誤りがなくなることが学習の終了条件なので
1	0901	0901	深層学習とは何か	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0901	0209	深層学習とは何か	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0901	1004	深層学習とは何か	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0901	1409	深層学習とは何か	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0901	0902	深層学習とは何か	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0901	1301	深層学習とは何か	個々の要素の間に i.i.d. の関係が成立しないもの
0	0901	0704	深層学習とは何か	以下の関数$L$の最小値を求めるという問題
0	0901	1205	深層学習とは何か	ある項目集合が頻出ならば，その部分集合も頻出である
0	0901	0510	深層学習とは何か	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0901	1001	深層学習とは何か	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0901	0810	深層学習とは何か	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0901	1501	深層学習とは何か	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0901	1110	深層学習とは何か	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0901	1306	深層学習とは何か	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0901	0411	深層学習とは何か	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0901	0713	深層学習とは何か	文書分類やバイオインフォマティックスなど
0	0901	0105	深層学習とは何か	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0901	0402	深層学習とは何か	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0901	0802	深層学習とは何か	特徴ベクトルの次元数
0	0901	0710	深層学習とは何か	もとの空間におけるデータ間の距離関係を保存
0	0901	0805	深層学習とは何か	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0901	0908	深層学習とは何か	シグモイド関数
0	0901	0906	深層学習とは何か	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0901	1301	深層学習とは何か	ひとまとまりの系列データを特定のクラスに識別する問題
0	0901	0803	深層学習とは何か	非線形識別面
1	1201	1201	頻出項目抽出とはなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	1201	0902	頻出項目抽出とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1201	1010	頻出項目抽出とはなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1201	1001	頻出項目抽出とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1201	0405	頻出項目抽出とはなんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1201	0811	頻出項目抽出とはなんですか	誤差が消失しません
0	1201	0715	頻出項目抽出とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1201	1502	頻出項目抽出とはなんですか	将棋や囲碁などを行うプログラム
0	1201	0509	頻出項目抽出とはなんですか	確率的最急勾配法
0	1201	0803	頻出項目抽出とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1201	0902	頻出項目抽出とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1201	0512	頻出項目抽出とはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1201	0114	頻出項目抽出とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1201	0906	頻出項目抽出とはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1201	1301	頻出項目抽出とはなんですか	形態素解析処理が典型的な問題
0	1201	0115	頻出項目抽出とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1201	1104	頻出項目抽出とはなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1201	0115	頻出項目抽出とはなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1201	1506	頻出項目抽出とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1201	0302	頻出項目抽出とはなんですか	カテゴリ形式の正解情報のこと
0	1201	0912	頻出項目抽出とはなんですか	畳み込みニューラルネットワーク
0	1201	0801	頻出項目抽出とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1201	0912	頻出項目抽出とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1201	0810	頻出項目抽出とはなんですか	誤差が小さくなって消失してしまう
0	1201	0901	頻出項目抽出とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
1	0902	0902	深層学習と他の手法との大きな違いは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0902	0512	深層学習と他の手法との大きな違いは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0902	0911	深層学習と他の手法との大きな違いは何ですか	ランダムに一定割合のユニットを消して学習を行う
0	0902	0802	深層学習と他の手法との大きな違いは何ですか	特徴ベクトルの次元数
0	0902	0911	深層学習と他の手法との大きな違いは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0902	0105	深層学習と他の手法との大きな違いは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0902	0712	深層学習と他の手法との大きな違いは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0902	0406	深層学習と他の手法との大きな違いは何ですか	各クラスから生じる特徴の尤もらしさを表す
0	0902	1302	深層学習と他の手法との大きな違いは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0902	1004	深層学習と他の手法との大きな違いは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0902	1304	深層学習と他の手法との大きな違いは何ですか	出力系列を参照する素性
0	0902	0606	深層学習と他の手法との大きな違いは何ですか	入力が少し変化したときに，出力も少し変化する
0	0902	0614	深層学習と他の手法との大きな違いは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0902	0211	深層学習と他の手法との大きな違いは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0902	1509	深層学習と他の手法との大きな違いは何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0902	0115	深層学習と他の手法との大きな違いは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0902	0801	深層学習と他の手法との大きな違いは何ですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0902	0906	深層学習と他の手法との大きな違いは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0902	0109	深層学習と他の手法との大きな違いは何ですか	正解が付いていない場合の学習
0	0902	1303	深層学習と他の手法との大きな違いは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0902	1409	深層学習と他の手法との大きな違いは何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0902	0512	深層学習と他の手法との大きな違いは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0902	1503	深層学習と他の手法との大きな違いは何ですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0902	1406	深層学習と他の手法との大きな違いは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0902	0114	深層学習と他の手法との大きな違いは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	0908	0908	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0908	1404	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0908	1408	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0908	0402	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	入力を観測した後で計算される確率
0	0908	1304	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	入力と対応させる素性
0	0908	0417	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	ネットワークの構造とアークの条件付き確率表
0	0908	0614	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	回帰木と線形回帰の双方のよいところを取った方法
0	0908	0610	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	片方を減らせば片方が増える
0	0908	1110	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0908	0204	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	特徴ベクトルの次元数を減らすこと
0	0908	0717	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	グリッド
0	0908	0901	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	深層学習に用いるニューラルネットワーク
0	0908	1509	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0908	0911	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	ドロップアウト
0	0908	0411	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0908	1309	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0908	0513	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0908	1506	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	その政策に従って行動したときの累積報酬の期待値で評価します
0	0908	0717	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	Grid search
0	0908	0109	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	正解が付いていない場合の学習
0	0908	0313	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0908	0413	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	変数間の独立性を表現できること
0	0908	0313	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0908	1408	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0908	0906	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
1	0912	0912	畳み込みニューラルネットワークはどのような分野でよく使われるか	画像認識
0	0912	0614	畳み込みニューラルネットワークはどのような分野でよく使われるか	回帰木と線形回帰の双方のよいところを取った方法
0	0912	0802	畳み込みニューラルネットワークはどのような分野でよく使われるか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0912	1110	畳み込みニューラルネットワークはどのような分野でよく使われるか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0912	1404	畳み込みニューラルネットワークはどのような分野でよく使われるか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0912	0514	畳み込みニューラルネットワークはどのような分野でよく使われるか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0912	0507	畳み込みニューラルネットワークはどのような分野でよく使われるか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0912	0610	畳み込みニューラルネットワークはどのような分野でよく使われるか	トレードオフの関係
0	0912	0902	畳み込みニューラルネットワークはどのような分野でよく使われるか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0912	0402	畳み込みニューラルネットワークはどのような分野でよく使われるか	入力を観測した後で計算される確率
0	0912	0512	畳み込みニューラルネットワークはどのような分野でよく使われるか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0912	0801	畳み込みニューラルネットワークはどのような分野でよく使われるか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0912	0917	畳み込みニューラルネットワークはどのような分野でよく使われるか	LSTMセル
0	0912	0313	畳み込みニューラルネットワークはどのような分野でよく使われるか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0912	1501	畳み込みニューラルネットワークはどのような分野でよく使われるか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0912	0901	畳み込みニューラルネットワークはどのような分野でよく使われるか	深層学習に用いるニューラルネットワーク
0	0912	0606	畳み込みニューラルネットワークはどのような分野でよく使われるか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0912	0114	畳み込みニューラルネットワークはどのような分野でよく使われるか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0912	0811	畳み込みニューラルネットワークはどのような分野でよく使われるか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0912	0513	畳み込みニューラルネットワークはどのような分野でよく使われるか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0912	0612	畳み込みニューラルネットワークはどのような分野でよく使われるか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0712	畳み込みニューラルネットワークはどのような分野でよく使われるか	カーネル関数が正定値関数という条件を満たすとき
0	0912	0505	畳み込みニューラルネットワークはどのような分野でよく使われるか	パーセプトロン
0	0912	0715	畳み込みニューラルネットワークはどのような分野でよく使われるか	複雑な非線形変換を求めるという操作を避ける方法
0	0912	0907	畳み込みニューラルネットワークはどのような分野でよく使われるか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
1	0912	0912	畳み込みニューラルネットワークとはどんなものか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0912	0512	畳み込みニューラルネットワークとはどんなものか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0912	0612	畳み込みニューラルネットワークとはどんなものか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0413	畳み込みニューラルネットワークとはどんなものか	変数間の独立性を表現できること
0	0912	0810	畳み込みニューラルネットワークとはどんなものか	2006 年頃に考案された事前学習法
0	0912	0402	畳み込みニューラルネットワークとはどんなものか	入力を観測した後で計算される確率
0	0912	0114	畳み込みニューラルネットワークとはどんなものか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0912	1409	畳み込みニューラルネットワークとはどんなものか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0912	0710	畳み込みニューラルネットワークとはどんなものか	低次元の特徴ベクトルを高次元に写像
0	0912	0711	畳み込みニューラルネットワークとはどんなものか	カーネル関数
0	0912	1408	畳み込みニューラルネットワークとはどんなものか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0912	1012	畳み込みニューラルネットワークとはどんなものか	すべてのデータの重みは平等
0	0912	1502	畳み込みニューラルネットワークとはどんなものか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0912	0311	畳み込みニューラルネットワークとはどんなものか	集合の乱雑さ
0	0912	1214	畳み込みニューラルネットワークとはどんなものか	計算量が膨大であること
0	0912	0115	畳み込みニューラルネットワークとはどんなものか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0912	0406	畳み込みニューラルネットワークとはどんなものか	各クラスから生じる特徴の尤もらしさを表す
0	0912	0505	畳み込みニューラルネットワークとはどんなものか	パーセプトロン
0	0912	0801	畳み込みニューラルネットワークとはどんなものか	このモデルは生物の神経細胞のモデルであると考えられています
0	0912	1104	畳み込みニューラルネットワークとはどんなものか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0912	0612	畳み込みニューラルネットワークとはどんなものか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0811	畳み込みニューラルネットワークとはどんなものか	誤差が消失しません
0	0912	1303	畳み込みニューラルネットワークとはどんなものか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0912	1306	畳み込みニューラルネットワークとはどんなものか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0912	1108	畳み込みニューラルネットワークとはどんなものか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
1	1214	1214	FP-Growthアルゴリズムとはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1214	0808	FP-Growthアルゴリズムとはなんですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1214	0811	FP-Growthアルゴリズムとはなんですか	ReLu
0	1214	1506	FP-Growthアルゴリズムとはなんですか	後に得られる報酬ほど割り引いて計算するための係数
0	1214	0411	FP-Growthアルゴリズムとはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1214	1104	FP-Growthアルゴリズムとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1214	0404	FP-Growthアルゴリズムとはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1214	0406	FP-Growthアルゴリズムとはなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1214	0912	FP-Growthアルゴリズムとはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1214	0307	FP-Growthアルゴリズムとはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1214	1106	FP-Growthアルゴリズムとはなんですか	最も遠い事例対の距離を類似度とする
0	1214	1505	FP-Growthアルゴリズムとはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1214	0510	FP-Growthアルゴリズムとはなんですか	特徴空間上でクラスを分割する面
0	1214	0701	FP-Growthアルゴリズムとはなんですか	学習データからのマージンが最大となる識別境界線
0	1214	0116	FP-Growthアルゴリズムとはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	1214	1207	FP-Growthアルゴリズムとはなんですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1214	0105	FP-Growthアルゴリズムとはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1214	1004	FP-Growthアルゴリズムとはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1214	0802	FP-Growthアルゴリズムとはなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1214	0512	FP-Growthアルゴリズムとはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1214	0704	FP-Growthアルゴリズムとはなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1214	0115	FP-Growthアルゴリズムとはなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1214	0715	FP-Growthアルゴリズムとはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1214	0901	FP-Growthアルゴリズムとはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1214	1306	FP-Growthアルゴリズムとはなんですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
1	0917	0917	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0917	1313	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0917	0116	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	学習データが教師あり／教師なしの混在となっているもの
0	0917	1214	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0917	1406	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0917	0114	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0917	0701	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0917	0707	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0917	1502	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	将棋や囲碁などを行うプログラム
0	0917	0111	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0917	0102	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	現在，人が行っている知的な判断を代わりに行う技術
0	0917	0505	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0917	0612	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0917	0801	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0917	0114	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0917	1215	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0917	0305	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	仮説に対して課す制約
0	0917	0511	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0917	1409	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0917	1106	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	最も近い事例対の距離を類似度とする
0	0917	0701	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	識別境界線と最も近いデータとの距離
0	0917	1106	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0917	1116	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0917	1001	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0917	0801	リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	1302	1302	系列ラベリング問題とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1302	0507	系列ラベリング問題とはなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1302	0811	系列ラベリング問題とはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1302	1108	系列ラベリング問題とはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1302	1303	系列ラベリング問題とはなんですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1302	0405	系列ラベリング問題とはなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	1302	0907	系列ラベリング問題とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1302	0402	系列ラベリング問題とはなんですか	入力を観測した後で計算される確率
0	1302	1507	系列ラベリング問題とはなんですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1302	0701	系列ラベリング問題とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	1302	0614	系列ラベリング問題とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	1302	0911	系列ラベリング問題とはなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	1302	0611	系列ラベリング問題とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	1302	0404	系列ラベリング問題とはなんですか	事前確率
0	1302	0614	系列ラベリング問題とはなんですか	モデル木
0	1302	0710	系列ラベリング問題とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1302	0614	系列ラベリング問題とはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1302	0906	系列ラベリング問題とはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1302	1502	系列ラベリング問題とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1302	0105	系列ラベリング問題とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1302	0811	系列ラベリング問題とはなんですか	引数が負のときは0，0以上のときはその値を出力
0	1302	0704	系列ラベリング問題とはなんですか	ラグランジュの未定乗数法を不等式制約条件
0	1302	1001	系列ラベリング問題とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1302	1001	系列ラベリング問題とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1302	0112	系列ラベリング問題とはなんですか	線形回帰，回帰木，モデル木など
1	0901	0901	深層学習のポイントは何	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0901	0209	深層学習のポイントは何	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0901	0811	深層学習のポイントは何	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0901	1301	深層学習のポイントは何	形態素解析処理が典型的な問題
0	0901	1501	深層学習のポイントは何	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0901	0114	深層学習のポイントは何	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0901	0906	深層学習のポイントは何	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0901	0102	深層学習のポイントは何	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0901	1201	深層学習のポイントは何	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0901	0810	深層学習のポイントは何	重みの修正量が層を戻るにつれて小さくなってゆく
0	0901	1502	深層学習のポイントは何	将棋や囲碁などを行うプログラム
0	0901	0803	深層学習のポイントは何	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0901	0701	深層学習のポイントは何	サポートベクトルマシン
0	0901	0116	深層学習のポイントは何	学習データが教師あり／教師なしの混在となっているもの
0	0901	0902	深層学習のポイントは何	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	0703	深層学習のポイントは何	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0901	0513	深層学習のポイントは何	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0901	0811	深層学習のポイントは何	ReLu
0	0901	0906	深層学習のポイントは何	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0901	0916	深層学習のポイントは何	リカレントニューラルネットワーク
0	0901	0616	深層学習のポイントは何	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0901	1214	深層学習のポイントは何	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0901	1112	深層学習のポイントは何	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0901	0606	深層学習のポイントは何	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0901	0906	深層学習のポイントは何	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
1	0402	0402	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	最大事後確率則
0	0402	0416	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	値が真となる確率を知りたいノードが表す変数
0	0402	0404	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0402	0701	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0402	0906	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0402	1403	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	多次元でも「次元の呪い」にかかっていない，ということ
0	0402	1012	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0402	0802	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	特徴空間上では線形識別面を設定すること
0	0402	0514	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0402	0103	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0402	0205	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0402	0602	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	正解情報$y$が数値であるということ
0	0402	0209	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0402	1506	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0402	0502	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0402	0907	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0402	1510	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0402	1220	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0402	0707	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0402	0810	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0402	1506	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	政策
0	0402	1408	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0402	0715	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	複雑な非線形変換を求めるという操作を避ける方法
0	0402	1001	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0402	0811	事後確率が最大となるクラスを識別結果とする方法を何と言いますか	半分の領域で勾配が1になるので
1	1001	1001	アンサンブル学習とはどういうものですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1001	0105	アンサンブル学習とはどういうものですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1001	0402	アンサンブル学習とはどういうものですか	入力を観測した後で計算される確率
0	1001	0111	アンサンブル学習とはどういうものですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1001	0805	アンサンブル学習とはどういうものですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1001	0411	アンサンブル学習とはどういうものですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1001	0606	アンサンブル学習とはどういうものですか	入力が少し変化したときに，出力も少し変化する
0	1001	0611	アンサンブル学習とはどういうものですか	識別における決定木の考え方を回帰問題に適用する方法
0	1001	0102	アンサンブル学習とはどういうものですか	現在，人が行っている知的な判断を代わりに行う技術
0	1001	0707	アンサンブル学習とはどういうものですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1001	0901	アンサンブル学習とはどういうものですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1001	1301	アンサンブル学習とはどういうものですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1001	0411	アンサンブル学習とはどういうものですか	確率のm推定という考え方を用います
0	1001	0114	アンサンブル学習とはどういうものですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1001	0503	アンサンブル学習とはどういうものですか	様々な数値データに対して多く用いられる統計モデル
0	1001	1309	アンサンブル学習とはどういうものですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1001	0502	アンサンブル学習とはどういうものですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1001	0708	アンサンブル学習とはどういうものですか	制約を弱める変数
0	1001	1111	アンサンブル学習とはどういうものですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1001	0416	アンサンブル学習とはどういうものですか	値が真となる確率を知りたいノードが表す変数
0	1001	0508	アンサンブル学習とはどういうものですか	二乗誤差を最小にするように識別関数を調整する方法
0	1001	0410	アンサンブル学習とはどういうものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1001	0701	アンサンブル学習とはどういうものですか	サポートベクトルマシン
0	1001	1407	アンサンブル学習とはどういうものですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1001	0917	アンサンブル学習とはどういうものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
1	1004	1004	バギングとはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1004	0417	バギングとはなんですか	ネットワークの構造とアークの条件付き確率表
0	1004	0416	バギングとはなんですか	値が真となる確率を知りたいノードが表す変数
0	1004	1506	バギングとはなんですか	最適政策$\pi^*$を獲得すること
0	1004	0614	バギングとはなんですか	回帰木と線形回帰の双方のよいところを取った
0	1004	0510	バギングとはなんですか	特徴空間上でクラスを分割する面
0	1004	1103	バギングとはなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	1004	1114	バギングとはなんですか	クラスタリング結果のデータ数の分布
0	1004	0713	バギングとはなんですか	文書分類やバイオインフォマティックスなど
0	1004	0715	バギングとはなんですか	カーネルトリック
0	1004	0211	バギングとはなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1004	0906	バギングとはなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	1004	0803	バギングとはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1004	0811	バギングとはなんですか	半分の領域で勾配が1になるので
0	1004	0701	バギングとはなんですか	識別境界線と最も近いデータとの距離
0	1004	0901	バギングとはなんですか	音声認識・画像認識・自然言語処理など
0	1004	0611	バギングとはなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1004	1110	バギングとはなんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1004	0411	バギングとはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1004	1403	バギングとはなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1004	0802	バギングとはなんですか	隠れ層
0	1004	0917	バギングとはなんですか	入力ゲート・出力ゲート・忘却ゲート
0	1004	1104	バギングとはなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1004	0808	バギングとはなんですか	入力の重み付き和の微分
0	1004	0502	バギングとはなんですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
1	1009	1009	通常の決定木とランダムフォレストの違いはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	1009	0701	通常の決定木とランダムフォレストの違いはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1009	0612	通常の決定木とランダムフォレストの違いはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1009	1509	通常の決定木とランダムフォレストの違いはなんですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1009	0911	通常の決定木とランダムフォレストの違いはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1009	0102	通常の決定木とランダムフォレストの違いはなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1009	1406	通常の決定木とランダムフォレストの違いはなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1009	0607	通常の決定木とランダムフォレストの違いはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1009	1219	通常の決定木とランダムフォレストの違いはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1009	0702	通常の決定木とランダムフォレストの違いはなんですか	識別面は平面を仮定する
0	1009	0109	通常の決定木とランダムフォレストの違いはなんですか	正解が付いていない場合の学習
0	1009	0614	通常の決定木とランダムフォレストの違いはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	1009	0707	通常の決定木とランダムフォレストの違いはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1009	1306	通常の決定木とランダムフォレストの違いはなんですか	条件付き確率場（Conditional Random Field: CRF）
0	1009	1510	通常の決定木とランダムフォレストの違いはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1009	0508	通常の決定木とランダムフォレストの違いはなんですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	1009	1405	通常の決定木とランダムフォレストの違いはなんですか	半教師あり学習は文書分類問題によく適用されます
0	1009	0614	通常の決定木とランダムフォレストの違いはなんですか	線形回帰式
0	1009	0901	通常の決定木とランダムフォレストの違いはなんですか	深層学習に用いるニューラルネットワーク
0	1009	1412	通常の決定木とランダムフォレストの違いはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1009	0711	通常の決定木とランダムフォレストの違いはなんですか	カーネル関数
0	1009	0404	通常の決定木とランダムフォレストの違いはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1009	0803	通常の決定木とランダムフォレストの違いはなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	1009	0906	通常の決定木とランダムフォレストの違いはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1009	0117	通常の決定木とランダムフォレストの違いはなんですか	学習データの一部にだけ正解が与えられている場合
1	1012	1012	ブースティングのアイディアは何ですか	各データに重みを付け，そのもとで識別器を作成します
0	1012	1004	ブースティングのアイディアは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1012	0109	ブースティングのアイディアは何ですか	学習データに正解が付いている場合の学習
0	1012	0209	ブースティングのアイディアは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1012	0606	ブースティングのアイディアは何ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1012	1110	ブースティングのアイディアは何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1012	0206	ブースティングのアイディアは何ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1012	0701	ブースティングのアイディアは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1012	1001	ブースティングのアイディアは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1012	0701	ブースティングのアイディアは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1012	0911	ブースティングのアイディアは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1012	1103	ブースティングのアイディアは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1012	0708	ブースティングのアイディアは何ですか	制約を弱める変数
0	1012	0606	ブースティングのアイディアは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	1012	0811	ブースティングのアイディアは何ですか	引数が負のときは0，0以上のときはその値を出力
0	1012	1306	ブースティングのアイディアは何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1012	0513	ブースティングのアイディアは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1012	1310	ブースティングのアイディアは何ですか	確率的非決定性オートマトンの一種
0	1012	0805	ブースティングのアイディアは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1012	0111	ブースティングのアイディアは何ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	1012	0917	ブースティングのアイディアは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	1012	0204	ブースティングのアイディアは何ですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	1012	1111	ブースティングのアイディアは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1012	0505	ブースティングのアイディアは何ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1012	0717	ブースティングのアイディアは何ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
1	1015	1015	勾配ブースティングとは何ですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	1015	0508	勾配ブースティングとは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1015	0802	勾配ブースティングとは何ですか	隠れ層
0	1015	0402	勾配ブースティングとは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1015	0206	勾配ブースティングとは何ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	1015	0901	勾配ブースティングとは何ですか	音声認識・画像認識・自然言語処理など
0	1015	0702	勾配ブースティングとは何ですか	識別面は平面を仮定する
0	1015	0906	勾配ブースティングとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1015	1214	勾配ブースティングとは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1015	0711	勾配ブースティングとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1015	0808	勾配ブースティングとは何ですか	シグモイド関数の微分
0	1015	0701	勾配ブースティングとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1015	0710	勾配ブースティングとは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1015	0717	勾配ブースティングとは何ですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	1015	0802	勾配ブースティングとは何ですか	識別対象のクラス数
0	1015	0614	勾配ブースティングとは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1015	0717	勾配ブースティングとは何ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	1015	0912	勾配ブースティングとは何ですか	畳み込みニューラルネットワーク
0	1015	0112	勾配ブースティングとは何ですか	線形回帰，回帰木，モデル木など
0	1015	0305	勾配ブースティングとは何ですか	仮説に対して課す制約
0	1015	1506	勾配ブースティングとは何ですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1015	0209	勾配ブースティングとは何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1015	1220	勾配ブースティングとは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1015	1106	勾配ブースティングとは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1015	0614	勾配ブースティングとは何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
1	1015	1015	損失関数にはどのようなものがありますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1015	0803	損失関数にはどのようなものがありますか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1015	0114	損失関数にはどのようなものがありますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1015	0114	損失関数にはどのようなものがありますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1015	0407	損失関数にはどのようなものがありますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1015	0803	損失関数にはどのようなものがありますか	重みパラメータに対しては線形で，入力を非線形変換する
0	1015	1306	損失関数にはどのようなものがありますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1015	0907	損失関数にはどのようなものがありますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1015	1106	損失関数にはどのようなものがありますか	クラスタの重心間の距離を類似度とする
0	1015	0911	損失関数にはどのようなものがありますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1015	0908	損失関数にはどのようなものがありますか	シグモイド関数
0	1015	1012	損失関数にはどのようなものがありますか	各データに重みを付け，そのもとで識別器を作成します
0	1015	0205	損失関数にはどのようなものがありますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1015	0612	損失関数にはどのようなものがありますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1015	1010	損失関数にはどのようなものがありますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1015	0114	損失関数にはどのようなものがありますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1015	0701	損失関数にはどのようなものがありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1015	0614	損失関数にはどのようなものがありますか	回帰木と線形回帰の双方のよいところを取った
0	1015	0416	損失関数にはどのようなものがありますか	値が真となる確率を知りたいノードが表す変数
0	1015	1214	損失関数にはどのようなものがありますか	計算量が膨大であること
0	1015	0906	損失関数にはどのようなものがありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1015	0404	損失関数にはどのようなものがありますか	事前確率
0	1015	0711	損失関数にはどのようなものがありますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1015	0901	損失関数にはどのようなものがありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1015	0411	損失関数にはどのようなものがありますか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
1	0109	0109	教師なし学習とはどういうものですか	正解が付いていない場合の学習
0	0109	1220	教師なし学習とはどういうものですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0109	0805	教師なし学習とはどういうものですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0109	0507	教師なし学習とはどういうものですか	パーセプトロンの収束定理
0	0109	1406	教師なし学習とはどういうものですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0109	1106	教師なし学習とはどういうものですか	クラスタの重心間の距離を類似度とする
0	0109	0405	教師なし学習とはどういうものですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0109	0606	教師なし学習とはどういうものですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0109	1214	教師なし学習とはどういうものですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0109	1505	教師なし学習とはどういうものですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0109	0502	教師なし学習とはどういうものですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0109	1303	教師なし学習とはどういうものですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0109	0902	教師なし学習とはどういうものですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0109	1004	教師なし学習とはどういうものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0109	0810	教師なし学習とはどういうものですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0109	0710	教師なし学習とはどういうものですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0109	0117	教師なし学習とはどういうものですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0109	1306	教師なし学習とはどういうものですか	条件付き確率場（Conditional Random Field: CRF）
0	0109	1412	教師なし学習とはどういうものですか	近くのノードは同じクラスになりやすいという仮定
0	0109	0902	教師なし学習とはどういうものですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0109	0710	教師なし学習とはどういうものですか	もとの空間におけるデータ間の距離関係を保存
0	0109	0811	教師なし学習とはどういうものですか	ユニットの活性化関数を工夫する方法があります
0	0109	1401	教師なし学習とはどういうものですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0109	0410	教師なし学習とはどういうものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0109	0306	教師なし学習とはどういうものですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
1	0114	0114	モデル推定とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0114	0104	モデル推定とは何ですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0114	0413	モデル推定とは何ですか	変数間の独立性を表現できること
0	0114	0412	モデル推定とは何ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0114	0311	モデル推定とは何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0114	0701	モデル推定とは何ですか	サポートベクトルマシン
0	0114	0404	モデル推定とは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0114	0112	モデル推定とは何ですか	線形回帰，回帰木，モデル木など
0	0114	1215	モデル推定とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0114	0611	モデル推定とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0114	0512	モデル推定とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0114	0505	モデル推定とは何ですか	ニューラルネットワーク
0	0114	1405	モデル推定とは何ですか	半教師あり学習は文書分類問題によく適用されます
0	0114	1411	モデル推定とは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0114	0611	モデル推定とは何ですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0114	0717	モデル推定とは何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0114	1411	モデル推定とは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0114	0612	モデル推定とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0114	1506	モデル推定とは何ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0114	0912	モデル推定とは何ですか	畳み込みニューラルネットワーク
0	0114	1506	モデル推定とは何ですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0114	0505	モデル推定とは何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0114	1103	モデル推定とは何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	0114	0614	モデル推定とは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0114	1505	モデル推定とは何ですか	「マルコフ性」を持つ確率過程における意思決定問題
1	0114	0114	クラスタリングはどのような手法ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0114	0801	クラスタリングはどのような手法ですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0114	0614	クラスタリングはどのような手法ですか	回帰木と線形回帰の双方のよいところを取った
0	0114	0906	クラスタリングはどのような手法ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0114	1306	クラスタリングはどのような手法ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0114	0313	クラスタリングはどのような手法ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0114	0606	クラスタリングはどのような手法ですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0114	1502	クラスタリングはどのような手法ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0114	0506	クラスタリングはどのような手法ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0114	0111	クラスタリングはどのような手法ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0114	1007	クラスタリングはどのような手法ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0114	1114	クラスタリングはどのような手法ですか	クラスタリング結果のデータ数の分布
0	0114	1412	クラスタリングはどのような手法ですか	近くのノードは同じクラスになりやすいという仮定
0	0114	0111	クラスタリングはどのような手法ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0114	0109	クラスタリングはどのような手法ですか	学習データに正解が付いている場合の学習
0	0114	0901	クラスタリングはどのような手法ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0114	0801	クラスタリングはどのような手法ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0114	1201	クラスタリングはどのような手法ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0114	0710	クラスタリングはどのような手法ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0114	0402	クラスタリングはどのような手法ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0114	1310	クラスタリングはどのような手法ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0114	1106	クラスタリングはどのような手法ですか	クラスタの重心間の距離を類似度とする
0	0114	0715	クラスタリングはどのような手法ですか	カーネルトリック
0	0114	0407	クラスタリングはどのような手法ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0114	1301	クラスタリングはどのような手法ですか	個々の要素の間に i.i.d. の関係が成立しないもの
1	1103	1103	内的結合とは何ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	1103	1110	内的結合とは何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1103	0708	内的結合とは何ですか	制約を弱める変数
0	1103	0508	内的結合とは何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	1103	0513	内的結合とは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1103	0313	内的結合とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1103	0901	内的結合とは何ですか	深層学習に用いるニューラルネットワーク
0	1103	0717	内的結合とは何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1103	1214	内的結合とは何ですか	計算量が膨大であること
0	1103	0902	内的結合とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1103	0512	内的結合とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1103	1215	内的結合とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1103	0616	内的結合とは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1103	0906	内的結合とは何ですか	入力に近い側の処理
0	1103	0805	内的結合とは何ですか	誤差逆伝播法
0	1103	0306	内的結合とは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1103	1403	内的結合とは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1103	0113	内的結合とは何ですか	入力データに潜む規則性を学習すること
0	1103	0808	内的結合とは何ですか	入力の重み付き和の微分
0	1103	0508	内的結合とは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1103	0916	内的結合とは何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	1103	1310	内的結合とは何ですか	確率的非決定性オートマトンの一種
0	1103	0505	内的結合とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1103	0304	内的結合とは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1103	1510	内的結合とは何ですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
1	1103	1103	外的分離とは何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	1103	0911	外的分離とは何ですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	1103	0902	外的分離とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1103	0514	外的分離とは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	1103	0708	外的分離とは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1103	0116	外的分離とは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1103	0917	外的分離とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	1103	1410	外的分離とは何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1103	0614	外的分離とは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1103	0917	外的分離とは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	1103	0912	外的分離とは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1103	0707	外的分離とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1103	0111	外的分離とは何ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	1103	0711	外的分離とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1103	0113	外的分離とは何ですか	入力データに潜む規則性
0	1103	0411	外的分離とは何ですか	確率のm推定という考え方を用います
0	1103	0911	外的分離とは何ですか	ランダムに一定割合のユニットを消して学習を行う
0	1103	0514	外的分離とは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	1103	0711	外的分離とは何ですか	カーネル関数
0	1103	0702	外的分離とは何ですか	識別面は平面を仮定する
0	1103	1506	外的分離とは何ですか	政策
0	1103	0908	外的分離とは何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1103	0701	外的分離とは何ですか	識別境界線と最も近いデータとの距離
0	1103	1411	外的分離とは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1103	1106	外的分離とは何ですか	最も遠い事例対の距離を類似度とする
1	1103	1103	分割最適化手法とは何ですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	1103	0715	分割最適化手法とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1103	0512	分割最適化手法とは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1103	1301	分割最適化手法とは何ですか	形態素解析処理が典型的な問題
0	1103	1508	分割最適化手法とは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1103	0805	分割最適化手法とは何ですか	誤差逆伝播法
0	1103	0605	分割最適化手法とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1103	0616	分割最適化手法とは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1103	0810	分割最適化手法とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1103	0606	分割最適化手法とは何ですか	山の尾根という意味
0	1103	0105	分割最適化手法とは何ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	1103	0111	分割最適化手法とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1103	0402	分割最適化手法とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1103	0811	分割最適化手法とは何ですか	半分の領域で勾配が1になるので
0	1103	0907	分割最適化手法とは何ですか	事前学習法
0	1103	0115	分割最適化手法とは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1103	1007	分割最適化手法とは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1103	0711	分割最適化手法とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1103	0508	分割最適化手法とは何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1103	0901	分割最適化手法とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1103	0405	分割最適化手法とは何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1103	1214	分割最適化手法とは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1103	0514	分割最適化手法とは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	1103	0508	分割最適化手法とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	1103	0114	分割最適化手法とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
1	1104	1104	階層的クラスタリングとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1104	0102	階層的クラスタリングとは何ですか	現在，人が行っている知的な判断を代わりに行う技術
0	1104	0611	階層的クラスタリングとは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1104	1108	階層的クラスタリングとは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1104	0907	階層的クラスタリングとは何ですか	事前学習法
0	1104	0704	階層的クラスタリングとは何ですか	ラグランジュの未定乗数法
0	1104	0209	階層的クラスタリングとは何ですか	正例がどれだけ正しく判定されているかという指標
0	1104	0601	階層的クラスタリングとは何ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1104	1505	階層的クラスタリングとは何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1104	0701	階層的クラスタリングとは何ですか	学習データからのマージンが最大となる識別境界線
0	1104	0116	階層的クラスタリングとは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1104	1506	階層的クラスタリングとは何ですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1104	0902	階層的クラスタリングとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1104	0906	階層的クラスタリングとは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1104	1309	階層的クラスタリングとは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1104	0311	階層的クラスタリングとは何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1104	0805	階層的クラスタリングとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1104	0701	階層的クラスタリングとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1104	1301	階層的クラスタリングとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1104	0901	階層的クラスタリングとは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1104	1004	階層的クラスタリングとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1104	1205	階層的クラスタリングとは何ですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1104	0710	階層的クラスタリングとは何ですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1104	0115	階層的クラスタリングとは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1104	1411	階層的クラスタリングとは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
1	1106	1106	重心法とは何ですか	クラスタの重心間の距離を類似度とする
0	1106	1302	重心法とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1106	0305	重心法とは何ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1106	1301	重心法とは何ですか	動画像の分類や音声で入力された単語の識別などの問題
0	1106	0707	重心法とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1106	1304	重心法とは何ですか	出力系列を参照する素性
0	1106	1408	重心法とは何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1106	0117	重心法とは何ですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	1106	0901	重心法とは何ですか	表現学習
0	1106	1412	重心法とは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1106	0802	重心法とは何ですか	隠れ層
0	1106	0512	重心法とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1106	0315	重心法とは何ですか	分割後のデータの分散
0	1106	1012	重心法とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1106	1209	重心法とは何ですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1106	0402	重心法とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1106	0810	重心法とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1106	0508	重心法とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	1106	0606	重心法とは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	1106	1201	重心法とは何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1106	0102	重心法とは何ですか	現在，人が行っている知的な判断を代わりに行う技術
0	1106	0808	重心法とは何ですか	入力の重み付き和の微分
0	1106	0711	重心法とは何ですか	カーネル関数
0	1106	1205	重心法とは何ですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1106	0917	重心法とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
1	1106	1106	Ward法とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1106	0811	Ward法とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	1106	1104	Ward法とは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1106	0911	Ward法とは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1106	0411	Ward法とは何ですか	確率のm推定という考え方を用います
0	1106	0906	Ward法とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1106	0514	Ward法とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1106	1502	Ward法とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1106	0404	Ward法とは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1106	0901	Ward法とは何ですか	深層学習に用いるニューラルネットワーク
0	1106	0402	Ward法とは何ですか	統計的識別手法
0	1106	1110	Ward法とは何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1106	0510	Ward法とは何ですか	特徴空間上でクラスを分割する面
0	1106	0803	Ward法とは何ですか	非線形識別面
0	1106	1012	Ward法とは何ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	1106	1103	Ward法とは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1106	0507	Ward法とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1106	0110	Ward法とは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1106	0911	Ward法とは何ですか	ドロップアウト
0	1106	0102	Ward法とは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1106	0411	Ward法とは何ですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1106	0811	Ward法とは何ですか	ユニットの活性化関数を工夫する方法があります
0	1106	0907	Ward法とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1106	0407	Ward法とは何ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	1106	0514	Ward法とは何ですか	ランダムに学習データを一つ
1	1111	1111	外れ値とは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1111	0605	外れ値とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1111	0612	外れ値とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1111	1205	外れ値とは何ですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	1111	1106	外れ値とは何ですか	最も近い事例対の距離を類似度とする
0	1111	0811	外れ値とは何ですか	半分の領域で勾配が1になるので
0	1111	0410	外れ値とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1111	0506	外れ値とは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1111	0911	外れ値とは何ですか	ランダムに一定割合のユニットを消して学習を行う
0	1111	0307	外れ値とは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	1111	0304	外れ値とは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1111	0801	外れ値とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1111	0115	外れ値とは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1111	1310	外れ値とは何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	1111	1214	外れ値とは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1111	0803	外れ値とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1111	0114	外れ値とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1111	0906	外れ値とは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1111	0710	外れ値とは何ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	1111	0410	外れ値とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1111	0111	外れ値とは何ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	1111	0912	外れ値とは何ですか	畳み込みニューラルネットワーク
0	1111	1112	外れ値とは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	1111	0114	外れ値とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1111	0802	外れ値とは何ですか	識別対象のクラス数
1	1112	1112	局所異常因子とは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	1112	0402	局所異常因子とは何ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1112	0411	局所異常因子とは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1112	0917	局所異常因子とは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	1112	0701	局所異常因子とは何ですか	線形で識別できないデータに対応するため
0	1112	0707	局所異常因子とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1112	0416	局所異常因子とは何ですか	値が真となる確率を知りたいノードが表す変数
0	1112	0605	局所異常因子とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1112	0801	局所異常因子とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1112	0811	局所異常因子とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	1112	0710	局所異常因子とは何ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	1112	0704	局所異常因子とは何ですか	以下の関数$L$の最小値を求めるという問題
0	1112	1407	局所異常因子とは何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1112	0901	局所異常因子とは何ですか	Deep Neural Network (DNN) 
0	1112	0710	局所異常因子とは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1112	1502	局所異常因子とは何ですか	将棋や囲碁などを行うプログラム
0	1112	0512	局所異常因子とは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1112	1209	局所異常因子とは何ですか	この値が高いほど，得られる情報の多い規則であること
0	1112	0701	局所異常因子とは何ですか	識別境界線と最も近いデータとの距離
0	1112	0803	局所異常因子とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1112	1310	局所異常因子とは何ですか	確率的非決定性オートマトンの一種
0	1112	0305	局所異常因子とは何ですか	仮説に対して課す制約
0	1112	0413	局所異常因子とは何ですか	変数間の独立性を表現できること
0	1112	0810	局所異常因子とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1112	1301	局所異常因子とは何ですか	動画像の分類や音声で入力された単語の識別などの問題
1	1201	1201	頻出項目抽出とは何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1201	0114	頻出項目抽出とは何ですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1201	0505	頻出項目抽出とは何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1201	0311	頻出項目抽出とは何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1201	0411	頻出項目抽出とは何ですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1201	0612	頻出項目抽出とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1201	0109	頻出項目抽出とは何ですか	正解が付いていない場合の学習
0	1201	1012	頻出項目抽出とは何ですか	各データに重みを付け，そのもとで識別器を作成します
0	1201	1012	頻出項目抽出とは何ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	1201	1407	頻出項目抽出とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1201	1310	頻出項目抽出とは何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	1201	0810	頻出項目抽出とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1201	0715	頻出項目抽出とは何ですか	カーネルトリック
0	1201	1510	頻出項目抽出とは何ですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	1201	0209	頻出項目抽出とは何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1201	0612	頻出項目抽出とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1201	1407	頻出項目抽出とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1201	0906	頻出項目抽出とは何ですか	多階層構造でもそのまま適用できます
0	1201	0505	頻出項目抽出とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1201	1506	頻出項目抽出とは何ですか	最適政策$\pi^*$を獲得すること
0	1201	0715	頻出項目抽出とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1201	0601	頻出項目抽出とは何ですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	1201	0715	頻出項目抽出とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1201	0811	頻出項目抽出とは何ですか	ユニットの活性化関数を工夫する方法
0	1201	0902	頻出項目抽出とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
1	1207	1207	a prioriアルゴリズムとは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1207	1111	a prioriアルゴリズムとは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1207	0908	a prioriアルゴリズムとは何ですか	シグモイド関数
0	1207	1303	a prioriアルゴリズムとは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1207	0708	a prioriアルゴリズムとは何ですか	制約を弱める変数
0	1207	0614	a prioriアルゴリズムとは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1207	0606	a prioriアルゴリズムとは何ですか	入力が少し変化したときに，出力も少し変化する
0	1207	0802	a prioriアルゴリズムとは何ですか	隠れ層
0	1207	1110	a prioriアルゴリズムとは何ですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1207	0610	a prioriアルゴリズムとは何ですか	真のモデルとの距離
0	1207	0511	a prioriアルゴリズムとは何ですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	1207	1012	a prioriアルゴリズムとは何ですか	すべてのデータの重みは平等
0	1207	1405	a prioriアルゴリズムとは何ですか	半教師あり学習は文書分類問題によく適用されます
0	1207	1008	a prioriアルゴリズムとは何ですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1207	1309	a prioriアルゴリズムとは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1207	0614	a prioriアルゴリズムとは何ですか	線形回帰式
0	1207	1502	a prioriアルゴリズムとは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1207	1203	a prioriアルゴリズムとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1207	0114	a prioriアルゴリズムとは何ですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1207	1403	a prioriアルゴリズムとは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1207	0901	a prioriアルゴリズムとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1207	0611	a prioriアルゴリズムとは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1207	1001	a prioriアルゴリズムとは何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1207	1410	a prioriアルゴリズムとは何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1207	0912	a prioriアルゴリズムとは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
1	1209	1209	確信度とは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	1209	0514	確信度とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1209	0505	確信度とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1209	1103	確信度とは何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	1209	1403	確信度とは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1209	1111	確信度とは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1209	0901	確信度とは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1209	1506	確信度とは何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1209	0605	確信度とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1209	0105	確信度とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1209	0715	確信度とは何ですか	識別面
0	1209	1301	確信度とは何ですか	連続音声認識
0	1209	0610	確信度とは何ですか	真のモデルとの距離
0	1209	1007	確信度とは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1209	1004	確信度とは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1209	1505	確信度とは何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1209	0916	確信度とは何ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1209	0801	確信度とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1209	0606	確信度とは何ですか	山の尾根という意味
0	1209	0416	確信度とは何ですか	値が真となる確率を知りたいノードが表す変数
0	1209	0810	確信度とは何ですか	誤差が小さくなって消失してしまう
0	1209	0502	確信度とは何ですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	1209	0802	確信度とは何ですか	特徴ベクトルの次元数
0	1209	0907	確信度とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1209	1510	確信度とは何ですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
1	1214	1214	FP-Grouthとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1214	0616	FP-Grouthとは何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	1214	1007	FP-Grouthとは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1214	1106	FP-Grouthとは何ですか	クラスタの重心間の距離を類似度とする
0	1214	1301	FP-Grouthとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1214	0614	FP-Grouthとは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1214	1114	FP-Grouthとは何ですか	クラスタリング結果のデータ数の分布から
0	1214	0701	FP-Grouthとは何ですか	識別境界線と最も近いデータとの距離
0	1214	0417	FP-Grouthとは何ですか	ネットワークの構造とアークの条件付き確率
0	1214	0402	FP-Grouthとは何ですか	統計的識別手法
0	1214	0513	FP-Grouthとは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1214	0901	FP-Grouthとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1214	1407	FP-Grouthとは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1214	0503	FP-Grouthとは何ですか	様々な数値データに対して多く用いられる統計モデル
0	1214	0811	FP-Grouthとは何ですか	ユニットの活性化関数を工夫する方法があります
0	1214	0912	FP-Grouthとは何ですか	畳み込みニューラルネットワーク
0	1214	0902	FP-Grouthとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1214	1108	FP-Grouthとは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1214	0502	FP-Grouthとは何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1214	0514	FP-Grouthとは何ですか	ランダムに学習データを一つ
0	1214	0503	FP-Grouthとは何ですか	様々な数値データに対して多く用いられる統計モデル
0	1214	0715	FP-Grouthとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1214	0611	FP-Grouthとは何ですか	回帰
0	1214	0404	FP-Grouthとは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1214	0204	FP-Grouthとは何ですか	特徴ベクトルの次元数を減らすこと
1	1215	1215	FP 木とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1215	0713	FP 木とは何ですか	文書分類やバイオインフォマティックスなど
0	1215	0509	FP 木とは何ですか	確率的最急勾配法
0	1215	1303	FP 木とは何ですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1215	0810	FP 木とは何ですか	誤差が小さくなって消失してしまう
0	1215	0802	FP 木とは何ですか	入力層・出力層の数に応じた適当な数
0	1215	0803	FP 木とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1215	1503	FP 木とは何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	1215	1205	FP 木とは何ですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	1215	0102	FP 木とは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1215	0802	FP 木とは何ですか	多層パーセプトロンあるいはニューラルネットワーク
0	1215	0811	FP 木とは何ですか	ユニットの活性化関数を工夫する方法
0	1215	1214	FP 木とは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1215	1209	FP 木とは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	1215	0409	FP 木とは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1215	0105	FP 木とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1215	0411	FP 木とは何ですか	確率のm推定という考え方を用います
0	1215	1008	FP 木とは何ですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1215	1304	FP 木とは何ですか	入力と対応させる素性
0	1215	0702	FP 木とは何ですか	識別面は平面を仮定する
0	1215	0311	FP 木とは何ですか	集合の乱雑さ
0	1215	1214	FP 木とは何ですか	計算量が膨大であること
0	1215	0701	FP 木とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1215	0204	FP 木とは何ですか	特徴ベクトルの次元数を減らすこと
0	1215	0710	FP 木とは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
1	1219	1219	協調フィルタリングとは何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1219	0104	協調フィルタリングとは何ですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	1219	1403	協調フィルタリングとは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1219	1303	協調フィルタリングとは何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1219	1407	協調フィルタリングとは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1219	0810	協調フィルタリングとは何ですか	勾配消失問題
0	1219	1009	協調フィルタリングとは何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	1219	0102	協調フィルタリングとは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1219	0805	協調フィルタリングとは何ですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	1219	0508	協調フィルタリングとは何ですか	最小二乗法
0	1219	0908	協調フィルタリングとは何ですか	ユークリッド距離
0	1219	0402	協調フィルタリングとは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1219	0803	協調フィルタリングとは何ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1219	0304	協調フィルタリングとは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1219	1207	協調フィルタリングとは何ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1219	0402	協調フィルタリングとは何ですか	最大事後確率則
0	1219	0710	協調フィルタリングとは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1219	0514	協調フィルタリングとは何ですか	ランダムに学習データを一つ
0	1219	1001	協調フィルタリングとは何ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	1219	0707	協調フィルタリングとは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1219	0305	協調フィルタリングとは何ですか	仮説に対して課す制約
0	1219	0316	協調フィルタリングとは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1219	1508	協調フィルタリングとは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1219	0306	協調フィルタリングとは何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	1219	1501	協調フィルタリングとは何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
1	1106	1106	単連結法とは何ですか	最も近い事例対の距離を類似度とする
0	1106	1403	単連結法とは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1106	1306	単連結法とは何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1106	0404	単連結法とは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1106	0614	単連結法とは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1106	0801	単連結法とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1106	0810	単連結法とは何ですか	勾配消失問題
0	1106	0906	単連結法とは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1106	0907	単連結法とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1106	0802	単連結法とは何ですか	識別対象のクラス数
0	1106	0810	単連結法とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1106	0810	単連結法とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1106	1404	単連結法とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	1106	1502	単連結法とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1106	0402	単連結法とは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1106	1406	単連結法とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1106	0114	単連結法とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1106	0801	単連結法とは何ですか	このモデルは生物の神経細胞のモデルであると考えられています
0	1106	0305	単連結法とは何ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1106	1407	単連結法とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1106	0316	単連結法とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1106	0614	単連結法とは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1106	0510	単連結法とは何ですか	特徴空間上でクラスを分割する面
0	1106	0114	単連結法とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1106	0402	単連結法とは何ですか	統計的識別手法
1	1106	1106	完全連結法とは何ですか	最も遠い事例対の距離を類似度とする
0	1106	0607	完全連結法とは何ですか	Lasso回帰
0	1106	0206	完全連結法とは何ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	1106	0504	完全連結法とは何ですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1106	1409	完全連結法とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1106	1001	完全連結法とは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1106	0402	完全連結法とは何ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1106	0306	完全連結法とは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1106	1004	完全連結法とは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1106	0907	完全連結法とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1106	0417	完全連結法とは何ですか	ネットワークの構造とアークの条件付き確率表
0	1106	1412	完全連結法とは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1106	0911	完全連結法とは何ですか	ドロップアウト
0	1106	1220	完全連結法とは何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1106	0306	完全連結法とは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1106	0502	完全連結法とは何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1106	0906	完全連結法とは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1106	0803	完全連結法とは何ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1106	0102	完全連結法とは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1106	0114	完全連結法とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1106	1012	完全連結法とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1106	1304	完全連結法とは何ですか	出力系列を参照する素性
0	1106	0908	完全連結法とは何ですか	シグモイド関数
0	1106	0606	完全連結法とは何ですか	山の尾根という意味
0	1106	1012	完全連結法とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
1	0702	0702	サポートベクトルマシンの識別面はどのような形になりますか	識別面は平面を仮定する
0	0702	0505	サポートベクトルマシンの識別面はどのような形になりますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0702	0708	サポートベクトルマシンの識別面はどのような形になりますか	制約を弱める変数
0	0702	0907	サポートベクトルマシンの識別面はどのような形になりますか	事前学習法
0	0702	0306	サポートベクトルマシンの識別面はどのような形になりますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0702	0901	サポートベクトルマシンの識別面はどのような形になりますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0702	0209	サポートベクトルマシンの識別面はどのような形になりますか	正例がどれだけ正しく判定されているかという指標
0	0702	1110	サポートベクトルマシンの識別面はどのような形になりますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0702	0205	サポートベクトルマシンの識別面はどのような形になりますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0702	0810	サポートベクトルマシンの識別面はどのような形になりますか	勾配消失問題
0	0702	1502	サポートベクトルマシンの識別面はどのような形になりますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0702	1408	サポートベクトルマシンの識別面はどのような形になりますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0702	0406	サポートベクトルマシンの識別面はどのような形になりますか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0702	1116	サポートベクトルマシンの識別面はどのような形になりますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0702	0113	サポートベクトルマシンの識別面はどのような形になりますか	入力データに潜む規則性
0	0702	0803	サポートベクトルマシンの識別面はどのような形になりますか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0702	0901	サポートベクトルマシンの識別面はどのような形になりますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0702	0810	サポートベクトルマシンの識別面はどのような形になりますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0702	0104	サポートベクトルマシンの識別面はどのような形になりますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0702	1111	サポートベクトルマシンの識別面はどのような形になりますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0702	0614	サポートベクトルマシンの識別面はどのような形になりますか	回帰木と線形回帰の双方のよいところを取った方法
0	0702	0614	サポートベクトルマシンの識別面はどのような形になりますか	回帰木と線形回帰の双方のよいところを取った方法
0	0702	0305	サポートベクトルマシンの識別面はどのような形になりますか	仮説に対して課す制約
0	0702	0901	サポートベクトルマシンの識別面はどのような形になりますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0702	0416	サポートベクトルマシンの識別面はどのような形になりますか	値が真となる確率を知りたいノードが表す変数
1	0711	0711	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	カーネル関数
0	0711	0616	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0711	0601	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0711	0605	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0711	0901	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	音声認識・画像認識・自然言語処理など
0	0711	0114	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	階層的クラスタリングや k-means 法
0	0711	0805	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0711	0410	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0711	0311	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	集合の乱雑さ
0	0711	0513	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0711	1503	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0711	0313	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0711	1412	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	近くのノードは同じクラスになりやすいという仮定
0	0711	1506	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0711	1106	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	最も遠い事例対の距離を類似度とする
0	0711	1110	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0711	0810	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0711	0713	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	文書分類やバイオインフォマティックスなど
0	0711	0103	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0711	1219	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0711	0505	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0711	0602	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	正解情報$y$が数値であるということ
0	0711	0507	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0711	0507	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	パーセプトロンの収束定理
0	0711	0402	もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
1	0715	0715	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	カーネルトリック
0	0715	0912	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	畳み込みニューラルネットワーク
0	0715	0802	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	識別対象のクラス数
0	0715	0710	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	もとの空間におけるデータ間の距離関係を保存
0	0715	0908	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0715	1207	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0715	0407	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0715	1209	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	規則の条件部が起こったときに結論部が起こる割合
0	0715	1108	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0715	0902	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0715	0605	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0715	0105	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0715	0306	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0715	0616	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0715	0717	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0715	0912	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	画像認識
0	0715	0901	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	深層学習に用いるニューラルネットワーク
0	0715	0811	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	ReLu
0	0715	1404	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0715	0410	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0715	1110	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0715	0917	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0715	0416	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	値が真となる確率を知りたいノードが表す変数
0	0715	0708	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	制約を弱める変数
0	0715	0912	複雑な非線形変換を求めるという操作を避ける方法をなんといいますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
1	0715	0715	カーネル関数を定めて識別面を得る方法はなんですか	カーネルトリック
0	0715	1409	カーネル関数を定めて識別面を得る方法はなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0715	0606	カーネル関数を定めて識別面を得る方法はなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0715	1502	カーネル関数を定めて識別面を得る方法はなんですか	将棋や囲碁などを行うプログラム
0	0715	0114	カーネル関数を定めて識別面を得る方法はなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0715	1509	カーネル関数を定めて識別面を得る方法はなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0715	0708	カーネル関数を定めて識別面を得る方法はなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0715	0611	カーネル関数を定めて識別面を得る方法はなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0715	1405	カーネル関数を定めて識別面を得る方法はなんですか	半教師あり学習は文書分類問題によく適用されます
0	0715	1510	カーネル関数を定めて識別面を得る方法はなんですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0715	1004	カーネル関数を定めて識別面を得る方法はなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0715	0103	カーネル関数を定めて識別面を得る方法はなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0715	0717	カーネル関数を定めて識別面を得る方法はなんですか	連続値
0	0715	0117	カーネル関数を定めて識別面を得る方法はなんですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0715	0614	カーネル関数を定めて識別面を得る方法はなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0715	1209	カーネル関数を定めて識別面を得る方法はなんですか	この値が高いほど，得られる情報の多い規則であること
0	0715	1403	カーネル関数を定めて識別面を得る方法はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0715	0802	カーネル関数を定めて識別面を得る方法はなんですか	特徴空間上では線形識別面を設定すること
0	0715	0204	カーネル関数を定めて識別面を得る方法はなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0715	0116	カーネル関数を定めて識別面を得る方法はなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0715	0917	カーネル関数を定めて識別面を得る方法はなんですか	LSTMセル
0	0715	0912	カーネル関数を定めて識別面を得る方法はなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0715	0901	カーネル関数を定めて識別面を得る方法はなんですか	深層学習に用いるニューラルネットワーク
0	0715	0710	カーネル関数を定めて識別面を得る方法はなんですか	もとの空間におけるデータ間の距離関係を保存
0	0715	1001	カーネル関数を定めて識別面を得る方法はなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
1	0717	0717	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	Grid search
0	0717	0805	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0717	1406	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0717	0810	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	重みの修正量が層を戻るにつれて小さくなってゆく
0	0717	0710	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0717	0313	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0717	1012	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0717	0701	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0717	0505	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	生物の神経細胞の仕組みをモデル化したもの
0	0717	0715	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	複雑な非線形変換を求めるという操作を避ける方法
0	0717	0402	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	入力を観測した後で計算される確率
0	0717	0715	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	複雑な非線形変換を求めるという操作を避ける方法
0	0717	1215	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0717	0713	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	文書分類やバイオインフォマティックスなど
0	0717	0710	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	低次元の特徴ベクトルを高次元に写像
0	0717	0701	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0717	0514	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0717	0306	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0717	1404	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0717	0508	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0717	1219	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0717	0406	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0717	0906	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0717	0912	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	畳み込みニューラルネットワーク
0	0717	0409	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
1	0801	0801	ニューラルネットワークとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0801	0602	ニューラルネットワークとはなんですか	正解情報$y$が数値であるということ
0	0801	1106	ニューラルネットワークとはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0801	0313	ニューラルネットワークとはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0801	0710	ニューラルネットワークとはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0801	0114	ニューラルネットワークとはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0801	1301	ニューラルネットワークとはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0801	1209	ニューラルネットワークとはなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0801	0404	ニューラルネットワークとはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0801	1306	ニューラルネットワークとはなんですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0801	1505	ニューラルネットワークとはなんですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0801	0413	ニューラルネットワークとはなんですか	変数間の独立性を表現できること
0	0801	1407	ニューラルネットワークとはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0801	1214	ニューラルネットワークとはなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0801	1007	ニューラルネットワークとはなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0801	1111	ニューラルネットワークとはなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0801	0116	ニューラルネットワークとはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0801	0104	ニューラルネットワークとはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0801	0710	ニューラルネットワークとはなんですか	低次元の特徴ベクトルを高次元に写像
0	0801	1503	ニューラルネットワークとはなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0801	1303	ニューラルネットワークとはなんですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0801	1406	ニューラルネットワークとはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0801	0901	ニューラルネットワークとはなんですか	音声認識・画像認識・自然言語処理など
0	0801	0610	ニューラルネットワークとはなんですか	真のモデルとの距離
0	0801	0708	ニューラルネットワークとはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
1	0802	0802	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	多層パーセプトロンあるいはニューラルネットワーク
0	0802	0906	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0802	0315	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	分割後のデータの分散
0	0802	0405	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	尤度と事前確率の積を最大とするクラス
0	0802	1506	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	最適政策$\pi^*$を獲得すること
0	0802	0715	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	カーネルトリック
0	0802	0104	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0802	0510	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	特徴空間上でクラスを分割する面
0	0802	0115	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0802	1303	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0802	0114	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0802	1108	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0802	0908	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	シグモイド関数
0	0802	0717	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0802	1106	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	クラスタの重心間の距離を類似度とする
0	0802	0407	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0802	0111	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0802	0402	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	事後確率が最大となるクラスを識別結果とする方法
0	0802	0612	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0802	0306	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0802	1111	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0802	0402	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0802	0805	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	誤差逆伝播法
0	0802	0811	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	引数が負のときは0，0以上のときはその値を出力
0	0802	0701	パーセプトロンをノードとして，階層状に結合したものをなんといいますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0802	0802	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	特徴ベクトルの次元数
0	0802	1012	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0802	0701	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	学習データからのマージンが最大となる識別境界線
0	0802	0906	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0802	0211	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0802	0906	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0802	1203	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0802	0509	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	確率的最急勾配法
0	0802	1009	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0802	1501	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0802	0410	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0802	0803	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0802	0508	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0802	1103	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	異なったまとまり間の距離はなるべく遠くなるように
0	0802	0701	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0802	0701	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	識別境界線と最も近いデータとの距離
0	0802	0514	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0802	0103	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0802	0416	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	値が真となる確率を知りたいノードが表す変数
0	0802	0512	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0802	0506	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0802	0206	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0802	1303	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0802	0713	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	文書分類やバイオインフォマティックスなど
0	0802	0711	3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
1	0802	0802	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	識別対象のクラス数
0	0802	0906	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0802	1104	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0802	1104	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0802	1506	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0802	0902	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0802	0916	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	リカレントニューラルネットワーク
0	0802	1010	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0802	1304	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	入力と対応させる素性
0	0802	0313	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0802	0906	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0802	0514	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	ランダムに学習データを一つ
0	0802	0105	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0802	0514	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0802	1306	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0802	0307	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0802	0715	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	複雑な非線形変換を求めるという操作を避ける方法
0	0802	1201	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0802	0104	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0802	1306	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0802	0906	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	十分多くの層を持つニューラルネットワーク
0	0802	0916	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0802	1403	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	多次元でも「次元の呪い」にかかっていない，ということ
0	0802	1503	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0802	1306	3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか	条件付き確率場（Conditional Random Field: CRF）
1	0803	0803	なぜノードを階層的に組むと非線形識別面が実現できるのか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0803	0906	なぜノードを階層的に組むと非線形識別面が実現できるのか	十分多くの層を持つニューラルネットワーク
0	0803	0206	なぜノードを階層的に組むと非線形識別面が実現できるのか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0803	0313	なぜノードを階層的に組むと非線形識別面が実現できるのか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0803	1306	なぜノードを階層的に組むと非線形識別面が実現できるのか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0803	0410	なぜノードを階層的に組むと非線形識別面が実現できるのか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0803	1506	なぜノードを階層的に組むと非線形識別面が実現できるのか	最適政策$\pi^*$を獲得すること
0	0803	1001	なぜノードを階層的に組むと非線形識別面が実現できるのか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0803	1209	なぜノードを階層的に組むと非線形識別面が実現できるのか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0803	0917	なぜノードを階層的に組むと非線形識別面が実現できるのか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0803	0601	なぜノードを階層的に組むと非線形識別面が実現できるのか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0803	1505	なぜノードを階層的に組むと非線形識別面が実現できるのか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0803	0701	なぜノードを階層的に組むと非線形識別面が実現できるのか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0803	1106	なぜノードを階層的に組むと非線形識別面が実現できるのか	クラスタの重心間の距離を類似度とする
0	0803	0917	なぜノードを階層的に組むと非線形識別面が実現できるのか	入力ゲート・出力ゲート・忘却ゲート
0	0803	0701	なぜノードを階層的に組むと非線形識別面が実現できるのか	識別境界線と最も近いデータとの距離
0	0803	0711	なぜノードを階層的に組むと非線形識別面が実現できるのか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0803	0511	なぜノードを階層的に組むと非線形識別面が実現できるのか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0803	0109	なぜノードを階層的に組むと非線形識別面が実現できるのか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0803	0514	なぜノードを階層的に組むと非線形識別面が実現できるのか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0803	0514	なぜノードを階層的に組むと非線形識別面が実現できるのか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0803	0810	なぜノードを階層的に組むと非線形識別面が実現できるのか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0803	1110	なぜノードを階層的に組むと非線形識別面が実現できるのか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0803	0902	なぜノードを階層的に組むと非線形識別面が実現できるのか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0803	0708	なぜノードを階層的に組むと非線形識別面が実現できるのか	制約を弱める変数
1	0805	0805	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	誤差逆伝播法
0	0805	1106	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0805	0507	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0805	0307	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0805	0907	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0805	0607	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0805	0901	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	音声認識・画像認識・自然言語処理など
0	0805	1502	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0805	0708	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	制約を弱める変数
0	0805	1310	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	0805	1409	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0805	0605	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0805	0606	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0805	1403	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0805	1502	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0805	1306	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0805	1114	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	クラスタリング結果のデータ数の分布
0	0805	0502	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0805	0404	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0805	1104	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0805	0802	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	多層パーセプトロンあるいはニューラルネットワーク
0	0805	0507	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0805	1410	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	学習初期の誤りに強いということ
0	0805	0305	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0805	0117	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか	学習データの一部にだけ正解が与えられている場合
1	0810	0810	多段階に誤差逆伝播法を適用すると問題はありますか	誤差が小さくなって消失してしまう
0	0810	0612	多段階に誤差逆伝播法を適用すると問題はありますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0810	0901	多段階に誤差逆伝播法を適用すると問題はありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0810	0205	多段階に誤差逆伝播法を適用すると問題はありますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0810	0901	多段階に誤差逆伝播法を適用すると問題はありますか	深層学習に用いるニューラルネットワーク
0	0810	0701	多段階に誤差逆伝播法を適用すると問題はありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	0802	多段階に誤差逆伝播法を適用すると問題はありますか	特徴ベクトルの次元数
0	0810	1303	多段階に誤差逆伝播法を適用すると問題はありますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0810	1409	多段階に誤差逆伝播法を適用すると問題はありますか	自分が出した誤りを指摘してくれる他人がいない
0	0810	1301	多段階に誤差逆伝播法を適用すると問題はありますか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0810	0701	多段階に誤差逆伝播法を適用すると問題はありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	0114	多段階に誤差逆伝播法を適用すると問題はありますか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0810	0906	多段階に誤差逆伝播法を適用すると問題はありますか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0810	0115	多段階に誤差逆伝播法を適用すると問題はありますか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0810	0715	多段階に誤差逆伝播法を適用すると問題はありますか	カーネルトリック
0	0810	1106	多段階に誤差逆伝播法を適用すると問題はありますか	最も近い事例対の距離を類似度とする
0	0810	1007	多段階に誤差逆伝播法を適用すると問題はありますか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0810	0702	多段階に誤差逆伝播法を適用すると問題はありますか	識別面は平面を仮定する
0	0810	1103	多段階に誤差逆伝播法を適用すると問題はありますか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0810	0514	多段階に誤差逆伝播法を適用すると問題はありますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0810	0504	多段階に誤差逆伝播法を適用すると問題はありますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0810	1506	多段階に誤差逆伝播法を適用すると問題はありますか	最適政策$\pi^*$を獲得すること
0	0810	0907	多段階に誤差逆伝播法を適用すると問題はありますか	事前学習法
0	0810	0715	多段階に誤差逆伝播法を適用すると問題はありますか	識別面
0	0810	1505	多段階に誤差逆伝播法を適用すると問題はありますか	「マルコフ性」を持つ確率過程における意思決定問題
1	0802	0802	中間層はいくつ用意しますか	入力層・出力層の数に応じた適当な数
0	0802	0614	中間層はいくつ用意しますか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0802	1506	中間層はいくつ用意しますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0802	1310	中間層はいくつ用意しますか	確率的非決定性オートマトンの一種
0	0802	0701	中間層はいくつ用意しますか	学習データからのマージンが最大となる識別境界線
0	0802	0612	中間層はいくつ用意しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0802	1214	中間層はいくつ用意しますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0802	0307	中間層はいくつ用意しますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0802	0710	中間層はいくつ用意しますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0802	0102	中間層はいくつ用意しますか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0802	0315	中間層はいくつ用意しますか	分割後のデータの分散
0	0802	0111	中間層はいくつ用意しますか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0802	0507	中間層はいくつ用意しますか	パーセプトロンの収束定理
0	0802	1111	中間層はいくつ用意しますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0802	0510	中間層はいくつ用意しますか	特徴空間上でクラスを分割する面
0	0802	0513	中間層はいくつ用意しますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0802	0701	中間層はいくつ用意しますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0802	0704	中間層はいくつ用意しますか	以下の関数$L$の最小値を求めるという問題
0	0802	0316	中間層はいくつ用意しますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0802	0112	中間層はいくつ用意しますか	線形回帰，回帰木，モデル木など
0	0802	0612	中間層はいくつ用意しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0802	0402	中間層はいくつ用意しますか	入力を観測した後で計算される確率
0	0802	1303	中間層はいくつ用意しますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0802	0901	中間層はいくつ用意しますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0802	0908	中間層はいくつ用意しますか	シグモイド関数
1	0803	0803	基底関数ベクトルによる非線形識別面はどのように実現されていますか	重みパラメータに対しては線形で，入力を非線形変換する
0	0803	0710	基底関数ベクトルによる非線形識別面はどのように実現されていますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0803	0912	基底関数ベクトルによる非線形識別面はどのように実現されていますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0803	0315	基底関数ベクトルによる非線形識別面はどのように実現されていますか	分割後のデータの分散
0	0803	0907	基底関数ベクトルによる非線形識別面はどのように実現されていますか	事前学習法
0	0803	1301	基底関数ベクトルによる非線形識別面はどのように実現されていますか	連続音声認識
0	0803	0912	基底関数ベクトルによる非線形識別面はどのように実現されていますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0803	1303	基底関数ベクトルによる非線形識別面はどのように実現されていますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0803	1301	基底関数ベクトルによる非線形識別面はどのように実現されていますか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0803	0117	基底関数ベクトルによる非線形識別面はどのように実現されていますか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0803	1103	基底関数ベクトルによる非線形識別面はどのように実現されていますか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0803	0205	基底関数ベクトルによる非線形識別面はどのように実現されていますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0803	0708	基底関数ベクトルによる非線形識別面はどのように実現されていますか	制約を弱める変数
0	0803	0614	基底関数ベクトルによる非線形識別面はどのように実現されていますか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0803	0704	基底関数ベクトルによる非線形識別面はどのように実現されていますか	ラグランジュの未定乗数法
0	0803	0313	基底関数ベクトルによる非線形識別面はどのように実現されていますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0803	0805	基底関数ベクトルによる非線形識別面はどのように実現されていますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0803	0906	基底関数ベクトルによる非線形識別面はどのように実現されていますか	十分多くの層
0	0803	0901	基底関数ベクトルによる非線形識別面はどのように実現されていますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0803	0114	基底関数ベクトルによる非線形識別面はどのように実現されていますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0803	1412	基底関数ベクトルによる非線形識別面はどのように実現されていますか	近くのノードは同じクラスになりやすいという仮定
0	0803	1110	基底関数ベクトルによる非線形識別面はどのように実現されていますか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0803	1015	基底関数ベクトルによる非線形識別面はどのように実現されていますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0803	0505	基底関数ベクトルによる非線形識別面はどのように実現されていますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0803	0805	基底関数ベクトルによる非線形識別面はどのように実現されていますか	誤差逆伝播法
1	0901	0901	深層学習はどのような分野で用いられていますか	音声認識・画像認識・自然言語処理など
0	0901	1303	深層学習はどのような分野で用いられていますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0901	0701	深層学習はどのような分野で用いられていますか	サポートベクトルマシン
0	0901	1106	深層学習はどのような分野で用いられていますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0901	0406	深層学習はどのような分野で用いられていますか	各クラスから生じる特徴の尤もらしさを表す
0	0901	0715	深層学習はどのような分野で用いられていますか	複雑な非線形変換を求めるという操作を避ける方法
0	0901	1309	深層学習はどのような分野で用いられていますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0901	0912	深層学習はどのような分野で用いられていますか	画像認識
0	0901	0402	深層学習はどのような分野で用いられていますか	事後確率が最大となるクラスを識別結果とする方法
0	0901	0811	深層学習はどのような分野で用いられていますか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0901	0610	深層学習はどのような分野で用いられていますか	トレードオフの関係
0	0901	0502	深層学習はどのような分野で用いられていますか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0901	0316	深層学習はどのような分野で用いられていますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0901	1008	深層学習はどのような分野で用いられていますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0901	0612	深層学習はどのような分野で用いられていますか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0901	0911	深層学習はどのような分野で用いられていますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0901	0405	深層学習はどのような分野で用いられていますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0901	0508	深層学習はどのような分野で用いられていますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0901	0707	深層学習はどのような分野で用いられていますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0901	1301	深層学習はどのような分野で用いられていますか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0901	0302	深層学習はどのような分野で用いられていますか	カテゴリ形式の正解情報のこと
0	0901	0305	深層学習はどのような分野で用いられていますか	仮説に対して課す制約
0	0901	0713	深層学習はどのような分野で用いられていますか	文書分類やバイオインフォマティックスなど
0	0901	0306	深層学習はどのような分野で用いられていますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0901	1209	深層学習はどのような分野で用いられていますか	規則の条件部が起こったときに結論部が起こる割合
1	0901	0901	深層学習とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0901	0407	深層学習とはなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0901	1408	深層学習とはなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0901	0105	深層学習とはなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0901	0811	深層学習とはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0901	0906	深層学習とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0901	1409	深層学習とはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0901	0712	深層学習とはなんですか	カーネル関数が正定値関数という条件を満たすとき
0	0901	0616	深層学習とはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0901	1412	深層学習とはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0901	0907	深層学習とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0901	0113	深層学習とはなんですか	入力データに潜む規則性を学習すること
0	0901	0701	深層学習とはなんですか	線形で識別できないデータに対応するため
0	0901	0114	深層学習とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0901	1103	深層学習とはなんですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0901	0710	深層学習とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0901	1104	深層学習とはなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0901	0701	深層学習とはなんですか	サポートベクトルマシン
0	0901	0717	深層学習とはなんですか	連続値
0	0901	0708	深層学習とはなんですか	制約を弱める変数
0	0901	0114	深層学習とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0901	1219	深層学習とはなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0901	0917	深層学習とはなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0901	0908	深層学習とはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0901	0917	深層学習とはなんですか	LSTMセル
1	0902	0902	深層学習と他の識別問題との違いはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0902	0102	深層学習と他の識別問題との違いはなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0902	0114	深層学習と他の識別問題との違いはなんですか	階層的クラスタリングや k-means 法
0	0902	1111	深層学習と他の識別問題との違いはなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0902	0512	深層学習と他の識別問題との違いはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0902	0908	深層学習と他の識別問題との違いはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0902	1001	深層学習と他の識別問題との違いはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0902	0105	深層学習と他の識別問題との違いはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0902	0606	深層学習と他の識別問題との違いはなんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0902	0708	深層学習と他の識別問題との違いはなんですか	制約を弱める変数
0	0902	0805	深層学習と他の識別問題との違いはなんですか	誤差逆伝播法
0	0902	1407	深層学習と他の識別問題との違いはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0902	0601	深層学習と他の識別問題との違いはなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0902	0717	深層学習と他の識別問題との違いはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0902	0416	深層学習と他の識別問題との違いはなんですか	値が真となる確率を知りたいノードが表す変数
0	0902	1408	深層学習と他の識別問題との違いはなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0902	0906	深層学習と他の識別問題との違いはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0902	1409	深層学習と他の識別問題との違いはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0902	0810	深層学習と他の識別問題との違いはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0902	0502	深層学習と他の識別問題との違いはなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0902	1506	深層学習と他の識別問題との違いはなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0902	1009	深層学習と他の識別問題との違いはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0902	0114	深層学習と他の識別問題との違いはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0902	0411	深層学習と他の識別問題との違いはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0902	0611	深層学習と他の識別問題との違いはなんですか	識別における決定木の考え方を回帰問題に適用する方法
1	0903	0903	深層学習に用いられるニューラルネットワークはどのような種類がありますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0903	1110	深層学習に用いられるニューラルネットワークはどのような種類がありますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0903	1409	深層学習に用いられるニューラルネットワークはどのような種類がありますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0903	0901	深層学習に用いられるニューラルネットワークはどのような種類がありますか	表現学習
0	0903	0109	深層学習に用いられるニューラルネットワークはどのような種類がありますか	正解が付いていない場合の学習
0	0903	1406	深層学習に用いられるニューラルネットワークはどのような種類がありますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0903	0710	深層学習に用いられるニューラルネットワークはどのような種類がありますか	低次元の特徴ベクトルを高次元に写像
0	0903	0502	深層学習に用いられるニューラルネットワークはどのような種類がありますか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0903	1214	深層学習に用いられるニューラルネットワークはどのような種類がありますか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0903	0802	深層学習に用いられるニューラルネットワークはどのような種類がありますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0903	0114	深層学習に用いられるニューラルネットワークはどのような種類がありますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0903	0204	深層学習に用いられるニューラルネットワークはどのような種類がありますか	特徴ベクトルの次元数を減らすこと
0	0903	0105	深層学習に用いられるニューラルネットワークはどのような種類がありますか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0903	1001	深層学習に用いられるニューラルネットワークはどのような種類がありますか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0903	0416	深層学習に用いられるニューラルネットワークはどのような種類がありますか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0903	0713	深層学習に用いられるニューラルネットワークはどのような種類がありますか	文書分類やバイオインフォマティックスなど
0	0903	1104	深層学習に用いられるニューラルネットワークはどのような種類がありますか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0903	0711	深層学習に用いられるニューラルネットワークはどのような種類がありますか	カーネル関数
0	0903	0715	深層学習に用いられるニューラルネットワークはどのような種類がありますか	識別面
0	0903	0901	深層学習に用いられるニューラルネットワークはどのような種類がありますか	Deep Neural Network (DNN) 
0	0903	0810	深層学習に用いられるニューラルネットワークはどのような種類がありますか	勾配消失問題
0	0903	0505	深層学習に用いられるニューラルネットワークはどのような種類がありますか	パーセプトロン
0	0903	0707	深層学習に用いられるニューラルネットワークはどのような種類がありますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0903	1103	深層学習に用いられるニューラルネットワークはどのような種類がありますか	異なったまとまり間の距離はなるべく遠くなるように
0	0903	1404	深層学習に用いられるニューラルネットワークはどのような種類がありますか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
1	0906	0906	多階層ニューラルネットワークとはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0906	0802	多階層ニューラルネットワークとはなんですか	識別対象のクラス数
0	0906	0803	多階層ニューラルネットワークとはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0906	1110	多階層ニューラルネットワークとはなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0906	0810	多階層ニューラルネットワークとはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0906	0104	多階層ニューラルネットワークとはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0906	0907	多階層ニューラルネットワークとはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0906	1106	多階層ニューラルネットワークとはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0906	1106	多階層ニューラルネットワークとはなんですか	最も近い事例対の距離を類似度とする
0	0906	0602	多階層ニューラルネットワークとはなんですか	正解情報$y$が数値であるということ
0	0906	0404	多階層ニューラルネットワークとはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0906	0810	多階層ニューラルネットワークとはなんですか	2006 年頃に考案された事前学習法
0	0906	0704	多階層ニューラルネットワークとはなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0906	0710	多階層ニューラルネットワークとはなんですか	もとの空間におけるデータ間の距離関係を保存
0	0906	1219	多階層ニューラルネットワークとはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0906	0717	多階層ニューラルネットワークとはなんですか	グリッド
0	0906	0810	多階層ニューラルネットワークとはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0906	0405	多階層ニューラルネットワークとはなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0906	0505	多階層ニューラルネットワークとはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0906	0109	多階層ニューラルネットワークとはなんですか	正解が付いていない場合の学習
0	0906	0111	多階層ニューラルネットワークとはなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0906	1501	多階層ニューラルネットワークとはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0906	1407	多階層ニューラルネットワークとはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0906	0811	多階層ニューラルネットワークとはなんですか	引数が負のときは0，0以上のときはその値を出力
0	0906	0206	多階層ニューラルネットワークとはなんですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
1	0906	0906	特徴抽出を学習するには何が必要か	十分多くの層を持つニューラルネットワーク
0	0906	0701	特徴抽出を学習するには何が必要か	識別境界線と最も近いデータとの距離
0	0906	0612	特徴抽出を学習するには何が必要か	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0906	0805	特徴抽出を学習するには何が必要か	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0906	0504	特徴抽出を学習するには何が必要か	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0906	1106	特徴抽出を学習するには何が必要か	最も遠い事例対の距離を類似度とする
0	0906	1306	特徴抽出を学習するには何が必要か	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0906	0701	特徴抽出を学習するには何が必要か	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0906	1009	特徴抽出を学習するには何が必要か	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0906	0717	特徴抽出を学習するには何が必要か	連続値
0	0906	0211	特徴抽出を学習するには何が必要か	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0906	1306	特徴抽出を学習するには何が必要か	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0906	1112	特徴抽出を学習するには何が必要か	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0906	1403	特徴抽出を学習するには何が必要か	多次元でも「次元の呪い」にかかっていない，ということ
0	0906	0802	特徴抽出を学習するには何が必要か	入力層・出力層の数に応じた適当な数
0	0906	0109	特徴抽出を学習するには何が必要か	正解が付いていない場合の学習
0	0906	0313	特徴抽出を学習するには何が必要か	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0906	0801	特徴抽出を学習するには何が必要か	このモデルは生物の神経細胞のモデルであると考えられています
0	0906	1301	特徴抽出を学習するには何が必要か	個々の要素の間に i.i.d. の関係が成立しないもの
0	0906	1010	特徴抽出を学習するには何が必要か	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0906	0717	特徴抽出を学習するには何が必要か	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0906	0510	特徴抽出を学習するには何が必要か	特徴空間上でクラスを分割する面
0	0906	0115	特徴抽出を学習するには何が必要か	パターンマイニング
0	0906	0917	特徴抽出を学習するには何が必要か	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0906	1506	特徴抽出を学習するには何が必要か	その政策に従って行動したときの累積報酬の期待値で評価します
1	0906	0906	階層が多いニューラルネットワークの学習の問題点は？	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0906	1220	階層が多いニューラルネットワークの学習の問題点は？	まばらなデータを低次元行列の積に分解する方法の一つ
0	0906	0803	階層が多いニューラルネットワークの学習の問題点は？	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0906	0110	階層が多いニューラルネットワークの学習の問題点は？	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0906	0611	階層が多いニューラルネットワークの学習の問題点は？	回帰
0	0906	1405	階層が多いニューラルネットワークの学習の問題点は？	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0906	0111	階層が多いニューラルネットワークの学習の問題点は？	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0906	1015	階層が多いニューラルネットワークの学習の問題点は？	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0906	0113	階層が多いニューラルネットワークの学習の問題点は？	入力データに潜む規則性
0	0906	0409	階層が多いニューラルネットワークの学習の問題点は？	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0906	0412	階層が多いニューラルネットワークの学習の問題点は？	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0906	1116	階層が多いニューラルネットワークの学習の問題点は？	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0906	1502	階層が多いニューラルネットワークの学習の問題点は？	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0906	0701	階層が多いニューラルネットワークの学習の問題点は？	線形で識別できないデータに対応するため
0	0906	0711	階層が多いニューラルネットワークの学習の問題点は？	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0906	0305	階層が多いニューラルネットワークの学習の問題点は？	仮説に対して課す制約
0	0906	1112	階層が多いニューラルネットワークの学習の問題点は？	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0906	0908	階層が多いニューラルネットワークの学習の問題点は？	ユークリッド距離
0	0906	0502	階層が多いニューラルネットワークの学習の問題点は？	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0906	1506	階層が多いニューラルネットワークの学習の問題点は？	最適政策$\pi^*$を獲得すること
0	0906	0116	階層が多いニューラルネットワークの学習の問題点は？	学習データが教師あり／教師なしの混在となっているもの
0	0906	0506	階層が多いニューラルネットワークの学習の問題点は？	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0906	0917	階層が多いニューラルネットワークの学習の問題点は？	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0906	0917	階層が多いニューラルネットワークの学習の問題点は？	LSTM
0	0906	0104	階層が多いニューラルネットワークの学習の問題点は？	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
1	0908	0908	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	シグモイド関数
0	0908	0810	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	勾配消失問題
0	0908	1116	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0908	1506	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0908	0802	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0908	0402	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0908	0506	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0908	0305	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	仮説に対して課す制約
0	0908	0711	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0908	0209	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0908	0114	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0908	0115	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0908	0917	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0908	0508	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0908	0204	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0908	0606	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0908	0416	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0908	0906	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0908	0102	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0908	1209	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	この値が高いほど，得られる情報の多い規則であること
0	0908	0901	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	深層学習に用いるニューラルネットワーク
0	0908	0803	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0908	0701	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0908	0805	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0908	0313	入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
1	0911	0911	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	ドロップアウト
0	0911	0209	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0911	0211	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0911	0302	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	カテゴリ形式の正解情報のこと
0	0911	0114	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0911	0502	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0911	1112	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0911	1505	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	「マルコフ性」を持つ確率過程における意思決定問題
0	0911	0112	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	線形回帰，回帰木，モデル木など
0	0911	0411	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0911	0912	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	画像認識
0	0911	0411	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0911	0614	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	回帰木と線形回帰の双方のよいところを取った方法
0	0911	1506	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	最適政策$\pi^*$を獲得すること
0	0911	0605	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0911	0606	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0911	0805	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	誤差逆伝播法
0	0911	0117	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0911	0311	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	集合の乱雑さ
0	0911	0805	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	誤差逆伝播法
0	0911	0906	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0911	0204	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0911	0906	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0911	0109	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	正解が付いていない場合の学習
0	0911	1108	ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
1	0912	0912	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	畳み込みニューラルネットワーク
0	0912	0805	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	誤差逆伝播法
0	0912	0811	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	半分の領域で勾配が1になるので
0	0912	0305	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	仮説に対して課す制約
0	0912	0903	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0912	0406	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0912	1502	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	将棋や囲碁などを行うプログラム
0	0912	0611	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0912	0916	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	リカレントニューラルネットワーク
0	0912	1214	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	計算量が膨大であること
0	0912	0810	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	勾配消失問題
0	0912	1209	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0912	0803	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	非線形識別面
0	0912	0901	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0912	0715	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0912	0717	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	グリッド
0	0912	0109	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	正解が付いていない場合の学習
0	0912	0612	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0701	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0912	1219	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	どの個人がどの商品を購入したかが記録されているデータ
0	0912	0111	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0912	0717	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0912	1209	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0912	0811	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0912	0405	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
1	0912	0912	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	畳み込みニューラルネットワーク
0	0912	1506	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0912	0508	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0912	0612	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0211	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0912	1412	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0912	1403	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0912	0710	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0912	0701	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0912	0510	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	特徴空間上でクラスを分割する面
0	0912	1112	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0912	0508	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	最小二乗法
0	0912	0708	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	制約を弱める変数
0	0912	0502	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	SVM
0	0912	0811	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	引数が負のときは0，0以上のときはその値を出力
0	0912	0701	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	識別境界線と最も近いデータとの距離
0	0912	0111	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0912	0917	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0912	0907	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0912	1509	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0912	1406	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0912	0717	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	グリッド
0	0912	1001	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0912	0911	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	ドロップアウト
0	0912	0710	画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
1	0916	0916	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	リカレントニューラルネットワーク
0	0916	0402	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	入力を観測した後で計算される確率
0	0916	0507	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0916	1409	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0916	0701	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	サポートベクトルマシン
0	0916	0811	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	ユニットの活性化関数を工夫する方法があります
0	0916	0614	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	回帰木と線形回帰の双方のよいところを取った
0	0916	0205	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0916	1408	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0916	1301	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0916	0901	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0916	0610	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	トレードオフの関係
0	0916	0610	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	学習結果の散らばり具合
0	0916	1303	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0916	1502	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0916	1012	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0916	0712	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0916	0104	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0916	0611	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0916	1106	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0916	0810	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0916	0717	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	Grid search
0	0916	0901	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0916	1409	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0916	0514	中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
1	0917	0917	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0917	0510	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	特徴空間上でクラスを分割する面
0	0917	0803	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0917	0710	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	もとの空間におけるデータ間の距離関係を保存
0	0917	1508	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0917	0916	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0917	0612	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0917	1310	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0917	0907	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	事前学習法
0	0917	0416	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0917	0209	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0917	0105	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0917	0717	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0917	0505	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	ニューラルネットワーク
0	0917	0612	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0917	0701	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	サポートベクトルマシン
0	0917	0612	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0917	0702	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	識別面は平面を仮定する
0	0917	1215	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0917	0211	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0917	0710	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0917	0707	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0917	0406	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	各クラスから生じる特徴の尤もらしさを表す
0	0917	1207	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0917	0204	リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか	特徴ベクトルの次元数を減らすこと
1	0917	0917	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	LSTM
0	0917	0514	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0917	0611	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	回帰
0	0917	0304	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	個々の事例から，あるクラスについて共通点を見つけること
0	0917	0505	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0917	0901	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0917	0416	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	値が真となる確率を知りたいノードが表す変数
0	0917	0805	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0917	1303	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0917	0907	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0917	1506	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	最適政策$\pi^*$を獲得すること
0	0917	0715	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	カーネルトリック
0	0917	1116	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0917	0105	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0917	0510	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	特徴空間上でクラスを分割する面
0	0917	1505	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0917	0402	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	統計的識別手法
0	0917	0313	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0917	0104	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0917	0805	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0917	0209	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0917	1219	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0917	0512	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0917	0104	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0917	0906	リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか	十分多くの層
1	0917	0917	LSTMで置き換えられるメモリユニットの名前はなんですか	LSTMセル
0	0917	0901	LSTMで置き換えられるメモリユニットの名前はなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0917	1407	LSTMで置き換えられるメモリユニットの名前はなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0917	0701	LSTMで置き換えられるメモリユニットの名前はなんですか	識別境界線と最も近いデータとの距離
0	0917	0513	LSTMで置き換えられるメモリユニットの名前はなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0917	0701	LSTMで置き換えられるメモリユニットの名前はなんですか	識別境界線と最も近いデータとの距離
0	0917	1116	LSTMで置き換えられるメモリユニットの名前はなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0917	1015	LSTMで置き換えられるメモリユニットの名前はなんですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0917	1001	LSTMで置き換えられるメモリユニットの名前はなんですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0917	0710	LSTMで置き換えられるメモリユニットの名前はなんですか	もとの空間におけるデータ間の距離関係を保存
0	0917	0111	LSTMで置き換えられるメモリユニットの名前はなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0917	0906	LSTMで置き換えられるメモリユニットの名前はなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0917	0402	LSTMで置き換えられるメモリユニットの名前はなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0917	0901	LSTMで置き換えられるメモリユニットの名前はなんですか	深層学習に用いるニューラルネットワーク
0	0917	0612	LSTMで置き換えられるメモリユニットの名前はなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0917	0416	LSTMで置き換えられるメモリユニットの名前はなんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0917	1012	LSTMで置き換えられるメモリユニットの名前はなんですか	すべてのデータの重みは平等
0	0917	0208	LSTMで置き換えられるメモリユニットの名前はなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0917	1104	LSTMで置き換えられるメモリユニットの名前はなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0917	0811	LSTMで置き換えられるメモリユニットの名前はなんですか	引数が負のときは0，0以上のときはその値を出力
0	0917	0114	LSTMで置き換えられるメモリユニットの名前はなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0917	1116	LSTMで置き換えられるメモリユニットの名前はなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0917	1001	LSTMで置き換えられるメモリユニットの名前はなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0917	0610	LSTMで置き換えられるメモリユニットの名前はなんですか	片方を減らせば片方が増える
0	0917	1001	LSTMで置き換えられるメモリユニットの名前はなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0917	0917	LSTMセルが持つ3つのゲートは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	0917	0805	LSTMセルが持つ3つのゲートは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0917	0402	LSTMセルが持つ3つのゲートは何ですか	入力を観測した後で計算される確率
0	0917	0901	LSTMセルが持つ3つのゲートは何ですか	音声認識・画像認識・自然言語処理など
0	0917	0710	LSTMセルが持つ3つのゲートは何ですか	もとの空間におけるデータ間の距離関係を保存
0	0917	0204	LSTMセルが持つ3つのゲートは何ですか	特徴ベクトルの次元数を減らすこと
0	0917	0901	LSTMセルが持つ3つのゲートは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0917	0413	LSTMセルが持つ3つのゲートは何ですか	変数間の独立性を表現できること
0	0917	0109	LSTMセルが持つ3つのゲートは何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0917	0715	LSTMセルが持つ3つのゲートは何ですか	カーネルトリック
0	0917	1201	LSTMセルが持つ3つのゲートは何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0917	0112	LSTMセルが持つ3つのゲートは何ですか	線形回帰，回帰木，モデル木など
0	0917	0502	LSTMセルが持つ3つのゲートは何ですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0917	0717	LSTMセルが持つ3つのゲートは何ですか	Grid search
0	0917	0313	LSTMセルが持つ3つのゲートは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0917	0703	LSTMセルが持つ3つのゲートは何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0917	1106	LSTMセルが持つ3つのゲートは何ですか	最も遠い事例対の距離を類似度とする
0	0917	1403	LSTMセルが持つ3つのゲートは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0917	0911	LSTMセルが持つ3つのゲートは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0917	0901	LSTMセルが持つ3つのゲートは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0917	1411	LSTMセルが持つ3つのゲートは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0917	0810	LSTMセルが持つ3つのゲートは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0917	0402	LSTMセルが持つ3つのゲートは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0917	1501	LSTMセルが持つ3つのゲートは何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0917	0114	LSTMセルが持つ3つのゲートは何ですか	階層的クラスタリングや k-means 法
1	0901	0901	深層学習に用いるニューラルネットワークをなんとよぶか	Deep Neural Network (DNN) 
0	0901	1106	深層学習に用いるニューラルネットワークをなんとよぶか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0901	0406	深層学習に用いるニューラルネットワークをなんとよぶか	各クラスから生じる特徴の尤もらしさを表す
0	0901	0113	深層学習に用いるニューラルネットワークをなんとよぶか	入力データに潜む規則性を学習すること
0	0901	1411	深層学習に用いるニューラルネットワークをなんとよぶか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0901	1310	深層学習に用いるニューラルネットワークをなんとよぶか	確率的非決定性オートマトンの一種
0	0901	0514	深層学習に用いるニューラルネットワークをなんとよぶか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0901	0801	深層学習に用いるニューラルネットワークをなんとよぶか	このモデルは生物の神経細胞のモデルであると考えられています
0	0901	0810	深層学習に用いるニューラルネットワークをなんとよぶか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0901	0114	深層学習に用いるニューラルネットワークをなんとよぶか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0901	1008	深層学習に用いるニューラルネットワークをなんとよぶか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0901	0402	深層学習に用いるニューラルネットワークをなんとよぶか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0901	1403	深層学習に用いるニューラルネットワークをなんとよぶか	多次元でも「次元の呪い」にかかっていない，ということ
0	0901	1301	深層学習に用いるニューラルネットワークをなんとよぶか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0901	0402	深層学習に用いるニューラルネットワークをなんとよぶか	事後確率が最大となるクラスを識別結果とする方法
0	0901	0514	深層学習に用いるニューラルネットワークをなんとよぶか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0901	0416	深層学習に用いるニューラルネットワークをなんとよぶか	値が真となる確率を知りたいノードが表す変数
0	0901	0304	深層学習に用いるニューラルネットワークをなんとよぶか	個々の事例から，あるクラスについて共通点を見つけること
0	0901	0510	深層学習に用いるニューラルネットワークをなんとよぶか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0901	0409	深層学習に用いるニューラルネットワークをなんとよぶか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0901	0803	深層学習に用いるニューラルネットワークをなんとよぶか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0901	1309	深層学習に用いるニューラルネットワークをなんとよぶか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0901	0917	深層学習に用いるニューラルネットワークをなんとよぶか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0901	0503	深層学習に用いるニューラルネットワークをなんとよぶか	様々な数値データに対して多く用いられる統計モデル
0	0901	1103	深層学習に用いるニューラルネットワークをなんとよぶか	異なったまとまり間の距離はなるべく遠くなるように
1	0404	0404	事前確率とは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0404	0508	事前確率とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	0404	0306	事前確率とは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0404	0701	事前確率とは何ですか	学習データからのマージンが最大となる識別境界線
0	0404	0810	事前確率とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0404	0811	事前確率とは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0404	1304	事前確率とは何ですか	入力と対応させる素性
0	0404	1503	事前確率とは何ですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0404	0307	事前確率とは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0404	0601	事前確率とは何ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0404	0906	事前確率とは何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0404	0209	事前確率とは何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0404	0302	事前確率とは何ですか	カテゴリ形式の正解情報のこと
0	0404	0912	事前確率とは何ですか	畳み込みニューラルネットワーク
0	0404	0513	事前確率とは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0404	0505	事前確率とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	0404	0114	事前確率とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0404	0708	事前確率とは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0404	0916	事前確率とは何ですか	リカレントニューラルネットワーク
0	0404	0614	事前確率とは何ですか	回帰木と線形回帰の双方のよいところを取った
0	0404	1111	事前確率とは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0404	0412	事前確率とは何ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0404	0413	事前確率とは何ですか	変数間の独立性を表現できること
0	0404	0702	事前確率とは何ですか	識別面は平面を仮定する
0	0404	1220	事前確率とは何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
1	0405	0405	尤度とは何ですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0405	0205	尤度とは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0405	1209	尤度とは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0405	0614	尤度とは何ですか	モデル木
0	0405	0114	尤度とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0405	0316	尤度とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0405	0211	尤度とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0405	0805	尤度とは何ですか	誤差逆伝播法
0	0405	0114	尤度とは何ですか	階層的クラスタリングや k-means 法
0	0405	1510	尤度とは何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0405	0417	尤度とは何ですか	ネットワークの構造とアークの条件付き確率
0	0405	0506	尤度とは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0405	1409	尤度とは何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0405	0808	尤度とは何ですか	入力の重み付き和の微分
0	0405	1405	尤度とは何ですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0405	0701	尤度とは何ですか	識別境界線と最も近いデータとの距離
0	0405	1001	尤度とは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0405	0808	尤度とは何ですか	シグモイド関数の微分
0	0405	0811	尤度とは何ですか	ユニットの活性化関数を工夫する方法
0	0405	1201	尤度とは何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0405	1412	尤度とは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0405	0911	尤度とは何ですか	ドロップアウト
0	0405	0117	尤度とは何ですか	学習データの一部にだけ正解が与えられている場合
0	0405	0514	尤度とは何ですか	ランダムに学習データを一つ
0	0405	1310	尤度とは何ですか	確率的非決定性オートマトンの一種
1	0407	0407	何故尤度の対数をとって計算するんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0407	0907	何故尤度の対数をとって計算するんですか	事前学習法
0	0407	1205	何故尤度の対数をとって計算するんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0407	1201	何故尤度の対数をとって計算するんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0407	0715	何故尤度の対数をとって計算するんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0407	0901	何故尤度の対数をとって計算するんですか	音声認識・画像認識・自然言語処理など
0	0407	0409	何故尤度の対数をとって計算するんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0407	1505	何故尤度の対数をとって計算するんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0407	1508	何故尤度の対数をとって計算するんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0407	0906	何故尤度の対数をとって計算するんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0407	1506	何故尤度の対数をとって計算するんですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0407	0209	何故尤度の対数をとって計算するんですか	正例がどれだけ正しく判定されているかという指標
0	0407	0802	何故尤度の対数をとって計算するんですか	特徴ベクトルの次元数
0	0407	0304	何故尤度の対数をとって計算するんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0407	0316	何故尤度の対数をとって計算するんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0407	1110	何故尤度の対数をとって計算するんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0407	1103	何故尤度の対数をとって計算するんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0407	0717	何故尤度の対数をとって計算するんですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0407	0803	何故尤度の対数をとって計算するんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0407	0114	何故尤度の対数をとって計算するんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0407	0711	何故尤度の対数をとって計算するんですか	カーネル関数
0	0407	1506	何故尤度の対数をとって計算するんですか	後に得られる報酬ほど割り引いて計算するための係数
0	0407	0110	何故尤度の対数をとって計算するんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0407	0416	何故尤度の対数をとって計算するんですか	値が真となる確率を知りたいノードが表す変数
0	0407	0710	何故尤度の対数をとって計算するんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
1	0411	0411	ゼロ頻度問題とは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0411	1506	ゼロ頻度問題とは何ですか	後に得られる報酬ほど割り引いて計算するための係数
0	0411	0209	ゼロ頻度問題とは何ですか	正例がどれだけ正しく判定されているかという指標
0	0411	1110	ゼロ頻度問題とは何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0411	0906	ゼロ頻度問題とは何ですか	多階層構造でもそのまま適用できます
0	0411	0917	ゼロ頻度問題とは何ですか	LSTMセル
0	0411	0717	ゼロ頻度問題とは何ですか	連続値
0	0411	0512	ゼロ頻度問題とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0411	0912	ゼロ頻度問題とは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0411	0917	ゼロ頻度問題とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0411	0711	ゼロ頻度問題とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0411	0612	ゼロ頻度問題とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0411	0115	ゼロ頻度問題とは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0411	0701	ゼロ頻度問題とは何ですか	線形で識別できないデータに対応するため
0	0411	1303	ゼロ頻度問題とは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0411	0602	ゼロ頻度問題とは何ですか	正解情報$y$が数値であるということ
0	0411	0802	ゼロ頻度問題とは何ですか	入力層・出力層の数に応じた適当な数
0	0411	0402	ゼロ頻度問題とは何ですか	入力を観測した後で計算される確率
0	0411	0712	ゼロ頻度問題とは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0411	0704	ゼロ頻度問題とは何ですか	以下の関数$L$の最小値を求めるという問題
0	0411	0105	ゼロ頻度問題とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0411	0409	ゼロ頻度問題とは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0411	1502	ゼロ頻度問題とは何ですか	将棋や囲碁などを行うプログラム
0	0411	0901	ゼロ頻度問題とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0411	0707	ゼロ頻度問題とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
1	1301	1301	系列データとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1301	0701	系列データとは何ですか	識別境界線と最も近いデータとの距離
0	1301	0810	系列データとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1301	0110	系列データとは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1301	0906	系列データとは何ですか	十分多くの層
0	1301	0917	系列データとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1301	0917	系列データとは何ですか	LSTM
0	1301	1209	系列データとは何ですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1301	0610	系列データとは何ですか	トレードオフの関係
0	1301	1406	系列データとは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1301	0509	系列データとは何ですか	確率的最急勾配法
0	1301	0616	系列データとは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1301	0404	系列データとは何ですか	事前確率
0	1301	0111	系列データとは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1301	1303	系列データとは何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1301	1001	系列データとは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1301	1506	系列データとは何ですか	各状態でどの行為を取ればよいのかという意思決定規則
0	1301	1111	系列データとは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1301	1409	系列データとは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1301	0406	系列データとは何ですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1301	0802	系列データとは何ですか	識別対象のクラス数
0	1301	0801	系列データとは何ですか	このモデルは生物の神経細胞のモデルであると考えられています
0	1301	1201	系列データとは何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	1301	0416	系列データとは何ですか	値が真となる確率を知りたいノードが表す変数
0	1301	1303	系列データとは何ですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
1	1301	1301	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	形態素解析処理が典型的な問題
0	1301	0810	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1301	0901	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	音声認識・画像認識・自然言語処理など
0	1301	0710	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1301	1406	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1301	1310	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	確率的非決定性オートマトンの一種
0	1301	0801	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	0305	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1301	1215	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1301	0701	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	識別境界線と最も近いデータとの距離
0	1301	0801	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	1010	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1301	1008	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1301	1506	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	最適政策$\pi^*$を獲得すること
0	1301	0504	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1301	1004	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1301	0307	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1301	0701	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	サポートベクトルマシン
0	1301	0713	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	文書分類やバイオインフォマティックスなど
0	1301	0508	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	1301	0810	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1301	1306	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1301	0115	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1301	0302	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	カテゴリ形式の正解情報のこと
0	1301	0916	系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか	リカレントニューラルネットワーク
1	1301	1301	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1301	1402	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1301	0803	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1301	0802	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	多層パーセプトロンあるいはニューラルネットワーク
0	1301	1110	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	1301	0105	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1301	0413	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	変数間の独立性を表現できること
0	1301	0406	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1301	0906	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	多階層構造でもそのまま適用できます
0	1301	0313	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1301	1110	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1301	0606	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	1301	0901	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1301	1411	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1301	1407	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1301	0715	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	1301	0710	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1301	1108	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1301	0901	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	深層学習に用いるニューラルネットワーク
0	1301	1110	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1301	0701	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1301	0512	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1301	1104	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1301	0505	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	生物の神経細胞の仕組みをモデル化したもの
0	1301	1412	系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
1	1301	1301	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	1301	0902	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1301	0507	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1301	0503	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	様々な数値データに対して多く用いられる統計モデル
0	1301	0901	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	深層学習に用いるニューラルネットワーク
0	1301	0708	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	制約を弱める変数
0	1301	0109	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	正解が付いていない場合の学習
0	1301	0316	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1301	0307	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	1301	0701	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1301	0206	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1301	0209	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1301	0810	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1301	0102	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1301	0715	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	カーネルトリック
0	1301	0110	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1301	1405	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	半教師あり学習は文書分類問題によく適用されます
0	1301	0802	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	特徴空間上では線形識別面を設定すること
0	1301	1214	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1301	1012	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1301	0903	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1301	1001	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	1301	0513	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1301	0708	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	制約を弱める変数
0	1301	0315	系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか	分割後のデータの分散
1	1301	1301	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	動画像の分類や音声で入力された単語の識別などの問題
0	1301	1502	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1301	1010	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1301	0801	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	0701	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	サポートベクトルマシン
0	1301	0802	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	識別対象のクラス数
0	1301	1313	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1301	0917	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1301	0302	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	カテゴリ形式の正解情報のこと
0	1301	0508	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1301	0707	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1301	0805	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1301	0405	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	尤度と事前確率の積を最大とするクラス
0	1301	0701	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1301	0708	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	制約を満たさない程度を表すので，小さい方が望ましい
0	1301	0702	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	識別面は平面を仮定する
0	1301	1015	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1301	0717	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1301	0901	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1301	0710	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1301	0316	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1301	1303	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1301	0701	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1301	0810	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1301	1411	系列データの入力の系列長にかかわらず，出力の系列長が1である問題の例は何かありますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
1	1301	1301	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	連続音声認識
0	1301	0801	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	0514	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1301	1110	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1301	1009	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	1301	0315	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	分割後のデータの分散
0	1301	0702	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	識別面は平面を仮定する
0	1301	0209	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1301	0704	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	ラグランジュの未定乗数法
0	1301	0908	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1301	0114	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1301	1408	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1301	0906	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	入力に近い側の処理
0	1301	0109	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	正解が付いていない場合の学習
0	1301	0612	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1301	0402	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	統計的識別手法
0	1301	0402	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	入力を観測した後で計算される確率
0	1301	1409	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	自分が出した誤りを指摘してくれる他人がいない
0	1301	0313	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1301	0811	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	ユニットの活性化関数を工夫する方法があります
0	1301	1104	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1301	0715	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	識別面
0	1301	0104	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	1301	0313	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1301	0614	系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか	回帰木と線形回帰の双方のよいところを取った方法
1	1303	1303	系列ラベリングの典型的な問題は何かありますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	1303	1506	系列ラベリングの典型的な問題は何かありますか	最適政策$\pi^*$を獲得すること
0	1303	0109	系列ラベリングの典型的な問題は何かありますか	正解が付いていない場合の学習
0	1303	0504	系列ラベリングの典型的な問題は何かありますか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1303	0701	系列ラベリングの典型的な問題は何かありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1303	1301	系列ラベリングの典型的な問題は何かありますか	連続音声認識
0	1303	0102	系列ラベリングの典型的な問題は何かありますか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1303	0109	系列ラベリングの典型的な問題は何かありますか	正解が付いていない場合の学習
0	1303	0417	系列ラベリングの典型的な問題は何かありますか	ネットワークの構造とアークの条件付き確率表
0	1303	0916	系列ラベリングの典型的な問題は何かありますか	リカレントニューラルネットワーク
0	1303	0912	系列ラベリングの典型的な問題は何かありますか	畳み込みニューラルネットワーク
0	1303	0402	系列ラベリングの典型的な問題は何かありますか	入力を観測した後で計算される確率
0	1303	1506	系列ラベリングの典型的な問題は何かありますか	政策
0	1303	0701	系列ラベリングの典型的な問題は何かありますか	識別境界線と最も近いデータとの距離
0	1303	0803	系列ラベリングの典型的な問題は何かありますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1303	1110	系列ラベリングの典型的な問題は何かありますか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	1303	0911	系列ラベリングの典型的な問題は何かありますか	ランダムに一定割合のユニットを消して学習を行う
0	1303	0802	系列ラベリングの典型的な問題は何かありますか	特徴空間上では線形識別面を設定すること
0	1303	0717	系列ラベリングの典型的な問題は何かありますか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	1303	1010	系列ラベリングの典型的な問題は何かありますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1303	0114	系列ラベリングの典型的な問題は何かありますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1303	0313	系列ラベリングの典型的な問題は何かありますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1303	0701	系列ラベリングの典型的な問題は何かありますか	線形で識別できないデータに対応するため
0	1303	0715	系列ラベリングの典型的な問題は何かありますか	カーネルトリック
0	1303	0512	系列ラベリングの典型的な問題は何かありますか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
1	1306	1306	条件付き確率場の学習にはどのような性質を利用しますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1306	0612	条件付き確率場の学習にはどのような性質を利用しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1306	1409	条件付き確率場の学習にはどのような性質を利用しますか	自分が出した誤りを指摘してくれる他人がいない
0	1306	0802	条件付き確率場の学習にはどのような性質を利用しますか	入力層・出力層の数に応じた適当な数
0	1306	0508	条件付き確率場の学習にはどのような性質を利用しますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1306	1015	条件付き確率場の学習にはどのような性質を利用しますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1306	0614	条件付き確率場の学習にはどのような性質を利用しますか	回帰木と線形回帰の双方のよいところを取った方法
0	1306	0512	条件付き確率場の学習にはどのような性質を利用しますか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1306	0311	条件付き確率場の学習にはどのような性質を利用しますか	集合の乱雑さ
0	1306	0204	条件付き確率場の学習にはどのような性質を利用しますか	特徴ベクトルの次元数を減らすこと
0	1306	0612	条件付き確率場の学習にはどのような性質を利用しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1306	1106	条件付き確率場の学習にはどのような性質を利用しますか	最も遠い事例対の距離を類似度とする
0	1306	0104	条件付き確率場の学習にはどのような性質を利用しますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1306	1411	条件付き確率場の学習にはどのような性質を利用しますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1306	1302	条件付き確率場の学習にはどのような性質を利用しますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1306	0701	条件付き確率場の学習にはどのような性質を利用しますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1306	0315	条件付き確率場の学習にはどのような性質を利用しますか	分割後のデータの分散
0	1306	1110	条件付き確率場の学習にはどのような性質を利用しますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1306	1309	条件付き確率場の学習にはどのような性質を利用しますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1306	0504	条件付き確率場の学習にはどのような性質を利用しますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1306	0513	条件付き確率場の学習にはどのような性質を利用しますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1306	1301	条件付き確率場の学習にはどのような性質を利用しますか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1306	0606	条件付き確率場の学習にはどのような性質を利用しますか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	1306	0405	条件付き確率場の学習にはどのような性質を利用しますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	1306	0410	条件付き確率場の学習にはどのような性質を利用しますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
1	1306	1306	CRFとは何の略称ですか	条件付き確率場（Conditional Random Field: CRF）
0	1306	1510	CRFとは何の略称ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1306	1309	CRFとは何の略称ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1306	0616	CRFとは何の略称ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1306	0811	CRFとは何の略称ですか	ユニットの活性化関数を工夫する方法
0	1306	1103	CRFとは何の略称ですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	1306	0405	CRFとは何の略称ですか	尤度と事前確率の積を最大とするクラス
0	1306	1506	CRFとは何の略称ですか	各状態でどの行為を取ればよいのかという意思決定規則
0	1306	0912	CRFとは何の略称ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1306	0111	CRFとは何の略称ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	1306	0611	CRFとは何の略称ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1306	0614	CRFとは何の略称ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1306	0209	CRFとは何の略称ですか	正例がどれだけ正しく判定されているかという指標
0	1306	1301	CRFとは何の略称ですか	連続音声認識
0	1306	0701	CRFとは何の略称ですか	サポートベクトルマシン
0	1306	0715	CRFとは何の略称ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1306	1209	CRFとは何の略称ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1306	0502	CRFとは何の略称ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1306	0402	CRFとは何の略称ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1306	0115	CRFとは何の略称ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1306	0211	CRFとは何の略称ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1306	0715	CRFとは何の略称ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1306	0513	CRFとは何の略称ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1306	0911	CRFとは何の略称ですか	ドロップアウト
0	1306	0708	CRFとは何の略称ですか	制約を弱める変数
1	1310	1310	HMMとは何の略称ですか	Hidden Marcov Model: 隠れマルコフモデル
0	1310	0701	HMMとは何の略称ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1310	0805	HMMとは何の略称ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1310	0409	HMMとは何の略称ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1310	1405	HMMとは何の略称ですか	半教師あり学習は文書分類問題によく適用されます
0	1310	0612	HMMとは何の略称ですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	1310	0916	HMMとは何の略称ですか	リカレントニューラルネットワーク
0	1310	0708	HMMとは何の略称ですか	制約を弱める変数
0	1310	0701	HMMとは何の略称ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1310	0402	HMMとは何の略称ですか	入力を観測した後で計算される確率
0	1310	0901	HMMとは何の略称ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1310	1506	HMMとは何の略称ですか	政策
0	1310	0703	HMMとは何の略称ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	1310	0316	HMMとは何の略称ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1310	0808	HMMとは何の略称ですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1310	0512	HMMとは何の略称ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1310	1501	HMMとは何の略称ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1310	1001	HMMとは何の略称ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1310	0906	HMMとは何の略称ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1310	0612	HMMとは何の略称ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1310	0614	HMMとは何の略称ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1310	1207	HMMとは何の略称ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1310	0601	HMMとは何の略称ですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	1310	1301	HMMとは何の略称ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1310	0307	HMMとは何の略称ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
1	1310	1310	隠れマルコフモデルとは何ですか	確率的非決定性オートマトンの一種
0	1310	1301	隠れマルコフモデルとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1310	1112	隠れマルコフモデルとは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1310	0505	隠れマルコフモデルとは何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1310	0715	隠れマルコフモデルとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1310	0102	隠れマルコフモデルとは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1310	0710	隠れマルコフモデルとは何ですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1310	0316	隠れマルコフモデルとは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1310	0917	隠れマルコフモデルとは何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1310	1209	隠れマルコフモデルとは何ですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1310	0712	隠れマルコフモデルとは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	1310	0701	隠れマルコフモデルとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1310	1201	隠れマルコフモデルとは何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1310	1203	隠れマルコフモデルとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1310	0917	隠れマルコフモデルとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	1310	0802	隠れマルコフモデルとは何ですか	識別対象のクラス数
0	1310	0109	隠れマルコフモデルとは何ですか	正解が付いていない場合の学習
0	1310	0701	隠れマルコフモデルとは何ですか	サポートベクトルマシン
0	1310	0708	隠れマルコフモデルとは何ですか	制約を弱める変数
0	1310	0508	隠れマルコフモデルとは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1310	1209	隠れマルコフモデルとは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	1310	0801	隠れマルコフモデルとは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1310	0701	隠れマルコフモデルとは何ですか	マージン
0	1310	0610	隠れマルコフモデルとは何ですか	真のモデルとの距離
0	1310	0701	隠れマルコフモデルとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	1402	1402	半教師あり学習に適した数値特徴データとはどのようなものですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	1402	1103	半教師あり学習に適した数値特徴データとはどのようなものですか	異なったまとまり間の距離はなるべく遠くなるように
0	1402	0907	半教師あり学習に適した数値特徴データとはどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1402	1301	半教師あり学習に適した数値特徴データとはどのようなものですか	連続音声認識
0	1402	1404	半教師あり学習に適した数値特徴データとはどのようなものですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	1402	0715	半教師あり学習に適した数値特徴データとはどのようなものですか	カーネルトリック
0	1402	0717	半教師あり学習に適した数値特徴データとはどのようなものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1402	0505	半教師あり学習に適した数値特徴データとはどのようなものですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1402	0908	半教師あり学習に適した数値特徴データとはどのようなものですか	ユークリッド距離
0	1402	0708	半教師あり学習に適した数値特徴データとはどのようなものですか	制約を弱める変数
0	1402	1409	半教師あり学習に適した数値特徴データとはどのようなものですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1402	0802	半教師あり学習に適した数値特徴データとはどのようなものですか	隠れ層
0	1402	0115	半教師あり学習に適した数値特徴データとはどのようなものですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1402	1007	半教師あり学習に適した数値特徴データとはどのようなものですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1402	0103	半教師あり学習に適した数値特徴データとはどのようなものですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	1402	0808	半教師あり学習に適した数値特徴データとはどのようなものですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1402	0402	半教師あり学習に適した数値特徴データとはどのようなものですか	事後確率が最大となるクラスを識別結果とする方法
0	1402	1214	半教師あり学習に適した数値特徴データとはどのようなものですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1402	0901	半教師あり学習に適した数値特徴データとはどのようなものですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1402	0115	半教師あり学習に適した数値特徴データとはどのようなものですか	パターンマイニング
0	1402	0611	半教師あり学習に適した数値特徴データとはどのようなものですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1402	0715	半教師あり学習に適した数値特徴データとはどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	1402	1001	半教師あり学習に適した数値特徴データとはどのようなものですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1402	0505	半教師あり学習に適した数値特徴データとはどのようなものですか	パーセプトロン
0	1402	1302	半教師あり学習に適した数値特徴データとはどのようなものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
1	1406	1406	半教師ある学習の基本的な進め方はどういったものですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1406	0612	半教師ある学習の基本的な進め方はどういったものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	1406	0512	半教師ある学習の基本的な進め方はどういったものですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1406	1502	半教師ある学習の基本的な進め方はどういったものですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1406	0114	半教師ある学習の基本的な進め方はどういったものですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1406	0612	半教師ある学習の基本的な進め方はどういったものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1406	0606	半教師ある学習の基本的な進め方はどういったものですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1406	0606	半教師ある学習の基本的な進め方はどういったものですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	1406	0204	半教師ある学習の基本的な進め方はどういったものですか	特徴ベクトルの次元数を減らすこと
0	1406	1009	半教師ある学習の基本的な進め方はどういったものですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	1406	0901	半教師ある学習の基本的な進め方はどういったものですか	表現学習
0	1406	0906	半教師ある学習の基本的な進め方はどういったものですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1406	0902	半教師ある学習の基本的な進め方はどういったものですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1406	0901	半教師ある学習の基本的な進め方はどういったものですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1406	0411	半教師ある学習の基本的な進め方はどういったものですか	確率のm推定という考え方を用います
0	1406	0410	半教師ある学習の基本的な進め方はどういったものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1406	0313	半教師ある学習の基本的な進め方はどういったものですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1406	1505	半教師ある学習の基本的な進め方はどういったものですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1406	1306	半教師ある学習の基本的な進め方はどういったものですか	条件付き確率場（Conditional Random Field: CRF）
0	1406	0908	半教師ある学習の基本的な進め方はどういったものですか	シグモイド関数
0	1406	0811	半教師ある学習の基本的な進め方はどういったものですか	ユニットの活性化関数を工夫する方法
0	1406	0901	半教師ある学習の基本的な進め方はどういったものですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1406	0114	半教師ある学習の基本的な進め方はどういったものですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1406	0715	半教師ある学習の基本的な進め方はどういったものですか	複雑な非線形変換を求めるという操作を避ける方法
0	1406	0115	半教師ある学習の基本的な進め方はどういったものですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
1	1406	1406	半教師あり学習の識別器に適切なのはどのようなものですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1406	0611	半教師あり学習の識別器に適切なのはどのようなものですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1406	0402	半教師あり学習の識別器に適切なのはどのようなものですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1406	0912	半教師あり学習の識別器に適切なのはどのようなものですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1406	1505	半教師あり学習の識別器に適切なのはどのようなものですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1406	0503	半教師あり学習の識別器に適切なのはどのようなものですか	様々な数値データに対して多く用いられる統計モデル
0	1406	0514	半教師あり学習の識別器に適切なのはどのようなものですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1406	0117	半教師あり学習の識別器に適切なのはどのようなものですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	1406	0810	半教師あり学習の識別器に適切なのはどのようなものですか	2006 年頃に考案された事前学習法
0	1406	0509	半教師あり学習の識別器に適切なのはどのようなものですか	確率的最急勾配法
0	1406	1403	半教師あり学習の識別器に適切なのはどのようなものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1406	0908	半教師あり学習の識別器に適切なのはどのようなものですか	シグモイド関数
0	1406	1507	半教師あり学習の識別器に適切なのはどのようなものですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1406	0406	半教師あり学習の識別器に適切なのはどのようなものですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1406	0701	半教師あり学習の識別器に適切なのはどのようなものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1406	0505	半教師あり学習の識別器に適切なのはどのようなものですか	生物の神経細胞の仕組みをモデル化したもの
0	1406	0505	半教師あり学習の識別器に適切なのはどのようなものですか	生物の神経細胞の仕組みをモデル化したもの
0	1406	1304	半教師あり学習の識別器に適切なのはどのようなものですか	入力と対応させる素性
0	1406	0114	半教師あり学習の識別器に適切なのはどのようなものですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1406	1505	半教師あり学習の識別器に適切なのはどのようなものですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1406	0901	半教師あり学習の識別器に適切なのはどのようなものですか	表現学習
0	1406	0707	半教師あり学習の識別器に適切なのはどのようなものですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1406	0711	半教師あり学習の識別器に適切なのはどのようなものですか	カーネル関数
0	1406	0917	半教師あり学習の識別器に適切なのはどのようなものですか	LSTM
0	1406	1209	半教師あり学習の識別器に適切なのはどのようなものですか	この値が高いほど，得られる情報の多い規則であること
1	1407	1407	自己学習とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1407	0711	自己学習とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1407	0803	自己学習とは何ですか	非線形識別面
0	1407	0902	自己学習とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1407	0811	自己学習とは何ですか	ReLu
0	1407	0105	自己学習とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1407	0704	自己学習とは何ですか	以下の関数$L$の最小値を求めるという問題
0	1407	0205	自己学習とは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1407	0512	自己学習とは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1407	0512	自己学習とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1407	0605	自己学習とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1407	0902	自己学習とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	1407	0316	自己学習とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1407	1207	自己学習とは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1407	0710	自己学習とは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1407	0416	自己学習とは何ですか	アークを無向とみなした結合を考えたとき
0	1407	1306	自己学習とは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	1407	0109	自己学習とは何ですか	正解が付いていない場合の学習
0	1407	1302	自己学習とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1407	0204	自己学習とは何ですか	特徴ベクトルの次元数を減らすこと
0	1407	1404	自己学習とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	1407	0715	自己学習とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1407	1001	自己学習とは何ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	1407	0802	自己学習とは何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1407	0901	自己学習とは何ですか	音声認識・画像認識・自然言語処理など
1	1408	1408	自己学習の性質にはどのようなものがありますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1408	1405	自己学習の性質にはどのようなものがありますか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	1408	1116	自己学習の性質にはどのようなものがありますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1408	0313	自己学習の性質にはどのようなものがありますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1408	1405	自己学習の性質にはどのようなものがありますか	半教師あり学習は文書分類問題によく適用されます
0	1408	0504	自己学習の性質にはどのようなものがありますか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1408	1506	自己学習の性質にはどのようなものがありますか	その政策に従って行動したときの累積報酬の期待値で評価
0	1408	1506	自己学習の性質にはどのようなものがありますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1408	0612	自己学習の性質にはどのようなものがありますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1408	0701	自己学習の性質にはどのようなものがありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1408	0808	自己学習の性質にはどのようなものがありますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1408	0708	自己学習の性質にはどのようなものがありますか	制約を弱める変数
0	1408	0911	自己学習の性質にはどのようなものがありますか	ランダムに一定割合のユニットを消して学習を行う
0	1408	0402	自己学習の性質にはどのようなものがありますか	統計的識別手法
0	1408	0610	自己学習の性質にはどのようなものがありますか	片方を減らせば片方が増える
0	1408	0917	自己学習の性質にはどのようなものがありますか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1408	0410	自己学習の性質にはどのようなものがありますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1408	1219	自己学習の性質にはどのようなものがありますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1408	1103	自己学習の性質にはどのようなものがありますか	異なったまとまり間の距離はなるべく遠くなるように
0	1408	1403	自己学習の性質にはどのようなものがありますか	多次元でも「次元の呪い」にかかっていない，ということ
0	1408	1106	自己学習の性質にはどのようなものがありますか	クラスタの重心間の距離を類似度とする
0	1408	0411	自己学習の性質にはどのようなものがありますか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1408	0112	自己学習の性質にはどのようなものがありますか	線形回帰，回帰木，モデル木など
0	1408	1008	自己学習の性質にはどのようなものがありますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1408	1306	自己学習の性質にはどのようなものがありますか	条件付き確率場（Conditional Random Field: CRF）
1	1409	1409	共訓練とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1409	0810	共訓練とは何ですか	2006 年頃に考案された事前学習法
0	1409	1104	共訓練とは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1409	1302	共訓練とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1409	1209	共訓練とは何ですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1409	0315	共訓練とは何ですか	分割後のデータの分散
0	1409	0402	共訓練とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1409	0607	共訓練とは何ですか	「投げ縄」という意味
0	1409	0708	共訓練とは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1409	0715	共訓練とは何ですか	カーネルトリック
0	1409	0717	共訓練とは何ですか	グリッド
0	1409	0701	共訓練とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1409	1116	共訓練とは何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1409	1502	共訓練とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1409	0701	共訓練とは何ですか	識別境界線と最も近いデータとの距離
0	1409	1201	共訓練とは何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1409	1506	共訓練とは何ですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	1409	1301	共訓練とは何ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1409	0715	共訓練とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1409	0503	共訓練とは何ですか	様々な数値データに対して多く用いられる統計モデル
0	1409	0917	共訓練とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	1409	0907	共訓練とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1409	0616	共訓練とは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1409	0611	共訓練とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1409	0802	共訓練とは何ですか	隠れ層
1	1410	1410	共訓練の特徴は何ですか	学習初期の誤りに強いということ
0	1410	1001	共訓練の特徴は何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1410	0514	共訓練の特徴は何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	1410	0916	共訓練の特徴は何ですか	リカレントニューラルネットワーク
0	1410	0602	共訓練の特徴は何ですか	正解情報$y$が数値であるということ
0	1410	0402	共訓練の特徴は何ですか	入力を観測した後で計算される確率
0	1410	1219	共訓練の特徴は何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1410	0911	共訓練の特徴は何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1410	0509	共訓練の特徴は何ですか	確率的最急勾配法
0	1410	0708	共訓練の特徴は何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1410	0205	共訓練の特徴は何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1410	0606	共訓練の特徴は何ですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	1410	0115	共訓練の特徴は何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1410	0811	共訓練の特徴は何ですか	引数が負のときは0，0以上のときはその値を出力
0	1410	0611	共訓練の特徴は何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1410	0103	共訓練の特徴は何ですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	1410	1506	共訓練の特徴は何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1410	1301	共訓練の特徴は何ですか	形態素解析処理が典型的な問題
0	1410	0305	共訓練の特徴は何ですか	仮説に対して課す制約
0	1410	0911	共訓練の特徴は何ですか	ランダムに一定割合のユニットを消して学習を行う
0	1410	1404	共訓練の特徴は何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	1410	0402	共訓練の特徴は何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1410	0701	共訓練の特徴は何ですか	識別境界線と最も近いデータとの距離
0	1410	1110	共訓練の特徴は何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1410	0911	共訓練の特徴は何ですか	過学習が起きにくくなり，汎用性が高まることが報告されています
1	1410	1410	共訓練の問題点は何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1410	0505	共訓練の問題点は何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1410	0701	共訓練の問題点は何ですか	マージン
0	1410	1306	共訓練の問題点は何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1410	0708	共訓練の問題点は何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1410	1220	共訓練の問題点は何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1410	1108	共訓練の問題点は何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1410	0410	共訓練の問題点は何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1410	1408	共訓練の問題点は何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1410	1015	共訓練の問題点は何ですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1410	0113	共訓練の問題点は何ですか	入力データに潜む規則性
0	1410	1502	共訓練の問題点は何ですか	将棋や囲碁などを行うプログラム
0	1410	0111	共訓練の問題点は何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1410	0611	共訓練の問題点は何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1410	0114	共訓練の問題点は何ですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1410	0614	共訓練の問題点は何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1410	0204	共訓練の問題点は何ですか	特徴ベクトルの次元数を減らすこと
0	1410	0717	共訓練の問題点は何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1410	1215	共訓練の問題点は何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1410	0805	共訓練の問題点は何ですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	1410	1110	共訓練の問題点は何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1410	0901	共訓練の問題点は何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1410	0407	共訓練の問題点は何ですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1410	0413	共訓練の問題点は何ですか	変数間の独立性を表現できること
0	1410	0906	共訓練の問題点は何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
1	1411	1411	YATSIアルゴリズムとは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1411	0507	YATSIアルゴリズムとは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1411	1104	YATSIアルゴリズムとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1411	0901	YATSIアルゴリズムとは何ですか	深層学習に用いるニューラルネットワーク
0	1411	1409	YATSIアルゴリズムとは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1411	0409	YATSIアルゴリズムとは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1411	0510	YATSIアルゴリズムとは何ですか	特徴空間上でクラスを分割する面
0	1411	0701	YATSIアルゴリズムとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1411	0811	YATSIアルゴリズムとは何ですか	半分の領域で勾配が1になるので
0	1411	0906	YATSIアルゴリズムとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1411	0805	YATSIアルゴリズムとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1411	0411	YATSIアルゴリズムとは何ですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1411	1309	YATSIアルゴリズムとは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1411	1106	YATSIアルゴリズムとは何ですか	クラスタの重心間の距離を類似度とする
0	1411	0912	YATSIアルゴリズムとは何ですか	畳み込みニューラルネットワーク
0	1411	0417	YATSIアルゴリズムとは何ですか	ネットワークの構造とアークの条件付き確率
0	1411	0205	YATSIアルゴリズムとは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1411	1215	YATSIアルゴリズムとは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	1411	1404	YATSIアルゴリズムとは何ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	1411	0704	YATSIアルゴリズムとは何ですか	ラグランジュの未定乗数法を不等式制約条件
0	1411	1104	YATSIアルゴリズムとは何ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1411	0607	YATSIアルゴリズムとは何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1411	0506	YATSIアルゴリズムとは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1411	1402	YATSIアルゴリズムとは何ですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1411	1207	YATSIアルゴリズムとは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
1	1502	1502	強化学習とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1502	1406	強化学習とはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1502	0507	強化学習とはなんですか	全ての誤りがなくなることが学習の終了条件なので
0	1502	0306	強化学習とはなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1502	0611	強化学習とはなんですか	回帰
0	1502	0911	強化学習とはなんですか	ドロップアウト
0	1502	0701	強化学習とはなんですか	マージン
0	1502	0111	強化学習とはなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1502	0504	強化学習とはなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1502	1405	強化学習とはなんですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	1502	1010	強化学習とはなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1502	1409	強化学習とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1502	0502	強化学習とはなんですか	SVM
0	1502	0810	強化学習とはなんですか	勾配消失問題
0	1502	1108	強化学習とはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1502	0111	強化学習とはなんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	1502	1001	強化学習とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1502	0912	強化学習とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1502	0612	強化学習とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1502	0302	強化学習とはなんですか	カテゴリ形式の正解情報のこと
0	1502	0715	強化学習とはなんですか	識別面
0	1502	0901	強化学習とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1502	0803	強化学習とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1502	0701	強化学習とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1502	0710	強化学習とはなんですか	もとの空間におけるデータ間の距離関係を保存
1	1502	1502	強化学習は例えば何に使われていますか	将棋や囲碁などを行うプログラム
0	1502	0717	強化学習は例えば何に使われていますか	Grid search
0	1502	1203	強化学習は例えば何に使われていますか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1502	0405	強化学習は例えば何に使われていますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1502	1407	強化学習は例えば何に使われていますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1502	0710	強化学習は例えば何に使われていますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1502	0901	強化学習は例えば何に使われていますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1502	0313	強化学習は例えば何に使われていますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1502	1012	強化学習は例えば何に使われていますか	各データに重みを付け，そのもとで識別器を作成します
0	1502	0911	強化学習は例えば何に使われていますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1502	0902	強化学習は例えば何に使われていますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1502	1506	強化学習は例えば何に使われていますか	後に得られる報酬ほど割り引いて計算するための係数
0	1502	0611	強化学習は例えば何に使われていますか	識別における決定木の考え方を回帰問題に適用する方法
0	1502	0701	強化学習は例えば何に使われていますか	サポートベクトルマシン
0	1502	0605	強化学習は例えば何に使われていますか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1502	0802	強化学習は例えば何に使われていますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1502	0511	強化学習は例えば何に使われていますか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	1502	0205	強化学習は例えば何に使われていますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1502	0612	強化学習は例えば何に使われていますか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	1502	0505	強化学習は例えば何に使われていますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1502	0917	強化学習は例えば何に使われていますか	入力ゲート・出力ゲート・忘却ゲート
0	1502	1409	強化学習は例えば何に使われていますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1502	1301	強化学習は例えば何に使われていますか	形態素解析処理が典型的な問題
0	1502	0105	強化学習は例えば何に使われていますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1502	0908	強化学習は例えば何に使われていますか	ユークリッド距離
1	1503	1503	強化学習で、報酬が決定的な場合の学習はどのようなものですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	1503	0805	強化学習で、報酬が決定的な場合の学習はどのようなものですか	誤差逆伝播法
0	1503	0811	強化学習で、報酬が決定的な場合の学習はどのようなものですか	半分の領域で勾配が1になるので
0	1503	0204	強化学習で、報酬が決定的な場合の学習はどのようなものですか	特徴ベクトルの次元数を減らすこと
0	1503	0610	強化学習で、報酬が決定的な場合の学習はどのようなものですか	真のモデルとの距離
0	1503	0710	強化学習で、報酬が決定的な場合の学習はどのようなものですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1503	0305	強化学習で、報酬が決定的な場合の学習はどのようなものですか	仮説に対して課す制約
0	1503	0406	強化学習で、報酬が決定的な場合の学習はどのようなものですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1503	0907	強化学習で、報酬が決定的な場合の学習はどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1503	1505	強化学習で、報酬が決定的な場合の学習はどのようなものですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1503	0717	強化学習で、報酬が決定的な場合の学習はどのようなものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1503	1111	強化学習で、報酬が決定的な場合の学習はどのようなものですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1503	1411	強化学習で、報酬が決定的な場合の学習はどのようなものですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1503	1502	強化学習で、報酬が決定的な場合の学習はどのようなものですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1503	1214	強化学習で、報酬が決定的な場合の学習はどのようなものですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1503	0505	強化学習で、報酬が決定的な場合の学習はどのようなものですか	パーセプトロン
0	1503	0412	強化学習で、報酬が決定的な場合の学習はどのようなものですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	1503	0508	強化学習で、報酬が決定的な場合の学習はどのようなものですか	二乗誤差を最小にするように識別関数を調整する方法
0	1503	0708	強化学習で、報酬が決定的な場合の学習はどのようなものですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1503	1015	強化学習で、報酬が決定的な場合の学習はどのようなものですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1503	0211	強化学習で、報酬が決定的な場合の学習はどのようなものですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1503	1106	強化学習で、報酬が決定的な場合の学習はどのようなものですか	クラスタの重心間の距離を類似度とする
0	1503	0103	強化学習で、報酬が決定的な場合の学習はどのようなものですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	1503	0901	強化学習で、報酬が決定的な場合の学習はどのようなものですか	音声認識・画像認識・自然言語処理など
0	1503	1506	強化学習で、報酬が決定的な場合の学習はどのようなものですか	その政策に従って行動したときの累積報酬の期待値で評価
1	1505	1505	マルコフ決定過程とは何ですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1505	0110	マルコフ決定過程とは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1505	0311	マルコフ決定過程とは何ですか	集合の乱雑さ
0	1505	0810	マルコフ決定過程とは何ですか	2006 年頃に考案された事前学習法
0	1505	0906	マルコフ決定過程とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1505	0514	マルコフ決定過程とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1505	0802	マルコフ決定過程とは何ですか	特徴空間上では線形識別面を設定すること
0	1505	0302	マルコフ決定過程とは何ですか	カテゴリ形式の正解情報のこと
0	1505	0510	マルコフ決定過程とは何ですか	特徴空間上でクラスを分割する面
0	1505	0902	マルコフ決定過程とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1505	0802	マルコフ決定過程とは何ですか	識別対象のクラス数
0	1505	0901	マルコフ決定過程とは何ですか	深層学習に用いるニューラルネットワーク
0	1505	0313	マルコフ決定過程とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1505	0908	マルコフ決定過程とは何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1505	0115	マルコフ決定過程とは何ですか	パターンマイニング
0	1505	0409	マルコフ決定過程とは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1505	0307	マルコフ決定過程とは何ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	1505	0417	マルコフ決定過程とは何ですか	ネットワークの構造とアークの条件付き確率
0	1505	1407	マルコフ決定過程とは何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1505	0912	マルコフ決定過程とは何ですか	画像認識
0	1505	0601	マルコフ決定過程とは何ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1505	0411	マルコフ決定過程とは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1505	0906	マルコフ決定過程とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1505	0505	マルコフ決定過程とは何ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1505	1301	マルコフ決定過程とは何ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
1	1505	1505	マルコフ決定過程の性質はどのようなものですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1505	0708	マルコフ決定過程の性質はどのようなものですか	制約を弱める変数
0	1505	1302	マルコフ決定過程の性質はどのようなものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1505	0502	マルコフ決定過程の性質はどのようなものですか	SVM
0	1505	0307	マルコフ決定過程の性質はどのようなものですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1505	0211	マルコフ決定過程の性質はどのようなものですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1505	1110	マルコフ決定過程の性質はどのようなものですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1505	1108	マルコフ決定過程の性質はどのようなものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1505	0917	マルコフ決定過程の性質はどのようなものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	1505	0605	マルコフ決定過程の性質はどのようなものですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1505	1301	マルコフ決定過程の性質はどのようなものですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1505	0911	マルコフ決定過程の性質はどのようなものですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1505	0607	マルコフ決定過程の性質はどのようなものですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1505	1303	マルコフ決定過程の性質はどのようなものですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1505	1214	マルコフ決定過程の性質はどのようなものですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1505	0611	マルコフ決定過程の性質はどのようなものですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1505	0917	マルコフ決定過程の性質はどのようなものですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1505	0916	マルコフ決定過程の性質はどのようなものですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1505	0902	マルコフ決定過程の性質はどのようなものですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1505	0514	マルコフ決定過程の性質はどのようなものですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1505	1114	マルコフ決定過程の性質はどのようなものですか	クラスタリング結果のデータ数の分布から
0	1505	0412	マルコフ決定過程の性質はどのようなものですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1505	0311	マルコフ決定過程の性質はどのようなものですか	集合の乱雑さ
0	1505	0810	マルコフ決定過程の性質はどのようなものですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1505	0711	マルコフ決定過程の性質はどのようなものですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
1	1506	1506	マルコフ決定過程における学習プロセスはどのようなものですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	1506	0610	マルコフ決定過程における学習プロセスはどのようなものですか	真のモデルとの距離
0	1506	0704	マルコフ決定過程における学習プロセスはどのようなものですか	ラグランジュの未定乗数法
0	1506	0114	マルコフ決定過程における学習プロセスはどのようなものですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1506	1301	マルコフ決定過程における学習プロセスはどのようなものですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1506	0115	マルコフ決定過程における学習プロセスはどのようなものですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1506	1012	マルコフ決定過程における学習プロセスはどのようなものですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1506	1205	マルコフ決定過程における学習プロセスはどのようなものですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1506	0803	マルコフ決定過程における学習プロセスはどのようなものですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1506	0917	マルコフ決定過程における学習プロセスはどのようなものですか	LSTMセル
0	1506	1401	マルコフ決定過程における学習プロセスはどのようなものですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	1506	0510	マルコフ決定過程における学習プロセスはどのようなものですか	特徴空間上でクラスを分割する面
0	1506	0115	マルコフ決定過程における学習プロセスはどのようなものですか	パターンマイニング
0	1506	1219	マルコフ決定過程における学習プロセスはどのようなものですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1506	0710	マルコフ決定過程における学習プロセスはどのようなものですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	1506	0810	マルコフ決定過程における学習プロセスはどのようなものですか	2006 年頃に考案された事前学習法
0	1506	0911	マルコフ決定過程における学習プロセスはどのようなものですか	学習時の自由度を意図的に下げていること
0	1506	1108	マルコフ決定過程における学習プロセスはどのようなものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1506	0710	マルコフ決定過程における学習プロセスはどのようなものですか	低次元の特徴ベクトルを高次元に写像
0	1506	0112	マルコフ決定過程における学習プロセスはどのようなものですか	線形回帰，回帰木，モデル木など
0	1506	0105	マルコフ決定過程における学習プロセスはどのようなものですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1506	0708	マルコフ決定過程における学習プロセスはどのようなものですか	制約を弱める変数
0	1506	0412	マルコフ決定過程における学習プロセスはどのようなものですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1506	0702	マルコフ決定過程における学習プロセスはどのようなものですか	識別面は平面を仮定する
0	1506	0612	マルコフ決定過程における学習プロセスはどのようなものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
1	1506	1506	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	政策
0	1506	1201	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1506	0916	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	リカレントニューラルネットワーク
0	1506	0113	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	入力データに潜む規則性を学習すること
0	1506	0607	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	Lasso回帰
0	1506	0717	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	1506	0104	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1506	0404	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1506	1408	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1506	0701	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1506	1201	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	1506	1106	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	最も遠い事例対の距離を類似度とする
0	1506	0612	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	1506	1106	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	最も近い事例対の距離を類似度とする
0	1506	0612	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1506	1302	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1506	0102	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1506	0507	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	全ての誤りがなくなることが学習の終了条件なので
0	1506	1507	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1506	0205	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1506	0616	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1506	0206	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	1506	0111	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	1506	0506	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1506	0204	マルコフ決定過程における学習での意思決定規則のことを何と呼びますか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
1	1506	1506	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1506	0417	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	ネットワークの構造とアークの条件付き確率表
0	1506	0405	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1506	1108	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1506	0514	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1506	1306	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1506	0906	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	入力に近い側の処理
0	1506	0710	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	もとの空間におけるデータ間の距離関係を保存
0	1506	0916	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1506	1509	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1506	1303	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1506	0906	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1506	0205	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1506	0402	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	統計的識別手法
0	1506	0808	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1506	0407	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1506	0412	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1506	0508	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	二乗誤差を最小にするように識別関数を調整する方法
0	1506	0702	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	識別面は平面を仮定する
0	1506	1505	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	「マルコフ性」を持つ確率過程における意思決定問題
0	1506	0710	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1506	0701	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	サポートベクトルマシン
0	1506	0802	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	識別対象のクラス数
0	1506	0411	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1506	1304	マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか	入力と対応させる素性
1	1506	1506	マルコフ決定過程における学習の目標は何ですか	最適政策$\pi^*$を獲得すること
0	1506	1214	マルコフ決定過程における学習の目標は何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1506	0209	マルコフ決定過程における学習の目標は何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1506	0704	マルコフ決定過程における学習の目標は何ですか	ラグランジュの未定乗数法を不等式制約条件
0	1506	0802	マルコフ決定過程における学習の目標は何ですか	識別対象のクラス数
0	1506	0917	マルコフ決定過程における学習の目標は何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1506	0701	マルコフ決定過程における学習の目標は何ですか	学習データからのマージンが最大となる識別境界線
0	1506	0701	マルコフ決定過程における学習の目標は何ですか	識別境界線と最も近いデータとの距離
0	1506	0802	マルコフ決定過程における学習の目標は何ですか	特徴空間上では線形識別面を設定すること
0	1506	0808	マルコフ決定過程における学習の目標は何ですか	入力の重み付き和の微分
0	1506	0304	マルコフ決定過程における学習の目標は何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1506	0917	マルコフ決定過程における学習の目標は何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1506	1219	マルコフ決定過程における学習の目標は何ですか	どの個人がどの商品を購入したかが記録されているデータ
0	1506	0912	マルコフ決定過程における学習の目標は何ですか	畳み込みニューラルネットワーク
0	1506	0402	マルコフ決定過程における学習の目標は何ですか	統計的識別手法
0	1506	0611	マルコフ決定過程における学習の目標は何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1506	0906	マルコフ決定過程における学習の目標は何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1506	0708	マルコフ決定過程における学習の目標は何ですか	制約を弱める変数
0	1506	0508	マルコフ決定過程における学習の目標は何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1506	0805	マルコフ決定過程における学習の目標は何ですか	誤差逆伝播法
0	1506	1301	マルコフ決定過程における学習の目標は何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1506	1015	マルコフ決定過程における学習の目標は何ですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1506	0701	マルコフ決定過程における学習の目標は何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1506	0610	マルコフ決定過程における学習の目標は何ですか	真のモデルとの距離
0	1506	0906	マルコフ決定過程における学習の目標は何ですか	入力に近い側の処理
1	1507	1507	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1507	0505	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	生物の神経細胞の仕組みをモデル化したもの
0	1507	0715	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	1507	0611	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	回帰
0	1507	0607	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1507	0811	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	半分の領域で勾配が1になるので
0	1507	0708	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	制約を弱める変数
0	1507	0416	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	1507	0805	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1507	0514	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1507	0808	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	入力の重み付き和の微分
0	1507	1302	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1507	0611	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	識別における決定木の考え方を回帰問題に適用する方法
0	1507	0701	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	識別境界線と最も近いデータとの距離
0	1507	0512	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1507	1214	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1507	1306	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1507	0610	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	真のモデルとの距離
0	1507	1409	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1507	1104	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1507	0209	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	正例がどれだけ正しく判定されているかという指標
0	1507	1505	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1507	1214	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1507	0701	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1507	0306	マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
1	1508	1508	ベルマン方程式とは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1508	0405	ベルマン方程式とは何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1508	0512	ベルマン方程式とは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1508	0109	ベルマン方程式とは何ですか	正解が付いていない場合の学習
0	1508	0507	ベルマン方程式とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1508	0204	ベルマン方程式とは何ですか	特徴ベクトルの次元数を減らすこと
0	1508	0704	ベルマン方程式とは何ですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1508	1501	ベルマン方程式とは何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1508	1410	ベルマン方程式とは何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1508	1502	ベルマン方程式とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1508	0701	ベルマン方程式とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1508	0402	ベルマン方程式とは何ですか	統計的識別手法
0	1508	0316	ベルマン方程式とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1508	0803	ベルマン方程式とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1508	0801	ベルマン方程式とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1508	0701	ベルマン方程式とは何ですか	識別境界線と最も近いデータとの距離
0	1508	0114	ベルマン方程式とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1508	0711	ベルマン方程式とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1508	1103	ベルマン方程式とは何ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	1508	1220	ベルマン方程式とは何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1508	0901	ベルマン方程式とは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1508	1301	ベルマン方程式とは何ですか	連続音声認識
0	1508	1304	ベルマン方程式とは何ですか	出力系列を参照する素性
0	1508	0402	ベルマン方程式とは何ですか	最大事後確率則
0	1508	0715	ベルマン方程式とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
1	1509	1509	モデルベースの手法とはどのような場合ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1509	0803	モデルベースの手法とはどのような場合ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1509	0105	モデルベースの手法とはどのような場合ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	1509	0402	モデルベースの手法とはどのような場合ですか	事後確率
0	1509	1304	モデルベースの手法とはどのような場合ですか	出力系列を参照する素性
0	1509	0313	モデルベースの手法とはどのような場合ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1509	0801	モデルベースの手法とはどのような場合ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1509	0402	モデルベースの手法とはどのような場合ですか	入力を観測した後で計算される確率
0	1509	0505	モデルベースの手法とはどのような場合ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1509	0206	モデルベースの手法とはどのような場合ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	1509	0802	モデルベースの手法とはどのような場合ですか	特徴ベクトルの次元数
0	1509	0112	モデルベースの手法とはどのような場合ですか	線形回帰，回帰木，モデル木など
0	1509	0306	モデルベースの手法とはどのような場合ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1509	0209	モデルベースの手法とはどのような場合ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1509	1502	モデルベースの手法とはどのような場合ですか	将棋や囲碁などを行うプログラム
0	1509	0711	モデルベースの手法とはどのような場合ですか	カーネル関数
0	1509	0502	モデルベースの手法とはどのような場合ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1509	0707	モデルベースの手法とはどのような場合ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1509	0912	モデルベースの手法とはどのような場合ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1509	0406	モデルベースの手法とはどのような場合ですか	各クラスから生じる特徴の尤もらしさを表す
0	1509	0109	モデルベースの手法とはどのような場合ですか	正解が付いていない場合の学習
0	1509	1406	モデルベースの手法とはどのような場合ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1509	0805	モデルベースの手法とはどのような場合ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1509	0102	モデルベースの手法とはどのような場合ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	1509	0315	モデルベースの手法とはどのような場合ですか	分割後のデータの分散
1	1509	1509	モデルフリーの手法とはどのような場合ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1509	1004	モデルフリーの手法とはどのような場合ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1509	0912	モデルフリーの手法とはどのような場合ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1509	0811	モデルフリーの手法とはどのような場合ですか	引数が負のときは0，0以上のときはその値を出力
0	1509	0917	モデルフリーの手法とはどのような場合ですか	LSTM
0	1509	0104	モデルフリーの手法とはどのような場合ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1509	0305	モデルフリーの手法とはどのような場合ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1509	1505	モデルフリーの手法とはどのような場合ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1509	0402	モデルフリーの手法とはどのような場合ですか	事後確率が最大となるクラスを識別結果とする方法
0	1509	1209	モデルフリーの手法とはどのような場合ですか	この値が高いほど，得られる情報の多い規則であること
0	1509	0507	モデルフリーの手法とはどのような場合ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1509	0505	モデルフリーの手法とはどのような場合ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1509	0610	モデルフリーの手法とはどのような場合ですか	トレードオフの関係
0	1509	0906	モデルフリーの手法とはどのような場合ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1509	1106	モデルフリーの手法とはどのような場合ですか	最も近い事例対の距離を類似度とする
0	1509	0701	モデルフリーの手法とはどのような場合ですか	識別境界線と最も近いデータとの距離
0	1509	0810	モデルフリーの手法とはどのような場合ですか	誤差が小さくなって消失してしまう
0	1509	1510	モデルフリーの手法とはどのような場合ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1509	0109	モデルフリーの手法とはどのような場合ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	1509	0611	モデルフリーの手法とはどのような場合ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1509	1205	モデルフリーの手法とはどのような場合ですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	1509	0315	モデルフリーの手法とはどのような場合ですか	分割後のデータの分散
0	1509	0505	モデルフリーの手法とはどのような場合ですか	パーセプトロン
0	1509	0802	モデルフリーの手法とはどのような場合ですか	特徴空間上では線形識別面を設定すること
0	1509	0111	モデルフリーの手法とはどのような場合ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
1	1510	1510	ε-greedy法とは何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1510	1207	ε-greedy法とは何ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1510	0707	ε-greedy法とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1510	0912	ε-greedy法とは何ですか	画像認識
0	1510	0611	ε-greedy法とは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1510	0114	ε-greedy法とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1510	1112	ε-greedy法とは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	1510	0514	ε-greedy法とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1510	0611	ε-greedy法とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1510	0903	ε-greedy法とは何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1510	0801	ε-greedy法とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1510	0507	ε-greedy法とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1510	0514	ε-greedy法とは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	1510	0204	ε-greedy法とは何ですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	1510	0701	ε-greedy法とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1510	0209	ε-greedy法とは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1510	0211	ε-greedy法とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1510	1507	ε-greedy法とは何ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1510	1503	ε-greedy法とは何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	1510	1502	ε-greedy法とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1510	0710	ε-greedy法とは何ですか	低次元の特徴ベクトルを高次元に写像
0	1510	1303	ε-greedy法とは何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1510	0507	ε-greedy法とは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	1510	0708	ε-greedy法とは何ですか	制約を弱める変数
0	1510	0707	ε-greedy法とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
1	0502	0502	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0502	0801	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0502	0701	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0502	1310	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	確率的非決定性オートマトンの一種
0	0502	0109	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	正解が付いていない場合の学習
0	0502	1010	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0502	0402	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	最大事後確率則
0	0502	0611	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	識別における決定木の考え方を回帰問題に適用する方法
0	0502	0116	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	学習データが教師あり／教師なしの混在となっているもの
0	0502	1110	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0502	0715	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	カーネルトリック
0	0502	0811	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0502	1207	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0502	0115	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0502	0316	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0502	0805	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	誤差逆伝播法
0	0502	1506	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0502	1506	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	後に得られる報酬ほど割り引いて計算するための係数
0	0502	0508	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0502	0413	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	変数間の独立性を表現できること
0	0502	1309	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0502	0505	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	生物の神経細胞の仕組みをモデル化したもの
0	0502	1108	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0502	0412	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0502	1301	非線形性を持つデータに対して識別を行うにはどんな方法がありますか	動画像の分類や音声で入力された単語の識別などの問題
1	0505	0505	識別関数法とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0505	1001	識別関数法とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0505	1106	識別関数法とはなんですか	最も近い事例対の距離を類似度とする
0	0505	0801	識別関数法とはなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0505	1106	識別関数法とはなんですか	最も遠い事例対の距離を類似度とする
0	0505	0411	識別関数法とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0505	0701	識別関数法とはなんですか	サポートベクトルマシン
0	0505	1214	識別関数法とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0505	0906	識別関数法とはなんですか	十分多くの層を持つニューラルネットワーク
0	0505	0109	識別関数法とはなんですか	正解が付いていない場合の学習
0	0505	0611	識別関数法とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0505	0410	識別関数法とはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0505	0902	識別関数法とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0505	1110	識別関数法とはなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0505	0912	識別関数法とはなんですか	畳み込みニューラルネットワーク
0	0505	0901	識別関数法とはなんですか	深層学習に用いるニューラルネットワーク
0	0505	0104	識別関数法とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0505	1502	識別関数法とはなんですか	将棋や囲碁などを行うプログラム
0	0505	1309	識別関数法とはなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0505	0111	識別関数法とはなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0505	0610	識別関数法とはなんですか	トレードオフの関係
0	0505	0810	識別関数法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0505	0708	識別関数法とはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0505	0113	識別関数法とはなんですか	入力データに潜む規則性を学習すること
0	0505	1506	識別関数法とはなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
1	0508	0508	最小二乗法とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0508	1501	最小二乗法とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0508	0504	最小二乗法とはなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0508	0713	最小二乗法とはなんですか	文書分類やバイオインフォマティックスなど
0	0508	0511	最小二乗法とはなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0508	0402	最小二乗法とはなんですか	統計的識別手法
0	0508	0413	最小二乗法とはなんですか	変数間の独立性を表現できること
0	0508	0505	最小二乗法とはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0508	1303	最小二乗法とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0508	1502	最小二乗法とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0508	0911	最小二乗法とはなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0508	0901	最小二乗法とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0508	0313	最小二乗法とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0508	1503	最小二乗法とはなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0508	0715	最小二乗法とはなんですか	カーネルトリック
0	0508	1505	最小二乗法とはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0508	0502	最小二乗法とはなんですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0508	0701	最小二乗法とはなんですか	識別境界線と最も近いデータとの距離
0	0508	0701	最小二乗法とはなんですか	サポートベクトルマシン
0	0508	0717	最小二乗法とはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0508	0113	最小二乗法とはなんですか	入力データに潜む規則性を学習すること
0	0508	0906	最小二乗法とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0508	0313	最小二乗法とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0508	1412	最小二乗法とはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0508	0702	最小二乗法とはなんですか	識別面は平面を仮定する
1	0510	0510	識別面とはなんですか	特徴空間上でクラスを分割する面
0	0510	1301	識別面とはなんですか	連続音声認識
0	0510	1403	識別面とはなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0510	1301	識別面とはなんですか	形態素解析処理が典型的な問題
0	0510	0901	識別面とはなんですか	深層学習に用いるニューラルネットワーク
0	0510	0105	識別面とはなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0510	0906	識別面とはなんですか	十分多くの層を持つニューラルネットワーク
0	0510	1301	識別面とはなんですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0510	0612	識別面とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0510	0701	識別面とはなんですか	サポートベクトルマシン
0	0510	1310	識別面とはなんですか	確率的非決定性オートマトンの一種
0	0510	0811	識別面とはなんですか	半分の領域で勾配が1になるので
0	0510	0802	識別面とはなんですか	隠れ層
0	0510	0514	識別面とはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0510	1106	識別面とはなんですか	最も遠い事例対の距離を類似度とする
0	0510	0611	識別面とはなんですか	回帰
0	0510	0512	識別面とはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0510	0906	識別面とはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0510	0907	識別面とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0510	0903	識別面とはなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0510	0610	識別面とはなんですか	学習結果の散らばり具合
0	0510	0901	識別面とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0510	1012	識別面とはなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0510	0908	識別面とはなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0510	1114	識別面とはなんですか	クラスタリング結果のデータ数の分布から
1	0512	0512	最急勾配法とはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0512	0917	最急勾配法とはなんですか	LSTMセル
0	0512	1205	最急勾配法とはなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0512	0701	最急勾配法とはなんですか	識別境界線と最も近いデータとの距離
0	0512	0507	最急勾配法とはなんですか	パーセプトロンの収束定理
0	0512	0607	最急勾配法とはなんですか	Lasso回帰
0	0512	0906	最急勾配法とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0512	0112	最急勾配法とはなんですか	線形回帰，回帰木，モデル木など
0	0512	0409	最急勾配法とはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0512	1008	最急勾配法とはなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0512	0204	最急勾配法とはなんですか	特徴ベクトルの次元数を減らすこと
0	0512	0616	最急勾配法とはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0512	0901	最急勾配法とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0512	0701	最急勾配法とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0512	0711	最急勾配法とはなんですか	カーネル関数
0	0512	0305	最急勾配法とはなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0512	0810	最急勾配法とはなんですか	誤差が小さくなって消失してしまう
0	0512	0316	最急勾配法とはなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0512	1104	最急勾配法とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0512	0701	最急勾配法とはなんですか	サポートベクトルマシン
0	0512	0912	最急勾配法とはなんですか	畳み込みニューラルネットワーク
0	0512	0406	最急勾配法とはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0512	0717	最急勾配法とはなんですか	Grid search
0	0512	0801	最急勾配法とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0512	0302	最急勾配法とはなんですか	カテゴリ形式の正解情報のこと
1	0514	0514	最急勾配法の問題点とはなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0514	0612	最急勾配法の問題点とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0514	0710	最急勾配法の問題点とはなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0514	0803	最急勾配法の問題点とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0514	0412	最急勾配法の問題点とはなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0514	0710	最急勾配法の問題点とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0514	0901	最急勾配法の問題点とはなんですか	深層学習に用いるニューラルネットワーク
0	0514	1408	最急勾配法の問題点とはなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0514	0907	最急勾配法の問題点とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0514	0803	最急勾配法の問題点とはなんですか	非線形識別面
0	0514	0104	最急勾配法の問題点とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0514	0402	最急勾配法の問題点とはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0514	1409	最急勾配法の問題点とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0514	0917	最急勾配法の問題点とはなんですか	LSTM
0	0514	0917	最急勾配法の問題点とはなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0514	1306	最急勾配法の問題点とはなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0514	0307	最急勾配法の問題点とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0514	0810	最急勾配法の問題点とはなんですか	2006 年頃に考案された事前学習法
0	0514	0416	最急勾配法の問題点とはなんですか	値が真となる確率を知りたいノードが表す変数
0	0514	0614	最急勾配法の問題点とはなんですか	モデル木
0	0514	0607	最急勾配法の問題点とはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0514	1405	最急勾配法の問題点とはなんですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0514	1409	最急勾配法の問題点とはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0514	0109	最急勾配法の問題点とはなんですか	正解が付いていない場合の学習
0	0514	0307	最急勾配法の問題点とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
1	0514	0514	確率的最急勾配法とはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0514	0611	確率的最急勾配法とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0514	0703	確率的最急勾配法とはなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0514	1508	確率的最急勾配法とはなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0514	0406	確率的最急勾配法とはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0514	0614	確率的最急勾配法とはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0514	0811	確率的最急勾配法とはなんですか	ユニットの活性化関数を工夫する方法があります
0	0514	0411	確率的最急勾配法とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0514	0907	確率的最急勾配法とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0514	0912	確率的最急勾配法とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0514	0502	確率的最急勾配法とはなんですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0514	0109	確率的最急勾配法とはなんですか	正解が付いていない場合の学習
0	0514	1203	確率的最急勾配法とはなんですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0514	0702	確率的最急勾配法とはなんですか	識別面は平面を仮定する
0	0514	0612	確率的最急勾配法とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0514	1301	確率的最急勾配法とはなんですか	連続音声認識
0	0514	0901	確率的最急勾配法とはなんですか	表現学習
0	0514	0906	確率的最急勾配法とはなんですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0514	1108	確率的最急勾配法とはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0514	0315	確率的最急勾配法とはなんですか	分割後のデータの分散
0	0514	1301	確率的最急勾配法とはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0514	1004	確率的最急勾配法とはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0514	0209	確率的最急勾配法とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0514	1409	確率的最急勾配法とはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0514	0917	確率的最急勾配法とはなんですか	LSTMセル
1	0610	0610	分散とはなんですか	学習結果の散らばり具合
0	0610	1509	分散とはなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0610	0502	分散とはなんですか	SVM
0	0610	0616	分散とはなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0610	0906	分散とはなんですか	十分多くの層を持つニューラルネットワーク
0	0610	0614	分散とはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0610	0710	分散とはなんですか	もとの空間におけるデータ間の距離関係を保存
0	0610	0907	分散とはなんですか	事前学習法
0	0610	0313	分散とはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0610	0907	分散とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0610	1505	分散とはなんですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0610	0808	分散とはなんですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0610	0810	分散とはなんですか	2006 年頃に考案された事前学習法
0	0610	0916	分散とはなんですか	リカレントニューラルネットワーク
0	0610	0902	分散とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0610	0711	分散とはなんですか	カーネル関数
0	0610	0801	分散とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0610	1501	分散とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0610	1301	分散とはなんですか	形態素解析処理が典型的な問題
0	0610	0701	分散とはなんですか	サポートベクトルマシン
0	0610	1302	分散とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0610	1215	分散とはなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0610	1502	分散とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0610	0911	分散とはなんですか	学習時の自由度を意図的に下げていること
0	0610	0912	分散とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
1	0611	0611	回帰木とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0611	1501	回帰木とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0611	0811	回帰木とはなんですか	半分の領域で勾配が1になるので
0	0611	1301	回帰木とはなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0611	0412	回帰木とはなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0611	0109	回帰木とはなんですか	正解が付いていない場合の学習
0	0611	0614	回帰木とはなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0611	0407	回帰木とはなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0611	0509	回帰木とはなんですか	確率的最急勾配法
0	0611	0710	回帰木とはなんですか	もとの空間におけるデータ間の距離関係を保存
0	0611	0305	回帰木とはなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0611	1116	回帰木とはなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0611	1410	回帰木とはなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0611	0115	回帰木とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0611	0114	回帰木とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0611	0607	回帰木とはなんですか	Lasso回帰
0	0611	0912	回帰木とはなんですか	画像認識
0	0611	0803	回帰木とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0611	0209	回帰木とはなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0611	1301	回帰木とはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0611	0306	回帰木とはなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0611	0701	回帰木とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0611	0105	回帰木とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0611	0710	回帰木とはなんですか	もとの空間におけるデータ間の距離関係を保存
0	0611	0912	回帰木とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
1	0611	0611	回帰木の特徴はなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0611	0509	回帰木の特徴はなんですか	確率的最急勾配法
0	0611	0109	回帰木の特徴はなんですか	正解が付いていない場合の学習
0	0611	1207	回帰木の特徴はなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0611	0304	回帰木の特徴はなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0611	0906	回帰木の特徴はなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0611	0906	回帰木の特徴はなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0611	0510	回帰木の特徴はなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0611	0701	回帰木の特徴はなんですか	サポートベクトルマシン
0	0611	0708	回帰木の特徴はなんですか	制約を弱める変数
0	0611	0402	回帰木の特徴はなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0611	0507	回帰木の特徴はなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0611	0710	回帰木の特徴はなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0611	0605	回帰木の特徴はなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0611	0508	回帰木の特徴はなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0611	1015	回帰木の特徴はなんですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0611	0315	回帰木の特徴はなんですか	分割後のデータの分散
0	0611	0514	回帰木の特徴はなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0611	0908	回帰木の特徴はなんですか	シグモイド関数
0	0611	0514	回帰木の特徴はなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0611	1001	回帰木の特徴はなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0611	0717	回帰木の特徴はなんですか	Grid search
0	0611	0612	回帰木の特徴はなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0611	1306	回帰木の特徴はなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0611	0704	回帰木の特徴はなんですか	以下の関数$L$の最小値を求めるという問題
1	0612	0612	CARTとはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0612	1404	CARTとはなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0612	0606	CARTとはなんですか	入力が少し変化したときに，出力も少し変化する
0	0612	1220	CARTとはなんですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0612	1110	CARTとはなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0612	0402	CARTとはなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0612	1412	CARTとはなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0612	0505	CARTとはなんですか	パーセプトロン
0	0612	0805	CARTとはなんですか	誤差逆伝播法
0	0612	0801	CARTとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0612	0901	CARTとはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0612	1106	CARTとはなんですか	クラスタの重心間の距離を類似度とする
0	0612	1106	CARTとはなんですか	最も遠い事例対の距離を類似度とする
0	0612	0906	CARTとはなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0612	0911	CARTとはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0612	0114	CARTとはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0612	0102	CARTとはなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0612	0109	CARTとはなんですか	正解が付いていない場合の学習
0	0612	0802	CARTとはなんですか	多層パーセプトロンあるいはニューラルネットワーク
0	0612	0208	CARTとはなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0612	1110	CARTとはなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0612	0901	CARTとはなんですか	音声認識・画像認識・自然言語処理など
0	0612	0711	CARTとはなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0612	0912	CARTとはなんですか	畳み込みニューラルネットワーク
0	0612	0707	CARTとはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
1	0614	0614	モデル木とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0614	1112	モデル木とはなんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0614	0907	モデル木とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0614	1502	モデル木とはなんですか	将棋や囲碁などを行うプログラム
0	0614	0911	モデル木とはなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0614	0802	モデル木とはなんですか	入力層・出力層の数に応じた適当な数
0	0614	0109	モデル木とはなんですか	学習データに正解が付いている場合の学習
0	0614	0803	モデル木とはなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0614	1001	モデル木とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0614	1010	モデル木とはなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0614	0704	モデル木とはなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0614	0607	モデル木とはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0614	0315	モデル木とはなんですか	分割後のデータの分散
0	0614	0906	モデル木とはなんですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0614	1502	モデル木とはなんですか	将棋や囲碁などを行うプログラム
0	0614	0803	モデル木とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0614	0710	モデル木とはなんですか	もとの空間におけるデータ間の距離関係を保存
0	0614	0304	モデル木とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0614	0204	モデル木とはなんですか	特徴ベクトルの次元数を減らすこと
0	0614	0612	モデル木とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0614	1114	モデル木とはなんですか	クラスタリング結果のデータ数の分布
0	0614	0103	モデル木とはなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0614	0801	モデル木とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0614	0514	モデル木とはなんですか	ランダムに学習データを一つ
0	0614	0811	モデル木とはなんですか	ユニットの活性化関数を工夫する方法があります
1	0409	0409	最尤推定法とはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0409	0901	最尤推定法とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0409	0607	最尤推定法とはなんですか	「投げ縄」という意味
0	0409	0304	最尤推定法とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0409	0105	最尤推定法とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0409	1114	最尤推定法とはなんですか	クラスタリング結果のデータ数の分布
0	0409	0504	最尤推定法とはなんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0409	1404	最尤推定法とはなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0409	1412	最尤推定法とはなんですか	近くのノードは同じクラスになりやすいという仮定
0	0409	1301	最尤推定法とはなんですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0409	0104	最尤推定法とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0409	0114	最尤推定法とはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0409	0508	最尤推定法とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0409	0805	最尤推定法とはなんですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0409	0116	最尤推定法とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0409	0209	最尤推定法とはなんですか	正例がどれだけ正しく判定されているかという指標
0	0409	0717	最尤推定法とはなんですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0409	0810	最尤推定法とはなんですか	勾配消失問題
0	0409	0305	最尤推定法とはなんですか	仮説に対して課す制約
0	0409	0109	最尤推定法とはなんですか	学習データに正解が付いている場合の学習
0	0409	1506	最尤推定法とはなんですか	政策
0	0409	1209	最尤推定法とはなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0409	1303	最尤推定法とはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0409	0102	最尤推定法とはなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0409	0906	最尤推定法とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
1	0413	0413	ベイジアンネットワークのメリットは何ですか	変数間の独立性を表現できること
0	0413	0614	ベイジアンネットワークのメリットは何ですか	モデル木
0	0413	0803	ベイジアンネットワークのメリットは何ですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0413	0713	ベイジアンネットワークのメリットは何ですか	文書分類やバイオインフォマティックスなど
0	0413	0104	ベイジアンネットワークのメリットは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0413	1302	ベイジアンネットワークのメリットは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0413	0402	ベイジアンネットワークのメリットは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0413	0112	ベイジアンネットワークのメリットは何ですか	線形回帰，回帰木，モデル木など
0	0413	0204	ベイジアンネットワークのメリットは何ですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0413	0507	ベイジアンネットワークのメリットは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	0413	0811	ベイジアンネットワークのメリットは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0413	0404	ベイジアンネットワークのメリットは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0413	0104	ベイジアンネットワークのメリットは何ですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0413	1301	ベイジアンネットワークのメリットは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0413	0114	ベイジアンネットワークのメリットは何ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0413	1108	ベイジアンネットワークのメリットは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0413	0305	ベイジアンネットワークのメリットは何ですか	仮説に対して課す制約
0	0413	0907	ベイジアンネットワークのメリットは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0413	0614	ベイジアンネットワークのメリットは何ですか	線形回帰式
0	0413	0514	ベイジアンネットワークのメリットは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0413	0802	ベイジアンネットワークのメリットは何ですか	特徴空間上では線形識別面を設定すること
0	0413	0901	ベイジアンネットワークのメリットは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0413	0109	ベイジアンネットワークのメリットは何ですか	正解が付いていない場合の学習
0	0413	0602	ベイジアンネットワークのメリットは何ですか	正解情報$y$が数値であるということ
0	0413	1214	ベイジアンネットワークのメリットは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
1	0502	0502	統計モデルによるアプローチはどのようなときに有効ですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0502	1215	統計モデルによるアプローチはどのようなときに有効ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0502	0901	統計モデルによるアプローチはどのようなときに有効ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0502	0614	統計モデルによるアプローチはどのようなときに有効ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0502	0304	統計モデルによるアプローチはどのようなときに有効ですか	個々の事例から，あるクラスについて共通点を見つけること
0	0502	1012	統計モデルによるアプローチはどのようなときに有効ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0502	0508	統計モデルによるアプローチはどのようなときに有効ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0502	0701	統計モデルによるアプローチはどのようなときに有効ですか	線形で識別できないデータに対応するため
0	0502	0605	統計モデルによるアプローチはどのようなときに有効ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0502	0114	統計モデルによるアプローチはどのようなときに有効ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0502	0114	統計モデルによるアプローチはどのようなときに有効ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0502	0701	統計モデルによるアプローチはどのようなときに有効ですか	サポートベクトルマシン
0	0502	1303	統計モデルによるアプローチはどのようなときに有効ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0502	0901	統計モデルによるアプローチはどのようなときに有効ですか	深層学習に用いるニューラルネットワーク
0	0502	0205	統計モデルによるアプローチはどのようなときに有効ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0502	1409	統計モデルによるアプローチはどのようなときに有効ですか	自分が出した誤りを指摘してくれる他人がいない
0	0502	0105	統計モデルによるアプローチはどのようなときに有効ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0502	0805	統計モデルによるアプローチはどのようなときに有効ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0502	1111	統計モデルによるアプローチはどのようなときに有効ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0502	1111	統計モデルによるアプローチはどのようなときに有効ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0502	0916	統計モデルによるアプローチはどのようなときに有効ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0502	0902	統計モデルによるアプローチはどのようなときに有効ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0502	1506	統計モデルによるアプローチはどのようなときに有効ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0502	0701	統計モデルによるアプローチはどのようなときに有効ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0502	0701	統計モデルによるアプローチはどのようなときに有効ですか	識別境界線と最も近いデータとの距離
1	0508	0508	線形分離不可能なデータにはどのように対処しますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0508	0513	線形分離不可能なデータにはどのように対処しますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0508	1214	線形分離不可能なデータにはどのように対処しますか	計算量が膨大であること
0	0508	0411	線形分離不可能なデータにはどのように対処しますか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0508	0808	線形分離不可能なデータにはどのように対処しますか	入力の重み付き和の微分
0	0508	0908	線形分離不可能なデータにはどのように対処しますか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0508	1306	線形分離不可能なデータにはどのように対処しますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0508	1409	線形分離不可能なデータにはどのように対処しますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0508	0606	線形分離不可能なデータにはどのように対処しますか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0508	1506	線形分離不可能なデータにはどのように対処しますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0508	1110	線形分離不可能なデータにはどのように対処しますか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0508	0906	線形分離不可能なデータにはどのように対処しますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0508	0109	線形分離不可能なデータにはどのように対処しますか	正解が付いていない場合の学習
0	0508	0111	線形分離不可能なデータにはどのように対処しますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0508	0704	線形分離不可能なデータにはどのように対処しますか	以下の関数$L$の最小値を求めるという問題
0	0508	0811	線形分離不可能なデータにはどのように対処しますか	ユニットの活性化関数を工夫する方法があります
0	0508	0612	線形分離不可能なデータにはどのように対処しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0508	0510	線形分離不可能なデータにはどのように対処しますか	特徴空間上でクラスを分割する面
0	0508	0505	線形分離不可能なデータにはどのように対処しますか	ニューラルネットワーク
0	0508	0811	線形分離不可能なデータにはどのように対処しますか	引数が負のときは0，0以上のときはその値を出力
0	0508	0313	線形分離不可能なデータにはどのように対処しますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0508	0701	線形分離不可能なデータにはどのように対処しますか	識別境界線と最も近いデータとの距離
0	0508	1106	線形分離不可能なデータにはどのように対処しますか	最も近い事例対の距離を類似度とする
0	0508	0710	線形分離不可能なデータにはどのように対処しますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0508	0313	線形分離不可能なデータにはどのように対処しますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
1	0510	0510	識別モデルはどのように作りますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0510	1106	識別モデルはどのように作りますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0510	0114	識別モデルはどのように作りますか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0510	0903	識別モデルはどのように作りますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0510	0701	識別モデルはどのように作りますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0510	1209	識別モデルはどのように作りますか	規則の条件部が起こったときに結論部が起こる割合
0	0510	0302	識別モデルはどのように作りますか	カテゴリ形式の正解情報のこと
0	0510	0614	識別モデルはどのように作りますか	回帰木と線形回帰の双方のよいところを取った方法
0	0510	1209	識別モデルはどのように作りますか	規則の条件部が起こったときに結論部が起こる割合
0	0510	0912	識別モデルはどのように作りますか	畳み込みニューラルネットワーク
0	0510	0307	識別モデルはどのように作りますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0510	1306	識別モデルはどのように作りますか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0510	0611	識別モデルはどのように作りますか	識別における決定木の考え方を回帰問題に適用する方法
0	0510	0810	識別モデルはどのように作りますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0510	0404	識別モデルはどのように作りますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0510	0402	識別モデルはどのように作りますか	事後確率
0	0510	0305	識別モデルはどのように作りますか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0510	0701	識別モデルはどのように作りますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0510	0701	識別モデルはどのように作りますか	サポートベクトルマシン
0	0510	0509	識別モデルはどのように作りますか	確率的最急勾配法
0	0510	1106	識別モデルはどのように作りますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0510	0413	識別モデルはどのように作りますか	変数間の独立性を表現できること
0	0510	0902	識別モデルはどのように作りますか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0510	0110	識別モデルはどのように作りますか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0510	0701	識別モデルはどのように作りますか	識別境界線と最も近いデータとの距離
1	0601	0601	回帰問題とはなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0601	0413	回帰問題とはなんですか	変数間の独立性を表現できること
0	0601	1110	回帰問題とはなんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0601	0607	回帰問題とはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0601	0901	回帰問題とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0601	0316	回帰問題とはなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0601	0302	回帰問題とはなんですか	カテゴリ形式の正解情報のこと
0	0601	0916	回帰問題とはなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0601	0508	回帰問題とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0601	1001	回帰問題とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0601	1104	回帰問題とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0601	0504	回帰問題とはなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0601	1404	回帰問題とはなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0601	0711	回帰問題とはなんですか	カーネル関数
0	0601	0109	回帰問題とはなんですか	正解が付いていない場合の学習
0	0601	0907	回帰問題とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0601	0514	回帰問題とはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0601	0602	回帰問題とはなんですか	数値型の正解情報のこと
0	0601	0409	回帰問題とはなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0601	0901	回帰問題とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0601	1301	回帰問題とはなんですか	連続音声認識
0	0601	0908	回帰問題とはなんですか	シグモイド関数
0	0601	1301	回帰問題とはなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0601	0612	回帰問題とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0601	0404	回帰問題とはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
1	0607	0607	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	Lasso回帰
0	0607	0611	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0607	0405	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0607	0701	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0607	0803	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	重みパラメータに対しては線形で，入力を非線形変換する
0	0607	0404	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0607	0908	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0607	0502	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0607	0912	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	畳み込みニューラルネットワーク
0	0607	1404	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0607	0803	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0607	0416	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	アークを無向とみなした結合を考えたとき
0	0607	0611	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	識別における決定木の考え方を回帰問題に適用する方法
0	0607	0508	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0607	0514	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0607	0912	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	畳み込みニューラルネットワーク
0	0607	0505	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	パーセプトロン
0	0607	0611	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	識別における決定木の考え方を回帰問題に適用する方法
0	0607	1015	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0607	0405	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0607	0811	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	引数が負のときは0，0以上のときはその値を出力
0	0607	0505	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0607	0505	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	ニューラルネットワーク
0	0607	0901	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	深層学習に用いるニューラルネットワーク
0	0607	1207	パラメータ$\bm{w}$の絶対値を正則化項とするものをなんといいますか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
1	0610	0610	バイアスと分散はどのような関係ですか	トレードオフの関係
0	0610	0109	バイアスと分散はどのような関係ですか	正解が付いていない場合の学習
0	0610	0802	バイアスと分散はどのような関係ですか	識別対象のクラス数
0	0610	0404	バイアスと分散はどのような関係ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0610	0612	バイアスと分散はどのような関係ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0610	0901	バイアスと分散はどのような関係ですか	表現学習
0	0610	0402	バイアスと分散はどのような関係ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0610	0614	バイアスと分散はどのような関係ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0610	0902	バイアスと分散はどのような関係ですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0610	1309	バイアスと分散はどのような関係ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0610	0701	バイアスと分散はどのような関係ですか	サポートベクトルマシン
0	0610	0901	バイアスと分散はどのような関係ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0610	1301	バイアスと分散はどのような関係ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0610	0115	バイアスと分散はどのような関係ですか	パターンマイニング
0	0610	0508	バイアスと分散はどのような関係ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0610	0509	バイアスと分散はどのような関係ですか	確率的最急勾配法
0	0610	0901	バイアスと分散はどのような関係ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0610	1506	バイアスと分散はどのような関係ですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0610	0701	バイアスと分散はどのような関係ですか	線形で識別できないデータに対応するため
0	0610	0907	バイアスと分散はどのような関係ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0610	0114	バイアスと分散はどのような関係ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0610	0507	バイアスと分散はどのような関係ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0610	0811	バイアスと分散はどのような関係ですか	ReLu
0	0610	0611	バイアスと分散はどのような関係ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0610	0209	バイアスと分散はどのような関係ですか	正例がどれだけ正しく判定されているかという指標
1	0611	0611	回帰木の特徴はどんものですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0611	0105	回帰木の特徴はどんものですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0611	0614	回帰木の特徴はどんものですか	回帰木と線形回帰の双方のよいところを取った方法
0	0611	0912	回帰木の特徴はどんものですか	画像認識
0	0611	1205	回帰木の特徴はどんものですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0611	0110	回帰木の特徴はどんものですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0611	0111	回帰木の特徴はどんものですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0611	0505	回帰木の特徴はどんものですか	生物の神経細胞の仕組みをモデル化したもの
0	0611	0113	回帰木の特徴はどんものですか	入力データに潜む規則性
0	0611	0307	回帰木の特徴はどんものですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0611	0701	回帰木の特徴はどんものですか	識別境界線と最も近いデータとの距離
0	0611	0612	回帰木の特徴はどんものですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0611	0708	回帰木の特徴はどんものですか	制約を弱める変数
0	0611	1506	回帰木の特徴はどんものですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0611	0402	回帰木の特徴はどんものですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0611	0606	回帰木の特徴はどんものですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0611	1209	回帰木の特徴はどんものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0611	1412	回帰木の特徴はどんものですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0611	1108	回帰木の特徴はどんものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0611	1209	回帰木の特徴はどんものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0611	1116	回帰木の特徴はどんものですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0611	0801	回帰木の特徴はどんものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0611	1411	回帰木の特徴はどんものですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0611	0614	回帰木の特徴はどんものですか	回帰木と線形回帰の双方のよいところを取った
0	0611	0803	回帰木の特徴はどんものですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
1	0611	0611	決定木はどのような考えですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0611	1309	決定木はどのような考えですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0611	0811	決定木はどのような考えですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0611	0104	決定木はどのような考えですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0611	0404	決定木はどのような考えですか	事前確率
0	0611	0710	決定木はどのような考えですか	もとの空間におけるデータ間の距離関係を保存
0	0611	0906	決定木はどのような考えですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0611	0702	決定木はどのような考えですか	識別面は平面を仮定する
0	0611	1304	決定木はどのような考えですか	出力系列を参照する素性
0	0611	1306	決定木はどのような考えですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0611	0708	決定木はどのような考えですか	制約を弱める変数
0	0611	1012	決定木はどのような考えですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0611	0206	決定木はどのような考えですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0611	0507	決定木はどのような考えですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0611	0402	決定木はどのような考えですか	統計的識別手法
0	0611	0707	決定木はどのような考えですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0611	0102	決定木はどのような考えですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0611	0307	決定木はどのような考えですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0611	0508	決定木はどのような考えですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0611	0506	決定木はどのような考えですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0611	1509	決定木はどのような考えですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0611	0901	決定木はどのような考えですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0611	0917	決定木はどのような考えですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0611	0514	決定木はどのような考えですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0611	0803	決定木はどのような考えですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
1	0612	0612	CARTはどのような決定木ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0612	1303	CARTはどのような決定木ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0612	0902	CARTはどのような決定木ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0612	0610	CARTはどのような決定木ですか	片方を減らせば片方が増える
0	0612	1503	CARTはどのような決定木ですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0612	0316	CARTはどのような決定木ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0612	1001	CARTはどのような決定木ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0612	1503	CARTはどのような決定木ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0612	1012	CARTはどのような決定木ですか	すべてのデータの重みは平等
0	0612	0811	CARTはどのような決定木ですか	半分の領域で勾配が1になるので
0	0612	0614	CARTはどのような決定木ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0612	1219	CARTはどのような決定木ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0612	0708	CARTはどのような決定木ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0612	1502	CARTはどのような決定木ですか	将棋や囲碁などを行うプログラム
0	0612	0908	CARTはどのような決定木ですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0612	0508	CARTはどのような決定木ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0612	0514	CARTはどのような決定木ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0612	1409	CARTはどのような決定木ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0612	1506	CARTはどのような決定木ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0612	0402	CARTはどのような決定木ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0612	0902	CARTはどのような決定木ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0612	0505	CARTはどのような決定木ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0612	0717	CARTはどのような決定木ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0612	0610	CARTはどのような決定木ですか	真のモデルとの距離
0	0612	0810	CARTはどのような決定木ですか	重みの修正量が層を戻るにつれて小さくなってゆく
1	0614	0614	回帰木と線形回帰の双方のよいところを取った方法はなんですか	モデル木
0	0614	1505	回帰木と線形回帰の双方のよいところを取った方法はなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0614	0316	回帰木と線形回帰の双方のよいところを取った方法はなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0614	0901	回帰木と線形回帰の双方のよいところを取った方法はなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0614	0803	回帰木と線形回帰の双方のよいところを取った方法はなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0614	0412	回帰木と線形回帰の双方のよいところを取った方法はなんですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0614	0413	回帰木と線形回帰の双方のよいところを取った方法はなんですか	変数間の独立性を表現できること
0	0614	1503	回帰木と線形回帰の双方のよいところを取った方法はなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0614	0114	回帰木と線形回帰の双方のよいところを取った方法はなんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0614	0808	回帰木と線形回帰の双方のよいところを取った方法はなんですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0614	1505	回帰木と線形回帰の双方のよいところを取った方法はなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0614	0209	回帰木と線形回帰の双方のよいところを取った方法はなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0614	0717	回帰木と線形回帰の双方のよいところを取った方法はなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0614	1506	回帰木と線形回帰の双方のよいところを取った方法はなんですか	政策
0	0614	0810	回帰木と線形回帰の双方のよいところを取った方法はなんですか	誤差が小さくなって消失してしまう
0	0614	0610	回帰木と線形回帰の双方のよいところを取った方法はなんですか	学習結果の散らばり具合
0	0614	0313	回帰木と線形回帰の双方のよいところを取った方法はなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0614	0810	回帰木と線形回帰の双方のよいところを取った方法はなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0614	0209	回帰木と線形回帰の双方のよいところを取った方法はなんですか	正例がどれだけ正しく判定されているかという指標
0	0614	0701	回帰木と線形回帰の双方のよいところを取った方法はなんですか	線形で識別できないデータに対応するため
0	0614	0406	回帰木と線形回帰の双方のよいところを取った方法はなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0614	0701	回帰木と線形回帰の双方のよいところを取った方法はなんですか	学習データからのマージンが最大となる識別境界線
0	0614	0110	回帰木と線形回帰の双方のよいところを取った方法はなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0614	0508	回帰木と線形回帰の双方のよいところを取った方法はなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0614	1108	回帰木と線形回帰の双方のよいところを取った方法はなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
1	0614	0614	モデル木ではリーフの値をどうしますか	線形回帰式
0	0614	0701	モデル木ではリーフの値をどうしますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0614	1103	モデル木ではリーフの値をどうしますか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0614	0708	モデル木ではリーフの値をどうしますか	制約を満たさない程度を表すので，小さい方が望ましい
0	0614	1110	モデル木ではリーフの値をどうしますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0614	0404	モデル木ではリーフの値をどうしますか	事前確率
0	0614	0703	モデル木ではリーフの値をどうしますか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0614	0204	モデル木ではリーフの値をどうしますか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0614	0204	モデル木ではリーフの値をどうしますか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0614	0917	モデル木ではリーフの値をどうしますか	LSTM
0	0614	0407	モデル木ではリーフの値をどうしますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0614	0907	モデル木ではリーフの値をどうしますか	事前学習法
0	0614	0211	モデル木ではリーフの値をどうしますか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0614	0114	モデル木ではリーフの値をどうしますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0614	0112	モデル木ではリーフの値をどうしますか	線形回帰，回帰木，モデル木など
0	0614	0906	モデル木ではリーフの値をどうしますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0614	0701	モデル木ではリーフの値をどうしますか	学習データからのマージンが最大となる識別境界線
0	0614	1104	モデル木ではリーフの値をどうしますか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0614	0811	モデル木ではリーフの値をどうしますか	ユニットの活性化関数を工夫する方法があります
0	0614	0514	モデル木ではリーフの値をどうしますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0614	1406	モデル木ではリーフの値をどうしますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0614	0810	モデル木ではリーフの値をどうしますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0614	0901	モデル木ではリーフの値をどうしますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0614	0805	モデル木ではリーフの値をどうしますか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0614	0304	モデル木ではリーフの値をどうしますか	個々の事例から，あるクラスについて共通点を見つけること
1	0614	0614	モデル木はどのような方法ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0614	0313	モデル木はどのような方法ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0614	0512	モデル木はどのような方法ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0614	1310	モデル木はどのような方法ですか	確率的非決定性オートマトンの一種
0	0614	0710	モデル木はどのような方法ですか	低次元の特徴ベクトルを高次元に写像
0	0614	0912	モデル木はどのような方法ですか	畳み込みニューラルネットワーク
0	0614	1215	モデル木はどのような方法ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0614	1103	モデル木はどのような方法ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0614	1106	モデル木はどのような方法ですか	クラスタの重心間の距離を類似度とする
0	0614	1403	モデル木はどのような方法ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0614	0708	モデル木はどのような方法ですか	制約を弱める変数
0	0614	1114	モデル木はどのような方法ですか	クラスタリング結果のデータ数の分布
0	0614	0810	モデル木はどのような方法ですか	2006 年頃に考案された事前学習法
0	0614	1220	モデル木はどのような方法ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0614	0906	モデル木はどのような方法ですか	十分多くの層
0	0614	0701	モデル木はどのような方法ですか	識別境界線と最も近いデータとの距離
0	0614	1304	モデル木はどのような方法ですか	入力と対応させる素性
0	0614	0715	モデル木はどのような方法ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0614	0509	モデル木はどのような方法ですか	確率的最急勾配法
0	0614	0402	モデル木はどのような方法ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0614	1410	モデル木はどのような方法ですか	学習初期の誤りに強いということ
0	0614	0206	モデル木はどのような方法ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0614	0606	モデル木はどのような方法ですか	山の尾根という意味
0	0614	1309	モデル木はどのような方法ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0614	0109	モデル木はどのような方法ですか	正解が付いていない場合の学習
1	0514	0514	最急勾配法の欠点はなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0514	0906	最急勾配法の欠点はなんですか	十分多くの層
0	0514	0313	最急勾配法の欠点はなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0514	1301	最急勾配法の欠点はなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0514	0802	最急勾配法の欠点はなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0514	1509	最急勾配法の欠点はなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0514	0802	最急勾配法の欠点はなんですか	識別対象のクラス数
0	0514	1508	最急勾配法の欠点はなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0514	0701	最急勾配法の欠点はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0514	0307	最急勾配法の欠点はなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0514	0307	最急勾配法の欠点はなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0514	0802	最急勾配法の欠点はなんですか	入力層・出力層の数に応じた適当な数
0	0514	0612	最急勾配法の欠点はなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0514	0701	最急勾配法の欠点はなんですか	サポートベクトルマシン
0	0514	0211	最急勾配法の欠点はなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0514	0105	最急勾配法の欠点はなんですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0514	0113	最急勾配法の欠点はなんですか	入力データに潜む規則性
0	0514	0407	最急勾配法の欠点はなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0514	0717	最急勾配法の欠点はなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0514	0602	最急勾配法の欠点はなんですか	数値型の正解情報のこと
0	0514	0508	最急勾配法の欠点はなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0514	0802	最急勾配法の欠点はなんですか	隠れ層
0	0514	1108	最急勾配法の欠点はなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0514	0306	最急勾配法の欠点はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0514	0114	最急勾配法の欠点はなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	0514	0514	確率的最急勾配法とはどういうものですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0514	1303	確率的最急勾配法とはどういうものですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0514	1407	確率的最急勾配法とはどういうものですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0514	0412	確率的最急勾配法とはどういうものですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0514	1411	確率的最急勾配法とはどういうものですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0514	0505	確率的最急勾配法とはどういうものですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0514	0917	確率的最急勾配法とはどういうものですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0514	1110	確率的最急勾配法とはどういうものですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0514	0701	確率的最急勾配法とはどういうものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0514	1012	確率的最急勾配法とはどういうものですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0514	0907	確率的最急勾配法とはどういうものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0514	1219	確率的最急勾配法とはどういうものですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0514	0712	確率的最急勾配法とはどういうものですか	カーネル関数が正定値関数という条件を満たすとき
0	0514	0704	確率的最急勾配法とはどういうものですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0514	0114	確率的最急勾配法とはどういうものですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0514	0502	確率的最急勾配法とはどういうものですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0514	1103	確率的最急勾配法とはどういうものですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0514	0912	確率的最急勾配法とはどういうものですか	画像認識
0	0514	1116	確率的最急勾配法とはどういうものですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0514	1203	確率的最急勾配法とはどういうものですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0514	0511	確率的最急勾配法とはどういうものですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0514	1510	確率的最急勾配法とはどういうものですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0514	0803	確率的最急勾配法とはどういうものですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0514	0610	確率的最急勾配法とはどういうものですか	学習結果の散らばり具合
0	0514	0611	確率的最急勾配法とはどういうものですか	識別における決定木の考え方を回帰問題に適用する方法
1	0701	0701	マージンとは何ですか	識別境界線と最も近いデータとの距離
0	0701	1406	マージンとは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0701	1103	マージンとは何ですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0701	0315	マージンとは何ですか	分割後のデータの分散
0	0701	0717	マージンとは何ですか	連続値
0	0701	0810	マージンとは何ですか	勾配消失問題
0	0701	0606	マージンとは何ですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0701	1007	マージンとは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0701	0901	マージンとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0701	1104	マージンとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0701	0610	マージンとは何ですか	学習結果の散らばり具合
0	0701	0713	マージンとは何ですか	文書分類やバイオインフォマティックスなど
0	0701	0810	マージンとは何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0701	0614	マージンとは何ですか	モデル木
0	0701	1106	マージンとは何ですか	クラスタの重心間の距離を類似度とする
0	0701	1503	マージンとは何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0701	0711	マージンとは何ですか	カーネル関数
0	0701	0901	マージンとは何ですか	Deep Neural Network (DNN) 
0	0701	1009	マージンとは何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0701	0811	マージンとは何ですか	引数が負のときは0，0以上のときはその値を出力
0	0701	0811	マージンとは何ですか	ユニットの活性化関数を工夫する方法があります
0	0701	0805	マージンとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0701	1108	マージンとは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0701	0417	マージンとは何ですか	ネットワークの構造とアークの条件付き確率
0	0701	0711	マージンとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
1	0717	0717	グリッドサーチとはどういうものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0717	1505	グリッドサーチとはどういうものですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0717	0509	グリッドサーチとはどういうものですか	確率的最急勾配法
0	0717	0404	グリッドサーチとはどういうものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0717	0505	グリッドサーチとはどういうものですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0717	0405	グリッドサーチとはどういうものですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0717	1403	グリッドサーチとはどういうものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0717	1214	グリッドサーチとはどういうものですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0717	0708	グリッドサーチとはどういうものですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0717	1012	グリッドサーチとはどういうものですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0717	0304	グリッドサーチとはどういうものですか	個々の事例から，あるクラスについて共通点を見つけること
0	0717	0614	グリッドサーチとはどういうものですか	線形回帰式
0	0717	0917	グリッドサーチとはどういうものですか	LSTM
0	0717	1302	グリッドサーチとはどういうものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0717	1306	グリッドサーチとはどういうものですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0717	1214	グリッドサーチとはどういうものですか	計算量が膨大であること
0	0717	0810	グリッドサーチとはどういうものですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0717	0508	グリッドサーチとはどういうものですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0717	0404	グリッドサーチとはどういうものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0717	0514	グリッドサーチとはどういうものですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0717	0205	グリッドサーチとはどういうものですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0717	1108	グリッドサーチとはどういうものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0717	0616	グリッドサーチとはどういうものですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0717	1409	グリッドサーチとはどういうものですか	自分が出した誤りを指摘してくれる他人がいない
0	0717	0701	グリッドサーチとはどういうものですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
1	0701	0701	SVMとは何の略称ですか	サポートベクトルマシン
0	0701	0803	SVMとは何の略称ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0701	0916	SVMとは何の略称ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0701	0912	SVMとは何の略称ですか	畳み込みニューラルネットワーク
0	0701	0405	SVMとは何の略称ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0701	0710	SVMとは何の略称ですか	もとの空間におけるデータ間の距離関係を保存
0	0701	0111	SVMとは何の略称ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0701	0109	SVMとは何の略称ですか	正解が付いていない場合の学習
0	0701	0204	SVMとは何の略称ですか	特徴ベクトルの次元数を減らすこと
0	0701	1412	SVMとは何の略称ですか	近くのノードは同じクラスになりやすいという仮定
0	0701	1104	SVMとは何の略称ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0701	1509	SVMとは何の略称ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0701	1407	SVMとは何の略称ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0701	0801	SVMとは何の略称ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0701	1104	SVMとは何の略称ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0701	0105	SVMとは何の略称ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0701	0514	SVMとは何の略称ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0701	0109	SVMとは何の略称ですか	正解が付いていない場合の学習
0	0701	1406	SVMとは何の略称ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0701	0717	SVMとは何の略称ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0701	0105	SVMとは何の略称ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0701	0906	SVMとは何の略称ですか	十分多くの層
0	0701	1301	SVMとは何の略称ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0701	0805	SVMとは何の略称ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0701	0708	SVMとは何の略称ですか	制約を満たさない程度を表すので，小さい方が望ましい
1	0701	0701	SVMとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0701	0912	SVMとは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0701	0614	SVMとは何ですか	回帰木と線形回帰の双方のよいところを取った
0	0701	0717	SVMとは何ですか	Grid search
0	0701	0102	SVMとは何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0701	0704	SVMとは何ですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0701	0707	SVMとは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0701	1402	SVMとは何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0701	1306	SVMとは何ですか	条件付き確率場（Conditional Random Field: CRF）
0	0701	0116	SVMとは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	0701	1506	SVMとは何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0701	1110	SVMとは何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0701	1412	SVMとは何ですか	近くのノードは同じクラスになりやすいという仮定
0	0701	0302	SVMとは何ですか	カテゴリ形式の正解情報のこと
0	0701	1106	SVMとは何ですか	クラスタの重心間の距離を類似度とする
0	0701	0911	SVMとは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0701	1303	SVMとは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0701	0610	SVMとは何ですか	片方を減らせば片方が増える
0	0701	1004	SVMとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0701	1411	SVMとは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0701	1309	SVMとは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0701	0611	SVMとは何ですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0701	1114	SVMとは何ですか	クラスタリング結果のデータ数の分布から
0	0701	0811	SVMとは何ですか	ユニットの活性化関数を工夫する方法
0	0701	0502	SVMとは何ですか	SVM
1	0701	0701	マージンは大きいほうがいいんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0701	0512	マージンは大きいほうがいいんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0701	0405	マージンは大きいほうがいいんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0701	1409	マージンは大きいほうがいいんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0701	0109	マージンは大きいほうがいいんですか	正解が付いていない場合の学習
0	0701	0505	マージンは大きいほうがいいんですか	生物の神経細胞の仕組みをモデル化したもの
0	0701	0211	マージンは大きいほうがいいんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0701	1104	マージンは大きいほうがいいんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0701	0402	マージンは大きいほうがいいんですか	入力を観測した後で計算される確率
0	0701	0103	マージンは大きいほうがいいんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0701	0114	マージンは大きいほうがいいんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0701	0507	マージンは大きいほうがいいんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0701	0710	マージンは大きいほうがいいんですか	もとの空間におけるデータ間の距離関係を保存
0	0701	0906	マージンは大きいほうがいいんですか	入力に近い側の処理
0	0701	0708	マージンは大きいほうがいいんですか	制約を弱める変数
0	0701	0911	マージンは大きいほうがいいんですか	ランダムに一定割合のユニットを消して学習を行う
0	0701	0109	マージンは大きいほうがいいんですか	正解が付いていない場合の学習
0	0701	0104	マージンは大きいほうがいいんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0701	0513	マージンは大きいほうがいいんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0701	1103	マージンは大きいほうがいいんですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0701	1001	マージンは大きいほうがいいんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0701	0504	マージンは大きいほうがいいんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0701	0102	マージンは大きいほうがいいんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0701	0416	マージンは大きいほうがいいんですか	アークを無向とみなした結合を考えたとき
0	0701	1220	マージンは大きいほうがいいんですか	まばらなデータを低次元行列の積に分解する方法の一つ
1	0703	0703	目的関数と距離の式が違うのはなぜですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0703	1001	目的関数と距離の式が違うのはなぜですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0703	0906	目的関数と距離の式が違うのはなぜですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0703	0906	目的関数と距離の式が違うのはなぜですか	十分多くの層を持つニューラルネットワーク
0	0703	0811	目的関数と距離の式が違うのはなぜですか	引数が負のときは0，0以上のときはその値を出力
0	0703	0901	目的関数と距離の式が違うのはなぜですか	Deep Neural Network (DNN) 
0	0703	1215	目的関数と距離の式が違うのはなぜですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0703	0102	目的関数と距離の式が違うのはなぜですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0703	0708	目的関数と距離の式が違うのはなぜですか	制約を弱める変数
0	0703	0901	目的関数と距離の式が違うのはなぜですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0703	0104	目的関数と距離の式が違うのはなぜですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0703	0916	目的関数と距離の式が違うのはなぜですか	リカレントニューラルネットワーク
0	0703	0901	目的関数と距離の式が違うのはなぜですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0703	1301	目的関数と距離の式が違うのはなぜですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0703	0103	目的関数と距離の式が違うのはなぜですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0703	0305	目的関数と距離の式が違うのはなぜですか	仮説に対して課す制約
0	0703	0514	目的関数と距離の式が違うのはなぜですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0703	0717	目的関数と距離の式が違うのはなぜですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0703	0502	目的関数と距離の式が違うのはなぜですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0703	0610	目的関数と距離の式が違うのはなぜですか	学習結果の散らばり具合
0	0703	0111	目的関数と距離の式が違うのはなぜですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0703	0610	目的関数と距離の式が違うのはなぜですか	学習結果の散らばり具合
0	0703	1009	目的関数と距離の式が違うのはなぜですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0703	0512	目的関数と距離の式が違うのはなぜですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0703	0413	目的関数と距離の式が違うのはなぜですか	変数間の独立性を表現できること
1	0801	0801	ニューラルネットワークとはどういうものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0801	0503	ニューラルネットワークとはどういうものですか	様々な数値データに対して多く用いられる統計モデル
0	0801	0903	ニューラルネットワークとはどういうものですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0801	1402	ニューラルネットワークとはどういうものですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0801	1108	ニューラルネットワークとはどういうものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0801	0205	ニューラルネットワークとはどういうものですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0801	0808	ニューラルネットワークとはどういうものですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0801	0701	ニューラルネットワークとはどういうものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0801	0906	ニューラルネットワークとはどういうものですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0801	0612	ニューラルネットワークとはどういうものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0801	0803	ニューラルネットワークとはどういうものですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0801	1004	ニューラルネットワークとはどういうものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0801	0509	ニューラルネットワークとはどういうものですか	確率的最急勾配法
0	0801	0802	ニューラルネットワークとはどういうものですか	隠れ層
0	0801	0506	ニューラルネットワークとはどういうものですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0801	0805	ニューラルネットワークとはどういうものですか	誤差逆伝播法
0	0801	1301	ニューラルネットワークとはどういうものですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0801	0601	ニューラルネットワークとはどういうものですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0801	0209	ニューラルネットワークとはどういうものですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0801	0912	ニューラルネットワークとはどういうものですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0801	1403	ニューラルネットワークとはどういうものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0801	0717	ニューラルネットワークとはどういうものですか	Grid search
0	0801	0209	ニューラルネットワークとはどういうものですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0801	1508	ニューラルネットワークとはどういうものですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0801	0404	ニューラルネットワークとはどういうものですか	事前確率
1	0803	0803	なぜノードを階層的に組むと非線形識別面が実現できるのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0803	0206	なぜノードを階層的に組むと非線形識別面が実現できるのですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0803	1008	なぜノードを階層的に組むと非線形識別面が実現できるのですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0803	0614	なぜノードを階層的に組むと非線形識別面が実現できるのですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0803	0505	なぜノードを階層的に組むと非線形識別面が実現できるのですか	生物の神経細胞の仕組みをモデル化したもの
0	0803	1012	なぜノードを階層的に組むと非線形識別面が実現できるのですか	各データに重みを付け，そのもとで識別器を作成します
0	0803	1306	なぜノードを階層的に組むと非線形識別面が実現できるのですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0803	0307	なぜノードを階層的に組むと非線形識別面が実現できるのですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0803	1015	なぜノードを階層的に組むと非線形識別面が実現できるのですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0803	0805	なぜノードを階層的に組むと非線形識別面が実現できるのですか	誤差逆伝播法
0	0803	0109	なぜノードを階層的に組むと非線形識別面が実現できるのですか	正解が付いていない場合の学習
0	0803	0411	なぜノードを階層的に組むと非線形識別面が実現できるのですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0803	0411	なぜノードを階層的に組むと非線形識別面が実現できるのですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0803	0802	なぜノードを階層的に組むと非線形識別面が実現できるのですか	隠れ層
0	0803	0702	なぜノードを階層的に組むと非線形識別面が実現できるのですか	識別面は平面を仮定する
0	0803	0503	なぜノードを階層的に組むと非線形識別面が実現できるのですか	様々な数値データに対して多く用いられる統計モデル
0	0803	0801	なぜノードを階層的に組むと非線形識別面が実現できるのですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0803	0912	なぜノードを階層的に組むと非線形識別面が実現できるのですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0803	1505	なぜノードを階層的に組むと非線形識別面が実現できるのですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0803	0912	なぜノードを階層的に組むと非線形識別面が実現できるのですか	画像認識
0	0803	1501	なぜノードを階層的に組むと非線形識別面が実現できるのですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0803	0701	なぜノードを階層的に組むと非線形識別面が実現できるのですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0803	1411	なぜノードを階層的に組むと非線形識別面が実現できるのですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0803	1111	なぜノードを階層的に組むと非線形識別面が実現できるのですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0803	0209	なぜノードを階層的に組むと非線形識別面が実現できるのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
1	0811	0811	活性化関数にReLUを用いると誤差が失われないのはなぜですか	半分の領域で勾配が1になるので
0	0811	0115	活性化関数にReLUを用いると誤差が失われないのはなぜですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0811	1409	活性化関数にReLUを用いると誤差が失われないのはなぜですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0811	0717	活性化関数にReLUを用いると誤差が失われないのはなぜですか	連続値
0	0811	1506	活性化関数にReLUを用いると誤差が失われないのはなぜですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0811	1303	活性化関数にReLUを用いると誤差が失われないのはなぜですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0811	0906	活性化関数にReLUを用いると誤差が失われないのはなぜですか	入力に近い側の処理
0	0811	0701	活性化関数にReLUを用いると誤差が失われないのはなぜですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0811	1502	活性化関数にReLUを用いると誤差が失われないのはなぜですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0811	1001	活性化関数にReLUを用いると誤差が失われないのはなぜですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0811	0707	活性化関数にReLUを用いると誤差が失われないのはなぜですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0811	0205	活性化関数にReLUを用いると誤差が失われないのはなぜですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0811	1104	活性化関数にReLUを用いると誤差が失われないのはなぜですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0811	1106	活性化関数にReLUを用いると誤差が失われないのはなぜですか	最も遠い事例対の距離を類似度とする
0	0811	0616	活性化関数にReLUを用いると誤差が失われないのはなぜですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0811	0313	活性化関数にReLUを用いると誤差が失われないのはなぜですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0811	0906	活性化関数にReLUを用いると誤差が失われないのはなぜですか	十分多くの層を持つニューラルネットワーク
0	0811	0505	活性化関数にReLUを用いると誤差が失われないのはなぜですか	ニューラルネットワーク
0	0811	0802	活性化関数にReLUを用いると誤差が失われないのはなぜですか	入力層・出力層の数に応じた適当な数
0	0811	0912	活性化関数にReLUを用いると誤差が失われないのはなぜですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0811	0506	活性化関数にReLUを用いると誤差が失われないのはなぜですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0811	0611	活性化関数にReLUを用いると誤差が失われないのはなぜですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0811	0117	活性化関数にReLUを用いると誤差が失われないのはなぜですか	学習データの一部にだけ正解が与えられている場合
0	0811	0313	活性化関数にReLUを用いると誤差が失われないのはなぜですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0811	1209	活性化関数にReLUを用いると誤差が失われないのはなぜですか	この値が高いほど，得られる情報の多い規則であること
1	0805	0805	誤差逆伝播法とは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0805	0114	誤差逆伝播法とは何ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0805	0305	誤差逆伝播法とは何ですか	仮説に対して課す制約
0	0805	0402	誤差逆伝播法とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	0805	0406	誤差逆伝播法とは何ですか	各クラスから生じる特徴の尤もらしさを表す
0	0805	0801	誤差逆伝播法とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0805	1503	誤差逆伝播法とは何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0805	0109	誤差逆伝播法とは何ですか	正解が付いていない場合の学習
0	0805	1502	誤差逆伝播法とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0805	0614	誤差逆伝播法とは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0805	1007	誤差逆伝播法とは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0805	0906	誤差逆伝播法とは何ですか	十分多くの層を持つニューラルネットワーク
0	0805	1114	誤差逆伝播法とは何ですか	クラスタリング結果のデータ数の分布
0	0805	1309	誤差逆伝播法とは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0805	1214	誤差逆伝播法とは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0805	1409	誤差逆伝播法とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0805	1306	誤差逆伝播法とは何ですか	条件付き確率場（Conditional Random Field: CRF）
0	0805	0702	誤差逆伝播法とは何ですか	識別面は平面を仮定する
0	0805	0114	誤差逆伝播法とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0805	0514	誤差逆伝播法とは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0805	0109	誤差逆伝播法とは何ですか	正解が付いていない場合の学習
0	0805	0407	誤差逆伝播法とは何ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0805	0811	誤差逆伝播法とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	0805	1310	誤差逆伝播法とは何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0805	1306	誤差逆伝播法とは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
1	0808	0808	1つ目の式の右辺の第1項は何ですか	シグモイド関数の微分
0	0808	0811	1つ目の式の右辺の第1項は何ですか	引数が負のときは0，0以上のときはその値を出力
0	0808	1309	1つ目の式の右辺の第1項は何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0808	0105	1つ目の式の右辺の第1項は何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0808	0413	1つ目の式の右辺の第1項は何ですか	変数間の独立性を表現できること
0	0808	0614	1つ目の式の右辺の第1項は何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0808	1203	1つ目の式の右辺の第1項は何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0808	0504	1つ目の式の右辺の第1項は何ですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0808	1108	1つ目の式の右辺の第1項は何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0808	1303	1つ目の式の右辺の第1項は何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0808	1103	1つ目の式の右辺の第1項は何ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0808	1510	1つ目の式の右辺の第1項は何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0808	1106	1つ目の式の右辺の第1項は何ですか	最も遠い事例対の距離を類似度とする
0	0808	0409	1つ目の式の右辺の第1項は何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0808	0503	1つ目の式の右辺の第1項は何ですか	様々な数値データに対して多く用いられる統計モデル
0	0808	0204	1つ目の式の右辺の第1項は何ですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0808	1505	1つ目の式の右辺の第1項は何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0808	1114	1つ目の式の右辺の第1項は何ですか	クラスタリング結果のデータ数の分布から
0	0808	1408	1つ目の式の右辺の第1項は何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0808	0114	1つ目の式の右辺の第1項は何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0808	0811	1つ目の式の右辺の第1項は何ですか	ユニットの活性化関数を工夫する方法があります
0	0808	1505	1つ目の式の右辺の第1項は何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0808	0903	1つ目の式の右辺の第1項は何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0808	0113	1つ目の式の右辺の第1項は何ですか	入力データに潜む規則性
0	0808	1508	1つ目の式の右辺の第1項は何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
1	0808	0808	1つ目の式の右辺の第2項は何ですか	入力の重み付き和の微分
0	0808	0512	1つ目の式の右辺の第2項は何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0808	0802	1つ目の式の右辺の第2項は何ですか	隠れ層
0	0808	0410	1つ目の式の右辺の第2項は何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0808	0801	1つ目の式の右辺の第2項は何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0808	0402	1つ目の式の右辺の第2項は何ですか	事後確率
0	0808	0114	1つ目の式の右辺の第2項は何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0808	0805	1つ目の式の右辺の第2項は何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0808	0313	1つ目の式の右辺の第2項は何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0808	1106	1つ目の式の右辺の第2項は何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0808	1510	1つ目の式の右辺の第2項は何ですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0808	1111	1つ目の式の右辺の第2項は何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0808	1104	1つ目の式の右辺の第2項は何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0808	0701	1つ目の式の右辺の第2項は何ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0808	0611	1つ目の式の右辺の第2項は何ですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0808	0711	1つ目の式の右辺の第2項は何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0808	0701	1つ目の式の右辺の第2項は何ですか	識別境界線と最も近いデータとの距離
0	0808	1214	1つ目の式の右辺の第2項は何ですか	計算量が膨大であること
0	0808	0717	1つ目の式の右辺の第2項は何ですか	Grid search
0	0808	0704	1つ目の式の右辺の第2項は何ですか	ラグランジュの未定乗数法を不等式制約条件
0	0808	0508	1つ目の式の右辺の第2項は何ですか	最小二乗法
0	0808	1408	1つ目の式の右辺の第2項は何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0808	0908	1つ目の式の右辺の第2項は何ですか	シグモイド関数
0	0808	0402	1つ目の式の右辺の第2項は何ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0808	0110	1つ目の式の右辺の第2項は何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
1	0810	0810	勾配消失問題とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0810	1106	勾配消失問題とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0810	1509	勾配消失問題とは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0810	0911	勾配消失問題とは何ですか	ドロップアウト
0	0810	1502	勾配消失問題とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0810	1104	勾配消失問題とは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0810	1409	勾配消失問題とは何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0810	1004	勾配消失問題とは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0810	1215	勾配消失問題とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0810	1214	勾配消失問題とは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0810	0701	勾配消失問題とは何ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0810	1004	勾配消失問題とは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0810	0917	勾配消失問題とは何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0810	0711	勾配消失問題とは何ですか	カーネル関数
0	0810	1001	勾配消失問題とは何ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0810	0506	勾配消失問題とは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0810	1103	勾配消失問題とは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0810	1411	勾配消失問題とは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0810	0901	勾配消失問題とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0810	0410	勾配消失問題とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0810	0811	勾配消失問題とは何ですか	ユニットの活性化関数を工夫する方法があります
0	0810	0811	勾配消失問題とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	0810	0416	勾配消失問題とは何ですか	値が真となる確率を知りたいノードが表す変数
0	0810	0907	勾配消失問題とは何ですか	事前学習法
0	0810	1406	勾配消失問題とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
1	0901	0901	深層学習とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0901	0402	深層学習とは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0901	0211	深層学習とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0901	1010	深層学習とは何ですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0901	0404	深層学習とは何ですか	事前確率
0	0901	1214	深層学習とは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0901	0507	深層学習とは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	0901	0410	深層学習とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0901	0417	深層学習とは何ですか	ネットワークの構造とアークの条件付き確率
0	0901	1402	深層学習とは何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0901	0404	深層学習とは何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0901	0410	深層学習とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0901	0610	深層学習とは何ですか	学習結果の散らばり具合
0	0901	1001	深層学習とは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0901	0902	深層学習とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	1008	深層学習とは何ですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0901	0109	深層学習とは何ですか	学習データに正解が付いている場合の学習
0	0901	0810	深層学習とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0901	1502	深層学習とは何ですか	将棋や囲碁などを行うプログラム
0	0901	0605	深層学習とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0901	0701	深層学習とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	0801	深層学習とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0901	0810	深層学習とは何ですか	誤差が小さくなって消失してしまう
0	0901	0508	深層学習とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	0901	0614	深層学習とは何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
1	0901	0901	表現学習とは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0901	0916	表現学習とは何ですか	リカレントニューラルネットワーク
0	0901	0115	表現学習とは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0901	1203	表現学習とは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0901	0111	表現学習とは何ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0901	0701	表現学習とは何ですか	識別境界線と最も近いデータとの距離
0	0901	1508	表現学習とは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0901	0507	表現学習とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0901	0902	表現学習とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0901	0514	表現学習とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0901	0802	表現学習とは何ですか	特徴空間上では線形識別面を設定すること
0	0901	0917	表現学習とは何ですか	LSTM
0	0901	0801	表現学習とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0901	0805	表現学習とは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0901	0702	表現学習とは何ですか	識別面は平面を仮定する
0	0901	0302	表現学習とは何ですか	カテゴリ形式の正解情報のこと
0	0901	0902	表現学習とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	1015	表現学習とは何ですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0901	0717	表現学習とは何ですか	Grid search
0	0901	0715	表現学習とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0901	0109	表現学習とは何ですか	正解が付いていない場合の学習
0	0901	0104	表現学習とは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0901	0605	表現学習とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0901	1215	表現学習とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0901	0505	表現学習とは何ですか	ニューラルネットワーク
1	0902	0902	深層学習では何を特徴とするのですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0902	0114	深層学習では何を特徴とするのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0902	0916	深層学習では何を特徴とするのですか	リカレントニューラルネットワーク
0	0902	0204	深層学習では何を特徴とするのですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0902	0811	深層学習では何を特徴とするのですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0902	0711	深層学習では何を特徴とするのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0902	0906	深層学習では何を特徴とするのですか	多階層構造でもそのまま適用できます
0	0902	0712	深層学習では何を特徴とするのですか	カーネル関数が正定値関数という条件を満たすとき
0	0902	0611	深層学習では何を特徴とするのですか	識別における決定木の考え方を回帰問題に適用する方法
0	0902	0113	深層学習では何を特徴とするのですか	入力データに潜む規則性
0	0902	0912	深層学習では何を特徴とするのですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0902	0503	深層学習では何を特徴とするのですか	様々な数値データに対して多く用いられる統計モデル
0	0902	0209	深層学習では何を特徴とするのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0902	0417	深層学習では何を特徴とするのですか	ネットワークの構造とアークの条件付き確率
0	0902	1301	深層学習では何を特徴とするのですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0902	0715	深層学習では何を特徴とするのですか	カーネルトリック
0	0902	1220	深層学習では何を特徴とするのですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0902	0903	深層学習では何を特徴とするのですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0902	1215	深層学習では何を特徴とするのですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0902	0111	深層学習では何を特徴とするのですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0902	1110	深層学習では何を特徴とするのですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0902	1503	深層学習では何を特徴とするのですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0902	0302	深層学習では何を特徴とするのですか	カテゴリ形式の正解情報のこと
0	0902	0803	深層学習では何を特徴とするのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0902	1001	深層学習では何を特徴とするのですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0906	0906	多階層ニューラルネットワークとは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0906	1219	多階層ニューラルネットワークとは何ですか	どの個人がどの商品を購入したかが記録されているデータ
0	0906	1505	多階層ニューラルネットワークとは何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0906	0405	多階層ニューラルネットワークとは何ですか	尤度と事前確率の積を最大とするクラス
0	0906	0306	多階層ニューラルネットワークとは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0906	0104	多階層ニューラルネットワークとは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0906	0805	多階層ニューラルネットワークとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0906	1209	多階層ニューラルネットワークとは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0906	1214	多階層ニューラルネットワークとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0906	0409	多階層ニューラルネットワークとは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0906	0601	多階層ニューラルネットワークとは何ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0906	0810	多階層ニューラルネットワークとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0906	0610	多階層ニューラルネットワークとは何ですか	学習結果の散らばり具合
0	0906	1220	多階層ニューラルネットワークとは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0906	0601	多階層ニューラルネットワークとは何ですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0906	0307	多階層ニューラルネットワークとは何ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0906	0701	多階層ニューラルネットワークとは何ですか	線形で識別できないデータに対応するため
0	0906	1108	多階層ニューラルネットワークとは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0906	1306	多階層ニューラルネットワークとは何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0906	0710	多階層ニューラルネットワークとは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0906	0410	多階層ニューラルネットワークとは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0906	0710	多階層ニューラルネットワークとは何ですか	もとの空間におけるデータ間の距離関係を保存
0	0906	0901	多階層ニューラルネットワークとは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0906	0209	多階層ニューラルネットワークとは何ですか	正例がどれだけ正しく判定されているかという指標
0	0906	0701	多階層ニューラルネットワークとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0906	0906	誤差逆伝搬法は多階層構造でも利用できますか	多階層構造でもそのまま適用できます
0	0906	0404	誤差逆伝搬法は多階層構造でも利用できますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0906	1015	誤差逆伝搬法は多階層構造でも利用できますか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0906	0908	誤差逆伝搬法は多階層構造でも利用できますか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0906	1502	誤差逆伝搬法は多階層構造でも利用できますか	将棋や囲碁などを行うプログラム
0	0906	0204	誤差逆伝搬法は多階層構造でも利用できますか	特徴ベクトルの次元数を減らすこと
0	0906	0611	誤差逆伝搬法は多階層構造でも利用できますか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0906	0701	誤差逆伝搬法は多階層構造でも利用できますか	識別境界線と最も近いデータとの距離
0	0906	0402	誤差逆伝搬法は多階層構造でも利用できますか	入力を観測した後で計算される確率
0	0906	0114	誤差逆伝搬法は多階層構造でも利用できますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0906	1114	誤差逆伝搬法は多階層構造でも利用できますか	クラスタリング結果のデータ数の分布
0	0906	0412	誤差逆伝搬法は多階層構造でも利用できますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0906	0911	誤差逆伝搬法は多階層構造でも利用できますか	ドロップアウト
0	0906	1410	誤差逆伝搬法は多階層構造でも利用できますか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0906	1411	誤差逆伝搬法は多階層構造でも利用できますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0906	0803	誤差逆伝搬法は多階層構造でも利用できますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0906	1110	誤差逆伝搬法は多階層構造でも利用できますか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0906	0307	誤差逆伝搬法は多階層構造でも利用できますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0906	0205	誤差逆伝搬法は多階層構造でも利用できますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0906	1215	誤差逆伝搬法は多階層構造でも利用できますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0906	0505	誤差逆伝搬法は多階層構造でも利用できますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0906	0205	誤差逆伝搬法は多階層構造でも利用できますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0906	1201	誤差逆伝搬法は多階層構造でも利用できますか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0906	1506	誤差逆伝搬法は多階層構造でも利用できますか	各状態でどの行為を取ればよいのかという意思決定規則
0	0906	0717	誤差逆伝搬法は多階層構造でも利用できますか	グリッド
1	0907	0907	事前学習法とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0907	0906	事前学習法とは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0907	0205	事前学習法とは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0907	0712	事前学習法とは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0907	0713	事前学習法とは何ですか	文書分類やバイオインフォマティックスなど
0	0907	0112	事前学習法とは何ですか	線形回帰，回帰木，モデル木など
0	0907	1214	事前学習法とは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0907	1209	事前学習法とは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	0907	0402	事前学習法とは何ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0907	0803	事前学習法とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0907	1411	事前学習法とは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0907	0304	事前学習法とは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	0907	0211	事前学習法とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0907	0502	事前学習法とは何ですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0907	0810	事前学習法とは何ですか	勾配消失問題
0	0907	0701	事前学習法とは何ですか	サポートベクトルマシン
0	0907	1402	事前学習法とは何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0907	0611	事前学習法とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0907	0307	事前学習法とは何ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0907	0710	事前学習法とは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0907	1303	事前学習法とは何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0907	1301	事前学習法とは何ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0907	0902	事前学習法とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0907	0413	事前学習法とは何ですか	変数間の独立性を表現できること
0	0907	1306	事前学習法とは何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
1	0901	0901	Deep Neural Network とは何ですか	深層学習に用いるニューラルネットワーク
0	0901	0703	Deep Neural Network とは何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0901	0810	Deep Neural Network とは何ですか	誤差が小さくなって消失してしまう
0	0901	0802	Deep Neural Network とは何ですか	特徴ベクトルの次元数
0	0901	0614	Deep Neural Network とは何ですか	モデル木
0	0901	0701	Deep Neural Network とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	0811	Deep Neural Network とは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0901	0302	Deep Neural Network とは何ですか	カテゴリ形式の正解情報のこと
0	0901	0717	Deep Neural Network とは何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0901	0708	Deep Neural Network とは何ですか	制約を弱める変数
0	0901	0606	Deep Neural Network とは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0901	1501	Deep Neural Network とは何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0901	0412	Deep Neural Network とは何ですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0901	0616	Deep Neural Network とは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0901	0204	Deep Neural Network とは何ですか	特徴ベクトルの次元数を減らすこと
0	0901	0514	Deep Neural Network とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0901	0407	Deep Neural Network とは何ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0901	1310	Deep Neural Network とは何ですか	確率的非決定性オートマトンの一種
0	0901	1506	Deep Neural Network とは何ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0901	0316	Deep Neural Network とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0901	0917	Deep Neural Network とは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	0901	1502	Deep Neural Network とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0901	0701	Deep Neural Network とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	1510	Deep Neural Network とは何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0901	0805	Deep Neural Network とは何ですか	誤差逆伝播法
1	0704	0704	マージンを最大にする識別面の計算はどうするの	ラグランジュの未定乗数法を不等式制約条件
0	0704	0805	マージンを最大にする識別面の計算はどうするの	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0704	0901	マージンを最大にする識別面の計算はどうするの	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0704	0917	マージンを最大にする識別面の計算はどうするの	LSTM
0	0704	1503	マージンを最大にする識別面の計算はどうするの	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0704	1214	マージンを最大にする識別面の計算はどうするの	計算量が膨大であること
0	0704	0612	マージンを最大にする識別面の計算はどうするの	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0704	1001	マージンを最大にする識別面の計算はどうするの	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0704	1405	マージンを最大にする識別面の計算はどうするの	半教師あり学習は文書分類問題によく適用されます
0	0704	0710	マージンを最大にする識別面の計算はどうするの	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0704	0715	マージンを最大にする識別面の計算はどうするの	複雑な非線形変換を求めるという操作を避ける方法
0	0704	0810	マージンを最大にする識別面の計算はどうするの	重みの修正量が層を戻るにつれて小さくなってゆく
0	0704	1310	マージンを最大にする識別面の計算はどうするの	Hidden Marcov Model: 隠れマルコフモデル
0	0704	0906	マージンを最大にする識別面の計算はどうするの	入力に近い側の処理
0	0704	0607	マージンを最大にする識別面の計算はどうするの	Lasso回帰
0	0704	0711	マージンを最大にする識別面の計算はどうするの	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0704	1203	マージンを最大にする識別面の計算はどうするの	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0704	1209	マージンを最大にする識別面の計算はどうするの	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0704	0808	マージンを最大にする識別面の計算はどうするの	入力の重み付き和の微分
0	0704	1111	マージンを最大にする識別面の計算はどうするの	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0704	0708	マージンを最大にする識別面の計算はどうするの	制約を満たさない程度を表すので，小さい方が望ましい
0	0704	0701	マージンを最大にする識別面の計算はどうするの	識別境界線と最も近いデータとの距離
0	0704	1007	マージンを最大にする識別面の計算はどうするの	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0704	1306	マージンを最大にする識別面の計算はどうするの	条件付き確率場（Conditional Random Field: CRF）
0	0704	0405	マージンを最大にする識別面の計算はどうするの	尤度と事前確率の積を最大とするクラス
1	0707	0707	ソフトマージンによる際の識別面の設定の仕方	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0707	1008	ソフトマージンによる際の識別面の設定の仕方	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0707	0704	ソフトマージンによる際の識別面の設定の仕方	以下の関数$L$の最小値を求めるという問題
0	0707	1012	ソフトマージンによる際の識別面の設定の仕方	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0707	1114	ソフトマージンによる際の識別面の設定の仕方	クラスタリング結果のデータ数の分布から
0	0707	0503	ソフトマージンによる際の識別面の設定の仕方	様々な数値データに対して多く用いられる統計モデル
0	0707	1303	ソフトマージンによる際の識別面の設定の仕方	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0707	0409	ソフトマージンによる際の識別面の設定の仕方	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0707	0416	ソフトマージンによる際の識別面の設定の仕方	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0707	0104	ソフトマージンによる際の識別面の設定の仕方	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0707	0114	ソフトマージンによる際の識別面の設定の仕方	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0707	0916	ソフトマージンによる際の識別面の設定の仕方	リカレントニューラルネットワーク
0	0707	0606	ソフトマージンによる際の識別面の設定の仕方	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0707	0612	ソフトマージンによる際の識別面の設定の仕方	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0707	1301	ソフトマージンによる際の識別面の設定の仕方	形態素解析処理が典型的な問題
0	0707	0912	ソフトマージンによる際の識別面の設定の仕方	画像認識
0	0707	0402	ソフトマージンによる際の識別面の設定の仕方	統計的識別手法
0	0707	0114	ソフトマージンによる際の識別面の設定の仕方	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0707	1407	ソフトマージンによる際の識別面の設定の仕方	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0707	1112	ソフトマージンによる際の識別面の設定の仕方	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0707	1405	ソフトマージンによる際の識別面の設定の仕方	半教師あり学習は文書分類問題によく適用されます
0	0707	0514	ソフトマージンによる際の識別面の設定の仕方	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0707	0907	ソフトマージンによる際の識別面の設定の仕方	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0707	0117	ソフトマージンによる際の識別面の設定の仕方	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0707	0701	ソフトマージンによる際の識別面の設定の仕方	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0708	0708	スラック変数はなにをするものか	制約を弱める変数
0	0708	1007	スラック変数はなにをするものか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0708	0902	スラック変数はなにをするものか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0708	0704	スラック変数はなにをするものか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0708	1310	スラック変数はなにをするものか	確率的非決定性オートマトンの一種
0	0708	1219	スラック変数はなにをするものか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0708	0605	スラック変数はなにをするものか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0708	0206	スラック変数はなにをするものか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0708	1001	スラック変数はなにをするものか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0708	1304	スラック変数はなにをするものか	出力系列を参照する素性
0	0708	0502	スラック変数はなにをするものか	SVM
0	0708	0208	スラック変数はなにをするものか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0708	0503	スラック変数はなにをするものか	様々な数値データに対して多く用いられる統計モデル
0	0708	0610	スラック変数はなにをするものか	トレードオフの関係
0	0708	0514	スラック変数はなにをするものか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0708	0912	スラック変数はなにをするものか	畳み込みニューラルネットワーク
0	0708	0601	スラック変数はなにをするものか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0708	0417	スラック変数はなにをするものか	ネットワークの構造とアークの条件付き確率
0	0708	1406	スラック変数はなにをするものか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0708	0808	スラック変数はなにをするものか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0708	0610	スラック変数はなにをするものか	片方を減らせば片方が増える
0	0708	0509	スラック変数はなにをするものか	確率的最急勾配法
0	0708	0701	スラック変数はなにをするものか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0708	1310	スラック変数はなにをするものか	Hidden Marcov Model: 隠れマルコフモデル
0	0708	1104	スラック変数はなにをするものか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
1	0710	0710	二次元から三次元の変換・写像で気をつけることはなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0710	0711	二次元から三次元の変換・写像で気をつけることはなんですか	カーネル関数
0	0710	1304	二次元から三次元の変換・写像で気をつけることはなんですか	入力と対応させる素性
0	0710	0602	二次元から三次元の変換・写像で気をつけることはなんですか	正解情報$y$が数値であるということ
0	0710	1111	二次元から三次元の変換・写像で気をつけることはなんですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0710	1508	二次元から三次元の変換・写像で気をつけることはなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0710	0810	二次元から三次元の変換・写像で気をつけることはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0710	1407	二次元から三次元の変換・写像で気をつけることはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0710	0605	二次元から三次元の変換・写像で気をつけることはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0710	0701	二次元から三次元の変換・写像で気をつけることはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0710	1214	二次元から三次元の変換・写像で気をつけることはなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0710	0205	二次元から三次元の変換・写像で気をつけることはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0710	1313	二次元から三次元の変換・写像で気をつけることはなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0710	0803	二次元から三次元の変換・写像で気をつけることはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0710	0811	二次元から三次元の変換・写像で気をつけることはなんですか	誤差が消失しません
0	0710	1404	二次元から三次元の変換・写像で気をつけることはなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0710	1110	二次元から三次元の変換・写像で気をつけることはなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0710	0406	二次元から三次元の変換・写像で気をつけることはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0710	0115	二次元から三次元の変換・写像で気をつけることはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0710	0507	二次元から三次元の変換・写像で気をつけることはなんですか	パーセプトロンの収束定理
0	0710	1209	二次元から三次元の変換・写像で気をつけることはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0710	0717	二次元から三次元の変換・写像で気をつけることはなんですか	Grid search
0	0710	1505	二次元から三次元の変換・写像で気をつけることはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0710	0402	二次元から三次元の変換・写像で気をつけることはなんですか	事後確率
0	0710	1510	二次元から三次元の変換・写像で気をつけることはなんですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
1	0710	0710	特徴ベクトルの次元が増えるとどうなるのか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0710	0114	特徴ベクトルの次元が増えるとどうなるのか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0710	0901	特徴ベクトルの次元が増えるとどうなるのか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0710	0514	特徴ベクトルの次元が増えるとどうなるのか	ランダムに学習データを一つ
0	0710	0917	特徴ベクトルの次元が増えるとどうなるのか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0710	1104	特徴ベクトルの次元が増えるとどうなるのか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0710	0707	特徴ベクトルの次元が増えるとどうなるのか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0710	0903	特徴ベクトルの次元が増えるとどうなるのか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0710	1001	特徴ベクトルの次元が増えるとどうなるのか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0710	1114	特徴ベクトルの次元が増えるとどうなるのか	クラスタリング結果のデータ数の分布から
0	0710	0405	特徴ベクトルの次元が増えるとどうなるのか	尤度と事前確率の積を最大とするクラス
0	0710	0701	特徴ベクトルの次元が増えるとどうなるのか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0710	1301	特徴ベクトルの次元が増えるとどうなるのか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0710	0404	特徴ベクトルの次元が増えるとどうなるのか	事前確率
0	0710	0109	特徴ベクトルの次元が増えるとどうなるのか	正解が付いていない場合の学習
0	0710	0117	特徴ベクトルの次元が増えるとどうなるのか	学習データの一部にだけ正解が与えられている場合
0	0710	0907	特徴ベクトルの次元が増えるとどうなるのか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0710	0211	特徴ベクトルの次元が増えるとどうなるのか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0710	1108	特徴ベクトルの次元が増えるとどうなるのか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0710	0306	特徴ベクトルの次元が増えるとどうなるのか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0710	0916	特徴ベクトルの次元が増えるとどうなるのか	リカレントニューラルネットワーク
0	0710	0102	特徴ベクトルの次元が増えるとどうなるのか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0710	0105	特徴ベクトルの次元が増えるとどうなるのか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0710	0701	特徴ベクトルの次元が増えるとどうなるのか	サポートベクトルマシン
0	0710	0912	特徴ベクトルの次元が増えるとどうなるのか	畳み込みニューラルネットワーク
1	0701	0701	SVMの正式名はなんですか	サポートベクトルマシン
0	0701	1506	SVMの正式名はなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0701	0917	SVMの正式名はなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0701	0901	SVMの正式名はなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0701	0205	SVMの正式名はなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0701	1004	SVMの正式名はなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0701	0511	SVMの正式名はなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0701	0912	SVMの正式名はなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0701	0710	SVMの正式名はなんですか	もとの空間におけるデータ間の距離関係を保存
0	0701	0901	SVMの正式名はなんですか	音声認識・画像認識・自然言語処理など
0	0701	1412	SVMの正式名はなんですか	近くのノードは同じクラスになりやすいという仮定
0	0701	0614	SVMの正式名はなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0701	0702	SVMの正式名はなんですか	識別面は平面を仮定する
0	0701	1306	SVMの正式名はなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0701	0907	SVMの正式名はなんですか	事前学習法
0	0701	0607	SVMの正式名はなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0701	1106	SVMの正式名はなんですか	最も近い事例対の距離を類似度とする
0	0701	0703	SVMの正式名はなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0701	0907	SVMの正式名はなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0701	0514	SVMの正式名はなんですか	ランダムに学習データを一つ
0	0701	0601	SVMの正式名はなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0701	0315	SVMの正式名はなんですか	分割後のデータの分散
0	0701	0304	SVMの正式名はなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0701	1502	SVMの正式名はなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0701	1403	SVMの正式名はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
1	0715	0715	カーネルトリックとは何か	複雑な非線形変換を求めるという操作を避ける方法
0	0715	0906	カーネルトリックとは何か	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0715	0708	カーネルトリックとは何か	制約を弱める変数
0	0715	0908	カーネルトリックとは何か	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0715	0406	カーネルトリックとは何か	各クラスから生じる特徴の尤もらしさを表す
0	0715	0902	カーネルトリックとは何か	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0715	0801	カーネルトリックとは何か	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0715	1306	カーネルトリックとは何か	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0715	0810	カーネルトリックとは何か	重みの修正量が層を戻るにつれて小さくなってゆく
0	0715	0614	カーネルトリックとは何か	回帰木と線形回帰の双方のよいところを取った方法
0	0715	1412	カーネルトリックとは何か	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0715	1108	カーネルトリックとは何か	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0715	0417	カーネルトリックとは何か	ネットワークの構造とアークの条件付き確率表
0	0715	0917	カーネルトリックとは何か	LSTMセル
0	0715	0606	カーネルトリックとは何か	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0715	0701	カーネルトリックとは何か	サポートベクトルマシン
0	0715	0717	カーネルトリックとは何か	Grid search
0	0715	1106	カーネルトリックとは何か	最も遠い事例対の距離を類似度とする
0	0715	0901	カーネルトリックとは何か	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0715	0717	カーネルトリックとは何か	Grid search
0	0715	1410	カーネルトリックとは何か	学習初期の誤りに強いということ
0	0715	0901	カーネルトリックとは何か	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0715	0802	カーネルトリックとは何か	特徴空間上では線形識別面を設定すること
0	0715	0508	カーネルトリックとは何か	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0715	0917	カーネルトリックとは何か	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
1	0801	0801	ニューラルネットワークはどのようなものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0801	0908	ニューラルネットワークはどのようなものですか	ユークリッド距離
0	0801	0402	ニューラルネットワークはどのようなものですか	統計的識別手法
0	0801	1302	ニューラルネットワークはどのようなものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0801	0614	ニューラルネットワークはどのようなものですか	モデル木
0	0801	1303	ニューラルネットワークはどのようなものですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0801	0105	ニューラルネットワークはどのようなものですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0801	0406	ニューラルネットワークはどのようなものですか	各クラスから生じる特徴の尤もらしさを表す
0	0801	1219	ニューラルネットワークはどのようなものですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0801	1104	ニューラルネットワークはどのようなものですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0801	0111	ニューラルネットワークはどのようなものですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0801	0907	ニューラルネットワークはどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0801	0411	ニューラルネットワークはどのようなものですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0801	0907	ニューラルネットワークはどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0801	0416	ニューラルネットワークはどのようなものですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0801	0811	ニューラルネットワークはどのようなものですか	ReLu
0	0801	1110	ニューラルネットワークはどのようなものですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0801	1116	ニューラルネットワークはどのようなものですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0801	1010	ニューラルネットワークはどのようなものですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0801	0606	ニューラルネットワークはどのようなものですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0801	1505	ニューラルネットワークはどのようなものですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0801	0811	ニューラルネットワークはどのようなものですか	ユニットの活性化関数を工夫する方法があります
0	0801	0916	ニューラルネットワークはどのようなものですか	リカレントニューラルネットワーク
0	0801	0704	ニューラルネットワークはどのようなものですか	ラグランジュの未定乗数法
0	0801	1010	ニューラルネットワークはどのようなものですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
1	0802	0802	ニューラルネットワークのモデルとはどのようなものか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0802	0417	ニューラルネットワークのモデルとはどのようなものか	ネットワークの構造とアークの条件付き確率
0	0802	0313	ニューラルネットワークのモデルとはどのようなものか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0802	0801	ニューラルネットワークのモデルとはどのようなものか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0802	1404	ニューラルネットワークのモデルとはどのようなものか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0802	0902	ニューラルネットワークのモデルとはどのようなものか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0802	0103	ニューラルネットワークのモデルとはどのようなものか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0802	1402	ニューラルネットワークのモデルとはどのようなものか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0802	0510	ニューラルネットワークのモデルとはどのようなものか	特徴空間上でクラスを分割する面
0	0802	0901	ニューラルネットワークのモデルとはどのようなものか	深層学習に用いるニューラルネットワーク
0	0802	0912	ニューラルネットワークのモデルとはどのようなものか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0802	0508	ニューラルネットワークのモデルとはどのようなものか	二乗誤差を最小にするように識別関数を調整する方法
0	0802	0701	ニューラルネットワークのモデルとはどのようなものか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0802	0906	ニューラルネットワークのモデルとはどのようなものか	十分多くの層を持つニューラルネットワーク
0	0802	0908	ニューラルネットワークのモデルとはどのようなものか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0802	0404	ニューラルネットワークのモデルとはどのようなものか	事前確率
0	0802	0506	ニューラルネットワークのモデルとはどのようなものか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0802	1509	ニューラルネットワークのモデルとはどのようなものか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0802	0109	ニューラルネットワークのモデルとはどのようなものか	学習データに正解が付いている場合の学習
0	0802	0911	ニューラルネットワークのモデルとはどのようなものか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0802	0908	ニューラルネットワークのモデルとはどのようなものか	シグモイド関数
0	0802	1004	ニューラルネットワークのモデルとはどのようなものか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0802	0316	ニューラルネットワークのモデルとはどのようなものか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0802	0514	ニューラルネットワークのモデルとはどのようなものか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0802	0607	ニューラルネットワークのモデルとはどのようなものか	「投げ縄」という意味
1	0805	0805	誤差逆伝播法とは何か	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0805	0802	誤差逆伝播法とは何か	多層パーセプトロンあるいはニューラルネットワーク
0	0805	1306	誤差逆伝播法とは何か	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0805	0417	誤差逆伝播法とは何か	ネットワークの構造とアークの条件付き確率表
0	0805	0703	誤差逆伝播法とは何か	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0805	0715	誤差逆伝播法とは何か	複雑な非線形変換を求めるという操作を避ける方法
0	0805	0907	誤差逆伝播法とは何か	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0805	0616	誤差逆伝播法とは何か	一般に非線形式ではデータにフィットしすぎてしまうため
0	0805	0115	誤差逆伝播法とは何か	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0805	1110	誤差逆伝播法とは何か	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0805	1103	誤差逆伝播法とは何か	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0805	0111	誤差逆伝播法とは何か	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0805	0610	誤差逆伝播法とは何か	トレードオフの関係
0	0805	0505	誤差逆伝播法とは何か	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0805	1412	誤差逆伝播法とは何か	近くのノードは同じクラスになりやすいという仮定
0	0805	0708	誤差逆伝播法とは何か	制約を弱める変数
0	0805	0701	誤差逆伝播法とは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0805	1409	誤差逆伝播法とは何か	自分が出した誤りを指摘してくれる他人がいない
0	0805	0405	誤差逆伝播法とは何か	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0805	0507	誤差逆伝播法とは何か	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0805	0209	誤差逆伝播法とは何か	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0805	0715	誤差逆伝播法とは何か	複雑な非線形変換を求めるという操作を避ける方法
0	0805	0102	誤差逆伝播法とは何か	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0805	0512	誤差逆伝播法とは何か	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0805	0113	誤差逆伝播法とは何か	入力データに潜む規則性
1	0810	0810	勾配消失問題とは何か	重みの修正量が層を戻るにつれて小さくなってゆく
0	0810	0614	勾配消失問題とは何か	回帰木と線形回帰の双方のよいところを取った方法
0	0810	0211	勾配消失問題とは何か	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0810	0701	勾配消失問題とは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	1106	勾配消失問題とは何か	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0810	0305	勾配消失問題とは何か	仮説に対して課す制約
0	0810	0114	勾配消失問題とは何か	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0810	0512	勾配消失問題とは何か	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0810	0802	勾配消失問題とは何か	識別対象のクラス数
0	0810	0708	勾配消失問題とは何か	制約を満たさない程度を表すので，小さい方が望ましい
0	0810	0505	勾配消失問題とは何か	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0810	0109	勾配消失問題とは何か	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0810	0503	勾配消失問題とは何か	様々な数値データに対して多く用いられる統計モデル
0	0810	0801	勾配消失問題とは何か	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0810	1506	勾配消失問題とは何か	後に得られる報酬ほど割り引いて計算するための係数
0	0810	0114	勾配消失問題とは何か	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0810	0911	勾配消失問題とは何か	学習時の自由度を意図的に下げていること
0	0810	0114	勾配消失問題とは何か	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0810	0715	勾配消失問題とは何か	複雑な非線形変換を求めるという操作を避ける方法
0	0810	0701	勾配消失問題とは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	0912	勾配消失問題とは何か	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0810	0802	勾配消失問題とは何か	特徴空間上では線形識別面を設定すること
0	0810	1103	勾配消失問題とは何か	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0810	1116	勾配消失問題とは何か	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0810	0109	勾配消失問題とは何か	正解が付いていない場合の学習
1	0810	0810	誤差逆伝播法における問題点は何か	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0810	1001	誤差逆伝播法における問題点は何か	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0810	0911	誤差逆伝播法における問題点は何か	ドロップアウト
0	0810	0503	誤差逆伝播法における問題点は何か	様々な数値データに対して多く用いられる統計モデル
0	0810	0715	誤差逆伝播法における問題点は何か	複雑な非線形変換を求めるという操作を避ける方法
0	0810	0907	誤差逆伝播法における問題点は何か	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0810	0407	誤差逆伝播法における問題点は何か	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0810	0508	誤差逆伝播法における問題点は何か	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0810	1108	誤差逆伝播法における問題点は何か	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0810	1214	誤差逆伝播法における問題点は何か	計算量が膨大であること
0	0810	0508	誤差逆伝播法における問題点は何か	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0810	0707	誤差逆伝播法における問題点は何か	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0810	0509	誤差逆伝播法における問題点は何か	確率的最急勾配法
0	0810	1012	誤差逆伝播法における問題点は何か	すべてのデータの重みは平等
0	0810	0708	誤差逆伝播法における問題点は何か	制約を弱める変数
0	0810	1209	誤差逆伝播法における問題点は何か	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0810	0510	誤差逆伝播法における問題点は何か	特徴空間上でクラスを分割する面
0	0810	0103	誤差逆伝播法における問題点は何か	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0810	0105	誤差逆伝播法における問題点は何か	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0810	0315	誤差逆伝播法における問題点は何か	分割後のデータの分散
0	0810	1508	誤差逆伝播法における問題点は何か	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0810	0902	誤差逆伝播法における問題点は何か	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0810	0701	誤差逆伝播法における問題点は何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	0402	誤差逆伝播法における問題点は何か	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0810	0402	誤差逆伝播法における問題点は何か	事後確率が最大となるクラスを識別結果とする方法
1	0810	0810	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0810	1407	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0810	0901	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0810	1205	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0810	0402	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	最大事後確率則
0	0810	0917	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0810	0102	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	現在，人が行っている知的な判断を代わりに行う技術
0	0810	0506	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0810	1201	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0810	1406	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0810	0313	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0810	1106	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	クラスタの重心間の距離を類似度とする
0	0810	0701	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	識別境界線と最も近いデータとの距離
0	0810	0505	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	パーセプトロン
0	0810	0109	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	正解が付いていない場合の学習
0	0810	0311	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	集合の乱雑さ
0	0810	1219	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	どの個人がどの商品を購入したかが記録されているデータ
0	0810	0610	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	真のモデルとの距離
0	0810	1106	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	クラスタの重心間の距離を類似度とする
0	0810	0611	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	識別における決定木の考え方を回帰問題に適用する方法
0	0810	0503	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	様々な数値データに対して多く用いられる統計モデル
0	0810	0803	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	非線形識別面
0	0810	1409	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0810	0701	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	1304	誤差逆伝播法による多階層ネットワークの学習は何故難しいのか	出力系列を参照する素性
1	0906	0906	多階層ニューラルネットワークとは何か	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0906	0614	多階層ニューラルネットワークとは何か	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0906	0513	多階層ニューラルネットワークとは何か	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0906	0302	多階層ニューラルネットワークとは何か	カテゴリ形式の正解情報のこと
0	0906	0112	多階層ニューラルネットワークとは何か	線形回帰，回帰木，モデル木など
0	0906	0508	多階層ニューラルネットワークとは何か	最小二乗法
0	0906	1409	多階層ニューラルネットワークとは何か	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0906	0701	多階層ニューラルネットワークとは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0906	0810	多階層ニューラルネットワークとは何か	重みの修正量が層を戻るにつれて小さくなってゆく
0	0906	0111	多階層ニューラルネットワークとは何か	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0906	1201	多階層ニューラルネットワークとは何か	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0906	1220	多階層ニューラルネットワークとは何か	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0906	0410	多階層ニューラルネットワークとは何か	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0906	0612	多階層ニューラルネットワークとは何か	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0906	0102	多階層ニューラルネットワークとは何か	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0906	0717	多階層ニューラルネットワークとは何か	Grid search
0	0906	0606	多階層ニューラルネットワークとは何か	入力が少し変化したときに，出力も少し変化する
0	0906	0411	多階層ニューラルネットワークとは何か	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0906	1201	多階層ニューラルネットワークとは何か	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0906	1106	多階層ニューラルネットワークとは何か	クラスタの重心間の距離を類似度とする
0	0906	1510	多階層ニューラルネットワークとは何か	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0906	0109	多階層ニューラルネットワークとは何か	正解が付いていない場合の学習
0	0906	0507	多階層ニューラルネットワークとは何か	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0906	1012	多階層ニューラルネットワークとは何か	すべてのデータの重みは平等
0	0906	0601	多階層ニューラルネットワークとは何か	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
1	0906	0906	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0906	0502	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	SVM
0	0906	0511	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0906	0717	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	Grid search
0	0906	0410	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0906	0715	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	カーネルトリック
0	0906	0416	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0906	1114	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	クラスタリング結果のデータ数の分布から
0	0906	0715	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	カーネルトリック
0	0906	1214	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0906	0612	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0906	0805	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	誤差逆伝播法
0	0906	0702	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	識別面は平面を仮定する
0	0906	0711	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0906	0901	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0906	1404	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0906	0416	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	アークを無向とみなした結合を考えたとき
0	0906	0907	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0906	0802	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	多層パーセプトロンあるいはニューラルネットワーク
0	0906	0402	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	事後確率
0	0906	0105	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0906	0104	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0906	1103	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0906	0505	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	生物の神経細胞の仕組みをモデル化したもの
0	0906	1214	多階層ニューラルネットワークにおける誤差逆伝播法の重みの修正の仕様はどのようなものか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
1	0901	0901	深層学習を言い換えると何になりますか	表現学習
0	0901	0701	深層学習を言い換えると何になりますか	線形で識別できないデータに対応するため
0	0901	0610	深層学習を言い換えると何になりますか	片方を減らせば片方が増える
0	0901	0702	深層学習を言い換えると何になりますか	識別面は平面を仮定する
0	0901	1503	深層学習を言い換えると何になりますか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0901	1112	深層学習を言い換えると何になりますか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0901	0209	深層学習を言い換えると何になりますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0901	1007	深層学習を言い換えると何になりますか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0901	1407	深層学習を言い換えると何になりますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0901	0701	深層学習を言い換えると何になりますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	1501	深層学習を言い換えると何になりますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0901	1402	深層学習を言い換えると何になりますか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0901	0114	深層学習を言い換えると何になりますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0901	0114	深層学習を言い換えると何になりますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0901	0514	深層学習を言い換えると何になりますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0901	1508	深層学習を言い換えると何になりますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0901	0510	深層学習を言い換えると何になりますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0901	1001	深層学習を言い換えると何になりますか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0901	0611	深層学習を言い換えると何になりますか	識別における決定木の考え方を回帰問題に適用する方法
0	0901	1306	深層学習を言い換えると何になりますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0901	0601	深層学習を言い換えると何になりますか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0901	1406	深層学習を言い換えると何になりますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0901	0601	深層学習を言い換えると何になりますか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0901	1304	深層学習を言い換えると何になりますか	入力と対応させる素性
0	0901	0205	深層学習を言い換えると何になりますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
1	0902	0902	深層学習におけるこれまでの識別問題との差はなにか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0902	1007	深層学習におけるこれまでの識別問題との差はなにか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0902	0917	深層学習におけるこれまでの識別問題との差はなにか	入力ゲート・出力ゲート・忘却ゲート
0	0902	0715	深層学習におけるこれまでの識別問題との差はなにか	複雑な非線形変換を求めるという操作を避ける方法
0	0902	0402	深層学習におけるこれまでの識別問題との差はなにか	事後確率
0	0902	1306	深層学習におけるこれまでの識別問題との差はなにか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0902	0514	深層学習におけるこれまでの識別問題との差はなにか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0902	1010	深層学習におけるこれまでの識別問題との差はなにか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0902	0701	深層学習におけるこれまでの識別問題との差はなにか	識別境界線と最も近いデータとの距離
0	0902	0502	深層学習におけるこれまでの識別問題との差はなにか	SVM
0	0902	0102	深層学習におけるこれまでの識別問題との差はなにか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0902	0402	深層学習におけるこれまでの識別問題との差はなにか	事後確率が最大となるクラスを識別結果とする方法
0	0902	1203	深層学習におけるこれまでの識別問題との差はなにか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0902	0707	深層学習におけるこれまでの識別問題との差はなにか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0902	0307	深層学習におけるこれまでの識別問題との差はなにか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0902	1209	深層学習におけるこれまでの識別問題との差はなにか	規則の条件部が起こったときに結論部が起こる割合
0	0902	1507	深層学習におけるこれまでの識別問題との差はなにか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0902	0606	深層学習におけるこれまでの識別問題との差はなにか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0902	0402	深層学習におけるこれまでの識別問題との差はなにか	事後確率が最大となるクラスを識別結果とする方法
0	0902	0508	深層学習におけるこれまでの識別問題との差はなにか	二乗誤差を最小にするように識別関数を調整する方法
0	0902	0105	深層学習におけるこれまでの識別問題との差はなにか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0902	0805	深層学習におけるこれまでの識別問題との差はなにか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0902	0811	深層学習におけるこれまでの識別問題との差はなにか	ユニットの活性化関数を工夫する方法があります
0	0902	0805	深層学習におけるこれまでの識別問題との差はなにか	誤差逆伝播法
0	0902	0701	深層学習におけるこれまでの識別問題との差はなにか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0906	0906	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0906	0802	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	特徴ベクトルの次元数
0	0906	0708	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	制約を弱める変数
0	0906	0901	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0906	1001	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0906	0711	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	カーネル関数
0	0906	0902	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0906	0105	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0906	0508	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	二乗誤差を最小にするように識別関数を調整する方法
0	0906	0402	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	統計的識別手法
0	0906	0911	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0906	1404	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0906	0316	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0906	0802	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	識別対象のクラス数
0	0906	0410	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0906	0802	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	多層パーセプトロンあるいはニューラルネットワーク
0	0906	0805	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0906	0614	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	回帰木と線形回帰の双方のよいところを取った方法
0	0906	0602	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	正解情報$y$が数値であるということ
0	0906	0505	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0906	0204	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0906	0411	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	確率のm推定という考え方を用います
0	0906	0313	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0906	0811	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	ユニットの活性化関数を工夫する方法
0	0906	0715	多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか	複雑な非線形変換を求めるという操作を避ける方法
1	0906	0906	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0906	1114	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	クラスタリング結果のデータ数の分布から
0	0906	1209	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	規則の条件部が起こったときに結論部が起こる割合
0	0906	0810	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	勾配消失問題
0	0906	0901	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	深層学習に用いるニューラルネットワーク
0	0906	0402	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0906	1214	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0906	0117	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	学習データの一部にだけ正解が与えられている場合
0	0906	0411	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0906	0512	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0906	1106	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0906	1104	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0906	0105	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0906	1219	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	どの個人がどの商品を購入したかが記録されているデータ
0	0906	0614	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	回帰木と線形回帰の双方のよいところを取った方法
0	0906	0717	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	Grid search
0	0906	0114	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0906	0614	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	線形回帰式
0	0906	1412	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	近くのノードは同じクラスになりやすいという仮定
0	0906	0711	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0906	0114	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0906	0112	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	線形回帰，回帰木，モデル木など
0	0906	1410	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	学習初期の誤りに強いということ
0	0906	0811	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	引数が負のときは0，0以上のときはその値を出力
0	0906	0109	多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
1	0906	0906	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0906	0402	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0906	0413	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	変数間の独立性を表現できること
0	0906	0805	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0906	0610	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	片方を減らせば片方が増える
0	0906	0411	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	確率のm推定という考え方を用います
0	0906	0410	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0906	0109	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	正解が付いていない場合の学習
0	0906	0810	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	勾配消失問題
0	0906	0306	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0906	1301	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	形態素解析処理が典型的な問題
0	0906	0901	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0906	0305	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0906	0707	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0906	0402	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	事後確率が最大となるクラスを識別結果とする方法
0	0906	0507	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	全ての誤りがなくなることが学習の終了条件なので
0	0906	1110	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0906	0209	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0906	0606	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	入力が少し変化したときに，出力も少し変化する
0	0906	0907	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0906	0611	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	識別における決定木の考え方を回帰問題に適用する方法
0	0906	0701	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0906	0508	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0906	0402	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	統計的識別手法
0	0906	0114	多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	0908	0908	オートエンコーダとは何か	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0908	0701	オートエンコーダとは何か	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0908	0901	オートエンコーダとは何か	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0908	0115	オートエンコーダとは何か	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0908	0614	オートエンコーダとは何か	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0908	0902	オートエンコーダとは何か	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0908	0717	オートエンコーダとは何か	Grid search
0	0908	0502	オートエンコーダとは何か	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0908	1009	オートエンコーダとは何か	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0908	0114	オートエンコーダとは何か	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0908	0606	オートエンコーダとは何か	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0908	0412	オートエンコーダとは何か	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0908	1410	オートエンコーダとは何か	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0908	0114	オートエンコーダとは何か	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0908	1507	オートエンコーダとは何か	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0908	1409	オートエンコーダとは何か	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0908	0502	オートエンコーダとは何か	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0908	0115	オートエンコーダとは何か	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0908	0402	オートエンコーダとは何か	統計的識別手法
0	0908	1214	オートエンコーダとは何か	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0908	1219	オートエンコーダとは何か	どの個人がどの商品を購入したかが記録されているデータ
0	0908	1209	オートエンコーダとは何か	この値が高いほど，得られる情報の多い規則であること
0	0908	1506	オートエンコーダとは何か	政策
0	0908	1108	オートエンコーダとは何か	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0908	0110	オートエンコーダとは何か	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
1	0912	0912	畳み込みニューラルネットワークとは何か	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0912	0810	畳み込みニューラルネットワークとは何か	重みの修正量が層を戻るにつれて小さくなってゆく
0	0912	0715	畳み込みニューラルネットワークとは何か	識別面
0	0912	1302	畳み込みニューラルネットワークとは何か	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0912	0803	畳み込みニューラルネットワークとは何か	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0912	0917	畳み込みニューラルネットワークとは何か	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0912	0810	畳み込みニューラルネットワークとは何か	重みの修正量が層を戻るにつれて小さくなってゆく
0	0912	1502	畳み込みニューラルネットワークとは何か	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0912	1306	畳み込みニューラルネットワークとは何か	条件付き確率場（Conditional Random Field: CRF）
0	0912	0507	畳み込みニューラルネットワークとは何か	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0912	0803	畳み込みニューラルネットワークとは何か	非線形識別面
0	0912	0614	畳み込みニューラルネットワークとは何か	回帰木と線形回帰の双方のよいところを取った方法
0	0912	0708	畳み込みニューラルネットワークとは何か	制約を弱める変数
0	0912	1106	畳み込みニューラルネットワークとは何か	最も近い事例対の距離を類似度とする
0	0912	1306	畳み込みニューラルネットワークとは何か	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0912	0503	畳み込みニューラルネットワークとは何か	様々な数値データに対して多く用いられる統計モデル
0	0912	1409	畳み込みニューラルネットワークとは何か	自分が出した誤りを指摘してくれる他人がいない
0	0912	0906	畳み込みニューラルネットワークとは何か	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0912	0112	畳み込みニューラルネットワークとは何か	線形回帰，回帰木，モデル木など
0	0912	0505	畳み込みニューラルネットワークとは何か	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0912	0111	畳み込みニューラルネットワークとは何か	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0912	0113	畳み込みニューラルネットワークとは何か	入力データに潜む規則性を学習すること
0	0912	1411	畳み込みニューラルネットワークとは何か	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0912	0802	畳み込みニューラルネットワークとは何か	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0912	0204	畳み込みニューラルネットワークとは何か	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
1	0916	0916	リカレントニューラルネットワークとは何か	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0916	0616	リカレントニューラルネットワークとは何か	一般に非線形式ではデータにフィットしすぎてしまうため
0	0916	0505	リカレントニューラルネットワークとは何か	生物の神経細胞の仕組みをモデル化したもの
0	0916	1506	リカレントニューラルネットワークとは何か	その政策に従って行動したときの累積報酬の期待値で評価
0	0916	1310	リカレントニューラルネットワークとは何か	Hidden Marcov Model: 隠れマルコフモデル
0	0916	0111	リカレントニューラルネットワークとは何か	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0916	0102	リカレントニューラルネットワークとは何か	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0916	0512	リカレントニューラルネットワークとは何か	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0916	1502	リカレントニューラルネットワークとは何か	将棋や囲碁などを行うプログラム
0	0916	1012	リカレントニューラルネットワークとは何か	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0916	0211	リカレントニューラルネットワークとは何か	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0916	0701	リカレントニューラルネットワークとは何か	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0916	1209	リカレントニューラルネットワークとは何か	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0916	0902	リカレントニューラルネットワークとは何か	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0916	0912	リカレントニューラルネットワークとは何か	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0916	0109	リカレントニューラルネットワークとは何か	正解が付いていない場合の学習
0	0916	1112	リカレントニューラルネットワークとは何か	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0916	0802	リカレントニューラルネットワークとは何か	隠れ層
0	0916	0313	リカレントニューラルネットワークとは何か	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0916	1502	リカレントニューラルネットワークとは何か	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0916	1301	リカレントニューラルネットワークとは何か	ひとまとまりの系列データを特定のクラスに識別する問題
0	0916	0911	リカレントニューラルネットワークとは何か	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0916	0114	リカレントニューラルネットワークとは何か	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0916	1302	リカレントニューラルネットワークとは何か	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0916	0109	リカレントニューラルネットワークとは何か	学習データに正解が付いている場合の学習
1	0917	0917	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0917	1110	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0917	0114	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0917	0508	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	二乗誤差を最小にするように識別関数を調整する方法
0	0917	0509	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	確率的最急勾配法
0	0917	1108	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0917	0514	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0917	1205	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0917	0701	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	線形で識別できないデータに対応するため
0	0917	0701	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0917	0616	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0917	0811	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	誤差が消失しません
0	0917	0711	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	カーネル関数
0	0917	0906	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0917	0811	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	引数が負のときは0，0以上のときはその値を出力
0	0917	1313	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0917	0306	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0917	0802	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	入力層・出力層の数に応じた適当な数
0	0917	0612	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0917	0510	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0917	0402	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0917	1304	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	出力系列を参照する素性
0	0917	0908	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	ユークリッド距離
0	0917	0402	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	統計的識別手法
0	0917	0701	リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
1	0701	0701	マージンとは何か	識別境界線と最も近いデータとの距離
0	0701	0610	マージンとは何か	学習結果の散らばり具合
0	0701	0205	マージンとは何か	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0701	1502	マージンとは何か	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0701	0114	マージンとは何か	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0701	0404	マージンとは何か	事前確率
0	0701	1501	マージンとは何か	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0701	0810	マージンとは何か	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0701	0116	マージンとは何か	学習データが教師あり／教師なしの混在となっているもの
0	0701	1301	マージンとは何か	個々の要素の間に i.i.d. の関係が成立しないもの
0	0701	0908	マージンとは何か	ユークリッド距離
0	0701	0407	マージンとは何か	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0701	0611	マージンとは何か	識別における決定木の考え方を回帰問題に適用する方法
0	0701	1110	マージンとは何か	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0701	0917	マージンとは何か	LSTM
0	0701	1406	マージンとは何か	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0701	1506	マージンとは何か	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0701	1106	マージンとは何か	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0701	0505	マージンとは何か	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0701	1106	マージンとは何か	最も近い事例対の距離を類似度とする
0	0701	0906	マージンとは何か	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0701	0104	マージンとは何か	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0701	1505	マージンとは何か	「マルコフ性」を持つ確率過程における意思決定問題
0	0701	0508	マージンとは何か	最小二乗法
0	0701	1301	マージンとは何か	形態素解析処理が典型的な問題
1	0701	0701	マージンが広いとどうなるのか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0701	1009	マージンが広いとどうなるのか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0701	1506	マージンが広いとどうなるのか	後に得られる報酬ほど割り引いて計算するための係数
0	0701	0805	マージンが広いとどうなるのか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0701	0102	マージンが広いとどうなるのか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0701	0610	マージンが広いとどうなるのか	真のモデルとの距離
0	0701	1404	マージンが広いとどうなるのか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0701	0209	マージンが広いとどうなるのか	正例がどれだけ正しく判定されているかという指標
0	0701	0614	マージンが広いとどうなるのか	モデル木
0	0701	0305	マージンが広いとどうなるのか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0701	0114	マージンが広いとどうなるのか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0701	0802	マージンが広いとどうなるのか	特徴ベクトルの次元数
0	0701	0614	マージンが広いとどうなるのか	回帰木と線形回帰の双方のよいところを取った方法
0	0701	0306	マージンが広いとどうなるのか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0701	0711	マージンが広いとどうなるのか	カーネル関数
0	0701	1116	マージンが広いとどうなるのか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0701	1219	マージンが広いとどうなるのか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0701	1502	マージンが広いとどうなるのか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0701	1219	マージンが広いとどうなるのか	どの個人がどの商品を購入したかが記録されているデータ
0	0701	0316	マージンが広いとどうなるのか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0701	1402	マージンが広いとどうなるのか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0701	1220	マージンが広いとどうなるのか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0701	1103	マージンが広いとどうなるのか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0701	0917	マージンが広いとどうなるのか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0701	0110	マージンが広いとどうなるのか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
1	0701	0701	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	サポートベクトルマシン
0	0701	1001	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0701	0315	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	分割後のデータの分散
0	0701	0702	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	識別面は平面を仮定する
0	0701	1201	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0701	1214	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0701	0109	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	正解が付いていない場合の学習
0	0701	0801	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0701	0115	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0701	0901	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0701	1303	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0701	0502	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0701	1106	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0701	0916	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	リカレントニューラルネットワーク
0	0701	0410	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0701	0504	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0701	0713	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	文書分類やバイオインフォマティックスなど
0	0701	0508	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0701	1309	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0701	0810	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0701	0901	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	深層学習に用いるニューラルネットワーク
0	0701	0803	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0701	0801	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0701	0114	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0701	0411	学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法はなにか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
1	0710	0710	特徴次元が多いとどうなるのか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0710	0901	特徴次元が多いとどうなるのか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0710	1012	特徴次元が多いとどうなるのか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0710	0512	特徴次元が多いとどうなるのか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0710	0511	特徴次元が多いとどうなるのか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0710	0917	特徴次元が多いとどうなるのか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0710	1407	特徴次元が多いとどうなるのか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0710	0306	特徴次元が多いとどうなるのか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0710	1012	特徴次元が多いとどうなるのか	すべてのデータの重みは平等
0	0710	0611	特徴次元が多いとどうなるのか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0710	0901	特徴次元が多いとどうなるのか	深層学習に用いるニューラルネットワーク
0	0710	0204	特徴次元が多いとどうなるのか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0710	0701	特徴次元が多いとどうなるのか	サポートベクトルマシン
0	0710	0803	特徴次元が多いとどうなるのか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0710	0117	特徴次元が多いとどうなるのか	学習データの一部にだけ正解が与えられている場合
0	0710	1103	特徴次元が多いとどうなるのか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0710	0715	特徴次元が多いとどうなるのか	複雑な非線形変換を求めるという操作を避ける方法
0	0710	0901	特徴次元が多いとどうなるのか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0710	0514	特徴次元が多いとどうなるのか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0710	0205	特徴次元が多いとどうなるのか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0710	0803	特徴次元が多いとどうなるのか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0710	1506	特徴次元が多いとどうなるのか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0710	0402	特徴次元が多いとどうなるのか	統計的識別手法
0	0710	1001	特徴次元が多いとどうなるのか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0710	0805	特徴次元が多いとどうなるのか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
1	0802	0802	中間層の別名は何か	隠れ層
0	0802	0110	中間層の別名は何か	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0802	0612	中間層の別名は何か	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0802	0116	中間層の別名は何か	学習データが教師あり／教師なしの混在となっているもの
0	0802	0811	中間層の別名は何か	引数が負のときは0，0以上のときはその値を出力
0	0802	0708	中間層の別名は何か	制約を弱める変数
0	0802	1508	中間層の別名は何か	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0802	1111	中間層の別名は何か	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0802	0811	中間層の別名は何か	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0802	0606	中間層の別名は何か	山の尾根という意味
0	0802	0801	中間層の別名は何か	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0802	0610	中間層の別名は何か	トレードオフの関係
0	0802	0717	中間層の別名は何か	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0802	1406	中間層の別名は何か	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0802	0906	中間層の別名は何か	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0802	0514	中間層の別名は何か	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0802	0307	中間層の別名は何か	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0802	1411	中間層の別名は何か	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0802	1301	中間層の別名は何か	動画像の分類や音声で入力された単語の識別などの問題
0	0802	0512	中間層の別名は何か	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0802	1407	中間層の別名は何か	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0802	1506	中間層の別名は何か	後に得られる報酬ほど割り引いて計算するための係数
0	0802	0306	中間層の別名は何か	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0802	0505	中間層の別名は何か	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0802	0912	中間層の別名は何か	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
1	0402	0402	統計的識別とはなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0402	1509	統計的識別とはなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0402	0906	統計的識別とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0402	0708	統計的識別とはなんですか	制約を弱める変数
0	0402	0803	統計的識別とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0402	0111	統計的識別とはなんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0402	0405	統計的識別とはなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0402	0509	統計的識別とはなんですか	確率的最急勾配法
0	0402	0715	統計的識別とはなんですか	カーネルトリック
0	0402	0901	統計的識別とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0701	統計的識別とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0402	0204	統計的識別とはなんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0402	0811	統計的識別とはなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0402	1506	統計的識別とはなんですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0402	0801	統計的識別とはなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0402	0304	統計的識別とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0402	0115	統計的識別とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0402	1110	統計的識別とはなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0402	0616	統計的識別とはなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0402	0411	統計的識別とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0402	0901	統計的識別とはなんですか	Deep Neural Network (DNN) 
0	0402	1411	統計的識別とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0402	0411	統計的識別とはなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0402	0903	統計的識別とはなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0402	0916	統計的識別とはなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
1	0404	0404	事前確率とはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0404	1215	事前確率とはなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0404	0505	事前確率とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0404	0116	事前確率とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0404	0614	事前確率とはなんですか	モデル木
0	0404	0505	事前確率とはなんですか	ニューラルネットワーク
0	0404	1114	事前確率とはなんですか	クラスタリング結果のデータ数の分布から
0	0404	0513	事前確率とはなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0404	0508	事前確率とはなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0404	0507	事前確率とはなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0404	0712	事前確率とはなんですか	カーネル関数が正定値関数という条件を満たすとき
0	0404	0205	事前確率とはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0404	1012	事前確率とはなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0404	0506	事前確率とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0404	1510	事前確率とはなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0404	1501	事前確率とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0404	1309	事前確率とはなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0404	1506	事前確率とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0404	1302	事前確率とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0404	0701	事前確率とはなんですか	線形で識別できないデータに対応するため
0	0404	0605	事前確率とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0404	1106	事前確率とはなんですか	最も遠い事例対の距離を類似度とする
0	0404	1301	事前確率とはなんですか	連続音声認識
0	0404	0808	事前確率とはなんですか	シグモイド関数の微分
0	0404	1506	事前確率とはなんですか	後に得られる報酬ほど割り引いて計算するための係数
1	0402	0402	事後確率とはなんですか	入力を観測した後で計算される確率
0	0402	1015	事後確率とはなんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0402	0507	事後確率とはなんですか	パーセプトロンの収束定理
0	0402	0901	事後確率とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0402	0708	事後確率とはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0402	0614	事後確率とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0402	1110	事後確率とはなんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0402	1303	事後確率とはなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0402	1508	事後確率とはなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0402	0506	事後確率とはなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0402	0114	事後確率とはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0402	1406	事後確率とはなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0402	0616	事後確率とはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0402	0803	事後確率とはなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0402	1411	事後確率とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0402	0104	事後確率とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0402	0116	事後確率とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0402	0116	事後確率とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0402	0912	事後確率とはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0402	0901	事後確率とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0901	事後確率とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0810	事後確率とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0402	1407	事後確率とはなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0402	0610	事後確率とはなんですか	トレードオフの関係
0	0402	0907	事後確率とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
1	0402	0402	事後確率とは何ですか	入力を観測した後で計算される確率
0	0402	1302	事後確率とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0402	1304	事後確率とは何ですか	出力系列を参照する素性
0	0402	0801	事後確率とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0402	0802	事後確率とは何ですか	入力層・出力層の数に応じた適当な数
0	0402	0508	事後確率とは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0402	0610	事後確率とは何ですか	真のモデルとの距離
0	0402	1220	事後確率とは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0402	0916	事後確率とは何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0402	0416	事後確率とは何ですか	アークを無向とみなした結合を考えたとき
0	0402	0912	事後確率とは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0402	0701	事後確率とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0402	0209	事後確率とは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0402	0413	事後確率とは何ですか	変数間の独立性を表現できること
0	0402	1404	事後確率とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0402	0206	事後確率とは何ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0402	0611	事後確率とは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0402	0912	事後確率とは何ですか	畳み込みニューラルネットワーク
0	0402	1405	事後確率とは何ですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0402	1301	事後確率とは何ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0402	0606	事後確率とは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0402	0302	事後確率とは何ですか	カテゴリ形式の正解情報のこと
0	0402	0417	事後確率とは何ですか	ネットワークの構造とアークの条件付き確率
0	0402	0306	事後確率とは何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0402	1111	事後確率とは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
1	0402	0402	統計的識別ってどうやってやるんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0402	0503	統計的識別ってどうやってやるんですか	様々な数値データに対して多く用いられる統計モデル
0	0402	1108	統計的識別ってどうやってやるんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0402	0204	統計的識別ってどうやってやるんですか	特徴ベクトルの次元数を減らすこと
0	0402	1215	統計的識別ってどうやってやるんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0402	0916	統計的識別ってどうやってやるんですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0402	1503	統計的識別ってどうやってやるんですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0402	0316	統計的識別ってどうやってやるんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0402	0701	統計的識別ってどうやってやるんですか	サポートベクトルマシン
0	0402	0416	統計的識別ってどうやってやるんですか	値が真となる確率を知りたいノードが表す変数
0	0402	1104	統計的識別ってどうやってやるんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0402	0805	統計的識別ってどうやってやるんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0402	0508	統計的識別ってどうやってやるんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0402	0204	統計的識別ってどうやってやるんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0402	0102	統計的識別ってどうやってやるんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0402	0702	統計的識別ってどうやってやるんですか	識別面は平面を仮定する
0	0402	1303	統計的識別ってどうやってやるんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0402	0104	統計的識別ってどうやってやるんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0402	1209	統計的識別ってどうやってやるんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0402	0811	統計的識別ってどうやってやるんですか	ReLu
0	0402	0704	統計的識別ってどうやってやるんですか	ラグランジュの未定乗数法
0	0402	1508	統計的識別ってどうやってやるんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0402	0505	統計的識別ってどうやってやるんですか	生物の神経細胞の仕組みをモデル化したもの
0	0402	0903	統計的識別ってどうやってやるんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0402	0607	統計的識別ってどうやってやるんですか	Lasso回帰
1	0402	0402	最大事後確率則ってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0402	0907	最大事後確率則ってなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0402	1201	最大事後確率則ってなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0402	0616	最大事後確率則ってなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0402	0413	最大事後確率則ってなんですか	変数間の独立性を表現できること
0	0402	0612	最大事後確率則ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0402	0710	最大事後確率則ってなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0402	0113	最大事後確率則ってなんですか	入力データに潜む規則性を学習すること
0	0402	1106	最大事後確率則ってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0402	0514	最大事後確率則ってなんですか	ランダムに学習データを一つ
0	0402	1201	最大事後確率則ってなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0402	0701	最大事後確率則ってなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0402	0708	最大事後確率則ってなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0402	0601	最大事後確率則ってなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0402	0906	最大事後確率則ってなんですか	十分多くの層を持つニューラルネットワーク
0	0402	0901	最大事後確率則ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0803	最大事後確率則ってなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0402	0208	最大事後確率則ってなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0402	0911	最大事後確率則ってなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0402	0808	最大事後確率則ってなんですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0402	1001	最大事後確率則ってなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0402	1301	最大事後確率則ってなんですか	動画像の分類や音声で入力された単語の識別などの問題
0	0402	1306	最大事後確率則ってなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0402	0113	最大事後確率則ってなんですか	入力データに潜む規則性
0	0402	0304	最大事後確率則ってなんですか	個々の事例から，あるクラスについて共通点を見つけること
1	0402	0402	条件付き確率ってどうやって求めるんですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0402	0506	条件付き確率ってどうやって求めるんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0402	0803	条件付き確率ってどうやって求めるんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0402	1506	条件付き確率ってどうやって求めるんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0402	1112	条件付き確率ってどうやって求めるんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0402	0116	条件付き確率ってどうやって求めるんですか	学習データが教師あり／教師なしの混在となっているもの
0	0402	0802	条件付き確率ってどうやって求めるんですか	識別対象のクラス数
0	0402	0305	条件付き確率ってどうやって求めるんですか	仮説に対して課す制約
0	0402	0805	条件付き確率ってどうやって求めるんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0402	1012	条件付き確率ってどうやって求めるんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0402	1501	条件付き確率ってどうやって求めるんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0402	0209	条件付き確率ってどうやって求めるんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0402	0901	条件付き確率ってどうやって求めるんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0702	条件付き確率ってどうやって求めるんですか	識別面は平面を仮定する
0	0402	0901	条件付き確率ってどうやって求めるんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0402	0306	条件付き確率ってどうやって求めるんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0402	0313	条件付き確率ってどうやって求めるんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0402	1303	条件付き確率ってどうやって求めるんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0402	1219	条件付き確率ってどうやって求めるんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0402	0701	条件付き確率ってどうやって求めるんですか	サポートベクトルマシン
0	0402	0509	条件付き確率ってどうやって求めるんですか	確率的最急勾配法
0	0402	0610	条件付き確率ってどうやって求めるんですか	トレードオフの関係
0	0402	0715	条件付き確率ってどうやって求めるんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0402	0412	条件付き確率ってどうやって求めるんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0402	1209	条件付き確率ってどうやって求めるんですか	この値が高いほど，得られる情報の多い規則であること
1	0405	0405	事後確率が最大になるクラスはどうやって得られますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0405	0917	事後確率が最大になるクラスはどうやって得られますか	LSTM
0	0405	1301	事後確率が最大になるクラスはどうやって得られますか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0405	1407	事後確率が最大になるクラスはどうやって得られますか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0405	0411	事後確率が最大になるクラスはどうやって得られますか	確率のm推定という考え方を用います
0	0405	0616	事後確率が最大になるクラスはどうやって得られますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0405	0206	事後確率が最大になるクラスはどうやって得られますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0405	0114	事後確率が最大になるクラスはどうやって得られますか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0405	0901	事後確率が最大になるクラスはどうやって得られますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0405	0115	事後確率が最大になるクラスはどうやって得られますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0405	1505	事後確率が最大になるクラスはどうやって得られますか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0405	1106	事後確率が最大になるクラスはどうやって得られますか	最も近い事例対の距離を類似度とする
0	0405	1403	事後確率が最大になるクラスはどうやって得られますか	多次元でも「次元の呪い」にかかっていない，ということ
0	0405	0316	事後確率が最大になるクラスはどうやって得られますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0405	0204	事後確率が最大になるクラスはどうやって得られますか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0405	0917	事後確率が最大になるクラスはどうやって得られますか	LSTMセル
0	0405	1510	事後確率が最大になるクラスはどうやって得られますか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0405	0307	事後確率が最大になるクラスはどうやって得られますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0405	0507	事後確率が最大になるクラスはどうやって得られますか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0405	0906	事後確率が最大になるクラスはどうやって得られますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0405	0409	事後確率が最大になるクラスはどうやって得られますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0405	0610	事後確率が最大になるクラスはどうやって得られますか	学習結果の散らばり具合
0	0405	0811	事後確率が最大になるクラスはどうやって得られますか	ユニットの活性化関数を工夫する方法があります
0	0405	0916	事後確率が最大になるクラスはどうやって得られますか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0405	1306	事後確率が最大になるクラスはどうやって得られますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
1	1001	1001	アンサンブル学習とは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1001	0906	アンサンブル学習とは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1001	0901	アンサンブル学習とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1001	0211	アンサンブル学習とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1001	1505	アンサンブル学習とは何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1001	0316	アンサンブル学習とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1001	1304	アンサンブル学習とは何ですか	入力と対応させる素性
0	1001	0209	アンサンブル学習とは何ですか	正例がどれだけ正しく判定されているかという指標
0	1001	0717	アンサンブル学習とは何ですか	Grid search
0	1001	1111	アンサンブル学習とは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1001	0701	アンサンブル学習とは何ですか	サポートベクトルマシン
0	1001	0605	アンサンブル学習とは何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1001	1309	アンサンブル学習とは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1001	1106	アンサンブル学習とは何ですか	最も近い事例対の距離を類似度とする
0	1001	0503	アンサンブル学習とは何ですか	様々な数値データに対して多く用いられる統計モデル
0	1001	1409	アンサンブル学習とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1001	0701	アンサンブル学習とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1001	0911	アンサンブル学習とは何ですか	ドロップアウト
0	1001	0602	アンサンブル学習とは何ですか	数値型の正解情報のこと
0	1001	1104	アンサンブル学習とは何ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1001	1007	アンサンブル学習とは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1001	0906	アンサンブル学習とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1001	0607	アンサンブル学習とは何ですか	Lasso回帰
0	1001	1215	アンサンブル学習とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1001	1303	アンサンブル学習とは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
1	1001	1001	誤りが独立であるとは何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1001	1209	誤りが独立であるとは何ですか	この値が高いほど，得られる情報の多い規則であること
0	1001	0717	誤りが独立であるとは何ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	1001	0606	誤りが独立であるとは何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	1001	0717	誤りが独立であるとは何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1001	0911	誤りが独立であるとは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1001	1310	誤りが独立であるとは何ですか	確率的非決定性オートマトンの一種
0	1001	0311	誤りが独立であるとは何ですか	集合の乱雑さ
0	1001	0115	誤りが独立であるとは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1001	1502	誤りが独立であるとは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1001	1410	誤りが独立であるとは何ですか	学習初期の誤りに強いということ
0	1001	0810	誤りが独立であるとは何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1001	0502	誤りが独立であるとは何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1001	0802	誤りが独立であるとは何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1001	1303	誤りが独立であるとは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1001	0810	誤りが独立であるとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1001	1112	誤りが独立であるとは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1001	0302	誤りが独立であるとは何ですか	カテゴリ形式の正解情報のこと
0	1001	0211	誤りが独立であるとは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1001	0901	誤りが独立であるとは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1001	1106	誤りが独立であるとは何ですか	最も近い事例対の距離を類似度とする
0	1001	0701	誤りが独立であるとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1001	1219	誤りが独立であるとは何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1001	0416	誤りが独立であるとは何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	1001	0105	誤りが独立であるとは何ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
1	1001	1001	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	1001	0109	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	学習データに正解が付いている場合の学習
0	1001	1303	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1001	0802	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	識別対象のクラス数
0	1001	0510	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1001	0702	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	識別面は平面を仮定する
0	1001	0504	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1001	0906	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1001	0803	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1001	1505	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1001	0114	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1001	1009	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	1001	0316	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1001	0715	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	複雑な非線形変換を求めるという操作を避ける方法
0	1001	1108	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1001	0802	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	特徴ベクトルの次元数
0	1001	0810	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1001	0103	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	1001	0508	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	二乗誤差を最小にするように識別関数を調整する方法
0	1001	0906	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	多階層構造でもそのまま適用できます
0	1001	1310	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	確率的非決定性オートマトンの一種
0	1001	0906	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1001	0715	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	複雑な非線形変換を求めるという操作を避ける方法
0	1001	1301	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1001	0112	なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか	線形回帰，回帰木，モデル木など
1	1010	1010	ブースティングとはなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1010	0803	ブースティングとはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1010	0204	ブースティングとはなんですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	1010	0302	ブースティングとはなんですか	カテゴリ形式の正解情報のこと
0	1010	0114	ブースティングとはなんですか	階層的クラスタリングや k-means 法
0	1010	0115	ブースティングとはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1010	0505	ブースティングとはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	1010	1106	ブースティングとはなんですか	クラスタの重心間の距離を類似度とする
0	1010	0801	ブースティングとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1010	0713	ブースティングとはなんですか	文書分類やバイオインフォマティックスなど
0	1010	0404	ブースティングとはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1010	1104	ブースティングとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1010	0412	ブースティングとはなんですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	1010	0912	ブースティングとはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1010	0114	ブースティングとはなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1010	1110	ブースティングとはなんですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	1010	0611	ブースティングとはなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1010	1008	ブースティングとはなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1010	0917	ブースティングとはなんですか	LSTM
0	1010	0307	ブースティングとはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	1010	1402	ブースティングとはなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1010	0601	ブースティングとはなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	1010	0109	ブースティングとはなんですか	正解が付いていない場合の学習
0	1010	0916	ブースティングとはなんですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	1010	1303	ブースティングとはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
1	0104	0104	深層学習が他の機械学習手法と異なる点はなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0104	0411	深層学習が他の機械学習手法と異なる点はなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0104	1112	深層学習が他の機械学習手法と異なる点はなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0104	0711	深層学習が他の機械学習手法と異なる点はなんですか	カーネル関数
0	0104	0508	深層学習が他の機械学習手法と異なる点はなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0104	0612	深層学習が他の機械学習手法と異なる点はなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0104	0906	深層学習が他の機械学習手法と異なる点はなんですか	多階層構造でもそのまま適用できます
0	0104	0802	深層学習が他の機械学習手法と異なる点はなんですか	特徴ベクトルの次元数
0	0104	0710	深層学習が他の機械学習手法と異なる点はなんですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0104	1303	深層学習が他の機械学習手法と異なる点はなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0104	0912	深層学習が他の機械学習手法と異なる点はなんですか	畳み込みニューラルネットワーク
0	0104	1219	深層学習が他の機械学習手法と異なる点はなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0104	0306	深層学習が他の機械学習手法と異なる点はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0104	1201	深層学習が他の機械学習手法と異なる点はなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0104	0513	深層学習が他の機械学習手法と異なる点はなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0104	1403	深層学習が他の機械学習手法と異なる点はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0104	0805	深層学習が他の機械学習手法と異なる点はなんですか	誤差逆伝播法
0	0104	1302	深層学習が他の機械学習手法と異なる点はなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0104	0810	深層学習が他の機械学習手法と異なる点はなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0104	0610	深層学習が他の機械学習手法と異なる点はなんですか	片方を減らせば片方が増える
0	0104	0204	深層学習が他の機械学習手法と異なる点はなんですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0104	0708	深層学習が他の機械学習手法と異なる点はなんですか	制約を弱める変数
0	0104	1506	深層学習が他の機械学習手法と異なる点はなんですか	最適政策$\pi^*$を獲得すること
0	0104	0803	深層学習が他の機械学習手法と異なる点はなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0104	0606	深層学習が他の機械学習手法と異なる点はなんですか	入力が少し変化したときに，出力も少し変化する
1	0104	0104	深層学習と機械学習の違いはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0104	0514	深層学習と機械学習の違いはなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0104	1405	深層学習と機械学習の違いはなんですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0104	0109	深層学習と機械学習の違いはなんですか	正解が付いていない場合の学習
0	0104	0902	深層学習と機械学習の違いはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0104	0902	深層学習と機械学習の違いはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0104	1001	深層学習と機械学習の違いはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0104	0810	深層学習と機械学習の違いはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0104	0113	深層学習と機械学習の違いはなんですか	入力データに潜む規則性
0	0104	0114	深層学習と機械学習の違いはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0104	0304	深層学習と機械学習の違いはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0104	1004	深層学習と機械学習の違いはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0104	1401	深層学習と機械学習の違いはなんですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0104	1303	深層学習と機械学習の違いはなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0104	0907	深層学習と機械学習の違いはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0104	0102	深層学習と機械学習の違いはなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0104	1310	深層学習と機械学習の違いはなんですか	確率的非決定性オートマトンの一種
0	0104	1106	深層学習と機械学習の違いはなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0104	1215	深層学習と機械学習の違いはなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0104	0512	深層学習と機械学習の違いはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0104	0505	深層学習と機械学習の違いはなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0104	1108	深層学習と機械学習の違いはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0104	0916	深層学習と機械学習の違いはなんですか	リカレントニューラルネットワーク
0	0104	1214	深層学習と機械学習の違いはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0104	0111	深層学習と機械学習の違いはなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
1	0104	0104	深層学習が得意な問題はなんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0104	0616	深層学習が得意な問題はなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0104	0507	深層学習が得意な問題はなんですか	パーセプトロンの収束定理
0	0104	1306	深層学習が得意な問題はなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0104	0802	深層学習が得意な問題はなんですか	識別対象のクラス数
0	0104	0115	深層学習が得意な問題はなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0104	0502	深層学習が得意な問題はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0104	0204	深層学習が得意な問題はなんですか	特徴ベクトルの次元数を減らすこと
0	0104	0902	深層学習が得意な問題はなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0104	1505	深層学習が得意な問題はなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0104	0811	深層学習が得意な問題はなんですか	ReLu
0	0104	0810	深層学習が得意な問題はなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0104	1001	深層学習が得意な問題はなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0104	0514	深層学習が得意な問題はなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0104	1015	深層学習が得意な問題はなんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0104	1104	深層学習が得意な問題はなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0104	1001	深層学習が得意な問題はなんですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0104	0906	深層学習が得意な問題はなんですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0104	0509	深層学習が得意な問題はなんですか	確率的最急勾配法
0	0104	0811	深層学習が得意な問題はなんですか	引数が負のときは0，0以上のときはその値を出力
0	0104	0906	深層学習が得意な問題はなんですか	十分多くの層
0	0104	1209	深層学習が得意な問題はなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0104	0711	深層学習が得意な問題はなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0104	0313	深層学習が得意な問題はなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0104	0711	深層学習が得意な問題はなんですか	カーネル関数
1	0105	0105	パターン認識とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0105	0504	パターン認識とはなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0105	1111	パターン認識とはなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0105	1106	パターン認識とはなんですか	クラスタの重心間の距離を類似度とする
0	0105	0715	パターン認識とはなんですか	カーネルトリック
0	0105	0204	パターン認識とはなんですか	特徴ベクトルの次元数を減らすこと
0	0105	0711	パターン認識とはなんですか	カーネル関数
0	0105	0701	パターン認識とはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0105	0114	パターン認識とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0105	0901	パターン認識とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0105	0509	パターン認識とはなんですか	確率的最急勾配法
0	0105	0710	パターン認識とはなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0105	0402	パターン認識とはなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0105	0114	パターン認識とはなんですか	階層的クラスタリングや k-means 法
0	0105	1103	パターン認識とはなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0105	0906	パターン認識とはなんですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0105	0701	パターン認識とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0105	1103	パターン認識とはなんですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0105	0104	パターン認識とはなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0105	0906	パターン認識とはなんですか	十分多くの層
0	0105	0901	パターン認識とはなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0105	0110	パターン認識とはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0105	0313	パターン認識とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0105	1406	パターン認識とはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0105	0306	パターン認識とはなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
1	1012	1012	バギングでは、個々のデータに対してどのように重みを設定しますか	すべてのデータの重みは平等
0	1012	0802	バギングでは、個々のデータに対してどのように重みを設定しますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1012	0305	バギングでは、個々のデータに対してどのように重みを設定しますか	仮説に対して課す制約
0	1012	1301	バギングでは、個々のデータに対してどのように重みを設定しますか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1012	0614	バギングでは、個々のデータに対してどのように重みを設定しますか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1012	0115	バギングでは、個々のデータに対してどのように重みを設定しますか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1012	0508	バギングでは、個々のデータに対してどのように重みを設定しますか	最小二乗法
0	1012	0103	バギングでは、個々のデータに対してどのように重みを設定しますか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	1012	1408	バギングでは、個々のデータに対してどのように重みを設定しますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1012	0607	バギングでは、個々のデータに対してどのように重みを設定しますか	Lasso回帰
0	1012	1201	バギングでは、個々のデータに対してどのように重みを設定しますか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	1012	0916	バギングでは、個々のデータに対してどのように重みを設定しますか	リカレントニューラルネットワーク
0	1012	1106	バギングでは、個々のデータに対してどのように重みを設定しますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1012	0316	バギングでは、個々のデータに対してどのように重みを設定しますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1012	1214	バギングでは、個々のデータに対してどのように重みを設定しますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1012	0715	バギングでは、個々のデータに対してどのように重みを設定しますか	複雑な非線形変換を求めるという操作を避ける方法
0	1012	0810	バギングでは、個々のデータに対してどのように重みを設定しますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1012	0209	バギングでは、個々のデータに対してどのように重みを設定しますか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1012	0901	バギングでは、個々のデータに対してどのように重みを設定しますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1012	0114	バギングでは、個々のデータに対してどのように重みを設定しますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1012	0109	バギングでは、個々のデータに対してどのように重みを設定しますか	正解が付いていない場合の学習
0	1012	1402	バギングでは、個々のデータに対してどのように重みを設定しますか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1012	0717	バギングでは、個々のデータに対してどのように重みを設定しますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1012	1501	バギングでは、個々のデータに対してどのように重みを設定しますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1012	0514	バギングでは、個々のデータに対してどのように重みを設定しますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
1	0105	0105	特徴抽出とはなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0105	0411	特徴抽出とはなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0105	0901	特徴抽出とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0105	0115	特徴抽出とはなんですか	パターンマイニング
0	0105	0313	特徴抽出とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0105	0616	特徴抽出とはなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0105	1008	特徴抽出とはなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0105	0417	特徴抽出とはなんですか	ネットワークの構造とアークの条件付き確率
0	0105	1510	特徴抽出とはなんですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0105	1505	特徴抽出とはなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0105	0917	特徴抽出とはなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0105	0906	特徴抽出とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0105	1301	特徴抽出とはなんですか	動画像の分類や音声で入力された単語の識別などの問題
0	0105	0311	特徴抽出とはなんですか	集合の乱雑さ
0	0105	0204	特徴抽出とはなんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0105	1106	特徴抽出とはなんですか	クラスタの重心間の距離を類似度とする
0	0105	0607	特徴抽出とはなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0105	1209	特徴抽出とはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0105	1502	特徴抽出とはなんですか	将棋や囲碁などを行うプログラム
0	0105	0701	特徴抽出とはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0105	1220	特徴抽出とはなんですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0105	0611	特徴抽出とはなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0105	0702	特徴抽出とはなんですか	識別面は平面を仮定する
0	0105	1110	特徴抽出とはなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0105	1310	特徴抽出とはなんですか	確率的非決定性オートマトンの一種
1	1012	1012	ブースティングでは、どのように識別器を作成するのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1012	0402	ブースティングでは、どのように識別器を作成するのですか	入力を観測した後で計算される確率
0	1012	0704	ブースティングでは、どのように識別器を作成するのですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1012	0315	ブースティングでは、どのように識別器を作成するのですか	分割後のデータの分散
0	1012	0514	ブースティングでは、どのように識別器を作成するのですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	1012	0311	ブースティングでは、どのように識別器を作成するのですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1012	1007	ブースティングでは、どのように識別器を作成するのですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1012	1506	ブースティングでは、どのように識別器を作成するのですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1012	0204	ブースティングでは、どのように識別器を作成するのですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	1012	0712	ブースティングでは、どのように識別器を作成するのですか	カーネル関数が正定値関数という条件を満たすとき
0	1012	0612	ブースティングでは、どのように識別器を作成するのですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	1012	1502	ブースティングでは、どのように識別器を作成するのですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1012	0811	ブースティングでは、どのように識別器を作成するのですか	ReLu
0	1012	0505	ブースティングでは、どのように識別器を作成するのですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1012	0614	ブースティングでは、どのように識別器を作成するのですか	回帰木と線形回帰の双方のよいところを取った方法
0	1012	1403	ブースティングでは、どのように識別器を作成するのですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1012	1015	ブースティングでは、どのように識別器を作成するのですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	1012	0701	ブースティングでは、どのように識別器を作成するのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1012	0708	ブースティングでは、どのように識別器を作成するのですか	制約を弱める変数
0	1012	0313	ブースティングでは、どのように識別器を作成するのですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1012	0911	ブースティングでは、どのように識別器を作成するのですか	ランダムに一定割合のユニットを消して学習を行う
0	1012	0917	ブースティングでは、どのように識別器を作成するのですか	LSTMセル
0	1012	1301	ブースティングでは、どのように識別器を作成するのですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1012	1507	ブースティングでは、どのように識別器を作成するのですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1012	0606	ブースティングでは、どのように識別器を作成するのですか	入力が少し変化したときに，出力も少し変化する
1	0115	0115	パターンマイニングとはどういう方法ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0115	0803	パターンマイニングとはどういう方法ですか	非線形識別面
0	0115	1012	パターンマイニングとはどういう方法ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0115	0712	パターンマイニングとはどういう方法ですか	カーネル関数が正定値関数という条件を満たすとき
0	0115	0906	パターンマイニングとはどういう方法ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0115	0505	パターンマイニングとはどういう方法ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0115	0413	パターンマイニングとはどういう方法ですか	変数間の独立性を表現できること
0	0115	0605	パターンマイニングとはどういう方法ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0115	0111	パターンマイニングとはどういう方法ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0115	0802	パターンマイニングとはどういう方法ですか	特徴ベクトルの次元数
0	0115	0513	パターンマイニングとはどういう方法ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0115	0901	パターンマイニングとはどういう方法ですか	音声認識・画像認識・自然言語処理など
0	0115	0209	パターンマイニングとはどういう方法ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0115	0810	パターンマイニングとはどういう方法ですか	誤差が小さくなって消失してしまう
0	0115	1104	パターンマイニングとはどういう方法ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0115	1009	パターンマイニングとはどういう方法ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0115	0109	パターンマイニングとはどういう方法ですか	学習データに正解が付いている場合の学習
0	0115	1306	パターンマイニングとはどういう方法ですか	条件付き確率場（Conditional Random Field: CRF）
0	0115	0610	パターンマイニングとはどういう方法ですか	片方を減らせば片方が増える
0	0115	1209	パターンマイニングとはどういう方法ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0115	0811	パターンマイニングとはどういう方法ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0115	0505	パターンマイニングとはどういう方法ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0115	0702	パターンマイニングとはどういう方法ですか	識別面は平面を仮定する
0	0115	0313	パターンマイニングとはどういう方法ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0115	0916	パターンマイニングとはどういう方法ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
1	0111	0111	識別問題にはどんな種類がありますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0111	0109	識別問題にはどんな種類がありますか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0111	0901	識別問題にはどんな種類がありますか	表現学習
0	0111	0911	識別問題にはどんな種類がありますか	ランダムに一定割合のユニットを消して学習を行う
0	0111	1310	識別問題にはどんな種類がありますか	Hidden Marcov Model: 隠れマルコフモデル
0	0111	0406	識別問題にはどんな種類がありますか	各クラスから生じる特徴の尤もらしさを表す
0	0111	0911	識別問題にはどんな種類がありますか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0111	0701	識別問題にはどんな種類がありますか	識別境界線と最も近いデータとの距離
0	0111	0304	識別問題にはどんな種類がありますか	個々の事例から，あるクラスについて共通点を見つけること
0	0111	1116	識別問題にはどんな種類がありますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0111	0805	識別問題にはどんな種類がありますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0111	0802	識別問題にはどんな種類がありますか	識別対象のクラス数
0	0111	0316	識別問題にはどんな種類がありますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0111	0810	識別問題にはどんな種類がありますか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0111	0903	識別問題にはどんな種類がありますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0111	1408	識別問題にはどんな種類がありますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0111	1501	識別問題にはどんな種類がありますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0111	0407	識別問題にはどんな種類がありますか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0111	0402	識別問題にはどんな種類がありますか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0111	0109	識別問題にはどんな種類がありますか	学習データに正解が付いている場合の学習
0	0111	0810	識別問題にはどんな種類がありますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0111	0610	識別問題にはどんな種類がありますか	学習結果の散らばり具合
0	0111	0701	識別問題にはどんな種類がありますか	サポートベクトルマシン
0	0111	1301	識別問題にはどんな種類がありますか	動画像の分類や音声で入力された単語の識別などの問題
0	0111	0715	識別問題にはどんな種類がありますか	複雑な非線形変換を求めるという操作を避ける方法
1	1012	1012	各識別器の結果は、どうするのですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	1012	1110	各識別器の結果は、どうするのですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1012	1505	各識別器の結果は、どうするのですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1012	0707	各識別器の結果は、どうするのですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1012	0402	各識別器の結果は、どうするのですか	事後確率
0	1012	0103	各識別器の結果は、どうするのですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	1012	1015	各識別器の結果は、どうするのですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	1012	0701	各識別器の結果は、どうするのですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	1012	0907	各識別器の結果は、どうするのですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1012	1015	各識別器の結果は、どうするのですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1012	1402	各識別器の結果は、どうするのですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1012	1110	各識別器の結果は、どうするのですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	1012	0906	各識別器の結果は、どうするのですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1012	0610	各識別器の結果は、どうするのですか	真のモデルとの距離
0	1012	0111	各識別器の結果は、どうするのですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	1012	1110	各識別器の結果は、どうするのですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	1012	1406	各識別器の結果は、どうするのですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1012	1106	各識別器の結果は、どうするのですか	最も近い事例対の距離を類似度とする
0	1012	1510	各識別器の結果は、どうするのですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	1012	0611	各識別器の結果は、どうするのですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1012	0412	各識別器の結果は、どうするのですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	1012	0811	各識別器の結果は、どうするのですか	ユニットの活性化関数を工夫する方法があります
0	1012	1410	各識別器の結果は、どうするのですか	学習初期の誤りに強いということ
0	1012	0701	各識別器の結果は、どうするのですか	マージン
0	1012	0102	各識別器の結果は、どうするのですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
1	0111	0111	2値分類とはなんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0111	1501	2値分類とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0111	0204	2値分類とはなんですか	特徴ベクトルの次元数を減らすこと
0	0111	0605	2値分類とはなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0111	0715	2値分類とはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0111	1502	2値分類とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0111	0611	2値分類とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0111	0313	2値分類とはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0111	1301	2値分類とはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0111	0810	2値分類とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0111	1502	2値分類とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0111	0115	2値分類とはなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0111	0713	2値分類とはなんですか	文書分類やバイオインフォマティックスなど
0	0111	1114	2値分類とはなんですか	クラスタリング結果のデータ数の分布
0	0111	0917	2値分類とはなんですか	LSTMセル
0	0111	0505	2値分類とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0111	0902	2値分類とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0111	0315	2値分類とはなんですか	分割後のデータの分散
0	0111	0402	2値分類とはなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0111	0512	2値分類とはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0111	0704	2値分類とはなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0111	0402	2値分類とはなんですか	事後確率
0	0111	0811	2値分類とはなんですか	引数が負のときは0，0以上のときはその値を出力
0	0111	0512	2値分類とはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0111	0109	2値分類とはなんですか	正解が付いていない場合の学習
1	0111	0111	識別の代表的な手法には何がありますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0111	0115	識別の代表的な手法には何がありますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0111	0912	識別の代表的な手法には何がありますか	畳み込みニューラルネットワーク
0	0111	0402	識別の代表的な手法には何がありますか	最大事後確率則
0	0111	0912	識別の代表的な手法には何がありますか	畳み込みニューラルネットワーク
0	0111	0508	識別の代表的な手法には何がありますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0111	0109	識別の代表的な手法には何がありますか	学習データに正解が付いている場合の学習
0	0111	0701	識別の代表的な手法には何がありますか	サポートベクトルマシン
0	0111	0717	識別の代表的な手法には何がありますか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0111	0701	識別の代表的な手法には何がありますか	識別境界線と最も近いデータとの距離
0	0111	0104	識別の代表的な手法には何がありますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0111	0711	識別の代表的な手法には何がありますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0111	1220	識別の代表的な手法には何がありますか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0111	0711	識別の代表的な手法には何がありますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0111	1201	識別の代表的な手法には何がありますか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0111	1502	識別の代表的な手法には何がありますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0111	0502	識別の代表的な手法には何がありますか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0111	1304	識別の代表的な手法には何がありますか	入力と対応させる素性
0	0111	0701	識別の代表的な手法には何がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0111	1506	識別の代表的な手法には何がありますか	政策
0	0111	0506	識別の代表的な手法には何がありますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0111	1104	識別の代表的な手法には何がありますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0111	1219	識別の代表的な手法には何がありますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0111	0911	識別の代表的な手法には何がありますか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0111	0109	識別の代表的な手法には何がありますか	正解が付いていない場合の学習
1	0112	0112	回帰の代表的な手法には何がありますか	線形回帰，回帰木，モデル木など
0	0112	0802	回帰の代表的な手法には何がありますか	特徴空間上では線形識別面を設定すること
0	0112	0906	回帰の代表的な手法には何がありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0112	1201	回帰の代表的な手法には何がありますか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0112	0105	回帰の代表的な手法には何がありますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0112	0616	回帰の代表的な手法には何がありますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0112	0514	回帰の代表的な手法には何がありますか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0112	0717	回帰の代表的な手法には何がありますか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0112	0701	回帰の代表的な手法には何がありますか	サポートベクトルマシン
0	0112	0704	回帰の代表的な手法には何がありますか	ラグランジュの未定乗数法
0	0112	1509	回帰の代表的な手法には何がありますか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0112	0416	回帰の代表的な手法には何がありますか	値が真となる確率を知りたいノードが表す変数
0	0112	0802	回帰の代表的な手法には何がありますか	特徴ベクトルの次元数
0	0112	0717	回帰の代表的な手法には何がありますか	グリッド
0	0112	0406	回帰の代表的な手法には何がありますか	各クラスから生じる特徴の尤もらしさを表す
0	0112	0410	回帰の代表的な手法には何がありますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0112	1015	回帰の代表的な手法には何がありますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0112	0109	回帰の代表的な手法には何がありますか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0112	0513	回帰の代表的な手法には何がありますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0112	0601	回帰の代表的な手法には何がありますか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0112	0208	回帰の代表的な手法には何がありますか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0112	0906	回帰の代表的な手法には何がありますか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0112	0901	回帰の代表的な手法には何がありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0112	0507	回帰の代表的な手法には何がありますか	パーセプトロンの収束定理
0	0112	0917	回帰の代表的な手法には何がありますか	入力ゲート・出力ゲート・忘却ゲート
1	0114	0114	モデル推定ってなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0114	0416	モデル推定ってなんですか	アークを無向とみなした結合を考えたとき
0	0114	0402	モデル推定ってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0114	0602	モデル推定ってなんですか	数値型の正解情報のこと
0	0114	0105	モデル推定ってなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0114	1306	モデル推定ってなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0114	1201	モデル推定ってなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0114	1403	モデル推定ってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0114	1409	モデル推定ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0114	0413	モデル推定ってなんですか	変数間の独立性を表現できること
0	0114	0109	モデル推定ってなんですか	正解が付いていない場合の学習
0	0114	0901	モデル推定ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0114	1407	モデル推定ってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0114	0707	モデル推定ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0114	0402	モデル推定ってなんですか	入力を観測した後で計算される確率
0	0114	0502	モデル推定ってなんですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0114	1310	モデル推定ってなんですか	確率的非決定性オートマトンの一種
0	0114	1302	モデル推定ってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0114	1304	モデル推定ってなんですか	出力系列を参照する素性
0	0114	1110	モデル推定ってなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0114	1103	モデル推定ってなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0114	1502	モデル推定ってなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0114	1214	モデル推定ってなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0114	1106	モデル推定ってなんですか	最も遠い事例対の距離を類似度とする
0	0114	0810	モデル推定ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
1	1015	1015	勾配ブースティングで、どのような損失関数が使われますか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	1015	0708	勾配ブースティングで、どのような損失関数が使われますか	制約を弱める変数
0	1015	0701	勾配ブースティングで、どのような損失関数が使われますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1015	0110	勾配ブースティングで、どのような損失関数が使われますか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1015	0607	勾配ブースティングで、どのような損失関数が使われますか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1015	1501	勾配ブースティングで、どのような損失関数が使われますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1015	0114	勾配ブースティングで、どのような損失関数が使われますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1015	0311	勾配ブースティングで、どのような損失関数が使われますか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1015	1008	勾配ブースティングで、どのような損失関数が使われますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1015	0810	勾配ブースティングで、どのような損失関数が使われますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1015	0715	勾配ブースティングで、どのような損失関数が使われますか	識別面
0	1015	1106	勾配ブースティングで、どのような損失関数が使われますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1015	0103	勾配ブースティングで、どのような損失関数が使われますか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	1015	0701	勾配ブースティングで、どのような損失関数が使われますか	識別境界線と最も近いデータとの距離
0	1015	0105	勾配ブースティングで、どのような損失関数が使われますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1015	0713	勾配ブースティングで、どのような損失関数が使われますか	文書分類やバイオインフォマティックスなど
0	1015	0612	勾配ブースティングで、どのような損失関数が使われますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1015	1412	勾配ブースティングで、どのような損失関数が使われますか	近くのノードは同じクラスになりやすいという仮定
0	1015	0704	勾配ブースティングで、どのような損失関数が使われますか	ラグランジュの未定乗数法を不等式制約条件
0	1015	1010	勾配ブースティングで、どのような損失関数が使われますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1015	0505	勾配ブースティングで、どのような損失関数が使われますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1015	1406	勾配ブースティングで、どのような損失関数が使われますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1015	0305	勾配ブースティングで、どのような損失関数が使われますか	仮説に対して課す制約
0	1015	1410	勾配ブースティングで、どのような損失関数が使われますか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1015	1111	勾配ブースティングで、どのような損失関数が使われますか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
1	0114	0114	クラスタリングって何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0114	0611	クラスタリングって何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0114	0315	クラスタリングって何ですか	分割後のデータの分散
0	0114	0906	クラスタリングって何ですか	入力に近い側の処理
0	0114	0610	クラスタリングって何ですか	トレードオフの関係
0	0114	0417	クラスタリングって何ですか	ネットワークの構造とアークの条件付き確率
0	0114	0514	クラスタリングって何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0114	0906	クラスタリングって何ですか	十分多くの層を持つニューラルネットワーク
0	0114	0907	クラスタリングって何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0114	0209	クラスタリングって何ですか	正例がどれだけ正しく判定されているかという指標
0	0114	0906	クラスタリングって何ですか	多階層構造でもそのまま適用できます
0	0114	0908	クラスタリングって何ですか	ユークリッド距離
0	0114	1510	クラスタリングって何ですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0114	0103	クラスタリングって何ですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0114	0409	クラスタリングって何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0114	0505	クラスタリングって何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0114	1409	クラスタリングって何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0114	0417	クラスタリングって何ですか	ネットワークの構造とアークの条件付き確率表
0	0114	0611	クラスタリングって何ですか	回帰
0	0114	0912	クラスタリングって何ですか	畳み込みニューラルネットワーク
0	0114	0606	クラスタリングって何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0114	0907	クラスタリングって何ですか	事前学習法
0	0114	0802	クラスタリングって何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0114	0311	クラスタリングって何ですか	集合の乱雑さ
0	0114	1306	クラスタリングって何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
1	0114	0114	密度推定とはなんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0114	0610	密度推定とはなんですか	真のモデルとの距離
0	0114	1302	密度推定とはなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0114	0906	密度推定とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0114	1402	密度推定とはなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0114	0810	密度推定とはなんですか	勾配消失問題
0	0114	1103	密度推定とはなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0114	1411	密度推定とはなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0114	1219	密度推定とはなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0114	1506	密度推定とはなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0114	1220	密度推定とはなんですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0114	1301	密度推定とはなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0114	0508	密度推定とはなんですか	最小二乗法
0	0114	0411	密度推定とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0114	0416	密度推定とはなんですか	値が真となる確率を知りたいノードが表す変数
0	0114	1110	密度推定とはなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0114	0614	密度推定とはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0114	0710	密度推定とはなんですか	低次元の特徴ベクトルを高次元に写像
0	0114	0601	密度推定とはなんですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0114	1219	密度推定とはなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0114	0803	密度推定とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0114	0708	密度推定とはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0114	0311	密度推定とはなんですか	集合の乱雑さ
0	0114	0802	密度推定とはなんですか	識別対象のクラス数
0	0114	0302	密度推定とはなんですか	カテゴリ形式の正解情報のこと
1	0109	0109	教師なし学習とはどういう学習法ですか	正解が付いていない場合の学習
0	0109	0901	教師なし学習とはどういう学習法ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0109	1303	教師なし学習とはどういう学習法ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0109	0710	教師なし学習とはどういう学習法ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0109	0311	教師なし学習とはどういう学習法ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0109	0906	教師なし学習とはどういう学習法ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0109	0505	教師なし学習とはどういう学習法ですか	生物の神経細胞の仕組みをモデル化したもの
0	0109	0509	教師なし学習とはどういう学習法ですか	確率的最急勾配法
0	0109	0708	教師なし学習とはどういう学習法ですか	制約を弱める変数
0	0109	1220	教師なし学習とはどういう学習法ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0109	0801	教師なし学習とはどういう学習法ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0109	0715	教師なし学習とはどういう学習法ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0109	0801	教師なし学習とはどういう学習法ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0109	1219	教師なし学習とはどういう学習法ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0109	0307	教師なし学習とはどういう学習法ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0109	0102	教師なし学習とはどういう学習法ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0109	0505	教師なし学習とはどういう学習法ですか	生物の神経細胞の仕組みをモデル化したもの
0	0109	0802	教師なし学習とはどういう学習法ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0109	0711	教師なし学習とはどういう学習法ですか	カーネル関数
0	0109	1406	教師なし学習とはどういう学習法ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0109	0508	教師なし学習とはどういう学習法ですか	二乗誤差を最小にするように識別関数を調整する方法
0	0109	1301	教師なし学習とはどういう学習法ですか	動画像の分類や音声で入力された単語の識別などの問題
0	0109	1201	教師なし学習とはどういう学習法ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0109	0205	教師なし学習とはどういう学習法ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0109	0811	教師なし学習とはどういう学習法ですか	ReLu
1	0114	0114	クラスタリングってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0114	1301	クラスタリングってなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0114	0711	クラスタリングってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0114	0710	クラスタリングってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0114	1409	クラスタリングってなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0114	1104	クラスタリングってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0114	1302	クラスタリングってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0114	1009	クラスタリングってなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0114	0901	クラスタリングってなんですか	深層学習に用いるニューラルネットワーク
0	0114	0811	クラスタリングってなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0114	0111	クラスタリングってなんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0114	1501	クラスタリングってなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0114	0209	クラスタリングってなんですか	正例がどれだけ正しく判定されているかという指標
0	0114	0514	クラスタリングってなんですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0114	0906	クラスタリングってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0114	0612	クラスタリングってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0114	1103	クラスタリングってなんですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0114	1207	クラスタリングってなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0114	0805	クラスタリングってなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0114	1506	クラスタリングってなんですか	政策
0	0114	0514	クラスタリングってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0114	0204	クラスタリングってなんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0114	0917	クラスタリングってなんですか	LSTM
0	0114	0104	クラスタリングってなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0114	0512	クラスタリングってなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
1	1104	1104	階層的クラスタリングってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1104	0703	階層的クラスタリングってなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	1104	1406	階層的クラスタリングってなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1104	1409	階層的クラスタリングってなんですか	自分が出した誤りを指摘してくれる他人がいない
0	1104	0808	階層的クラスタリングってなんですか	入力の重み付き和の微分
0	1104	0707	階層的クラスタリングってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1104	1220	階層的クラスタリングってなんですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1104	0402	階層的クラスタリングってなんですか	統計的識別手法
0	1104	1015	階層的クラスタリングってなんですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1104	1403	階層的クラスタリングってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1104	0601	階層的クラスタリングってなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1104	1508	階層的クラスタリングってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1104	0416	階層的クラスタリングってなんですか	値が真となる確率を知りたいノードが表す変数
0	1104	0908	階層的クラスタリングってなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1104	0701	階層的クラスタリングってなんですか	識別境界線と最も近いデータとの距離
0	1104	1209	階層的クラスタリングってなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	1104	0511	階層的クラスタリングってなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	1104	0811	階層的クラスタリングってなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1104	0506	階層的クラスタリングってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1104	0607	階層的クラスタリングってなんですか	Lasso回帰
0	1104	0113	階層的クラスタリングってなんですか	入力データに潜む規則性を学習すること
0	1104	0105	階層的クラスタリングってなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1104	0715	階層的クラスタリングってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1104	0917	階層的クラスタリングってなんですか	入力ゲート・出力ゲート・忘却ゲート
0	1104	1004	階層的クラスタリングってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
1	0114	0114	クラスタリングの代表的な手法には何がありますか	階層的クラスタリングや k-means 法
0	0114	0610	クラスタリングの代表的な手法には何がありますか	学習結果の散らばり具合
0	0114	0614	クラスタリングの代表的な手法には何がありますか	線形回帰式
0	0114	0404	クラスタリングの代表的な手法には何がありますか	事前確率
0	0114	1402	クラスタリングの代表的な手法には何がありますか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0114	0704	クラスタリングの代表的な手法には何がありますか	ラグランジュの未定乗数法
0	0114	0901	クラスタリングの代表的な手法には何がありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0114	0616	クラスタリングの代表的な手法には何がありますか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0114	0505	クラスタリングの代表的な手法には何がありますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0114	0811	クラスタリングの代表的な手法には何がありますか	半分の領域で勾配が1になるので
0	0114	0105	クラスタリングの代表的な手法には何がありますか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0114	0811	クラスタリングの代表的な手法には何がありますか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0114	0912	クラスタリングの代表的な手法には何がありますか	畳み込みニューラルネットワーク
0	0114	0908	クラスタリングの代表的な手法には何がありますか	ユークリッド距離
0	0114	1301	クラスタリングの代表的な手法には何がありますか	形態素解析処理が典型的な問題
0	0114	1207	クラスタリングの代表的な手法には何がありますか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0114	0305	クラスタリングの代表的な手法には何がありますか	仮説に対して課す制約
0	0114	0902	クラスタリングの代表的な手法には何がありますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0114	0808	クラスタリングの代表的な手法には何がありますか	入力の重み付き和の微分
0	0114	1106	クラスタリングの代表的な手法には何がありますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0114	1506	クラスタリングの代表的な手法には何がありますか	政策
0	0114	0810	クラスタリングの代表的な手法には何がありますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0114	1214	クラスタリングの代表的な手法には何がありますか	計算量が膨大であること
0	0114	0409	クラスタリングの代表的な手法には何がありますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0114	0509	クラスタリングの代表的な手法には何がありますか	確率的最急勾配法
1	1104	1104	階層的クラスタリングは、どのようなアルゴリズムですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1104	0105	階層的クラスタリングは、どのようなアルゴリズムですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1104	0811	階層的クラスタリングは、どのようなアルゴリズムですか	ユニットの活性化関数を工夫する方法があります
0	1104	1106	階層的クラスタリングは、どのようなアルゴリズムですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1104	1301	階層的クラスタリングは、どのようなアルゴリズムですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1104	0614	階層的クラスタリングは、どのようなアルゴリズムですか	モデル木
0	1104	1501	階層的クラスタリングは、どのようなアルゴリズムですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1104	0508	階層的クラスタリングは、どのようなアルゴリズムですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1104	1203	階層的クラスタリングは、どのようなアルゴリズムですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1104	1110	階層的クラスタリングは、どのようなアルゴリズムですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1104	0110	階層的クラスタリングは、どのようなアルゴリズムですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1104	0506	階層的クラスタリングは、どのようなアルゴリズムですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1104	1110	階層的クラスタリングは、どのようなアルゴリズムですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1104	1405	階層的クラスタリングは、どのようなアルゴリズムですか	半教師あり学習は文書分類問題によく適用されます
0	1104	0115	階層的クラスタリングは、どのようなアルゴリズムですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1104	0701	階層的クラスタリングは、どのようなアルゴリズムですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1104	1506	階層的クラスタリングは、どのようなアルゴリズムですか	後に得られる報酬ほど割り引いて計算するための係数
0	1104	0109	階層的クラスタリングは、どのようなアルゴリズムですか	正解が付いていない場合の学習
0	1104	0906	階層的クラスタリングは、どのようなアルゴリズムですか	十分多くの層を持つニューラルネットワーク
0	1104	0117	階層的クラスタリングは、どのようなアルゴリズムですか	学習データの一部にだけ正解が与えられている場合
0	1104	0209	階層的クラスタリングは、どのようなアルゴリズムですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1104	0710	階層的クラスタリングは、どのようなアルゴリズムですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1104	0113	階層的クラスタリングは、どのようなアルゴリズムですか	入力データに潜む規則性を学習すること
0	1104	1510	階層的クラスタリングは、どのようなアルゴリズムですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	1104	1010	階層的クラスタリングは、どのようなアルゴリズムですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
1	0115	0115	パターンマイニングの代表的な手法には何がありますか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0115	0102	パターンマイニングの代表的な手法には何がありますか	現在，人が行っている知的な判断を代わりに行う技術
0	0115	0917	パターンマイニングの代表的な手法には何がありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0115	0611	パターンマイニングの代表的な手法には何がありますか	回帰
0	0115	0502	パターンマイニングの代表的な手法には何がありますか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0115	0810	パターンマイニングの代表的な手法には何がありますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0115	0611	パターンマイニングの代表的な手法には何がありますか	識別における決定木の考え方を回帰問題に適用する方法
0	0115	0206	パターンマイニングの代表的な手法には何がありますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0115	0605	パターンマイニングの代表的な手法には何がありますか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0115	0503	パターンマイニングの代表的な手法には何がありますか	様々な数値データに対して多く用いられる統計モデル
0	0115	0508	パターンマイニングの代表的な手法には何がありますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0115	0903	パターンマイニングの代表的な手法には何がありますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0115	0607	パターンマイニングの代表的な手法には何がありますか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0115	1104	パターンマイニングの代表的な手法には何がありますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0115	0906	パターンマイニングの代表的な手法には何がありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0115	0112	パターンマイニングの代表的な手法には何がありますか	線形回帰，回帰木，モデル木など
0	0115	1009	パターンマイニングの代表的な手法には何がありますか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0115	1406	パターンマイニングの代表的な手法には何がありますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0115	0610	パターンマイニングの代表的な手法には何がありますか	学習結果の散らばり具合
0	0115	1505	パターンマイニングの代表的な手法には何がありますか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0115	1302	パターンマイニングの代表的な手法には何がありますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0115	0704	パターンマイニングの代表的な手法には何がありますか	ラグランジュの未定乗数法を不等式制約条件
0	0115	0701	パターンマイニングの代表的な手法には何がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0115	0313	パターンマイニングの代表的な手法には何がありますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0115	0911	パターンマイニングの代表的な手法には何がありますか	過学習が起きにくくなり，汎用性が高まることが報告されています
1	1108	1108	k-平均法とは、どのようなアルゴリズムなのですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1108	1010	k-平均法とは、どのようなアルゴリズムなのですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1108	0906	k-平均法とは、どのようなアルゴリズムなのですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1108	0313	k-平均法とは、どのようなアルゴリズムなのですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1108	0811	k-平均法とは、どのようなアルゴリズムなのですか	引数が負のときは0，0以上のときはその値を出力
0	1108	0710	k-平均法とは、どのようなアルゴリズムなのですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1108	0707	k-平均法とは、どのようなアルゴリズムなのですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1108	0307	k-平均法とは、どのようなアルゴリズムなのですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	1108	0114	k-平均法とは、どのようなアルゴリズムなのですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1108	0713	k-平均法とは、どのようなアルゴリズムなのですか	文書分類やバイオインフォマティックスなど
0	1108	0902	k-平均法とは、どのようなアルゴリズムなのですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1108	1407	k-平均法とは、どのようなアルゴリズムなのですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1108	0508	k-平均法とは、どのようなアルゴリズムなのですか	二乗誤差を最小にするように識別関数を調整する方法
0	1108	0315	k-平均法とは、どのようなアルゴリズムなのですか	分割後のデータの分散
0	1108	0503	k-平均法とは、どのようなアルゴリズムなのですか	様々な数値データに対して多く用いられる統計モデル
0	1108	0903	k-平均法とは、どのようなアルゴリズムなのですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1108	1103	k-平均法とは、どのようなアルゴリズムなのですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1108	1505	k-平均法とは、どのようなアルゴリズムなのですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1108	0715	k-平均法とは、どのようなアルゴリズムなのですか	複雑な非線形変換を求めるという操作を避ける方法
0	1108	1214	k-平均法とは、どのようなアルゴリズムなのですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1108	0606	k-平均法とは、どのようなアルゴリズムなのですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	1108	1209	k-平均法とは、どのようなアルゴリズムなのですか	規則の条件部が起こったときに結論部が起こる割合
0	1108	0704	k-平均法とは、どのようなアルゴリズムなのですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1108	0115	k-平均法とは、どのようなアルゴリズムなのですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1108	0901	k-平均法とは、どのようなアルゴリズムなのですか	深層学習に用いるニューラルネットワーク
1	1108	1108	k-平均法とはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1108	0402	k-平均法とはなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1108	0514	k-平均法とはなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	1108	0116	k-平均法とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	1108	0105	k-平均法とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1108	0304	k-平均法とはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	1108	0402	k-平均法とはなんですか	事後確率が最大となるクラスを識別結果とする方法
0	1108	0810	k-平均法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1108	0916	k-平均法とはなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1108	0614	k-平均法とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	1108	0805	k-平均法とはなんですか	誤差逆伝播法
0	1108	0113	k-平均法とはなんですか	入力データに潜む規則性
0	1108	0912	k-平均法とはなんですか	畳み込みニューラルネットワーク
0	1108	0612	k-平均法とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1108	1001	k-平均法とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1108	0704	k-平均法とはなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1108	0901	k-平均法とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1108	1301	k-平均法とはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1108	0903	k-平均法とはなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1108	0810	k-平均法とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1108	0810	k-平均法とはなんですか	勾配消失問題
0	1108	1304	k-平均法とはなんですか	入力と対応させる素性
0	1108	0402	k-平均法とはなんですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1108	0307	k-平均法とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1108	0911	k-平均法とはなんですか	ドロップアウト
1	0205	0205	主成分分析って何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0205	0704	主成分分析って何ですか	以下の関数$L$の最小値を求めるという問題
0	0205	0614	主成分分析って何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0205	0704	主成分分析って何ですか	ラグランジュの未定乗数法
0	0205	1302	主成分分析って何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0205	0112	主成分分析って何ですか	線形回帰，回帰木，モデル木など
0	0205	1110	主成分分析って何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0205	0611	主成分分析って何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0205	0711	主成分分析って何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0205	1108	主成分分析って何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0205	0902	主成分分析って何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0205	0204	主成分分析って何ですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0205	0404	主成分分析って何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0205	0810	主成分分析って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0205	0209	主成分分析って何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0205	0315	主成分分析って何ですか	分割後のデータの分散
0	0205	0602	主成分分析って何ですか	正解情報$y$が数値であるということ
0	0205	0513	主成分分析って何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0205	0712	主成分分析って何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0205	0908	主成分分析って何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0205	0206	主成分分析って何ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0205	0717	主成分分析って何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0205	1116	主成分分析って何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0205	0612	主成分分析って何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0205	0411	主成分分析って何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
1	0209	0209	精度って何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0209	1110	精度って何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0209	0505	精度って何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0209	0409	精度って何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0209	0710	精度って何ですか	もとの空間におけるデータ間の距離関係を保存
0	0209	0701	精度って何ですか	サポートベクトルマシン
0	0209	0607	精度って何ですか	Lasso回帰
0	0209	0602	精度って何ですか	数値型の正解情報のこと
0	0209	1001	精度って何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0209	0417	精度って何ですか	ネットワークの構造とアークの条件付き確率表
0	0209	0908	精度って何ですか	シグモイド関数
0	0209	0614	精度って何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0209	1116	精度って何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0209	1008	精度って何ですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0209	0411	精度って何ですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0209	1207	精度って何ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0209	0810	精度って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0209	0911	精度って何ですか	学習時の自由度を意図的に下げていること
0	0209	0901	精度って何ですか	Deep Neural Network (DNN) 
0	0209	0906	精度って何ですか	入力に近い側の処理
0	0209	0711	精度って何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0209	0916	精度って何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0209	1501	精度って何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0209	0404	精度って何ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0209	1410	精度って何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
1	1110	1110	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1110	0802	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	隠れ層
0	1110	0901	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1110	0810	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	2006 年頃に考案された事前学習法
0	1110	0411	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	確率のm推定という考え方を用います
0	1110	1501	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1110	0701	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1110	1108	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1110	1406	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1110	0708	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	制約を弱める変数
0	1110	1008	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	1110	0715	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	複雑な非線形変換を求めるという操作を避ける方法
0	1110	1004	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1110	1012	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1110	0711	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	カーネル関数
0	1110	0305	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	仮説に対して課す制約
0	1110	0410	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1110	0412	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	1110	1106	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	最も遠い事例対の距離を類似度とする
0	1110	1507	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1110	0606	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1110	0607	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1110	1505	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1110	0505	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1110	1112	X-meansアルゴリズムは、どのよなアルゴリズムなのですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
1	0209	0209	F値って何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0209	0810	F値って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0209	0907	F値って何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0209	0811	F値って何ですか	半分の領域で勾配が1になるので
0	0209	1007	F値って何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0209	0911	F値って何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0209	1301	F値って何ですか	形態素解析処理が典型的な問題
0	0209	0801	F値って何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0209	0514	F値って何ですか	ランダムに学習データを一つ
0	0209	0802	F値って何ですか	多層パーセプトロンあるいはニューラルネットワーク
0	0209	0903	F値って何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0209	0704	F値って何ですか	ラグランジュの未定乗数法
0	0209	0810	F値って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0209	0711	F値って何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0209	0715	F値って何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0209	0605	F値って何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0209	0616	F値って何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0209	1306	F値って何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0209	0114	F値って何ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0209	1009	F値って何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0209	0802	F値って何ですか	特徴ベクトルの次元数
0	0209	0513	F値って何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0209	0901	F値って何ですか	音声認識・画像認識・自然言語処理など
0	0209	0510	F値って何ですか	特徴空間上でクラスを分割する面
0	0209	1015	F値って何ですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
1	0211	0211	ROC曲線って何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0211	0416	ROC曲線って何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0211	0117	ROC曲線って何ですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0211	1111	ROC曲線って何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0211	0409	ROC曲線って何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0211	0105	ROC曲線って何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0211	0109	ROC曲線って何ですか	正解が付いていない場合の学習
0	0211	0803	ROC曲線って何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0211	0912	ROC曲線って何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0211	0402	ROC曲線って何ですか	事後確率が最大となるクラスを識別結果とする方法
0	0211	1501	ROC曲線って何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0211	0907	ROC曲線って何ですか	事前学習法
0	0211	0906	ROC曲線って何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0211	0103	ROC曲線って何ですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0211	0901	ROC曲線って何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0211	0111	ROC曲線って何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0211	0702	ROC曲線って何ですか	識別面は平面を仮定する
0	0211	1214	ROC曲線って何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0211	0606	ROC曲線って何ですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0211	0104	ROC曲線って何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0211	0801	ROC曲線って何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0211	0906	ROC曲線って何ですか	十分多くの層
0	0211	0701	ROC曲線って何ですか	マージン
0	0211	0607	ROC曲線って何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0211	0114	ROC曲線って何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	1110	1110	モデルの対数尤度とは何ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	1110	1506	モデルの対数尤度とは何ですか	政策
0	1110	0412	モデルの対数尤度とは何ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1110	0701	モデルの対数尤度とは何ですか	識別境界線と最も近いデータとの距離
0	1110	0917	モデルの対数尤度とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1110	0610	モデルの対数尤度とは何ですか	学習結果の散らばり具合
0	1110	0708	モデルの対数尤度とは何ですか	制約を弱める変数
0	1110	0907	モデルの対数尤度とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1110	0302	モデルの対数尤度とは何ですか	カテゴリ形式の正解情報のこと
0	1110	0509	モデルの対数尤度とは何ですか	確率的最急勾配法
0	1110	0811	モデルの対数尤度とは何ですか	ユニットの活性化関数を工夫する方法があります
0	1110	0908	モデルの対数尤度とは何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	1110	0811	モデルの対数尤度とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	1110	0514	モデルの対数尤度とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1110	0803	モデルの対数尤度とは何ですか	重みパラメータに対しては線形で，入力を非線形変換する
0	1110	0505	モデルの対数尤度とは何ですか	ニューラルネットワーク
0	1110	0611	モデルの対数尤度とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1110	0906	モデルの対数尤度とは何ですか	十分多くの層
0	1110	0109	モデルの対数尤度とは何ですか	正解が付いていない場合の学習
0	1110	0601	モデルの対数尤度とは何ですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	1110	0704	モデルの対数尤度とは何ですか	以下の関数$L$の最小値を求めるという問題
0	1110	1209	モデルの対数尤度とは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1110	0916	モデルの対数尤度とは何ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	1110	0906	モデルの対数尤度とは何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1110	0111	モデルの対数尤度とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
1	0304	0304	概念学習って何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	0304	1313	概念学習って何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0304	1501	概念学習って何ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0304	1010	概念学習って何ですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0304	1214	概念学習って何ですか	計算量が膨大であること
0	0304	0102	概念学習って何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0304	1509	概念学習って何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0304	0105	概念学習って何ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0304	0906	概念学習って何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0304	1106	概念学習って何ですか	最も近い事例対の距離を類似度とする
0	0304	1214	概念学習って何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0304	1304	概念学習って何ですか	出力系列を参照する素性
0	0304	0513	概念学習って何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0304	0906	概念学習って何ですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0304	0416	概念学習って何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0304	0710	概念学習って何ですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0304	1505	概念学習って何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0304	1502	概念学習って何ですか	将棋や囲碁などを行うプログラム
0	0304	1407	概念学習って何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0304	1306	概念学習って何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0304	0717	概念学習って何ですか	連続値
0	0304	1111	概念学習って何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0304	1104	概念学習って何ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0304	0306	概念学習って何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0304	1506	概念学習って何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
1	0306	0306	候補削除アルゴリズムってなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0306	0316	候補削除アルゴリズムってなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0306	0508	候補削除アルゴリズムってなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0306	1313	候補削除アルゴリズムってなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0306	0402	候補削除アルゴリズムってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0306	1406	候補削除アルゴリズムってなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0306	1310	候補削除アルゴリズムってなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	0306	0810	候補削除アルゴリズムってなんですか	2006 年頃に考案された事前学習法
0	0306	0811	候補削除アルゴリズムってなんですか	ユニットの活性化関数を工夫する方法があります
0	0306	0614	候補削除アルゴリズムってなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0306	1106	候補削除アルゴリズムってなんですか	最も近い事例対の距離を類似度とする
0	0306	0507	候補削除アルゴリズムってなんですか	パーセプトロンの収束定理
0	0306	0406	候補削除アルゴリズムってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0306	0514	候補削除アルゴリズムってなんですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0306	0701	候補削除アルゴリズムってなんですか	学習データからのマージンが最大となる識別境界線
0	0306	1106	候補削除アルゴリズムってなんですか	最も遠い事例対の距離を類似度とする
0	0306	0411	候補削除アルゴリズムってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0306	0701	候補削除アルゴリズムってなんですか	マージン
0	0306	0710	候補削除アルゴリズムってなんですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0306	0916	候補削除アルゴリズムってなんですか	リカレントニューラルネットワーク
0	0306	0701	候補削除アルゴリズムってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0306	0802	候補削除アルゴリズムってなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0306	0701	候補削除アルゴリズムってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0306	0601	候補削除アルゴリズムってなんですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0306	1012	候補削除アルゴリズムってなんですか	各データに重みを付け，そのもとで識別器を作成します
1	1111	1111	外れ値とはどういうものですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1111	1004	外れ値とはどういうものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1111	0611	外れ値とはどういうものですか	回帰
0	1111	1302	外れ値とはどういうものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1111	0717	外れ値とはどういうものですか	Grid search
0	1111	0614	外れ値とはどういうものですか	回帰木と線形回帰の双方のよいところを取った
0	1111	0811	外れ値とはどういうものですか	半分の領域で勾配が1になるので
0	1111	1409	外れ値とはどういうものですか	自分が出した誤りを指摘してくれる他人がいない
0	1111	0710	外れ値とはどういうものですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1111	0614	外れ値とはどういうものですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	1111	0810	外れ値とはどういうものですか	2006 年頃に考案された事前学習法
0	1111	0512	外れ値とはどういうものですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1111	1108	外れ値とはどういうものですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1111	0416	外れ値とはどういうものですか	値が真となる確率を知りたいノードが表す変数
0	1111	0901	外れ値とはどういうものですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1111	0906	外れ値とはどういうものですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1111	0811	外れ値とはどういうものですか	ReLu
0	1111	0410	外れ値とはどういうものですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1111	0610	外れ値とはどういうものですか	学習結果の散らばり具合
0	1111	0607	外れ値とはどういうものですか	「投げ縄」という意味
0	1111	0115	外れ値とはどういうものですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1111	0614	外れ値とはどういうものですか	回帰木と線形回帰の双方のよいところを取った方法
0	1111	0302	外れ値とはどういうものですか	カテゴリ形式の正解情報のこと
0	1111	0906	外れ値とはどういうものですか	十分多くの層を持つニューラルネットワーク
0	1111	0715	外れ値とはどういうものですか	カーネルトリック
1	0307	0307	決定木ってなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0307	0805	決定木ってなんですか	誤差逆伝播法
0	0307	1506	決定木ってなんですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0307	1506	決定木ってなんですか	最適政策$\pi^*$を獲得すること
0	0307	0715	決定木ってなんですか	カーネルトリック
0	0307	0614	決定木ってなんですか	モデル木
0	0307	0715	決定木ってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0307	1506	決定木ってなんですか	政策
0	0307	0209	決定木ってなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0307	0611	決定木ってなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0307	0607	決定木ってなんですか	Lasso回帰
0	0307	0109	決定木ってなんですか	学習データに正解が付いている場合の学習
0	0307	0409	決定木ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0307	0507	決定木ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0307	0902	決定木ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0307	1510	決定木ってなんですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0307	0901	決定木ってなんですか	深層学習に用いるニューラルネットワーク
0	0307	0114	決定木ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0307	0902	決定木ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0307	0508	決定木ってなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0307	0616	決定木ってなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0307	0402	決定木ってなんですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0307	0502	決定木ってなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0307	1412	決定木ってなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0307	0916	決定木ってなんですか	リカレントニューラルネットワーク
1	1112	1112	局所異常因子では、どのように外れ値を検知するのですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1112	0801	局所異常因子では、どのように外れ値を検知するのですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1112	0402	局所異常因子では、どのように外れ値を検知するのですか	統計的識別手法
0	1112	1313	局所異常因子では、どのように外れ値を検知するのですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1112	0112	局所異常因子では、どのように外れ値を検知するのですか	線形回帰，回帰木，モデル木など
0	1112	0505	局所異常因子では、どのように外れ値を検知するのですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1112	0708	局所異常因子では、どのように外れ値を検知するのですか	制約を弱める変数
0	1112	0611	局所異常因子では、どのように外れ値を検知するのですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1112	1412	局所異常因子では、どのように外れ値を検知するのですか	近くのノードは同じクラスになりやすいという仮定
0	1112	1012	局所異常因子では、どのように外れ値を検知するのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1112	1209	局所異常因子では、どのように外れ値を検知するのですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1112	0701	局所異常因子では、どのように外れ値を検知するのですか	サポートベクトルマシン
0	1112	0614	局所異常因子では、どのように外れ値を検知するのですか	回帰木と線形回帰の双方のよいところを取った方法
0	1112	0704	局所異常因子では、どのように外れ値を検知するのですか	ラグランジュの未定乗数法を不等式制約条件
0	1112	1010	局所異常因子では、どのように外れ値を検知するのですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1112	0805	局所異常因子では、どのように外れ値を検知するのですか	誤差逆伝播法
0	1112	1214	局所異常因子では、どのように外れ値を検知するのですか	計算量が膨大であること
0	1112	0701	局所異常因子では、どのように外れ値を検知するのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1112	1103	局所異常因子では、どのように外れ値を検知するのですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	1112	0104	局所異常因子では、どのように外れ値を検知するのですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1112	0115	局所異常因子では、どのように外れ値を検知するのですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1112	1001	局所異常因子では、どのように外れ値を検知するのですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	1112	0512	局所異常因子では、どのように外れ値を検知するのですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1112	0917	局所異常因子では、どのように外れ値を検知するのですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1112	0602	局所異常因子では、どのように外れ値を検知するのですか	正解情報$y$が数値であるということ
1	0313	0313	過学習って何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0313	0912	過学習って何ですか	画像認識
0	0313	1214	過学習って何ですか	計算量が膨大であること
0	0313	0209	過学習って何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0313	0206	過学習って何ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0313	0911	過学習って何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0313	0906	過学習って何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0313	1112	過学習って何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0313	0315	過学習って何ですか	分割後のデータの分散
0	0313	0801	過学習って何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0313	0808	過学習って何ですか	入力の重み付き和の微分
0	0313	0514	過学習って何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0313	1001	過学習って何ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0313	0614	過学習って何ですか	回帰木と線形回帰の双方のよいところを取った
0	0313	1502	過学習って何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0313	0710	過学習って何ですか	もとの空間におけるデータ間の距離関係を保存
0	0313	0908	過学習って何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0313	0901	過学習って何ですか	表現学習
0	0313	0607	過学習って何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0313	1301	過学習って何ですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0313	1012	過学習って何ですか	すべてのデータの重みは平等
0	0313	1001	過学習って何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0313	0512	過学習って何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0313	0109	過学習って何ですか	正解が付いていない場合の学習
0	0313	0114	過学習って何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
1	1114	1114	事前確率を求めるには何が必要ですか	クラスタリング結果のデータ数の分布
0	1114	1301	事前確率を求めるには何が必要ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1114	0410	事前確率を求めるには何が必要ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1114	0114	事前確率を求めるには何が必要ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1114	0701	事前確率を求めるには何が必要ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	1114	0302	事前確率を求めるには何が必要ですか	カテゴリ形式の正解情報のこと
0	1114	0701	事前確率を求めるには何が必要ですか	マージン
0	1114	0802	事前確率を求めるには何が必要ですか	隠れ層
0	1114	0104	事前確率を求めるには何が必要ですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	1114	1412	事前確率を求めるには何が必要ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1114	0117	事前確率を求めるには何が必要ですか	学習データの一部にだけ正解が与えられている場合
0	1114	0610	事前確率を求めるには何が必要ですか	片方を減らせば片方が増える
0	1114	0109	事前確率を求めるには何が必要ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	1114	1219	事前確率を求めるには何が必要ですか	どの個人がどの商品を購入したかが記録されているデータ
0	1114	1001	事前確率を求めるには何が必要ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1114	0908	事前確率を求めるには何が必要ですか	ユークリッド距離
0	1114	0102	事前確率を求めるには何が必要ですか	現在，人が行っている知的な判断を代わりに行う技術
0	1114	0404	事前確率を求めるには何が必要ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1114	0906	事前確率を求めるには何が必要ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1114	0601	事前確率を求めるには何が必要ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1114	0901	事前確率を求めるには何が必要ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1114	1310	事前確率を求めるには何が必要ですか	確率的非決定性オートマトンの一種
0	1114	0710	事前確率を求めるには何が必要ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1114	0109	事前確率を求めるには何が必要ですか	正解が付いていない場合の学習
0	1114	1012	事前確率を求めるには何が必要ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
1	0204	0204	特徴ベクトルの次元数が増えるとどんな問題がありますか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0204	0109	特徴ベクトルの次元数が増えるとどんな問題がありますか	正解が付いていない場合の学習
0	0204	1106	特徴ベクトルの次元数が増えるとどんな問題がありますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0204	0514	特徴ベクトルの次元数が増えるとどんな問題がありますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0204	0710	特徴ベクトルの次元数が増えるとどんな問題がありますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0204	0413	特徴ベクトルの次元数が増えるとどんな問題がありますか	変数間の独立性を表現できること
0	0204	0114	特徴ベクトルの次元数が増えるとどんな問題がありますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0204	0113	特徴ベクトルの次元数が増えるとどんな問題がありますか	入力データに潜む規則性を学習すること
0	0204	1401	特徴ベクトルの次元数が増えるとどんな問題がありますか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0204	1108	特徴ベクトルの次元数が増えるとどんな問題がありますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0204	0708	特徴ベクトルの次元数が増えるとどんな問題がありますか	制約を弱める変数
0	0204	0502	特徴ベクトルの次元数が増えるとどんな問題がありますか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0204	0504	特徴ベクトルの次元数が増えるとどんな問題がありますか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0204	0701	特徴ベクトルの次元数が増えるとどんな問題がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0204	0711	特徴ベクトルの次元数が増えるとどんな問題がありますか	カーネル関数
0	0204	0701	特徴ベクトルの次元数が増えるとどんな問題がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0204	0514	特徴ベクトルの次元数が増えるとどんな問題がありますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0204	0911	特徴ベクトルの次元数が増えるとどんな問題がありますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0204	0906	特徴ベクトルの次元数が増えるとどんな問題がありますか	十分多くの層
0	0204	0917	特徴ベクトルの次元数が増えるとどんな問題がありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0204	0917	特徴ベクトルの次元数が増えるとどんな問題がありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0204	0901	特徴ベクトルの次元数が増えるとどんな問題がありますか	深層学習に用いるニューラルネットワーク
0	0204	0917	特徴ベクトルの次元数が増えるとどんな問題がありますか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0204	0612	特徴ベクトルの次元数が増えるとどんな問題がありますか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0204	0412	特徴ベクトルの次元数が増えるとどんな問題がありますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
1	0115	0115	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	パターンマイニング
0	0115	1302	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0115	1106	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0115	0711	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	カーネル関数
0	0115	0413	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	変数間の独立性を表現できること
0	0115	0917	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0115	0906	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0115	0103	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0115	0906	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	多階層構造でもそのまま適用できます
0	0115	0610	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	片方を減らせば片方が増える
0	0115	0802	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	特徴ベクトルの次元数
0	0115	1114	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	クラスタリング結果のデータ数の分布から
0	0115	0406	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0115	0417	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	ネットワークの構造とアークの条件付き確率
0	0115	0502	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0115	1009	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0115	0606	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	入力が少し変化したときに，出力も少し変化する
0	0115	1015	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0115	1215	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0115	1301	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0115	0710	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	もとの空間におけるデータ間の距離関係を保存
0	0115	1111	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0115	0406	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0115	0411	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0115	1004	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法はなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
1	0103	0103	機械学習はどんな時に利用されますか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0103	0505	機械学習はどんな時に利用されますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0103	0808	機械学習はどんな時に利用されますか	入力の重み付き和の微分
0	0103	0811	機械学習はどんな時に利用されますか	ユニットの活性化関数を工夫する方法があります
0	0103	0512	機械学習はどんな時に利用されますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0103	1409	機械学習はどんな時に利用されますか	自分が出した誤りを指摘してくれる他人がいない
0	0103	0402	機械学習はどんな時に利用されますか	事後確率が最大となるクラスを識別結果とする方法
0	0103	0508	機械学習はどんな時に利用されますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0103	0908	機械学習はどんな時に利用されますか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0103	0409	機械学習はどんな時に利用されますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0103	0505	機械学習はどんな時に利用されますか	生物の神経細胞の仕組みをモデル化したもの
0	0103	0802	機械学習はどんな時に利用されますか	多層パーセプトロンあるいはニューラルネットワーク
0	0103	1301	機械学習はどんな時に利用されますか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0103	0306	機械学習はどんな時に利用されますか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0103	0416	機械学習はどんな時に利用されますか	値が真となる確率を知りたいノードが表す変数
0	0103	0901	機械学習はどんな時に利用されますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0103	0901	機械学習はどんな時に利用されますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0103	1012	機械学習はどんな時に利用されますか	すべてのデータの重みは平等
0	0103	0906	機械学習はどんな時に利用されますか	多階層構造でもそのまま適用できます
0	0103	0902	機械学習はどんな時に利用されますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0103	0711	機械学習はどんな時に利用されますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0103	0510	機械学習はどんな時に利用されますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0103	0917	機械学習はどんな時に利用されますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0103	1306	機械学習はどんな時に利用されますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0103	0602	機械学習はどんな時に利用されますか	正解情報$y$が数値であるということ
1	1116	1116	EM アルゴリズムとは、どのようなアルゴリズムですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1116	0715	EM アルゴリズムとは、どのようなアルゴリズムですか	複雑な非線形変換を求めるという操作を避ける方法
0	1116	0805	EM アルゴリズムとは、どのようなアルゴリズムですか	誤差逆伝播法
0	1116	0115	EM アルゴリズムとは、どのようなアルゴリズムですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1116	1407	EM アルゴリズムとは、どのようなアルゴリズムですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1116	0411	EM アルゴリズムとは、どのようなアルゴリズムですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1116	0503	EM アルゴリズムとは、どのようなアルゴリズムですか	様々な数値データに対して多く用いられる統計モデル
0	1116	0911	EM アルゴリズムとは、どのようなアルゴリズムですか	ドロップアウト
0	1116	1220	EM アルゴリズムとは、どのようなアルゴリズムですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1116	0504	EM アルゴリズムとは、どのようなアルゴリズムですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1116	0715	EM アルゴリズムとは、どのようなアルゴリズムですか	複雑な非線形変換を求めるという操作を避ける方法
0	1116	1214	EM アルゴリズムとは、どのようなアルゴリズムですか	計算量が膨大であること
0	1116	0902	EM アルゴリズムとは、どのようなアルゴリズムですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1116	0508	EM アルゴリズムとは、どのようなアルゴリズムですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1116	1004	EM アルゴリズムとは、どのようなアルゴリズムですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1116	0607	EM アルゴリズムとは、どのようなアルゴリズムですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1116	0906	EM アルゴリズムとは、どのようなアルゴリズムですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1116	1010	EM アルゴリズムとは、どのようなアルゴリズムですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1116	1001	EM アルゴリズムとは、どのようなアルゴリズムですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1116	0411	EM アルゴリズムとは、どのようなアルゴリズムですか	確率のm推定という考え方を用います
0	1116	0901	EM アルゴリズムとは、どのようなアルゴリズムですか	音声認識・画像認識・自然言語処理など
0	1116	0711	EM アルゴリズムとは、どのようなアルゴリズムですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1116	0901	EM アルゴリズムとは、どのようなアルゴリズムですか	Deep Neural Network (DNN) 
0	1116	0117	EM アルゴリズムとは、どのようなアルゴリズムですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	1116	1410	EM アルゴリズムとは、どのようなアルゴリズムですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
1	1203	1203	トランザクションとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1203	1304	トランザクションとは何ですか	出力系列を参照する素性
0	1203	1111	トランザクションとは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1203	0802	トランザクションとは何ですか	特徴空間上では線形識別面を設定すること
0	1203	1506	トランザクションとは何ですか	各状態でどの行為を取ればよいのかという意思決定規則
0	1203	1001	トランザクションとは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1203	1106	トランザクションとは何ですか	クラスタの重心間の距離を類似度とする
0	1203	0315	トランザクションとは何ですか	分割後のデータの分散
0	1203	0912	トランザクションとは何ですか	畳み込みニューラルネットワーク
0	1203	0803	トランザクションとは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1203	0109	トランザクションとは何ですか	正解が付いていない場合の学習
0	1203	0112	トランザクションとは何ですか	線形回帰，回帰木，モデル木など
0	1203	0505	トランザクションとは何ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1203	0811	トランザクションとは何ですか	半分の領域で勾配が1になるので
0	1203	0506	トランザクションとは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1203	0715	トランザクションとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1203	0715	トランザクションとは何ですか	カーネルトリック
0	1203	0917	トランザクションとは何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1203	1401	トランザクションとは何ですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	1203	1215	トランザクションとは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	1203	0902	トランザクションとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1203	1506	トランザクションとは何ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1203	0502	トランザクションとは何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1203	0508	トランザクションとは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1203	1214	トランザクションとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
1	1207	1207	Aprioriアルゴリズムとはどのようなアルゴリズムですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1207	0802	Aprioriアルゴリズムとはどのようなアルゴリズムですか	特徴空間上では線形識別面を設定すること
0	1207	0715	Aprioriアルゴリズムとはどのようなアルゴリズムですか	複雑な非線形変換を求めるという操作を避ける方法
0	1207	0405	Aprioriアルゴリズムとはどのようなアルゴリズムですか	尤度と事前確率の積を最大とするクラス
0	1207	0205	Aprioriアルゴリズムとはどのようなアルゴリズムですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1207	1106	Aprioriアルゴリズムとはどのようなアルゴリズムですか	最も近い事例対の距離を類似度とする
0	1207	0504	Aprioriアルゴリズムとはどのようなアルゴリズムですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1207	0802	Aprioriアルゴリズムとはどのようなアルゴリズムですか	特徴ベクトルの次元数
0	1207	1402	Aprioriアルゴリズムとはどのようなアルゴリズムですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	1207	0802	Aprioriアルゴリズムとはどのようなアルゴリズムですか	入力層・出力層の数に応じた適当な数
0	1207	0611	Aprioriアルゴリズムとはどのようなアルゴリズムですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1207	0104	Aprioriアルゴリズムとはどのようなアルゴリズムですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	1207	0710	Aprioriアルゴリズムとはどのようなアルゴリズムですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1207	0802	Aprioriアルゴリズムとはどのようなアルゴリズムですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1207	1502	Aprioriアルゴリズムとはどのようなアルゴリズムですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1207	1409	Aprioriアルゴリズムとはどのようなアルゴリズムですか	自分が出した誤りを指摘してくれる他人がいない
0	1207	0802	Aprioriアルゴリズムとはどのようなアルゴリズムですか	識別対象のクラス数
0	1207	0610	Aprioriアルゴリズムとはどのようなアルゴリズムですか	真のモデルとの距離
0	1207	0510	Aprioriアルゴリズムとはどのようなアルゴリズムですか	特徴空間上でクラスを分割する面
0	1207	0304	Aprioriアルゴリズムとはどのようなアルゴリズムですか	個々の事例から，あるクラスについて共通点を見つけること
0	1207	1007	Aprioriアルゴリズムとはどのようなアルゴリズムですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1207	0115	Aprioriアルゴリズムとはどのようなアルゴリズムですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1207	1411	Aprioriアルゴリズムとはどのようなアルゴリズムですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1207	0109	Aprioriアルゴリズムとはどのようなアルゴリズムですか	正解が付いていない場合の学習
0	1207	0606	Aprioriアルゴリズムとはどのようなアルゴリズムですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
1	1209	1209	確信度とはなんですか	規則の条件部が起こったときに結論部が起こる割合
0	1209	0616	確信度とはなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	1209	0711	確信度とはなんですか	カーネル関数
0	1209	0109	確信度とはなんですか	学習データに正解が付いている場合の学習
0	1209	0114	確信度とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1209	0313	確信度とはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1209	0114	確信度とはなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1209	0803	確信度とはなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1209	0509	確信度とはなんですか	確率的最急勾配法
0	1209	0614	確信度とはなんですか	モデル木
0	1209	0701	確信度とはなんですか	マージン
0	1209	0610	確信度とはなんですか	学習結果の散らばり具合
0	1209	1309	確信度とはなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1209	0908	確信度とはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	1209	0305	確信度とはなんですか	仮説に対して課す制約
0	1209	0505	確信度とはなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1209	1409	確信度とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1209	0607	確信度とはなんですか	「投げ縄」という意味
0	1209	0410	確信度とはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1209	0508	確信度とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	1209	0413	確信度とはなんですか	変数間の独立性を表現できること
0	1209	1001	確信度とはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1209	1103	確信度とはなんですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	1209	0402	確信度とはなんですか	事後確率
0	1209	0610	確信度とはなんですか	学習結果の散らばり具合
1	1209	1209	リフト値ってなんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1209	0413	リフト値ってなんですか	変数間の独立性を表現できること
0	1209	0504	リフト値ってなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1209	1502	リフト値ってなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1209	0404	リフト値ってなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1209	0211	リフト値ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1209	0907	リフト値ってなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1209	0507	リフト値ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1209	0104	リフト値ってなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1209	0611	リフト値ってなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	1209	1214	リフト値ってなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1209	1304	リフト値ってなんですか	出力系列を参照する素性
0	1209	0211	リフト値ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1209	1408	リフト値ってなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1209	1302	リフト値ってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1209	0110	リフト値ってなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1209	1214	リフト値ってなんですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1209	0610	リフト値ってなんですか	真のモデルとの距離
0	1209	0402	リフト値ってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	1209	0708	リフト値ってなんですか	制約を弱める変数
0	1209	0704	リフト値ってなんですか	ラグランジュの未定乗数法
0	1209	0701	リフト値ってなんですか	サポートベクトルマシン
0	1209	1103	リフト値ってなんですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1209	0109	リフト値ってなんですか	正解が付いていない場合の学習
0	1209	0114	リフト値ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	1209	1209	リフト値の値からどのようなことがわかりますか	この値が高いほど，得られる情報の多い規則であること
0	1209	0911	リフト値の値からどのようなことがわかりますか	ドロップアウト
0	1209	0502	リフト値の値からどのようなことがわかりますか	SVM
0	1209	0508	リフト値の値からどのようなことがわかりますか	二乗誤差を最小にするように識別関数を調整する方法
0	1209	0413	リフト値の値からどのようなことがわかりますか	変数間の独立性を表現できること
0	1209	0109	リフト値の値からどのようなことがわかりますか	正解が付いていない場合の学習
0	1209	0508	リフト値の値からどのようなことがわかりますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1209	0811	リフト値の値からどのようなことがわかりますか	引数が負のときは0，0以上のときはその値を出力
0	1209	1302	リフト値の値からどのようなことがわかりますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1209	0114	リフト値の値からどのようなことがわかりますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1209	1412	リフト値の値からどのようなことがわかりますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1209	0114	リフト値の値からどのようなことがわかりますか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1209	1412	リフト値の値からどのようなことがわかりますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1209	0805	リフト値の値からどのようなことがわかりますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1209	0902	リフト値の値からどのようなことがわかりますか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	1209	0702	リフト値の値からどのようなことがわかりますか	識別面は平面を仮定する
0	1209	1301	リフト値の値からどのようなことがわかりますか	ひとまとまりの系列データを特定のクラスに識別する問題
0	1209	0601	リフト値の値からどのようなことがわかりますか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	1209	0407	リフト値の値からどのようなことがわかりますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1209	0205	リフト値の値からどのようなことがわかりますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1209	0111	リフト値の値からどのようなことがわかりますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	1209	0109	リフト値の値からどのようなことがわかりますか	正解が付いていない場合の学習
0	1209	0715	リフト値の値からどのようなことがわかりますか	カーネルトリック
0	1209	1509	リフト値の値からどのようなことがわかりますか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1209	1301	リフト値の値からどのようなことがわかりますか	個々の要素の間に i.i.d. の関係が成立しないもの
1	1214	1214	FP-Growthアルゴリズムとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1214	0306	FP-Growthアルゴリズムとは何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	1214	1508	FP-Growthアルゴリズムとは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1214	0715	FP-Growthアルゴリズムとは何ですか	カーネルトリック
0	1214	0402	FP-Growthアルゴリズムとは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1214	0803	FP-Growthアルゴリズムとは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1214	0907	FP-Growthアルゴリズムとは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1214	0607	FP-Growthアルゴリズムとは何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1214	0715	FP-Growthアルゴリズムとは何ですか	識別面
0	1214	1306	FP-Growthアルゴリズムとは何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1214	0504	FP-Growthアルゴリズムとは何ですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1214	0701	FP-Growthアルゴリズムとは何ですか	サポートベクトルマシン
0	1214	0304	FP-Growthアルゴリズムとは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1214	0614	FP-Growthアルゴリズムとは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1214	0413	FP-Growthアルゴリズムとは何ですか	変数間の独立性を表現できること
0	1214	0803	FP-Growthアルゴリズムとは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1214	0614	FP-Growthアルゴリズムとは何ですか	回帰木と線形回帰の双方のよいところを取った
0	1214	0506	FP-Growthアルゴリズムとは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1214	1509	FP-Growthアルゴリズムとは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1214	0204	FP-Growthアルゴリズムとは何ですか	特徴ベクトルの次元数を減らすこと
0	1214	0907	FP-Growthアルゴリズムとは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1214	1306	FP-Growthアルゴリズムとは何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1214	0901	FP-Growthアルゴリズムとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1214	0810	FP-Growthアルゴリズムとは何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1214	0416	FP-Growthアルゴリズムとは何ですか	値が真となる確率を知りたいノードが表す変数
1	1214	1214	Aprioriアルゴリズムの問題点は何ですか	計算量が膨大であること
0	1214	0411	Aprioriアルゴリズムの問題点は何ですか	確率のm推定という考え方を用います
0	1214	0612	Aprioriアルゴリズムの問題点は何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1214	0912	Aprioriアルゴリズムの問題点は何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1214	1104	Aprioriアルゴリズムの問題点は何ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1214	0614	Aprioriアルゴリズムの問題点は何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1214	1209	Aprioriアルゴリズムの問題点は何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1214	0505	Aprioriアルゴリズムの問題点は何ですか	ニューラルネットワーク
0	1214	0513	Aprioriアルゴリズムの問題点は何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1214	0902	Aprioriアルゴリズムの問題点は何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1214	1502	Aprioriアルゴリズムの問題点は何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1214	0614	Aprioriアルゴリズムの問題点は何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	1214	1103	Aprioriアルゴリズムの問題点は何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	1214	0105	Aprioriアルゴリズムの問題点は何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1214	0109	Aprioriアルゴリズムの問題点は何ですか	学習データに正解が付いている場合の学習
0	1214	0906	Aprioriアルゴリズムの問題点は何ですか	多階層構造でもそのまま適用できます
0	1214	1310	Aprioriアルゴリズムの問題点は何ですか	確率的非決定性オートマトンの一種
0	1214	0402	Aprioriアルゴリズムの問題点は何ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1214	0811	Aprioriアルゴリズムの問題点は何ですか	半分の領域で勾配が1になるので
0	1214	0601	Aprioriアルゴリズムの問題点は何ですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	1214	1412	Aprioriアルゴリズムの問題点は何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1214	1108	Aprioriアルゴリズムの問題点は何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1214	1304	Aprioriアルゴリズムの問題点は何ですか	入力と対応させる素性
0	1214	0801	Aprioriアルゴリズムの問題点は何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1214	0610	Aprioriアルゴリズムの問題点は何ですか	片方を減らせば片方が増える
1	1214	1214	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1214	0811	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	ユニットの活性化関数を工夫する方法があります
0	1214	0614	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	回帰木と線形回帰の双方のよいところを取った方法
0	1214	0616	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1214	0305	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	仮説に対して課す制約
0	1214	0505	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	パーセプトロン
0	1214	1509	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1214	0717	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1214	0109	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	正解が付いていない場合の学習
0	1214	0417	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	ネットワークの構造とアークの条件付き確率
0	1214	0707	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1214	0305	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1214	1406	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1214	0502	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1214	0805	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1214	1508	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1214	0810	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	勾配消失問題
0	1214	0607	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	Lasso回帰
0	1214	0611	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	識別における決定木の考え方を回帰問題に適用する方法
0	1214	0906	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1214	1410	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	学習初期の誤りに強いということ
0	1214	1106	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	クラスタの重心間の距離を類似度とする
0	1214	0209	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	正例がどれだけ正しく判定されているかという指標
0	1214	0701	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	マージン
0	1214	0402	Aprioriアルゴリズムはなぜ計算量が膨大なのですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
1	1215	1215	FP-木とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	1215	0402	FP-木とは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1215	0701	FP-木とは何ですか	サポートベクトルマシン
0	1215	0808	FP-木とは何ですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1215	0911	FP-木とは何ですか	学習時の自由度を意図的に下げていること
0	1215	0901	FP-木とは何ですか	深層学習に用いるニューラルネットワーク
0	1215	1306	FP-木とは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	1215	0712	FP-木とは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	1215	0112	FP-木とは何ですか	線形回帰，回帰木，モデル木など
0	1215	1304	FP-木とは何ですか	出力系列を参照する素性
0	1215	1214	FP-木とは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1215	0612	FP-木とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	1215	0402	FP-木とは何ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1215	0313	FP-木とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1215	0803	FP-木とは何ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1215	0811	FP-木とは何ですか	引数が負のときは0，0以上のときはその値を出力
0	1215	1313	FP-木とは何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1215	1112	FP-木とは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1215	1506	FP-木とは何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1215	0903	FP-木とは何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1215	0710	FP-木とは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1215	1412	FP-木とは何ですか	近くのノードは同じクラスになりやすいという仮定
0	1215	1201	FP-木とは何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1215	0507	FP-木とは何ですか	パーセプトロンの収束定理
0	1215	0514	FP-木とは何ですか	ランダムに学習データを一つ
1	1219	1219	協調フィルタリングとはどのようなものですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1219	0405	協調フィルタリングとはどのようなものですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	1219	0114	協調フィルタリングとはどのようなものですか	階層的クラスタリングや k-means 法
0	1219	0111	協調フィルタリングとはどのようなものですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	1219	1403	協調フィルタリングとはどのようなものですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1219	1303	協調フィルタリングとはどのようなものですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	1219	1303	協調フィルタリングとはどのようなものですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1219	1303	協調フィルタリングとはどのようなものですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1219	0115	協調フィルタリングとはどのようなものですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1219	1301	協調フィルタリングとはどのようなものですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1219	0902	協調フィルタリングとはどのようなものですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	1219	0509	協調フィルタリングとはどのようなものですか	確率的最急勾配法
0	1219	1201	協調フィルタリングとはどのようなものですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1219	0802	協調フィルタリングとはどのようなものですか	特徴ベクトルの次元数
0	1219	0912	協調フィルタリングとはどのようなものですか	畳み込みニューラルネットワーク
0	1219	1012	協調フィルタリングとはどのようなものですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	1219	1506	協調フィルタリングとはどのようなものですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1219	0109	協調フィルタリングとはどのようなものですか	学習データに正解が付いている場合の学習
0	1219	0911	協調フィルタリングとはどのようなものですか	ドロップアウト
0	1219	0916	協調フィルタリングとはどのようなものですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	1219	0906	協調フィルタリングとはどのようなものですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1219	0715	協調フィルタリングとはどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	1219	0907	協調フィルタリングとはどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1219	1106	協調フィルタリングとはどのようなものですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1219	0508	協調フィルタリングとはどのようなものですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
1	1220	1220	Matrix Factorizationとは何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1220	0116	Matrix Factorizationとは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1220	0712	Matrix Factorizationとは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	1220	0704	Matrix Factorizationとは何ですか	以下の関数$L$の最小値を求めるという問題
0	1220	0805	Matrix Factorizationとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1220	0616	Matrix Factorizationとは何ですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1220	1214	Matrix Factorizationとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1220	0404	Matrix Factorizationとは何ですか	事前確率
0	1220	0209	Matrix Factorizationとは何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1220	0708	Matrix Factorizationとは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1220	0710	Matrix Factorizationとは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1220	1207	Matrix Factorizationとは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1220	0502	Matrix Factorizationとは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1220	0416	Matrix Factorizationとは何ですか	値が真となる確率を知りたいノードが表す変数
0	1220	0906	Matrix Factorizationとは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1220	1214	Matrix Factorizationとは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1220	1407	Matrix Factorizationとは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1220	1219	Matrix Factorizationとは何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1220	1410	Matrix Factorizationとは何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1220	0701	Matrix Factorizationとは何ですか	識別境界線と最も近いデータとの距離
0	1220	1313	Matrix Factorizationとは何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1220	0411	Matrix Factorizationとは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1220	1509	Matrix Factorizationとは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1220	0616	Matrix Factorizationとは何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	1220	0109	Matrix Factorizationとは何ですか	正解が付いていない場合の学習
1	1220	1220	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1220	0103	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	1220	0710	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1220	1302	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1220	1509	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1220	0811	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	引数が負のときは0，0以上のときはその値を出力
0	1220	0514	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	1220	1116	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1220	0810	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1220	1411	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1220	0907	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1220	1112	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1220	0802	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	入力層・出力層の数に応じた適当な数
0	1220	0704	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	ラグランジュの未定乗数法を不等式制約条件
0	1220	0701	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1220	0912	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	画像認識
0	1220	0701	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1220	1506	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1220	0611	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	識別における決定木の考え方を回帰問題に適用する方法
0	1220	0711	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1220	0810	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1220	1411	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1220	0404	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	事前確率
0	1220	0606	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	1220	0209	推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
1	1106	1106	Ward法はどういうものですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1106	0701	Ward法はどういうものですか	サポートベクトルマシン
0	1106	0607	Ward法はどういうものですか	「投げ縄」という意味
0	1106	0406	Ward法はどういうものですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1106	0811	Ward法はどういうものですか	ユニットの活性化関数を工夫する方法があります
0	1106	1306	Ward法はどういうものですか	条件付き確率場（Conditional Random Field: CRF）
0	1106	0801	Ward法はどういうものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1106	0906	Ward法はどういうものですか	多階層構造でもそのまま適用できます
0	1106	0701	Ward法はどういうものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1106	0708	Ward法はどういうものですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1106	0802	Ward法はどういうものですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1106	0803	Ward法はどういうものですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1106	0614	Ward法はどういうものですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	1106	0117	Ward法はどういうものですか	学習データの一部にだけ正解が与えられている場合
0	1106	0208	Ward法はどういうものですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	1106	0711	Ward法はどういうものですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1106	0612	Ward法はどういうものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1106	1302	Ward法はどういうものですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1106	0406	Ward法はどういうものですか	各クラスから生じる特徴の尤もらしさを表す
0	1106	1012	Ward法はどういうものですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1106	0802	Ward法はどういうものですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1106	0610	Ward法はどういうものですか	トレードオフの関係
0	1106	1313	Ward法はどういうものですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1106	0917	Ward法はどういうものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	1106	0607	Ward法はどういうものですか	Lasso回帰
1	1009	1009	通常の決定木とランダムフォレストとの違いは何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	1009	0701	通常の決定木とランダムフォレストとの違いは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1009	0611	通常の決定木とランダムフォレストとの違いは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1009	0512	通常の決定木とランダムフォレストとの違いは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1009	0906	通常の決定木とランダムフォレストとの違いは何ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	1009	1302	通常の決定木とランダムフォレストとの違いは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1009	0211	通常の決定木とランダムフォレストとの違いは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1009	1012	通常の決定木とランダムフォレストとの違いは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1009	0512	通常の決定木とランダムフォレストとの違いは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1009	1203	通常の決定木とランダムフォレストとの違いは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1009	0304	通常の決定木とランダムフォレストとの違いは何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1009	1116	通常の決定木とランダムフォレストとの違いは何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1009	0114	通常の決定木とランダムフォレストとの違いは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1009	0614	通常の決定木とランダムフォレストとの違いは何ですか	モデル木
0	1009	0805	通常の決定木とランダムフォレストとの違いは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1009	0513	通常の決定木とランダムフォレストとの違いは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1009	1402	通常の決定木とランダムフォレストとの違いは何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	1009	0901	通常の決定木とランダムフォレストとの違いは何ですか	音声認識・画像認識・自然言語処理など
0	1009	1001	通常の決定木とランダムフォレストとの違いは何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1009	1306	通常の決定木とランダムフォレストとの違いは何ですか	条件付き確率場（Conditional Random Field: CRF）
0	1009	0811	通常の決定木とランダムフォレストとの違いは何ですか	ユニットの活性化関数を工夫する方法
0	1009	0701	通常の決定木とランダムフォレストとの違いは何ですか	サポートベクトルマシン
0	1009	0504	通常の決定木とランダムフォレストとの違いは何ですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1009	0912	通常の決定木とランダムフォレストとの違いは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1009	1209	通常の決定木とランダムフォレストとの違いは何ですか	規則の条件部が起こったときに結論部が起こる割合
1	1010	1010	バギングやランダムフォレストとブースティングではどのような点で異なりますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1010	1012	バギングやランダムフォレストとブースティングではどのような点で異なりますか	各データに重みを付け，そのもとで識別器を作成します
0	1010	1106	バギングやランダムフォレストとブースティングではどのような点で異なりますか	クラスタの重心間の距離を類似度とする
0	1010	0912	バギングやランダムフォレストとブースティングではどのような点で異なりますか	畳み込みニューラルネットワーク
0	1010	0802	バギングやランダムフォレストとブースティングではどのような点で異なりますか	識別対象のクラス数
0	1010	0917	バギングやランダムフォレストとブースティングではどのような点で異なりますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	1010	0316	バギングやランダムフォレストとブースティングではどのような点で異なりますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1010	0111	バギングやランダムフォレストとブースティングではどのような点で異なりますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1010	0906	バギングやランダムフォレストとブースティングではどのような点で異なりますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1010	0611	バギングやランダムフォレストとブースティングではどのような点で異なりますか	識別における決定木の考え方を回帰問題に適用する方法
0	1010	0907	バギングやランダムフォレストとブースティングではどのような点で異なりますか	事前学習法
0	1010	0805	バギングやランダムフォレストとブースティングではどのような点で異なりますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1010	0611	バギングやランダムフォレストとブースティングではどのような点で異なりますか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1010	0810	バギングやランダムフォレストとブースティングではどのような点で異なりますか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1010	0901	バギングやランダムフォレストとブースティングではどのような点で異なりますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1010	0409	バギングやランダムフォレストとブースティングではどのような点で異なりますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1010	0901	バギングやランダムフォレストとブースティングではどのような点で異なりますか	表現学習
0	1010	0416	バギングやランダムフォレストとブースティングではどのような点で異なりますか	アークを無向とみなした結合を考えたとき
0	1010	1209	バギングやランダムフォレストとブースティングではどのような点で異なりますか	規則の条件部が起こったときに結論部が起こる割合
0	1010	0304	バギングやランダムフォレストとブースティングではどのような点で異なりますか	個々の事例から，あるクラスについて共通点を見つけること
0	1010	0601	バギングやランダムフォレストとブースティングではどのような点で異なりますか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	1010	0701	バギングやランダムフォレストとブースティングではどのような点で異なりますか	サポートベクトルマシン
0	1010	0510	バギングやランダムフォレストとブースティングではどのような点で異なりますか	特徴空間上でクラスを分割する面
0	1010	0710	バギングやランダムフォレストとブースティングではどのような点で異なりますか	もとの空間におけるデータ間の距離関係を保存
0	1010	0901	バギングやランダムフォレストとブースティングではどのような点で異なりますか	深層学習に用いるニューラルネットワーク
1	1403	1403	多様体仮定とは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1403	0502	多様体仮定とは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1403	0211	多様体仮定とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1403	0906	多様体仮定とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1403	0104	多様体仮定とは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1403	0505	多様体仮定とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1403	0908	多様体仮定とは何ですか	シグモイド関数
0	1403	0508	多様体仮定とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	1403	0803	多様体仮定とは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1403	0505	多様体仮定とは何ですか	パーセプトロン
0	1403	1106	多様体仮定とは何ですか	最も遠い事例対の距離を類似度とする
0	1403	1015	多様体仮定とは何ですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	1403	0509	多様体仮定とは何ですか	確率的最急勾配法
0	1403	0411	多様体仮定とは何ですか	確率のm推定という考え方を用います
0	1403	0114	多様体仮定とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1403	1409	多様体仮定とは何ですか	自分が出した誤りを指摘してくれる他人がいない
0	1403	0105	多様体仮定とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1403	0810	多様体仮定とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1403	0701	多様体仮定とは何ですか	識別境界線と最も近いデータとの距離
0	1403	0717	多様体仮定とは何ですか	Grid search
0	1403	1306	多様体仮定とは何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1403	1110	多様体仮定とは何ですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	1403	0111	多様体仮定とは何ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	1403	0305	多様体仮定とは何ですか	仮説に対して課す制約
0	1403	0715	多様体仮定とは何ですか	カーネルトリック
1	0406	0406	尤度とはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0406	1214	尤度とはなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0406	1001	尤度とはなんですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0406	1012	尤度とはなんですか	各データに重みを付け，そのもとで識別器を作成します
0	0406	0509	尤度とはなんですか	確率的最急勾配法
0	0406	0413	尤度とはなんですか	変数間の独立性を表現できること
0	0406	0908	尤度とはなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0406	0316	尤度とはなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0406	0811	尤度とはなんですか	ユニットの活性化関数を工夫する方法があります
0	0406	0313	尤度とはなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0406	0902	尤度とはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0406	0112	尤度とはなんですか	線形回帰，回帰木，モデル木など
0	0406	0307	尤度とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0406	0717	尤度とはなんですか	連続値
0	0406	0601	尤度とはなんですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0406	0109	尤度とはなんですか	正解が付いていない場合の学習
0	0406	0707	尤度とはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0406	0114	尤度とはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0406	0610	尤度とはなんですか	真のモデルとの距離
0	0406	0901	尤度とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0406	1201	尤度とはなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0406	0505	尤度とはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0406	0606	尤度とはなんですか	入力が少し変化したときに，出力も少し変化する
0	0406	0511	尤度とはなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0406	0606	尤度とはなんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
1	0409	0409	最尤推定法ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0409	1309	最尤推定法ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0409	0717	最尤推定法ってなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0409	1110	最尤推定法ってなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0409	0708	最尤推定法ってなんですか	制約を弱める変数
0	0409	0612	最尤推定法ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0409	0204	最尤推定法ってなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0409	0805	最尤推定法ってなんですか	誤差逆伝播法
0	0409	1015	最尤推定法ってなんですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0409	0717	最尤推定法ってなんですか	グリッド
0	0409	1110	最尤推定法ってなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0409	0707	最尤推定法ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0409	1306	最尤推定法ってなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0409	1209	最尤推定法ってなんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0409	0606	最尤推定法ってなんですか	山の尾根という意味
0	0409	0514	最尤推定法ってなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0409	1214	最尤推定法ってなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0409	0614	最尤推定法ってなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0409	1301	最尤推定法ってなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0409	0305	最尤推定法ってなんですか	仮説に対して課す制約
0	0409	1008	最尤推定法ってなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0409	1010	最尤推定法ってなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0409	0114	最尤推定法ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0409	0906	最尤推定法ってなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0409	0611	最尤推定法ってなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
1	0410	0410	ナイーブベイス識別法ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0410	0509	ナイーブベイス識別法ってなんですか	確率的最急勾配法
0	0410	1209	ナイーブベイス識別法ってなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0410	0412	ナイーブベイス識別法ってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0410	1309	ナイーブベイス識別法ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0410	0715	ナイーブベイス識別法ってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0410	0911	ナイーブベイス識別法ってなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0410	1104	ナイーブベイス識別法ってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0410	0116	ナイーブベイス識別法ってなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0410	1407	ナイーブベイス識別法ってなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0410	0912	ナイーブベイス識別法ってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0410	0311	ナイーブベイス識別法ってなんですか	集合の乱雑さ
0	0410	0901	ナイーブベイス識別法ってなんですか	深層学習に用いるニューラルネットワーク
0	0410	0402	ナイーブベイス識別法ってなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0410	0313	ナイーブベイス識別法ってなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0410	1406	ナイーブベイス識別法ってなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0410	0901	ナイーブベイス識別法ってなんですか	深層学習に用いるニューラルネットワーク
0	0410	0313	ナイーブベイス識別法ってなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0410	0204	ナイーブベイス識別法ってなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0410	0514	ナイーブベイス識別法ってなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0410	0901	ナイーブベイス識別法ってなんですか	音声認識・画像認識・自然言語処理など
0	0410	0114	ナイーブベイス識別法ってなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0410	1509	ナイーブベイス識別法ってなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0410	0704	ナイーブベイス識別法ってなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0410	0901	ナイーブベイス識別法ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
1	0411	0411	ゼロ頻度問題はどのような問題ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0411	0614	ゼロ頻度問題はどのような問題ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0411	0906	ゼロ頻度問題はどのような問題ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0411	1301	ゼロ頻度問題はどのような問題ですか	動画像の分類や音声で入力された単語の識別などの問題
0	0411	0712	ゼロ頻度問題はどのような問題ですか	カーネル関数が正定値関数という条件を満たすとき
0	0411	1209	ゼロ頻度問題はどのような問題ですか	規則の条件部が起こったときに結論部が起こる割合
0	0411	0404	ゼロ頻度問題はどのような問題ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0411	0611	ゼロ頻度問題はどのような問題ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0411	0412	ゼロ頻度問題はどのような問題ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0411	1401	ゼロ頻度問題はどのような問題ですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0411	0204	ゼロ頻度問題はどのような問題ですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0411	0717	ゼロ頻度問題はどのような問題ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0411	0610	ゼロ頻度問題はどのような問題ですか	学習結果の散らばり具合
0	0411	1412	ゼロ頻度問題はどのような問題ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0411	0313	ゼロ頻度問題はどのような問題ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0411	0901	ゼロ頻度問題はどのような問題ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0411	0710	ゼロ頻度問題はどのような問題ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0411	0612	ゼロ頻度問題はどのような問題ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0411	0505	ゼロ頻度問題はどのような問題ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0411	0912	ゼロ頻度問題はどのような問題ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0411	0204	ゼロ頻度問題はどのような問題ですか	特徴ベクトルの次元数を減らすこと
0	0411	0512	ゼロ頻度問題はどのような問題ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0411	0902	ゼロ頻度問題はどのような問題ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0411	0701	ゼロ頻度問題はどのような問題ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0411	1301	ゼロ頻度問題はどのような問題ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
1	0411	0411	ゼロ頻度問題はどうすれば回避できますか	確率のm推定という考え方を用います
0	0411	0908	ゼロ頻度問題はどうすれば回避できますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0411	1304	ゼロ頻度問題はどうすれば回避できますか	入力と対応させる素性
0	0411	0717	ゼロ頻度問題はどうすれば回避できますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0411	0315	ゼロ頻度問題はどうすれば回避できますか	分割後のデータの分散
0	0411	0416	ゼロ頻度問題はどうすれば回避できますか	値が真となる確率を知りたいノードが表す変数
0	0411	0701	ゼロ頻度問題はどうすれば回避できますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0411	0906	ゼロ頻度問題はどうすれば回避できますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0411	1408	ゼロ頻度問題はどうすれば回避できますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0411	0711	ゼロ頻度問題はどうすれば回避できますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0411	0513	ゼロ頻度問題はどうすれば回避できますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0411	1209	ゼロ頻度問題はどうすれば回避できますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0411	0307	ゼロ頻度問題はどうすれば回避できますか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0411	1012	ゼロ頻度問題はどうすれば回避できますか	すべてのデータの重みは平等
0	0411	0505	ゼロ頻度問題はどうすれば回避できますか	ニューラルネットワーク
0	0411	1406	ゼロ頻度問題はどうすれば回避できますか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0411	0508	ゼロ頻度問題はどうすれば回避できますか	二乗誤差を最小にするように識別関数を調整する方法
0	0411	0307	ゼロ頻度問題はどうすれば回避できますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0411	0803	ゼロ頻度問題はどうすれば回避できますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0411	0204	ゼロ頻度問題はどうすれば回避できますか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0411	0102	ゼロ頻度問題はどうすれば回避できますか	現在，人が行っている知的な判断を代わりに行う技術
0	0411	0105	ゼロ頻度問題はどうすれば回避できますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0411	1111	ゼロ頻度問題はどうすれば回避できますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0411	0901	ゼロ頻度問題はどうすれば回避できますか	深層学習に用いるニューラルネットワーク
0	0411	0717	ゼロ頻度問題はどうすれば回避できますか	連続値
1	0411	0411	確率のm推定ってなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0411	0917	確率のm推定ってなんですか	LSTM
0	0411	0412	確率のm推定ってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0411	0612	確率のm推定ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0411	0805	確率のm推定ってなんですか	誤差逆伝播法
0	0411	0505	確率のm推定ってなんですか	ニューラルネットワーク
0	0411	0507	確率のm推定ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0411	0802	確率のm推定ってなんですか	隠れ層
0	0411	0112	確率のm推定ってなんですか	線形回帰，回帰木，モデル木など
0	0411	0810	確率のm推定ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0411	0802	確率のm推定ってなんですか	特徴ベクトルの次元数
0	0411	1010	確率のm推定ってなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0411	0502	確率のm推定ってなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0411	0109	確率のm推定ってなんですか	正解が付いていない場合の学習
0	0411	0803	確率のm推定ってなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0411	1304	確率のm推定ってなんですか	入力と対応させる素性
0	0411	0901	確率のm推定ってなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0411	0801	確率のm推定ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0411	1503	確率のm推定ってなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0411	0209	確率のm推定ってなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0411	1403	確率のm推定ってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0411	0402	確率のm推定ってなんですか	統計的識別手法
0	0411	0602	確率のm推定ってなんですか	正解情報$y$が数値であるということ
0	0411	0307	確率のm推定ってなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0411	0803	確率のm推定ってなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
1	0412	0412	ベイジアンネットワークってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0412	0901	ベイジアンネットワークってなんですか	音声認識・画像認識・自然言語処理など
0	0412	1220	ベイジアンネットワークってなんですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0412	0901	ベイジアンネットワークってなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0412	0715	ベイジアンネットワークってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0412	0505	ベイジアンネットワークってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0412	0205	ベイジアンネットワークってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0412	0810	ベイジアンネットワークってなんですか	勾配消失問題
0	0412	1215	ベイジアンネットワークってなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0412	0612	ベイジアンネットワークってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0412	0802	ベイジアンネットワークってなんですか	多層パーセプトロンあるいはニューラルネットワーク
0	0412	0614	ベイジアンネットワークってなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0412	0801	ベイジアンネットワークってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0412	1508	ベイジアンネットワークってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0412	0903	ベイジアンネットワークってなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0412	0306	ベイジアンネットワークってなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0412	1219	ベイジアンネットワークってなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0412	0708	ベイジアンネットワークってなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0412	0607	ベイジアンネットワークってなんですか	Lasso回帰
0	0412	0606	ベイジアンネットワークってなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0412	0811	ベイジアンネットワークってなんですか	ユニットの活性化関数を工夫する方法があります
0	0412	0313	ベイジアンネットワークってなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0412	0704	ベイジアンネットワークってなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0412	0506	ベイジアンネットワークってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0412	0612	ベイジアンネットワークってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
1	0416	0416	目的変数ってなんですか	値が真となる確率を知りたいノードが表す変数
0	0416	0710	目的変数ってなんですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0416	0114	目的変数ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0416	1108	目的変数ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0416	1004	目的変数ってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0416	0413	目的変数ってなんですか	変数間の独立性を表現できること
0	0416	0715	目的変数ってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0416	1309	目的変数ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0416	0313	目的変数ってなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0416	0715	目的変数ってなんですか	カーネルトリック
0	0416	0411	目的変数ってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0416	0115	目的変数ってなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0416	1406	目的変数ってなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0416	0610	目的変数ってなんですか	学習結果の散らばり具合
0	0416	0717	目的変数ってなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0416	0711	目的変数ってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0416	0701	目的変数ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0416	0610	目的変数ってなんですか	真のモデルとの距離
0	0416	0701	目的変数ってなんですか	サポートベクトルマシン
0	0416	0111	目的変数ってなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0416	1509	目的変数ってなんですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0416	1402	目的変数ってなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0416	0610	目的変数ってなんですか	片方を減らせば片方が増える
0	0416	0906	目的変数ってなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0416	0803	目的変数ってなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
1	0105	0105	パターン認識ってなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0105	0205	パターン認識ってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0105	0402	パターン認識ってなんですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0105	0906	パターン認識ってなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0105	0406	パターン認識ってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0105	0711	パターン認識ってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0105	0701	パターン認識ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0105	0805	パターン認識ってなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0105	1506	パターン認識ってなんですか	後に得られる報酬ほど割り引いて計算するための係数
0	0105	1505	パターン認識ってなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0105	0710	パターン認識ってなんですか	低次元の特徴ベクトルを高次元に写像
0	0105	1302	パターン認識ってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0105	0710	パターン認識ってなんですか	もとの空間におけるデータ間の距離関係を保存
0	0105	1306	パターン認識ってなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0105	1306	パターン認識ってなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0105	0514	パターン認識ってなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0105	0701	パターン認識ってなんですか	識別境界線と最も近いデータとの距離
0	0105	1108	パターン認識ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0105	1110	パターン認識ってなんですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0105	1004	パターン認識ってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0105	0912	パターン認識ってなんですか	画像認識
0	0105	0810	パターン認識ってなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0105	0411	パターン認識ってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0105	0102	パターン認識ってなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0105	0911	パターン認識ってなんですか	学習時の自由度を意図的に下げていること
1	0701	0701	SVMってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0701	0111	SVMってなんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0701	0906	SVMってなんですか	十分多くの層を持つニューラルネットワーク
0	0701	0912	SVMってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0701	0805	SVMってなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0701	0514	SVMってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0701	1110	SVMってなんですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0701	0404	SVMってなんですか	事前確率
0	0701	1505	SVMってなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0701	0917	SVMってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0701	0703	SVMってなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0701	0316	SVMってなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0701	0410	SVMってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0701	0902	SVMってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0701	0109	SVMってなんですか	正解が付いていない場合の学習
0	0701	0410	SVMってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0701	0602	SVMってなんですか	正解情報$y$が数値であるということ
0	0701	0509	SVMってなんですか	確率的最急勾配法
0	0701	0908	SVMってなんですか	ユークリッド距離
0	0701	0811	SVMってなんですか	引数が負のときは0，0以上のときはその値を出力
0	0701	0901	SVMってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0701	1412	SVMってなんですか	近くのノードは同じクラスになりやすいという仮定
0	0701	0906	SVMってなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0701	0901	SVMってなんですか	表現学習
0	0701	0805	SVMってなんですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
1	0502	0502	ニューラルネットワークってなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0502	0205	ニューラルネットワークってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0502	0912	ニューラルネットワークってなんですか	畳み込みニューラルネットワーク
0	0502	0715	ニューラルネットワークってなんですか	カーネルトリック
0	0502	0508	ニューラルネットワークってなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0502	0508	ニューラルネットワークってなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0502	0514	ニューラルネットワークってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0502	0610	ニューラルネットワークってなんですか	真のモデルとの距離
0	0502	0204	ニューラルネットワークってなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0502	0810	ニューラルネットワークってなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0502	0506	ニューラルネットワークってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0502	0701	ニューラルネットワークってなんですか	学習データからのマージンが最大となる識別境界線
0	0502	1404	ニューラルネットワークってなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0502	0602	ニューラルネットワークってなんですか	正解情報$y$が数値であるということ
0	0502	0916	ニューラルネットワークってなんですか	リカレントニューラルネットワーク
0	0502	0406	ニューラルネットワークってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0502	0717	ニューラルネットワークってなんですか	連続値
0	0502	0510	ニューラルネットワークってなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0502	0801	ニューラルネットワークってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0502	0805	ニューラルネットワークってなんですか	誤差逆伝播法
0	0502	0611	ニューラルネットワークってなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0502	0801	ニューラルネットワークってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0502	1407	ニューラルネットワークってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0502	0103	ニューラルネットワークってなんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0502	0117	ニューラルネットワークってなんですか	学習データの一部にだけ正解が与えられている場合
1	0503	0503	正規分布ってなんですか	様々な数値データに対して多く用いられる統計モデル
0	0503	0610	正規分布ってなんですか	真のモデルとの距離
0	0503	0208	正規分布ってなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0503	0912	正規分布ってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0503	0810	正規分布ってなんですか	誤差が小さくなって消失してしまう
0	0503	0907	正規分布ってなんですか	事前学習法
0	0503	0715	正規分布ってなんですか	カーネルトリック
0	0503	1313	正規分布ってなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0503	0114	正規分布ってなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0503	1201	正規分布ってなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0503	1506	正規分布ってなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0503	0505	正規分布ってなんですか	パーセプトロン
0	0503	1309	正規分布ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0503	1506	正規分布ってなんですか	政策
0	0503	0109	正規分布ってなんですか	学習データに正解が付いている場合の学習
0	0503	0412	正規分布ってなんですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0503	1301	正規分布ってなんですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0503	0901	正規分布ってなんですか	深層学習に用いるニューラルネットワーク
0	0503	0304	正規分布ってなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0503	1112	正規分布ってなんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0503	0507	正規分布ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0503	0502	正規分布ってなんですか	SVM
0	0503	0810	正規分布ってなんですか	勾配消失問題
0	0503	0111	正規分布ってなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0503	0105	正規分布ってなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
1	0505	0505	識別モデルってなんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0505	0311	識別モデルってなんですか	集合の乱雑さ
0	0505	0104	識別モデルってなんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0505	0512	識別モデルってなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0505	1004	識別モデルってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0505	0917	識別モデルってなんですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0505	0412	識別モデルってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0505	0808	識別モデルってなんですか	入力の重み付き和の微分
0	0505	0416	識別モデルってなんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0505	0602	識別モデルってなんですか	正解情報$y$が数値であるということ
0	0505	0109	識別モデルってなんですか	正解が付いていない場合の学習
0	0505	0416	識別モデルってなんですか	アークを無向とみなした結合を考えたとき
0	0505	0707	識別モデルってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0505	1506	識別モデルってなんですか	政策
0	0505	0102	識別モデルってなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0505	0715	識別モデルってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0505	0610	識別モデルってなんですか	トレードオフの関係
0	0505	0911	識別モデルってなんですか	ランダムに一定割合のユニットを消して学習を行う
0	0505	0409	識別モデルってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0505	0901	識別モデルってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0505	1410	識別モデルってなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0505	1406	識別モデルってなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0505	1301	識別モデルってなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0505	0701	識別モデルってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0505	0717	識別モデルってなんですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
1	0505	0505	識別関数法ってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0505	0105	識別関数法ってなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0505	1409	識別関数法ってなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0505	0607	識別関数法ってなんですか	「投げ縄」という意味
0	0505	0305	識別関数法ってなんですか	仮説に対して課す制約
0	0505	1510	識別関数法ってなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0505	0901	識別関数法ってなんですか	音声認識・画像認識・自然言語処理など
0	0505	1214	識別関数法ってなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0505	0410	識別関数法ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0505	0405	識別関数法ってなんですか	尤度と事前確率の積を最大とするクラス
0	0505	0715	識別関数法ってなんですか	識別面
0	0505	1306	識別関数法ってなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0505	0704	識別関数法ってなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0505	0916	識別関数法ってなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0505	1010	識別関数法ってなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0505	0410	識別関数法ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0505	0602	識別関数法ってなんですか	正解情報$y$が数値であるということ
0	0505	0810	識別関数法ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0505	0510	識別関数法ってなんですか	特徴空間上でクラスを分割する面
0	0505	0713	識別関数法ってなんですか	文書分類やバイオインフォマティックスなど
0	0505	0502	識別関数法ってなんですか	SVM
0	0505	0115	識別関数法ってなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0505	0402	識別関数法ってなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0505	0315	識別関数法ってなんですか	分割後のデータの分散
0	0505	0402	識別関数法ってなんですか	事後確率
1	0505	0505	パーセプトロンってなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0505	0406	パーセプトロンってなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0505	0610	パーセプトロンってなんですか	真のモデルとの距離
0	0505	0912	パーセプトロンってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0505	0811	パーセプトロンってなんですか	引数が負のときは0，0以上のときはその値を出力
0	0505	0803	パーセプトロンってなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0505	1110	パーセプトロンってなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0505	1301	パーセプトロンってなんですか	連続音声認識
0	0505	0917	パーセプトロンってなんですか	LSTMセル
0	0505	0713	パーセプトロンってなんですか	文書分類やバイオインフォマティックスなど
0	0505	1506	パーセプトロンってなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0505	1502	パーセプトロンってなんですか	将棋や囲碁などを行うプログラム
0	0505	0610	パーセプトロンってなんですか	トレードオフの関係
0	0505	0616	パーセプトロンってなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0505	0908	パーセプトロンってなんですか	ユークリッド距離
0	0505	0512	パーセプトロンってなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0505	0906	パーセプトロンってなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0505	1112	パーセプトロンってなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0505	0402	パーセプトロンってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0505	0205	パーセプトロンってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0505	0305	パーセプトロンってなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0505	1502	パーセプトロンってなんですか	将棋や囲碁などを行うプログラム
0	0505	1009	パーセプトロンってなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0505	0109	パーセプトロンってなんですか	正解が付いていない場合の学習
0	0505	1404	パーセプトロンってなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
1	0507	0507	パーセプトロンの収束定理ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0507	1404	パーセプトロンの収束定理ってなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0507	1111	パーセプトロンの収束定理ってなんですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0507	1201	パーセプトロンの収束定理ってなんですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0507	1301	パーセプトロンの収束定理ってなんですか	形態素解析処理が典型的な問題
0	0507	0906	パーセプトロンの収束定理ってなんですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0507	1405	パーセプトロンの収束定理ってなんですか	半教師あり学習は文書分類問題によく適用されます
0	0507	1402	パーセプトロンの収束定理ってなんですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0507	0402	パーセプトロンの収束定理ってなんですか	最大事後確率則
0	0507	0111	パーセプトロンの収束定理ってなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0507	0802	パーセプトロンの収束定理ってなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0507	1110	パーセプトロンの収束定理ってなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0507	0307	パーセプトロンの収束定理ってなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0507	1205	パーセプトロンの収束定理ってなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0507	1407	パーセプトロンの収束定理ってなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0507	1104	パーセプトロンの収束定理ってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0507	1306	パーセプトロンの収束定理ってなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0507	0306	パーセプトロンの収束定理ってなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0507	1506	パーセプトロンの収束定理ってなんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0507	0911	パーセプトロンの収束定理ってなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0507	0906	パーセプトロンの収束定理ってなんですか	入力に近い側の処理
0	0507	0614	パーセプトロンの収束定理ってなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0507	1407	パーセプトロンの収束定理ってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0507	0916	パーセプトロンの収束定理ってなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0507	1304	パーセプトロンの収束定理ってなんですか	入力と対応させる素性
1	0508	0508	最小二乗法ってなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0508	0710	最小二乗法ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0508	0802	最小二乗法ってなんですか	入力層・出力層の数に応じた適当な数
0	0508	0906	最小二乗法ってなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0508	0512	最小二乗法ってなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0508	0404	最小二乗法ってなんですか	事前確率
0	0508	1301	最小二乗法ってなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0508	0701	最小二乗法ってなんですか	学習データからのマージンが最大となる識別境界線
0	0508	0902	最小二乗法ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0508	0903	最小二乗法ってなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0508	1010	最小二乗法ってなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0508	0205	最小二乗法ってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0508	0514	最小二乗法ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0508	0507	最小二乗法ってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0508	0307	最小二乗法ってなんですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0508	1010	最小二乗法ってなんですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0508	0805	最小二乗法ってなんですか	誤差逆伝播法
0	0508	0802	最小二乗法ってなんですか	識別対象のクラス数
0	0508	0116	最小二乗法ってなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0508	0906	最小二乗法ってなんですか	多階層構造でもそのまま適用できます
0	0508	0111	最小二乗法ってなんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0508	0510	最小二乗法ってなんですか	特徴空間上でクラスを分割する面
0	0508	0513	最小二乗法ってなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0508	0810	最小二乗法ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0508	0612	最小二乗法ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
1	0510	0510	識別面ってなんですか	特徴空間上でクラスを分割する面
0	0510	0704	識別面ってなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0510	0103	識別面ってなんですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0510	0906	識別面ってなんですか	十分多くの層を持つニューラルネットワーク
0	0510	0911	識別面ってなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0510	0802	識別面ってなんですか	多層パーセプトロンあるいはニューラルネットワーク
0	0510	1103	識別面ってなんですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0510	0801	識別面ってなんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0510	0606	識別面ってなんですか	山の尾根という意味
0	0510	0406	識別面ってなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0510	1205	識別面ってなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0510	0917	識別面ってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0510	0115	識別面ってなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0510	1007	識別面ってなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0510	0514	識別面ってなんですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0510	0801	識別面ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0510	0906	識別面ってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0510	0512	識別面ってなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0510	0917	識別面ってなんですか	LSTM
0	0510	0911	識別面ってなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0510	1407	識別面ってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0510	0109	識別面ってなんですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0510	0901	識別面ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0510	1001	識別面ってなんですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0510	0810	識別面ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
1	0701	0701	マージンってなんですか	識別境界線と最も近いデータとの距離
0	0701	0507	マージンってなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0701	0601	マージンってなんですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0701	0901	マージンってなんですか	Deep Neural Network (DNN) 
0	0701	0805	マージンってなんですか	誤差逆伝播法
0	0701	1201	マージンってなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0701	0407	マージンってなんですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0701	0405	マージンってなんですか	尤度と事前確率の積を最大とするクラス
0	0701	0404	マージンってなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0701	0810	マージンってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0701	0702	マージンってなんですか	識別面は平面を仮定する
0	0701	0712	マージンってなんですか	カーネル関数が正定値関数という条件を満たすとき
0	0701	1103	マージンってなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0701	1104	マージンってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0701	0508	マージンってなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0701	0602	マージンってなんですか	正解情報$y$が数値であるということ
0	0701	0610	マージンってなんですか	学習結果の散らばり具合
0	0701	1209	マージンってなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0701	0906	マージンってなんですか	十分多くの層を持つニューラルネットワーク
0	0701	0313	マージンってなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0701	1510	マージンってなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0701	0710	マージンってなんですか	低次元の特徴ベクトルを高次元に写像
0	0701	0917	マージンってなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0701	0304	マージンってなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0701	0612	マージンってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
1	0711	0711	カーネル関数ってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0711	0810	カーネル関数ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0711	0505	カーネル関数ってなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0711	0911	カーネル関数ってなんですか	ドロップアウト
0	0711	0505	カーネル関数ってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0711	0416	カーネル関数ってなんですか	値が真となる確率を知りたいノードが表す変数
0	0711	0710	カーネル関数ってなんですか	もとの空間におけるデータ間の距離関係を保存
0	0711	0811	カーネル関数ってなんですか	ユニットの活性化関数を工夫する方法があります
0	0711	1411	カーネル関数ってなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0711	0206	カーネル関数ってなんですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0711	0404	カーネル関数ってなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0711	0612	カーネル関数ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0711	0506	カーネル関数ってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0711	0612	カーネル関数ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0711	0810	カーネル関数ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0711	0204	カーネル関数ってなんですか	特徴ベクトルの次元数を減らすこと
0	0711	0114	カーネル関数ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0711	0407	カーネル関数ってなんですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0711	0304	カーネル関数ってなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0711	0115	カーネル関数ってなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0711	0901	カーネル関数ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0711	0409	カーネル関数ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0711	0607	カーネル関数ってなんですか	「投げ縄」という意味
0	0711	0605	カーネル関数ってなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0711	0114	カーネル関数ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	0708	0708	スラック変数とは何ですか	制約を弱める変数
0	0708	0901	スラック変数とは何ですか	表現学習
0	0708	1110	スラック変数とは何ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0708	1409	スラック変数とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0708	1209	スラック変数とは何ですか	この値が高いほど，得られる情報の多い規則であること
0	0708	0701	スラック変数とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0708	0805	スラック変数とは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0708	0715	スラック変数とは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0708	0707	スラック変数とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0708	0802	スラック変数とは何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0708	1505	スラック変数とは何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0708	0802	スラック変数とは何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0708	0710	スラック変数とは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0708	1405	スラック変数とは何ですか	半教師あり学習は文書分類問題によく適用されます
0	0708	0811	スラック変数とは何ですか	半分の領域で勾配が1になるので
0	0708	0115	スラック変数とは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0708	1001	スラック変数とは何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0708	0908	スラック変数とは何ですか	ユークリッド距離
0	0708	0204	スラック変数とは何ですか	特徴ベクトルの次元数を減らすこと
0	0708	0906	スラック変数とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0708	1301	スラック変数とは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0708	0710	スラック変数とは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0708	0502	スラック変数とは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0708	1507	スラック変数とは何ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0708	0111	スラック変数とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
1	0708	0708	なぜスラック変数が小さい方が良いのですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0708	0114	なぜスラック変数が小さい方が良いのですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0708	1502	なぜスラック変数が小さい方が良いのですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0708	0611	なぜスラック変数が小さい方が良いのですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0708	0612	なぜスラック変数が小さい方が良いのですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0708	0514	なぜスラック変数が小さい方が良いのですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0708	1505	なぜスラック変数が小さい方が良いのですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0708	0916	なぜスラック変数が小さい方が良いのですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0708	0205	なぜスラック変数が小さい方が良いのですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0708	0616	なぜスラック変数が小さい方が良いのですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0708	0912	なぜスラック変数が小さい方が良いのですか	畳み込みニューラルネットワーク
0	0708	0802	なぜスラック変数が小さい方が良いのですか	隠れ層
0	0708	0701	なぜスラック変数が小さい方が良いのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0708	0808	なぜスラック変数が小さい方が良いのですか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0708	0906	なぜスラック変数が小さい方が良いのですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0708	1106	なぜスラック変数が小さい方が良いのですか	最も近い事例対の距離を類似度とする
0	0708	0802	なぜスラック変数が小さい方が良いのですか	多層パーセプトロンあるいはニューラルネットワーク
0	0708	0115	なぜスラック変数が小さい方が良いのですか	パターンマイニング
0	0708	0711	なぜスラック変数が小さい方が良いのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0708	1009	なぜスラック変数が小さい方が良いのですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0708	0811	なぜスラック変数が小さい方が良いのですか	ユニットの活性化関数を工夫する方法
0	0708	1302	なぜスラック変数が小さい方が良いのですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0708	0505	なぜスラック変数が小さい方が良いのですか	パーセプトロン
0	0708	0514	なぜスラック変数が小さい方が良いのですか	ランダムに学習データを一つ
0	0708	1508	なぜスラック変数が小さい方が良いのですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
1	0715	0715	カーネルトリックとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0715	1010	カーネルトリックとは何ですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0715	0512	カーネルトリックとは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0715	0502	カーネルトリックとは何ですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	0715	0204	カーネルトリックとは何ですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0715	1502	カーネルトリックとは何ですか	将棋や囲碁などを行うプログラム
0	0715	0614	カーネルトリックとは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0715	0411	カーネルトリックとは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0715	0508	カーネルトリックとは何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0715	0711	カーネルトリックとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0715	0701	カーネルトリックとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0715	1403	カーネルトリックとは何ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0715	0907	カーネルトリックとは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0715	0514	カーネルトリックとは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0715	1302	カーネルトリックとは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0715	0305	カーネルトリックとは何ですか	仮説に対して課す制約
0	0715	0410	カーネルトリックとは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0715	0110	カーネルトリックとは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0715	0502	カーネルトリックとは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0715	0114	カーネルトリックとは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0715	0610	カーネルトリックとは何ですか	学習結果の散らばり具合
0	0715	0614	カーネルトリックとは何ですか	モデル木
0	0715	0116	カーネルトリックとは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	0715	0916	カーネルトリックとは何ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0715	0810	カーネルトリックとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
1	0801	0801	ニューラルネットワークとは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0801	1106	ニューラルネットワークとは何ですか	クラスタの重心間の距離を類似度とする
0	0801	0715	ニューラルネットワークとは何ですか	識別面
0	0801	0912	ニューラルネットワークとは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0801	0701	ニューラルネットワークとは何ですか	サポートベクトルマシン
0	0801	1104	ニューラルネットワークとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0801	0802	ニューラルネットワークとは何ですか	特徴空間上では線形識別面を設定すること
0	0801	0713	ニューラルネットワークとは何ですか	文書分類やバイオインフォマティックスなど
0	0801	0305	ニューラルネットワークとは何ですか	仮説に対して課す制約
0	0801	1214	ニューラルネットワークとは何ですか	計算量が膨大であること
0	0801	0917	ニューラルネットワークとは何ですか	LSTM
0	0801	0410	ニューラルネットワークとは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0801	0313	ニューラルネットワークとは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0801	0902	ニューラルネットワークとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0801	0105	ニューラルネットワークとは何ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0801	1112	ニューラルネットワークとは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0801	0114	ニューラルネットワークとは何ですか	階層的クラスタリングや k-means 法
0	0801	0701	ニューラルネットワークとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0801	0514	ニューラルネットワークとは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0801	0711	ニューラルネットワークとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0801	0114	ニューラルネットワークとは何ですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0801	0104	ニューラルネットワークとは何ですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0801	1008	ニューラルネットワークとは何ですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0801	0512	ニューラルネットワークとは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0801	0906	ニューラルネットワークとは何ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
1	0810	0810	勾配消失問題とはどのような問題ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0810	1302	勾配消失問題とはどのような問題ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0810	0607	勾配消失問題とはどのような問題ですか	Lasso回帰
0	0810	1408	勾配消失問題とはどのような問題ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0810	0805	勾配消失問題とはどのような問題ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0810	0611	勾配消失問題とはどのような問題ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0810	0612	勾配消失問題とはどのような問題ですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0810	0502	勾配消失問題とはどのような問題ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0810	1114	勾配消失問題とはどのような問題ですか	クラスタリング結果のデータ数の分布
0	0810	0701	勾配消失問題とはどのような問題ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0810	0209	勾配消失問題とはどのような問題ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0810	1015	勾配消失問題とはどのような問題ですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0810	1010	勾配消失問題とはどのような問題ですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0810	0912	勾配消失問題とはどのような問題ですか	画像認識
0	0810	0102	勾配消失問題とはどのような問題ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0810	1116	勾配消失問題とはどのような問題ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0810	1001	勾配消失問題とはどのような問題ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0810	0715	勾配消失問題とはどのような問題ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0810	0903	勾配消失問題とはどのような問題ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0810	1404	勾配消失問題とはどのような問題ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0810	1220	勾配消失問題とはどのような問題ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0810	1403	勾配消失問題とはどのような問題ですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0810	0901	勾配消失問題とはどのような問題ですか	表現学習
0	0810	0606	勾配消失問題とはどのような問題ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0810	1303	勾配消失問題とはどのような問題ですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
1	0811	0811	どのようにして勾配消失問題を解決しますか	ユニットの活性化関数を工夫する方法があります
0	0811	0701	どのようにして勾配消失問題を解決しますか	マージン
0	0811	0707	どのようにして勾配消失問題を解決しますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0811	0717	どのようにして勾配消失問題を解決しますか	グリッド
0	0811	0606	どのようにして勾配消失問題を解決しますか	山の尾根という意味
0	0811	0611	どのようにして勾配消失問題を解決しますか	識別における決定木の考え方を回帰問題に適用する方法
0	0811	0703	どのようにして勾配消失問題を解決しますか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0811	1304	どのようにして勾配消失問題を解決しますか	出力系列を参照する素性
0	0811	0116	どのようにして勾配消失問題を解決しますか	学習データが教師あり／教師なしの混在となっているもの
0	0811	0710	どのようにして勾配消失問題を解決しますか	低次元の特徴ベクトルを高次元に写像
0	0811	0514	どのようにして勾配消失問題を解決しますか	ランダムに学習データを一つ
0	0811	0903	どのようにして勾配消失問題を解決しますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0811	0710	どのようにして勾配消失問題を解決しますか	もとの空間におけるデータ間の距離関係を保存
0	0811	1510	どのようにして勾配消失問題を解決しますか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0811	1114	どのようにして勾配消失問題を解決しますか	クラスタリング結果のデータ数の分布
0	0811	1116	どのようにして勾配消失問題を解決しますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0811	1106	どのようにして勾配消失問題を解決しますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0811	1503	どのようにして勾配消失問題を解決しますか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0811	0701	どのようにして勾配消失問題を解決しますか	サポートベクトルマシン
0	0811	0404	どのようにして勾配消失問題を解決しますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0811	0616	どのようにして勾配消失問題を解決しますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0811	0607	どのようにして勾配消失問題を解決しますか	Lasso回帰
0	0811	0601	どのようにして勾配消失問題を解決しますか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0811	1008	どのようにして勾配消失問題を解決しますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0811	1012	どのようにして勾配消失問題を解決しますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
1	0901	0901	深層学習はどのようなものですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0901	0907	深層学習はどのようなものですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0901	0315	深層学習はどのようなものですか	分割後のデータの分散
0	0901	0908	深層学習はどのようなものですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0901	0710	深層学習はどのようなものですか	もとの空間におけるデータ間の距離関係を保存
0	0901	0610	深層学習はどのようなものですか	学習結果の散らばり具合
0	0901	1004	深層学習はどのようなものですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0901	0712	深層学習はどのようなものですか	カーネル関数が正定値関数という条件を満たすとき
0	0901	0708	深層学習はどのようなものですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0901	1402	深層学習はどのようなものですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0901	0404	深層学習はどのようなものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0901	1301	深層学習はどのようなものですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0901	0103	深層学習はどのようなものですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0901	0701	深層学習はどのようなものですか	識別境界線と最も近いデータとの距離
0	0901	0109	深層学習はどのようなものですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0901	0103	深層学習はどのようなものですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0901	1405	深層学習はどのようなものですか	半教師あり学習は文書分類問題によく適用されます
0	0901	1310	深層学習はどのようなものですか	確率的非決定性オートマトンの一種
0	0901	0514	深層学習はどのようなものですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0901	1215	深層学習はどのようなものですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0901	1104	深層学習はどのようなものですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0901	0505	深層学習はどのようなものですか	パーセプトロン
0	0901	0611	深層学習はどのようなものですか	識別における決定木の考え方を回帰問題に適用する方法
0	0901	0715	深層学習はどのようなものですか	識別面
0	0901	0701	深層学習はどのようなものですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
1	0901	0901	深層学習はどのような場面で応用されますか	音声認識・画像認識・自然言語処理など
0	0901	1209	深層学習はどのような場面で応用されますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0901	0209	深層学習はどのような場面で応用されますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0901	0508	深層学習はどのような場面で応用されますか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0901	0715	深層学習はどのような場面で応用されますか	複雑な非線形変換を求めるという操作を避ける方法
0	0901	0803	深層学習はどのような場面で応用されますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0901	1304	深層学習はどのような場面で応用されますか	入力と対応させる素性
0	0901	1505	深層学習はどのような場面で応用されますか	「マルコフ性」を持つ確率過程における意思決定問題
0	0901	1402	深層学習はどのような場面で応用されますか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0901	0205	深層学習はどのような場面で応用されますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0901	0502	深層学習はどのような場面で応用されますか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0901	1205	深層学習はどのような場面で応用されますか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0901	0416	深層学習はどのような場面で応用されますか	値が真となる確率を知りたいノードが表す変数
0	0901	1306	深層学習はどのような場面で応用されますか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0901	0114	深層学習はどのような場面で応用されますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0901	1214	深層学習はどのような場面で応用されますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0901	1207	深層学習はどのような場面で応用されますか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0901	0907	深層学習はどのような場面で応用されますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0901	0802	深層学習はどのような場面で応用されますか	隠れ層
0	0901	0514	深層学習はどのような場面で応用されますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0901	0111	深層学習はどのような場面で応用されますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0901	0407	深層学習はどのような場面で応用されますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0901	0306	深層学習はどのような場面で応用されますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0901	0908	深層学習はどのような場面で応用されますか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0901	0710	深層学習はどのような場面で応用されますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
1	0901	0901	表現学習ってなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0901	0601	表現学習ってなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0901	1508	表現学習ってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0901	1001	表現学習ってなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0901	0402	表現学習ってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0901	0906	表現学習ってなんですか	十分多くの層
0	0901	0115	表現学習ってなんですか	パターンマイニング
0	0901	0902	表現学習ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	0508	表現学習ってなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0901	0711	表現学習ってなんですか	カーネル関数
0	0901	0305	表現学習ってなんですか	仮説に対して課す制約
0	0901	0902	表現学習ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	0416	表現学習ってなんですか	値が真となる確率を知りたいノードが表す変数
0	0901	0801	表現学習ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0901	0411	表現学習ってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0901	0701	表現学習ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	0614	表現学習ってなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0901	0805	表現学習ってなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0901	0917	表現学習ってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0901	0810	表現学習ってなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0901	1108	表現学習ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0901	1509	表現学習ってなんですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0901	0112	表現学習ってなんですか	線形回帰，回帰木，モデル木など
0	0901	1502	表現学習ってなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0901	0405	表現学習ってなんですか	尤度と事前確率の積を最大とするクラス
1	0912	0912	畳み込みネットワークとは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0912	1108	畳み込みネットワークとは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0912	0612	畳み込みネットワークとは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0912	0911	畳み込みネットワークとは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0912	1408	畳み込みネットワークとは何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0912	0808	畳み込みネットワークとは何ですか	シグモイド関数の微分
0	0912	0901	畳み込みネットワークとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0912	0715	畳み込みネットワークとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0912	0610	畳み込みネットワークとは何ですか	トレードオフの関係
0	0912	0902	畳み込みネットワークとは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0912	0802	畳み込みネットワークとは何ですか	識別対象のクラス数
0	0912	0811	畳み込みネットワークとは何ですか	ユニットの活性化関数を工夫する方法
0	0912	0209	畳み込みネットワークとは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0912	0917	畳み込みネットワークとは何ですか	LSTM
0	0912	0701	畳み込みネットワークとは何ですか	マージン
0	0912	0611	畳み込みネットワークとは何ですか	回帰
0	0912	0811	畳み込みネットワークとは何ですか	引数が負のときは0，0以上のときはその値を出力
0	0912	0710	畳み込みネットワークとは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0912	0606	畳み込みネットワークとは何ですか	入力が少し変化したときに，出力も少し変化する
0	0912	0113	畳み込みネットワークとは何ですか	入力データに潜む規則性を学習すること
0	0912	0114	畳み込みネットワークとは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0912	0402	畳み込みネットワークとは何ですか	最大事後確率則
0	0912	0211	畳み込みネットワークとは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0912	0510	畳み込みネットワークとは何ですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0912	1104	畳み込みネットワークとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
1	0907	0907	事前学習とは何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0907	0803	事前学習とは何ですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0907	0901	事前学習とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0907	0104	事前学習とは何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0907	0307	事前学習とは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0907	0616	事前学習とは何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0907	0901	事前学習とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0907	0110	事前学習とは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0907	0610	事前学習とは何ですか	真のモデルとの距離
0	0907	1209	事前学習とは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	0907	1509	事前学習とは何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0907	1220	事前学習とは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0907	0906	事前学習とは何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0907	1410	事前学習とは何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0907	1001	事前学習とは何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0907	1408	事前学習とは何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0907	0917	事前学習とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0907	0802	事前学習とは何ですか	特徴空間上では線形識別面を設定すること
0	0907	0509	事前学習とは何ですか	確率的最急勾配法
0	0907	1007	事前学習とは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0907	1301	事前学習とは何ですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0907	1404	事前学習とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0907	1508	事前学習とは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0907	1509	事前学習とは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0907	1110	事前学習とは何ですか	事前にクラスタ数$k$を固定しなければいけないという問題点
1	0911	0911	ドロップアウトとは何ですか	ランダムに一定割合のユニットを消して学習を行う
0	0911	0802	ドロップアウトとは何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0911	0912	ドロップアウトとは何ですか	畳み込みニューラルネットワーク
0	0911	0502	ドロップアウトとは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0911	0811	ドロップアウトとは何ですか	ユニットの活性化関数を工夫する方法があります
0	0911	0409	ドロップアウトとは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0911	0402	ドロップアウトとは何ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0911	1111	ドロップアウトとは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0911	0901	ドロップアウトとは何ですか	音声認識・画像認識・自然言語処理など
0	0911	0717	ドロップアウトとは何ですか	連続値
0	0911	1116	ドロップアウトとは何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0911	0802	ドロップアウトとは何ですか	特徴ベクトルの次元数
0	0911	0105	ドロップアウトとは何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0911	0710	ドロップアウトとは何ですか	もとの空間におけるデータ間の距離関係を保存
0	0911	1106	ドロップアウトとは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0911	0311	ドロップアウトとは何ですか	集合の乱雑さ
0	0911	0115	ドロップアウトとは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0911	0111	ドロップアウトとは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0911	0110	ドロップアウトとは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0911	0711	ドロップアウトとは何ですか	カーネル関数
0	0911	0402	ドロップアウトとは何ですか	事後確率
0	0911	1404	ドロップアウトとは何ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0911	0906	ドロップアウトとは何ですか	十分多くの層
0	0911	0906	ドロップアウトとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0911	1310	ドロップアウトとは何ですか	Hidden Marcov Model: 隠れマルコフモデル
1	0911	0911	なぜドロップアウトによって過学習が回避できるのですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0911	1009	なぜドロップアウトによって過学習が回避できるのですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0911	0502	なぜドロップアウトによって過学習が回避できるのですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0911	1408	なぜドロップアウトによって過学習が回避できるのですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0911	1306	なぜドロップアウトによって過学習が回避できるのですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0911	1407	なぜドロップアウトによって過学習が回避できるのですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0911	0711	なぜドロップアウトによって過学習が回避できるのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0911	0610	なぜドロップアウトによって過学習が回避できるのですか	真のモデルとの距離
0	0911	0907	なぜドロップアウトによって過学習が回避できるのですか	事前学習法
0	0911	0110	なぜドロップアウトによって過学習が回避できるのですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0911	0306	なぜドロップアウトによって過学習が回避できるのですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0911	1106	なぜドロップアウトによって過学習が回避できるのですか	最も遠い事例対の距離を類似度とする
0	0911	0903	なぜドロップアウトによって過学習が回避できるのですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0911	0707	なぜドロップアウトによって過学習が回避できるのですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0911	0514	なぜドロップアウトによって過学習が回避できるのですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0911	1506	なぜドロップアウトによって過学習が回避できるのですか	政策
0	0911	0711	なぜドロップアウトによって過学習が回避できるのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0911	0114	なぜドロップアウトによって過学習が回避できるのですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0911	0103	なぜドロップアウトによって過学習が回避できるのですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0911	1001	なぜドロップアウトによって過学習が回避できるのですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0911	0715	なぜドロップアウトによって過学習が回避できるのですか	複雑な非線形変換を求めるという操作を避ける方法
0	0911	0717	なぜドロップアウトによって過学習が回避できるのですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0911	0701	なぜドロップアウトによって過学習が回避できるのですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0911	0810	なぜドロップアウトによって過学習が回避できるのですか	2006 年頃に考案された事前学習法
0	0911	0514	なぜドロップアウトによって過学習が回避できるのですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
1	0911	0911	ドロップアウトで過学習が防げるのはなぜですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0911	0906	ドロップアウトで過学習が防げるのはなぜですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0911	1205	ドロップアウトで過学習が防げるのはなぜですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0911	0402	ドロップアウトで過学習が防げるのはなぜですか	事後確率が最大となるクラスを識別結果とする方法
0	0911	0205	ドロップアウトで過学習が防げるのはなぜですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0911	0701	ドロップアウトで過学習が防げるのはなぜですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0911	0305	ドロップアウトで過学習が防げるのはなぜですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0911	1111	ドロップアウトで過学習が防げるのはなぜですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0911	1301	ドロップアウトで過学習が防げるのはなぜですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0911	0610	ドロップアウトで過学習が防げるのはなぜですか	真のモデルとの距離
0	0911	1010	ドロップアウトで過学習が防げるのはなぜですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0911	1004	ドロップアウトで過学習が防げるのはなぜですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0911	0701	ドロップアウトで過学習が防げるのはなぜですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0911	0901	ドロップアウトで過学習が防げるのはなぜですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0911	0514	ドロップアウトで過学習が防げるのはなぜですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0911	0307	ドロップアウトで過学習が防げるのはなぜですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0911	0601	ドロップアウトで過学習が防げるのはなぜですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0911	0607	ドロップアウトで過学習が防げるのはなぜですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0911	0901	ドロップアウトで過学習が防げるのはなぜですか	深層学習に用いるニューラルネットワーク
0	0911	0606	ドロップアウトで過学習が防げるのはなぜですか	山の尾根という意味
0	0911	0711	ドロップアウトで過学習が防げるのはなぜですか	カーネル関数
0	0911	1503	ドロップアウトで過学習が防げるのはなぜですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0911	0402	ドロップアウトで過学習が防げるのはなぜですか	統計的識別手法
0	0911	0702	ドロップアウトで過学習が防げるのはなぜですか	識別面は平面を仮定する
0	0911	0111	ドロップアウトで過学習が防げるのはなぜですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
1	0916	0916	リカレントネットワークの特徴は何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0916	0307	リカレントネットワークの特徴は何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0916	1303	リカレントネットワークの特徴は何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0916	1103	リカレントネットワークの特徴は何ですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0916	1502	リカレントネットワークの特徴は何ですか	将棋や囲碁などを行うプログラム
0	0916	0416	リカレントネットワークの特徴は何ですか	アークを無向とみなした結合を考えたとき
0	0916	1310	リカレントネットワークの特徴は何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0916	1214	リカレントネットワークの特徴は何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0916	1214	リカレントネットワークの特徴は何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0916	1410	リカレントネットワークの特徴は何ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0916	1201	リカレントネットワークの特徴は何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0916	1310	リカレントネットワークの特徴は何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0916	1505	リカレントネットワークの特徴は何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0916	0306	リカレントネットワークの特徴は何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0916	0409	リカレントネットワークの特徴は何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0916	0708	リカレントネットワークの特徴は何ですか	制約を弱める変数
0	0916	1209	リカレントネットワークの特徴は何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0916	0402	リカレントネットワークの特徴は何ですか	事後確率が最大となるクラスを識別結果とする方法
0	0916	1001	リカレントネットワークの特徴は何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0916	0906	リカレントネットワークの特徴は何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0916	0805	リカレントネットワークの特徴は何ですか	誤差逆伝播法
0	0916	0612	リカレントネットワークの特徴は何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0916	0507	リカレントネットワークの特徴は何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0916	0607	リカレントネットワークの特徴は何ですか	「投げ縄」という意味
0	0916	1001	リカレントネットワークの特徴は何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0901	0901	DNNってなんですか	深層学習に用いるニューラルネットワーク
0	0901	1220	DNNってなんですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0901	0306	DNNってなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0901	0805	DNNってなんですか	誤差逆伝播法
0	0901	0209	DNNってなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0901	0506	DNNってなんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0901	0111	DNNってなんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0901	0711	DNNってなんですか	カーネル関数
0	0901	0801	DNNってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0901	1012	DNNってなんですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0901	0911	DNNってなんですか	ドロップアウト
0	0901	0701	DNNってなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0901	0607	DNNってなんですか	「投げ縄」という意味
0	0901	0507	DNNってなんですか	パーセプトロンの収束定理
0	0901	0508	DNNってなんですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0901	0701	DNNってなんですか	識別境界線と最も近いデータとの距離
0	0901	0703	DNNってなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0901	0711	DNNってなんですか	カーネル関数
0	0901	0416	DNNってなんですか	アークを無向とみなした結合を考えたとき
0	0901	0916	DNNってなんですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0901	0313	DNNってなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0901	0206	DNNってなんですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0901	1403	DNNってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0901	0410	DNNってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0901	0305	DNNってなんですか	仮説に対して課す制約
1	0906	0906	多階層における誤差伝播法の問題点は何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0906	1106	多階層における誤差伝播法の問題点は何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0906	0616	多階層における誤差伝播法の問題点は何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0906	0605	多階層における誤差伝播法の問題点は何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0906	0916	多階層における誤差伝播法の問題点は何ですか	リカレントニューラルネットワーク
0	0906	0916	多階層における誤差伝播法の問題点は何ですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0906	0406	多階層における誤差伝播法の問題点は何ですか	各クラスから生じる特徴の尤もらしさを表す
0	0906	0908	多階層における誤差伝播法の問題点は何ですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0906	1110	多階層における誤差伝播法の問題点は何ですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0906	0602	多階層における誤差伝播法の問題点は何ですか	正解情報$y$が数値であるということ
0	0906	0209	多階層における誤差伝播法の問題点は何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0906	0715	多階層における誤差伝播法の問題点は何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0906	0109	多階層における誤差伝播法の問題点は何ですか	学習データに正解が付いている場合の学習
0	0906	0109	多階層における誤差伝播法の問題点は何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0906	0402	多階層における誤差伝播法の問題点は何ですか	事後確率が最大となるクラスを識別結果とする方法
0	0906	0912	多階層における誤差伝播法の問題点は何ですか	画像認識
0	0906	0606	多階層における誤差伝播法の問題点は何ですか	入力が少し変化したときに，出力も少し変化する
0	0906	1015	多階層における誤差伝播法の問題点は何ですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0906	1012	多階層における誤差伝播法の問題点は何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0906	1106	多階層における誤差伝播法の問題点は何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0906	0901	多階層における誤差伝播法の問題点は何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0906	0801	多階層における誤差伝播法の問題点は何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0906	0901	多階層における誤差伝播法の問題点は何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0906	0711	多階層における誤差伝播法の問題点は何ですか	カーネル関数
0	0906	0801	多階層における誤差伝播法の問題点は何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	0906	0906	誤差逆伝播法の問題点は何がありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0906	0112	誤差逆伝播法の問題点は何がありますか	線形回帰，回帰木，モデル木など
0	0906	0206	誤差逆伝播法の問題点は何がありますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0906	1501	誤差逆伝播法の問題点は何がありますか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0906	0614	誤差逆伝播法の問題点は何がありますか	回帰木と線形回帰の双方のよいところを取った方法
0	0906	0117	誤差逆伝播法の問題点は何がありますか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0906	1502	誤差逆伝播法の問題点は何がありますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0906	1508	誤差逆伝播法の問題点は何がありますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0906	0205	誤差逆伝播法の問題点は何がありますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0906	0607	誤差逆伝播法の問題点は何がありますか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0906	0404	誤差逆伝播法の問題点は何がありますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0906	0601	誤差逆伝播法の問題点は何がありますか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0906	0710	誤差逆伝播法の問題点は何がありますか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0906	0811	誤差逆伝播法の問題点は何がありますか	誤差が消失しません
0	0906	0505	誤差逆伝播法の問題点は何がありますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0906	0311	誤差逆伝播法の問題点は何がありますか	集合の乱雑さ
0	0906	0104	誤差逆伝播法の問題点は何がありますか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0906	0405	誤差逆伝播法の問題点は何がありますか	尤度と事前確率の積を最大とするクラス
0	0906	0508	誤差逆伝播法の問題点は何がありますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0906	1209	誤差逆伝播法の問題点は何がありますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0906	0507	誤差逆伝播法の問題点は何がありますか	全ての誤りがなくなることが学習の終了条件なので
0	0906	0512	誤差逆伝播法の問題点は何がありますか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0906	0715	誤差逆伝播法の問題点は何がありますか	識別面
0	0906	0405	誤差逆伝播法の問題点は何がありますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0906	1301	誤差逆伝播法の問題点は何がありますか	連続音声認識
1	0917	0917	LSTMとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0917	0313	LSTMとは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0917	1506	LSTMとは何ですか	政策
0	0917	0701	LSTMとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0917	0512	LSTMとは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0917	1214	LSTMとは何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0917	1304	LSTMとは何ですか	入力と対応させる素性
0	0917	0906	LSTMとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0917	0406	LSTMとは何ですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0917	0305	LSTMとは何ですか	仮説に対して課す制約
0	0917	0205	LSTMとは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0917	1407	LSTMとは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0917	0504	LSTMとは何ですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0917	1106	LSTMとは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0917	0711	LSTMとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0917	0611	LSTMとは何ですか	回帰
0	0917	1007	LSTMとは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0917	0611	LSTMとは何ですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0917	0811	LSTMとは何ですか	半分の領域で勾配が1になるので
0	0917	0801	LSTMとは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0917	1106	LSTMとは何ですか	最も遠い事例対の距離を類似度とする
0	0917	0111	LSTMとは何ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0917	0411	LSTMとは何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0917	0912	LSTMとは何ですか	畳み込みニューラルネットワーク
0	0917	1214	LSTMとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
1	0917	0917	LSTMと通常のユニットの違いは何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0917	0612	LSTMと通常のユニットの違いは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0917	0803	LSTMと通常のユニットの違いは何ですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0917	0508	LSTMと通常のユニットの違いは何ですか	最小二乗法
0	0917	0405	LSTMと通常のユニットの違いは何ですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0917	0901	LSTMと通常のユニットの違いは何ですか	音声認識・画像認識・自然言語処理など
0	0917	0209	LSTMと通常のユニットの違いは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0917	0116	LSTMと通常のユニットの違いは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	0917	0911	LSTMと通常のユニットの違いは何ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0917	0114	LSTMと通常のユニットの違いは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0917	0715	LSTMと通常のユニットの違いは何ですか	カーネルトリック
0	0917	0906	LSTMと通常のユニットの違いは何ですか	十分多くの層
0	0917	0508	LSTMと通常のユニットの違いは何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0917	0410	LSTMと通常のユニットの違いは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0917	1108	LSTMと通常のユニットの違いは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0917	1103	LSTMと通常のユニットの違いは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0917	1509	LSTMと通常のユニットの違いは何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0917	0802	LSTMと通常のユニットの違いは何ですか	隠れ層
0	0917	1506	LSTMと通常のユニットの違いは何ですか	最適政策$\pi^*$を獲得すること
0	0917	0512	LSTMと通常のユニットの違いは何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0917	0614	LSTMと通常のユニットの違いは何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0917	1007	LSTMと通常のユニットの違いは何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0917	1214	LSTMと通常のユニットの違いは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0917	0701	LSTMと通常のユニットの違いは何ですか	識別境界線と最も近いデータとの距離
0	0917	1409	LSTMと通常のユニットの違いは何ですか	自分が出した誤りを指摘してくれる他人がいない
1	0512	0512	ロジスティック識別器ってなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0512	0903	ロジスティック識別器ってなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0512	0505	ロジスティック識別器ってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0512	0610	ロジスティック識別器ってなんですか	学習結果の散らばり具合
0	0512	0410	ロジスティック識別器ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0512	0402	ロジスティック識別器ってなんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0512	1410	ロジスティック識別器ってなんですか	学習初期の誤りに強いということ
0	0512	0916	ロジスティック識別器ってなんですか	リカレントニューラルネットワーク
0	0512	0704	ロジスティック識別器ってなんですか	ラグランジュの未定乗数法
0	0512	0211	ロジスティック識別器ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0512	1402	ロジスティック識別器ってなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0512	1106	ロジスティック識別器ってなんですか	最も近い事例対の距離を類似度とする
0	0512	1506	ロジスティック識別器ってなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0512	0702	ロジスティック識別器ってなんですか	識別面は平面を仮定する
0	0512	0514	ロジスティック識別器ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0512	0114	ロジスティック識別器ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0512	1012	ロジスティック識別器ってなんですか	すべてのデータの重みは平等
0	0512	0917	ロジスティック識別器ってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0512	0906	ロジスティック識別器ってなんですか	入力に近い側の処理
0	0512	1409	ロジスティック識別器ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0512	0717	ロジスティック識別器ってなんですか	連続値
0	0512	1001	ロジスティック識別器ってなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0512	0602	ロジスティック識別器ってなんですか	正解情報$y$が数値であるということ
0	0512	0803	ロジスティック識別器ってなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0512	0513	ロジスティック識別器ってなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
1	0512	0512	最急勾配法ってなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0512	0902	最急勾配法ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0512	1506	最急勾配法ってなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0512	0802	最急勾配法ってなんですか	隠れ層
0	0512	1407	最急勾配法ってなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0512	0801	最急勾配法ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0512	1301	最急勾配法ってなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0512	1405	最急勾配法ってなんですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0512	1215	最急勾配法ってなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0512	1313	最急勾配法ってなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0512	0611	最急勾配法ってなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0512	0701	最急勾配法ってなんですか	サポートベクトルマシン
0	0512	1508	最急勾配法ってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0512	0713	最急勾配法ってなんですか	文書分類やバイオインフォマティックスなど
0	0512	0917	最急勾配法ってなんですか	入力ゲート・出力ゲート・忘却ゲート
0	0512	0109	最急勾配法ってなんですか	正解が付いていない場合の学習
0	0512	1103	最急勾配法ってなんですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0512	0810	最急勾配法ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0512	1404	最急勾配法ってなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0512	1108	最急勾配法ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0512	0102	最急勾配法ってなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0512	0803	最急勾配法ってなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0512	0404	最急勾配法ってなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0512	1503	最急勾配法ってなんですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0512	0811	最急勾配法ってなんですか	ユニットの活性化関数を工夫する方法があります
1	0514	0514	確率的最急勾配法ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0514	1106	確率的最急勾配法ってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0514	1001	確率的最急勾配法ってなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0514	1106	確率的最急勾配法ってなんですか	最も近い事例対の距離を類似度とする
0	0514	1506	確率的最急勾配法ってなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0514	0616	確率的最急勾配法ってなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0514	0611	確率的最急勾配法ってなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0514	0906	確率的最急勾配法ってなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0514	1506	確率的最急勾配法ってなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0514	1301	確率的最急勾配法ってなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0514	1106	確率的最急勾配法ってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0514	0801	確率的最急勾配法ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0514	0701	確率的最急勾配法ってなんですか	サポートベクトルマシン
0	0514	1116	確率的最急勾配法ってなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0514	0715	確率的最急勾配法ってなんですか	カーネルトリック
0	0514	0409	確率的最急勾配法ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0514	0907	確率的最急勾配法ってなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0514	0114	確率的最急勾配法ってなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0514	0412	確率的最急勾配法ってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0514	0703	確率的最急勾配法ってなんですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0514	1203	確率的最急勾配法ってなんですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0514	0304	確率的最急勾配法ってなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0514	1301	確率的最急勾配法ってなんですか	動画像の分類や音声で入力された単語の識別などの問題
0	0514	0810	確率的最急勾配法ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0514	1008	確率的最急勾配法ってなんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
1	0601	0601	回帰問題ってなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0601	1104	回帰問題ってなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0601	0502	回帰問題ってなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0601	0805	回帰問題ってなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0601	0114	回帰問題ってなんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0601	0810	回帰問題ってなんですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0601	0315	回帰問題ってなんですか	分割後のデータの分散
0	0601	1009	回帰問題ってなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0601	0906	回帰問題ってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0601	0402	回帰問題ってなんですか	事後確率が最大となるクラスを識別結果とする方法
0	0601	0409	回帰問題ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0601	1004	回帰問題ってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0601	1104	回帰問題ってなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0601	0803	回帰問題ってなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0601	1303	回帰問題ってなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0601	0502	回帰問題ってなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0601	0606	回帰問題ってなんですか	山の尾根という意味
0	0601	0606	回帰問題ってなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0601	0102	回帰問題ってなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0601	1215	回帰問題ってなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0601	0505	回帰問題ってなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0601	0912	回帰問題ってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0601	1103	回帰問題ってなんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0601	0311	回帰問題ってなんですか	集合の乱雑さ
0	0601	0701	回帰問題ってなんですか	識別境界線と最も近いデータとの距離
1	0602	0602	ターゲットってなんですか	数値型の正解情報のこと
0	0602	1509	ターゲットってなんですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0602	1502	ターゲットってなんですか	将棋や囲碁などを行うプログラム
0	0602	0701	ターゲットってなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0602	0204	ターゲットってなんですか	特徴ベクトルの次元数を減らすこと
0	0602	0916	ターゲットってなんですか	リカレントニューラルネットワーク
0	0602	0701	ターゲットってなんですか	識別境界線と最も近いデータとの距離
0	0602	0614	ターゲットってなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0602	1306	ターゲットってなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0602	1114	ターゲットってなんですか	クラスタリング結果のデータ数の分布から
0	0602	1310	ターゲットってなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	0602	0115	ターゲットってなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0602	0911	ターゲットってなんですか	学習時の自由度を意図的に下げていること
0	0602	0710	ターゲットってなんですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0602	0811	ターゲットってなんですか	ユニットの活性化関数を工夫する方法があります
0	0602	0402	ターゲットってなんですか	入力を観測した後で計算される確率
0	0602	1506	ターゲットってなんですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0602	1004	ターゲットってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0602	0614	ターゲットってなんですか	回帰木と線形回帰の双方のよいところを取った
0	0602	0810	ターゲットってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0602	0717	ターゲットってなんですか	連続値
0	0602	0205	ターゲットってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0602	1302	ターゲットってなんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0602	0711	ターゲットってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0602	0701	ターゲットってなんですか	サポートベクトルマシン
1	0605	0605	決定係数ってなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0605	0407	決定係数ってなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0605	0512	決定係数ってなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0605	1209	決定係数ってなんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0605	0710	決定係数ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0605	1104	決定係数ってなんですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0605	0307	決定係数ってなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0605	0701	決定係数ってなんですか	線形で識別できないデータに対応するため
0	0605	1309	決定係数ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0605	0901	決定係数ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0605	0510	決定係数ってなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0605	0906	決定係数ってなんですか	多階層構造でもそのまま適用できます
0	0605	0916	決定係数ってなんですか	特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつ
0	0605	0710	決定係数ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0605	0802	決定係数ってなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0605	0802	決定係数ってなんですか	識別対象のクラス数
0	0605	0601	決定係数ってなんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0605	0104	決定係数ってなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0605	0707	決定係数ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0605	1209	決定係数ってなんですか	この値が高いほど，得られる情報の多い規則であること
0	0605	0306	決定係数ってなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0605	1012	決定係数ってなんですか	各データに重みを付け，そのもとで識別器を作成します
0	0605	0810	決定係数ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0605	0701	決定係数ってなんですか	学習データからのマージンが最大となる識別境界線
0	0605	0701	決定係数ってなんですか	識別境界線と最も近いデータとの距離
1	0606	0606	正則化ってなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0606	1106	正則化ってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0606	0413	正則化ってなんですか	変数間の独立性を表現できること
0	0606	1214	正則化ってなんですか	計算量が膨大であること
0	0606	0410	正則化ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0606	0612	正則化ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0606	1001	正則化ってなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0606	1502	正則化ってなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0606	1508	正則化ってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0606	1310	正則化ってなんですか	確率的非決定性オートマトンの一種
0	0606	0701	正則化ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0606	0701	正則化ってなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0606	1110	正則化ってなんですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0606	0710	正則化ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0606	0901	正則化ってなんですか	Deep Neural Network (DNN) 
0	0606	1506	正則化ってなんですか	最適政策$\pi^*$を獲得すること
0	0606	1309	正則化ってなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0606	0616	正則化ってなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0606	0717	正則化ってなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0606	0112	正則化ってなんですか	線形回帰，回帰木，モデル木など
0	0606	0906	正則化ってなんですか	多階層構造でもそのまま適用できます
0	0606	1209	正則化ってなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0606	1303	正則化ってなんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0606	0906	正則化ってなんですか	十分多くの層を持つニューラルネットワーク
0	0606	1007	正則化ってなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
1	0606	0606	Ridge回帰ってなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0606	1110	Ridge回帰ってなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0606	0205	Ridge回帰ってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0606	0409	Ridge回帰ってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0606	0701	Ridge回帰ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0606	0105	Ridge回帰ってなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0606	0710	Ridge回帰ってなんですか	もとの空間におけるデータ間の距離関係を保存
0	0606	0810	Ridge回帰ってなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0606	1404	Ridge回帰ってなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0606	1015	Ridge回帰ってなんですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0606	1001	Ridge回帰ってなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0606	0710	Ridge回帰ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0606	1506	Ridge回帰ってなんですか	最適政策$\pi^*$を獲得すること
0	0606	0803	Ridge回帰ってなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0606	0109	Ridge回帰ってなんですか	正解が付いていない場合の学習
0	0606	0211	Ridge回帰ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0606	0417	Ridge回帰ってなんですか	ネットワークの構造とアークの条件付き確率表
0	0606	0811	Ridge回帰ってなんですか	半分の領域で勾配が1になるので
0	0606	0114	Ridge回帰ってなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0606	1110	Ridge回帰ってなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0606	0402	Ridge回帰ってなんですか	統計的識別手法
0	0606	0406	Ridge回帰ってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0606	0508	Ridge回帰ってなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0606	0209	Ridge回帰ってなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0606	1111	Ridge回帰ってなんですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
1	0606	0606	Ridgeってどういう意味ですか	山の尾根という意味
0	0606	0701	Ridgeってどういう意味ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0606	0901	Ridgeってどういう意味ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0606	0906	Ridgeってどういう意味ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0606	0911	Ridgeってどういう意味ですか	学習時の自由度を意図的に下げていること
0	0606	1214	Ridgeってどういう意味ですか	計算量が膨大であること
0	0606	0802	Ridgeってどういう意味ですか	識別対象のクラス数
0	0606	0513	Ridgeってどういう意味ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0606	0410	Ridgeってどういう意味ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0606	0715	Ridgeってどういう意味ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0606	1404	Ridgeってどういう意味ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0606	1301	Ridgeってどういう意味ですか	動画像の分類や音声で入力された単語の識別などの問題
0	0606	1410	Ridgeってどういう意味ですか	学習初期の誤りに強いということ
0	0606	1209	Ridgeってどういう意味ですか	規則の条件部が起こったときに結論部が起こる割合
0	0606	1502	Ridgeってどういう意味ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0606	0717	Ridgeってどういう意味ですか	グリッド
0	0606	1112	Ridgeってどういう意味ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0606	1409	Ridgeってどういう意味ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0606	0912	Ridgeってどういう意味ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0606	0810	Ridgeってどういう意味ですか	2006 年頃に考案された事前学習法
0	0606	0104	Ridgeってどういう意味ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0606	0105	Ridgeってどういう意味ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0606	0810	Ridgeってどういう意味ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0606	0410	Ridgeってどういう意味ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0606	0805	Ridgeってどういう意味ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
1	0607	0607	Lasso回帰ってなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0607	0115	Lasso回帰ってなんですか	パターンマイニング
0	0607	0916	Lasso回帰ってなんですか	リカレントニューラルネットワーク
0	0607	0406	Lasso回帰ってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0607	1301	Lasso回帰ってなんですか	形態素解析処理が典型的な問題
0	0607	1313	Lasso回帰ってなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0607	0802	Lasso回帰ってなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0607	0112	Lasso回帰ってなんですか	線形回帰，回帰木，モデル木など
0	0607	0614	Lasso回帰ってなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0607	0610	Lasso回帰ってなんですか	真のモデルとの距離
0	0607	0311	Lasso回帰ってなんですか	集合の乱雑さ
0	0607	0811	Lasso回帰ってなんですか	引数が負のときは0，0以上のときはその値を出力
0	0607	0808	Lasso回帰ってなんですか	シグモイド関数の微分
0	0607	0912	Lasso回帰ってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0607	0717	Lasso回帰ってなんですか	Grid search
0	0607	0209	Lasso回帰ってなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0607	0906	Lasso回帰ってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0607	1108	Lasso回帰ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0607	1407	Lasso回帰ってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0607	0416	Lasso回帰ってなんですか	値が真となる確率を知りたいノードが表す変数
0	0607	1407	Lasso回帰ってなんですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0607	0610	Lasso回帰ってなんですか	片方を減らせば片方が増える
0	0607	1409	Lasso回帰ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0607	0906	Lasso回帰ってなんですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0607	0811	Lasso回帰ってなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
1	0607	0607	lassoってどういう意味ですか	「投げ縄」という意味
0	0607	0402	lassoってどういう意味ですか	入力を観測した後で計算される確率
0	0607	0610	lassoってどういう意味ですか	学習結果の散らばり具合
0	0607	0917	lassoってどういう意味ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0607	0901	lassoってどういう意味ですか	音声認識・画像認識・自然言語処理など
0	0607	1009	lassoってどういう意味ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0607	1410	lassoってどういう意味ですか	学習初期の誤りに強いということ
0	0607	0707	lassoってどういう意味ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0607	1405	lassoってどういう意味ですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0607	1301	lassoってどういう意味ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0607	0606	lassoってどういう意味ですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0607	0906	lassoってどういう意味ですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0607	0102	lassoってどういう意味ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0607	0405	lassoってどういう意味ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0607	0917	lassoってどういう意味ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0607	1301	lassoってどういう意味ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0607	0510	lassoってどういう意味ですか	特徴空間上でクラスを分割する面
0	0607	1306	lassoってどういう意味ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0607	0316	lassoってどういう意味ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0607	0907	lassoってどういう意味ですか	事前学習法
0	0607	1301	lassoってどういう意味ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0607	0917	lassoってどういう意味ですか	LSTMセル
0	0607	0805	lassoってどういう意味ですか	誤差逆伝播法
0	0607	0111	lassoってどういう意味ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0607	0508	lassoってどういう意味ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
1	0610	0610	バイアスってなんですか	真のモデルとの距離
0	0610	1404	バイアスってなんですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0610	0906	バイアスってなんですか	多階層構造でもそのまま適用できます
0	0610	0406	バイアスってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0610	1010	バイアスってなんですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0610	1110	バイアスってなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0610	0512	バイアスってなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0610	0209	バイアスってなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0610	0901	バイアスってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0610	0902	バイアスってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0610	0313	バイアスってなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0610	0411	バイアスってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0610	1301	バイアスってなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0610	0102	バイアスってなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0610	0614	バイアスってなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0610	0409	バイアスってなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0610	0901	バイアスってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0610	0502	バイアスってなんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0610	0708	バイアスってなんですか	制約を弱める変数
0	0610	0411	バイアスってなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0610	0111	バイアスってなんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0610	0805	バイアスってなんですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0610	1303	バイアスってなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0610	1403	バイアスってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0610	1407	バイアスってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
1	0610	0610	分散ってなんですか	学習結果の散らばり具合
0	0610	0204	分散ってなんですか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0610	1112	分散ってなんですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0610	0710	分散ってなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0610	1409	分散ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0610	1304	分散ってなんですか	入力と対応させる素性
0	0610	0507	分散ってなんですか	パーセプトロンの収束定理
0	0610	0917	分散ってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0610	0514	分散ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0610	0114	分散ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0610	0102	分散ってなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0610	1506	分散ってなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0610	1301	分散ってなんですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0610	0707	分散ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0610	0417	分散ってなんですか	ネットワークの構造とアークの条件付き確率
0	0610	0114	分散ってなんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0610	1306	分散ってなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0610	0402	分散ってなんですか	事後確率
0	0610	0211	分散ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0610	0402	分散ってなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0610	0612	分散ってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0610	0901	分散ってなんですか	音声認識・画像認識・自然言語処理など
0	0610	0802	分散ってなんですか	特徴ベクトルの次元数
0	0610	0507	分散ってなんですか	全ての誤りがなくなることが学習の終了条件なので
0	0610	0711	分散ってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
1	0612	0612	CARTってなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0612	1215	CARTってなんですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0612	1219	CARTってなんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0612	0311	CARTってなんですか	集合の乱雑さ
0	0612	0906	CARTってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0612	0208	CARTってなんですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0612	0704	CARTってなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0612	0514	CARTってなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0612	0708	CARTってなんですか	制約を弱める変数
0	0612	0702	CARTってなんですか	識別面は平面を仮定する
0	0612	1108	CARTってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0612	0508	CARTってなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0612	0901	CARTってなんですか	深層学習に用いるニューラルネットワーク
0	0612	0715	CARTってなんですか	カーネルトリック
0	0612	1306	CARTってなんですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0612	1209	CARTってなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0612	0912	CARTってなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0612	1106	CARTってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0612	0805	CARTってなんですか	誤差逆伝播法
0	0612	0911	CARTってなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0612	0105	CARTってなんですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0612	0204	CARTってなんですか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	0612	0606	CARTってなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0612	1207	CARTってなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0612	0701	CARTってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0612	0612	Gini Impurityってなんですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0612	1310	Gini Impurityってなんですか	確率的非決定性オートマトンの一種
0	0612	0504	Gini Impurityってなんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0612	1015	Gini Impurityってなんですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0612	0508	Gini Impurityってなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0612	0109	Gini Impurityってなんですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0612	0115	Gini Impurityってなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0612	0513	Gini Impurityってなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0612	0102	Gini Impurityってなんですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0612	0115	Gini Impurityってなんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0612	1106	Gini Impurityってなんですか	最も近い事例対の距離を類似度とする
0	0612	0402	Gini Impurityってなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0612	1116	Gini Impurityってなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0612	0406	Gini Impurityってなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0612	0502	Gini Impurityってなんですか	SVM
0	0612	1004	Gini Impurityってなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0612	1510	Gini Impurityってなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0612	0701	Gini Impurityってなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0612	0402	Gini Impurityってなんですか	入力を観測した後で計算される確率
0	0612	0313	Gini Impurityってなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0612	0505	Gini Impurityってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0612	1505	Gini Impurityってなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0612	0711	Gini Impurityってなんですか	カーネル関数
0	0612	0104	Gini Impurityってなんですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0612	0801	Gini Impurityってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	0614	0614	モデル木ってなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0614	1407	モデル木ってなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0614	0612	モデル木ってなんですか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0614	0406	モデル木ってなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0614	0811	モデル木ってなんですか	ユニットの活性化関数を工夫する方法があります
0	0614	0901	モデル木ってなんですか	音声認識・画像認識・自然言語処理など
0	0614	0701	モデル木ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0614	1106	モデル木ってなんですか	最も遠い事例対の距離を類似度とする
0	0614	0701	モデル木ってなんですか	サポートベクトルマシン
0	0614	0402	モデル木ってなんですか	入力を観測した後で計算される確率
0	0614	1108	モデル木ってなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0614	0410	モデル木ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0614	0110	モデル木ってなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0614	1007	モデル木ってなんですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0614	1505	モデル木ってなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0614	1303	モデル木ってなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0614	0514	モデル木ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0614	1403	モデル木ってなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0614	0701	モデル木ってなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0614	1209	モデル木ってなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0614	0514	モデル木ってなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0614	0313	モデル木ってなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0614	0114	モデル木ってなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0614	0704	モデル木ってなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0614	0304	モデル木ってなんですか	個々の事例から，あるクラスについて共通点を見つけること
1	0103	0103	機械学習で用いられるビッグデータとは何ですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0103	0115	機械学習で用いられるビッグデータとは何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0103	0701	機械学習で用いられるビッグデータとは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0103	0805	機械学習で用いられるビッグデータとは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0103	0205	機械学習で用いられるビッグデータとは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0103	0917	機械学習で用いられるビッグデータとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0103	1412	機械学習で用いられるビッグデータとは何ですか	近くのノードは同じクラスになりやすいという仮定
0	0103	1410	機械学習で用いられるビッグデータとは何ですか	学習初期の誤りに強いということ
0	0103	0502	機械学習で用いられるビッグデータとは何ですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0103	0606	機械学習で用いられるビッグデータとは何ですか	入力が少し変化したときに，出力も少し変化する
0	0103	1209	機械学習で用いられるビッグデータとは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0103	0507	機械学習で用いられるビッグデータとは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	0103	1214	機械学習で用いられるビッグデータとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0103	0701	機械学習で用いられるビッグデータとは何ですか	サポートベクトルマシン
0	0103	1009	機械学習で用いられるビッグデータとは何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0103	1004	機械学習で用いられるビッグデータとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0103	1505	機械学習で用いられるビッグデータとは何ですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0103	0708	機械学習で用いられるビッグデータとは何ですか	制約を弱める変数
0	0103	0306	機械学習で用いられるビッグデータとは何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0103	0917	機械学習で用いられるビッグデータとは何ですか	LSTM
0	0103	1205	機械学習で用いられるビッグデータとは何ですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0103	1407	機械学習で用いられるビッグデータとは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0103	0111	機械学習で用いられるビッグデータとは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0103	1209	機械学習で用いられるビッグデータとは何ですか	規則の条件部が起こったときに結論部が起こる割合
0	0103	0102	機械学習で用いられるビッグデータとは何ですか	現在，人が行っている知的な判断を代わりに行う技術
1	0111	0111	識別では、なぜすべてのデータをきれいに分離しないのですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0111	1111	識別では、なぜすべてのデータをきれいに分離しないのですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0111	0305	識別では、なぜすべてのデータをきれいに分離しないのですか	仮説に対して課す制約
0	0111	0209	識別では、なぜすべてのデータをきれいに分離しないのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0111	0502	識別では、なぜすべてのデータをきれいに分離しないのですか	SVM
0	0111	0205	識別では、なぜすべてのデータをきれいに分離しないのですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0111	0911	識別では、なぜすべてのデータをきれいに分離しないのですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0111	0110	識別では、なぜすべてのデータをきれいに分離しないのですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0111	0502	識別では、なぜすべてのデータをきれいに分離しないのですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0111	0313	識別では、なぜすべてのデータをきれいに分離しないのですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0111	0717	識別では、なぜすべてのデータをきれいに分離しないのですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0111	0313	識別では、なぜすべてのデータをきれいに分離しないのですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0111	0701	識別では、なぜすべてのデータをきれいに分離しないのですか	マージン
0	0111	1106	識別では、なぜすべてのデータをきれいに分離しないのですか	クラスタの重心間の距離を類似度とする
0	0111	0911	識別では、なぜすべてのデータをきれいに分離しないのですか	ランダムに一定割合のユニットを消して学習を行う
0	0111	0117	識別では、なぜすべてのデータをきれいに分離しないのですか	学習データの一部にだけ正解が与えられている場合
0	0111	0912	識別では、なぜすべてのデータをきれいに分離しないのですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0111	1506	識別では、なぜすべてのデータをきれいに分離しないのですか	政策
0	0111	0103	識別では、なぜすべてのデータをきれいに分離しないのですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0111	1304	識別では、なぜすべてのデータをきれいに分離しないのですか	出力系列を参照する素性
0	0111	0410	識別では、なぜすべてのデータをきれいに分離しないのですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0111	0416	識別では、なぜすべてのデータをきれいに分離しないのですか	値が真となる確率を知りたいノードが表す変数
0	0111	1012	識別では、なぜすべてのデータをきれいに分離しないのですか	すべてのデータの重みは平等
0	0111	0109	識別では、なぜすべてのデータをきれいに分離しないのですか	正解が付いていない場合の学習
0	0111	0505	識別では、なぜすべてのデータをきれいに分離しないのですか	生物の神経細胞の仕組みをモデル化したもの
1	0205	0205	主成分分析とは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0205	0311	主成分分析とは何ですか	集合の乱雑さ
0	0205	0903	主成分分析とは何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0205	0602	主成分分析とは何ですか	正解情報$y$が数値であるということ
0	0205	0702	主成分分析とは何ですか	識別面は平面を仮定する
0	0205	0406	主成分分析とは何ですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0205	0710	主成分分析とは何ですか	もとの空間におけるデータ間の距離関係を保存
0	0205	0407	主成分分析とは何ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0205	1012	主成分分析とは何ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0205	0507	主成分分析とは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	0205	0614	主成分分析とは何ですか	線形回帰式
0	0205	0507	主成分分析とは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0205	0612	主成分分析とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0205	0508	主成分分析とは何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0205	1502	主成分分析とは何ですか	将棋や囲碁などを行うプログラム
0	0205	0211	主成分分析とは何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0205	0313	主成分分析とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0205	1111	主成分分析とは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0205	1303	主成分分析とは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0205	0402	主成分分析とは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0205	0412	主成分分析とは何ですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0205	1406	主成分分析とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0205	0614	主成分分析とは何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0205	0803	主成分分析とは何ですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0205	0111	主成分分析とは何ですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
1	0204	0204	なぜ前処理で次元削減を行うのですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0204	0802	なぜ前処理で次元削減を行うのですか	識別対象のクラス数
0	0204	1503	なぜ前処理で次元削減を行うのですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0204	0906	なぜ前処理で次元削減を行うのですか	十分多くの層
0	0204	1309	なぜ前処理で次元削減を行うのですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0204	0116	なぜ前処理で次元削減を行うのですか	学習データが教師あり／教師なしの混在となっているもの
0	0204	0917	なぜ前処理で次元削減を行うのですか	LSTM
0	0204	1412	なぜ前処理で次元削減を行うのですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0204	1510	なぜ前処理で次元削減を行うのですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0204	0810	なぜ前処理で次元削減を行うのですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0204	0902	なぜ前処理で次元削減を行うのですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0204	0416	なぜ前処理で次元削減を行うのですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0204	0402	なぜ前処理で次元削減を行うのですか	事後確率が最大となるクラスを識別結果とする方法
0	0204	0316	なぜ前処理で次元削減を行うのですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0204	0701	なぜ前処理で次元削減を行うのですか	サポートベクトルマシン
0	0204	0711	なぜ前処理で次元削減を行うのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0204	0715	なぜ前処理で次元削減を行うのですか	カーネルトリック
0	0204	0901	なぜ前処理で次元削減を行うのですか	深層学習に用いるニューラルネットワーク
0	0204	0304	なぜ前処理で次元削減を行うのですか	個々の事例から，あるクラスについて共通点を見つけること
0	0204	1303	なぜ前処理で次元削減を行うのですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0204	0701	なぜ前処理で次元削減を行うのですか	サポートベクトルマシン
0	0204	0112	なぜ前処理で次元削減を行うのですか	線形回帰，回帰木，モデル木など
0	0204	0810	なぜ前処理で次元削減を行うのですか	2006 年頃に考案された事前学習法
0	0204	1408	なぜ前処理で次元削減を行うのですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0204	0701	なぜ前処理で次元削減を行うのですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
1	0305	0305	バイアスって何ですか	仮説に対して課す制約
0	0305	1409	バイアスって何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0305	0411	バイアスって何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0305	0611	バイアスって何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0305	1012	バイアスって何ですか	すべてのデータの重みは平等
0	0305	0514	バイアスって何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0305	1012	バイアスって何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0305	1505	バイアスって何ですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0305	0906	バイアスって何ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0305	0109	バイアスって何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0305	0803	バイアスって何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0305	1007	バイアスって何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0305	0606	バイアスって何ですか	山の尾根という意味
0	0305	0114	バイアスって何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0305	0711	バイアスって何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0305	0917	バイアスって何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0305	1214	バイアスって何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0305	1303	バイアスって何ですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0305	0102	バイアスって何ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0305	0109	バイアスって何ですか	正解が付いていない場合の学習
0	0305	1201	バイアスって何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0305	0901	バイアスって何ですか	深層学習に用いるニューラルネットワーク
0	0305	0110	バイアスって何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0305	0811	バイアスって何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0305	0607	バイアスって何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
1	0307	0307	決定木とはどのような学習法ですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	0307	0105	決定木とはどのような学習法ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0307	0701	決定木とはどのような学習法ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0307	0115	決定木とはどのような学習法ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0307	0906	決定木とはどのような学習法ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0307	1501	決定木とはどのような学習法ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0307	1114	決定木とはどのような学習法ですか	クラスタリング結果のデータ数の分布から
0	0307	0204	決定木とはどのような学習法ですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0307	0701	決定木とはどのような学習法ですか	識別境界線と最も近いデータとの距離
0	0307	0610	決定木とはどのような学習法ですか	真のモデルとの距離
0	0307	1004	決定木とはどのような学習法ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0307	0906	決定木とはどのような学習法ですか	多階層構造でもそのまま適用できます
0	0307	0513	決定木とはどのような学習法ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0307	1505	決定木とはどのような学習法ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0307	1201	決定木とはどのような学習法ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0307	0102	決定木とはどのような学習法ですか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0307	1116	決定木とはどのような学習法ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0307	0711	決定木とはどのような学習法ですか	カーネル関数
0	0307	0111	決定木とはどのような学習法ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0307	0611	決定木とはどのような学習法ですか	回帰
0	0307	1214	決定木とはどのような学習法ですか	計算量が膨大であること
0	0307	1502	決定木とはどのような学習法ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0307	1411	決定木とはどのような学習法ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0307	1209	決定木とはどのような学習法ですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0307	0109	決定木とはどのような学習法ですか	正解が付いていない場合の学習
1	0311	0311	エントロピーって何ですか	集合の乱雑さ
0	0311	0306	エントロピーって何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0311	1303	エントロピーって何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	0311	0901	エントロピーって何ですか	深層学習に用いるニューラルネットワーク
0	0311	0514	エントロピーって何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0311	0416	エントロピーって何ですか	アークを無向とみなした結合を考えたとき
0	0311	0406	エントロピーって何ですか	各クラスから生じる特徴の尤もらしさを表す
0	0311	0605	エントロピーって何ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0311	0601	エントロピーって何ですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0311	1103	エントロピーって何ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0311	0307	エントロピーって何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0311	1301	エントロピーって何ですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0311	0710	エントロピーって何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0311	0803	エントロピーって何ですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0311	1001	エントロピーって何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0311	0614	エントロピーって何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0311	1505	エントロピーって何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0311	0811	エントロピーって何ですか	ユニットの活性化関数を工夫する方法
0	0311	0105	エントロピーって何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0311	1401	エントロピーって何ですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0311	0109	エントロピーって何ですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0311	0411	エントロピーって何ですか	確率のm推定という考え方を用います
0	0311	0916	エントロピーって何ですか	リカレントニューラルネットワーク
0	0311	0717	エントロピーって何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0311	1219	エントロピーって何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
1	0313	0313	過学習とはどのような状態ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0313	1303	過学習とはどのような状態ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0313	0901	過学習とはどのような状態ですか	音声認識・画像認識・自然言語処理など
0	0313	1209	過学習とはどのような状態ですか	規則の条件部が起こったときに結論部が起こる割合
0	0313	0805	過学習とはどのような状態ですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0313	0917	過学習とはどのような状態ですか	入力ゲート・出力ゲート・忘却ゲート
0	0313	1104	過学習とはどのような状態ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0313	0114	過学習とはどのような状態ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0313	1215	過学習とはどのような状態ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0313	0811	過学習とはどのような状態ですか	引数が負のときは0，0以上のときはその値を出力
0	0313	0614	過学習とはどのような状態ですか	線形回帰式
0	0313	0411	過学習とはどのような状態ですか	確率のm推定という考え方を用います
0	0313	0205	過学習とはどのような状態ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0313	0810	過学習とはどのような状態ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0313	0205	過学習とはどのような状態ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0313	0115	過学習とはどのような状態ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0313	0614	過学習とはどのような状態ですか	回帰木と線形回帰の双方のよいところを取った方法
0	0313	1404	過学習とはどのような状態ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0313	0206	過学習とはどのような状態ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0313	0602	過学習とはどのような状態ですか	数値型の正解情報のこと
0	0313	0802	過学習とはどのような状態ですか	特徴ベクトルの次元数
0	0313	1301	過学習とはどのような状態ですか	形態素解析処理が典型的な問題
0	0313	0701	過学習とはどのような状態ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0313	1108	過学習とはどのような状態ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0313	0611	過学習とはどのような状態ですか	回帰
1	1301	1301	系列データとはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1301	0708	系列データとはなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1301	0906	系列データとはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1301	1106	系列データとはなんですか	最も遠い事例対の距離を類似度とする
0	1301	1001	系列データとはなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1301	0906	系列データとはなんですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	1301	0805	系列データとはなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1301	0912	系列データとはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	1301	0313	系列データとはなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1301	1104	系列データとはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1301	0405	系列データとはなんですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1301	0906	系列データとはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1301	0606	系列データとはなんですか	入力が少し変化したときに，出力も少し変化する
0	1301	0211	系列データとはなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1301	0512	系列データとはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1301	0805	系列データとはなんですか	誤差逆伝播法
0	1301	0701	系列データとはなんですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1301	1108	系列データとはなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1301	1505	系列データとはなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1301	0801	系列データとはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	0902	系列データとはなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1301	0602	系列データとはなんですか	数値型の正解情報のこと
0	1301	1506	系列データとはなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1301	0701	系列データとはなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1301	0514	系列データとはなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
1	1301	1301	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1301	0917	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	LSTM
0	1301	0616	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1301	0614	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	回帰木と線形回帰の双方のよいところを取った
0	1301	0701	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1301	1509	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1301	1106	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1301	0502	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1301	0803	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1301	0514	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1301	0114	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1301	1106	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1301	0805	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1301	0707	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1301	0206	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1301	0902	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1301	0205	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1301	1501	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1301	1410	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	1301	0901	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	1301	0811	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1301	0708	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	制約を弱める変数
0	1301	0611	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	識別における決定木の考え方を回帰問題に適用する方法
0	1301	0801	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1301	1505	入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか	「マルコフ性」を持つ確率過程における意思決定問題
1	1303	1303	日本語の形態素列の特徴は何ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1303	0411	日本語の形態素列の特徴は何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1303	0514	日本語の形態素列の特徴は何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	1303	1114	日本語の形態素列の特徴は何ですか	クラスタリング結果のデータ数の分布から
0	1303	0508	日本語の形態素列の特徴は何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	1303	1201	日本語の形態素列の特徴は何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1303	0717	日本語の形態素列の特徴は何ですか	グリッド
0	1303	1209	日本語の形態素列の特徴は何ですか	この値が高いほど，得られる情報の多い規則であること
0	1303	0508	日本語の形態素列の特徴は何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1303	0502	日本語の形態素列の特徴は何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1303	0911	日本語の形態素列の特徴は何ですか	ドロップアウト
0	1303	0508	日本語の形態素列の特徴は何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1303	0405	日本語の形態素列の特徴は何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1303	1009	日本語の形態素列の特徴は何ですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	1303	1301	日本語の形態素列の特徴は何ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1303	0209	日本語の形態素列の特徴は何ですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1303	1502	日本語の形態素列の特徴は何ですか	将棋や囲碁などを行うプログラム
0	1303	1313	日本語の形態素列の特徴は何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1303	0912	日本語の形態素列の特徴は何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1303	0614	日本語の形態素列の特徴は何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1303	0109	日本語の形態素列の特徴は何ですか	正解が付いていない場合の学習
0	1303	0116	日本語の形態素列の特徴は何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1303	0715	日本語の形態素列の特徴は何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1303	0811	日本語の形態素列の特徴は何ですか	ユニットの活性化関数を工夫する方法があります
0	1303	0906	日本語の形態素列の特徴は何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
1	1303	1303	固定表現抽出とは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	1303	1106	固定表現抽出とは何ですか	最も遠い事例対の距離を類似度とする
0	1303	0510	固定表現抽出とは何ですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1303	0917	固定表現抽出とは何ですか	LSTM
0	1303	0209	固定表現抽出とは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1303	0510	固定表現抽出とは何ですか	特徴空間上でクラスを分割する面
0	1303	1304	固定表現抽出とは何ですか	入力と対応させる素性
0	1303	0313	固定表現抽出とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1303	1012	固定表現抽出とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1303	1112	固定表現抽出とは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	1303	0901	固定表現抽出とは何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1303	0402	固定表現抽出とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1303	0116	固定表現抽出とは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1303	0315	固定表現抽出とは何ですか	分割後のデータの分散
0	1303	0717	固定表現抽出とは何ですか	Grid search
0	1303	1111	固定表現抽出とは何ですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1303	0707	固定表現抽出とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1303	1010	固定表現抽出とは何ですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	1303	0907	固定表現抽出とは何ですか	事前学習法
0	1303	1012	固定表現抽出とは何ですか	各データに重みを付け，そのもとで識別器を作成します
0	1303	0902	固定表現抽出とは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1303	0717	固定表現抽出とは何ですか	グリッド
0	1303	1406	固定表現抽出とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1303	1111	固定表現抽出とは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1303	0114	固定表現抽出とは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
1	1304	1304	遷移素性とは何ですか	出力系列を参照する素性
0	1304	0611	遷移素性とは何ですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1304	0917	遷移素性とは何ですか	LSTMセル
0	1304	0612	遷移素性とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1304	1509	遷移素性とは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1304	0402	遷移素性とは何ですか	事後確率
0	1304	0717	遷移素性とは何ですか	連続値
0	1304	0801	遷移素性とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1304	0901	遷移素性とは何ですか	音声認識・画像認識・自然言語処理など
0	1304	0701	遷移素性とは何ですか	識別境界線と最も近いデータとの距離
0	1304	0811	遷移素性とは何ですか	ReLu
0	1304	1215	遷移素性とは何ですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1304	0402	遷移素性とは何ですか	事後確率が最大となるクラスを識別結果とする方法
0	1304	0406	遷移素性とは何ですか	各クラスから生じる特徴の尤もらしさを表す
0	1304	1111	遷移素性とは何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1304	1220	遷移素性とは何ですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	1304	0410	遷移素性とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1304	1106	遷移素性とは何ですか	最も遠い事例対の距離を類似度とする
0	1304	0906	遷移素性とは何ですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1304	1103	遷移素性とは何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	1304	0907	遷移素性とは何ですか	事前学習法
0	1304	0708	遷移素性とは何ですか	制約を弱める変数
0	1304	0316	遷移素性とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1304	1406	遷移素性とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1304	1302	遷移素性とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
1	1304	1304	観測素性とは何ですか	入力と対応させる素性
0	1304	0114	観測素性とは何ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	1304	0607	観測素性とは何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1304	1012	観測素性とは何ですか	すべてのデータの重みは平等
0	1304	0111	観測素性とは何ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	1304	0311	観測素性とは何ですか	集合の乱雑さ
0	1304	1205	観測素性とは何ですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1304	0601	観測素性とは何ですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	1304	0715	観測素性とは何ですか	カーネルトリック
0	1304	1103	観測素性とは何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	1304	0710	観測素性とは何ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1304	0406	観測素性とは何ですか	各クラスから生じる特徴の尤もらしさを表す
0	1304	0802	観測素性とは何ですか	特徴ベクトルの次元数
0	1304	0514	観測素性とは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	1304	0116	観測素性とは何ですか	学習データが教師あり／教師なしの混在となっているもの
0	1304	0402	観測素性とは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1304	1201	観測素性とは何ですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	1304	1406	観測素性とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1304	0810	観測素性とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1304	0704	観測素性とは何ですか	以下の関数$L$の最小値を求めるという問題
0	1304	1205	観測素性とは何ですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	1304	1306	観測素性とは何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1304	0416	観測素性とは何ですか	値が真となる確率を知りたいノードが表す変数
0	1304	1207	観測素性とは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1304	0811	観測素性とは何ですか	ユニットの活性化関数を工夫する方法があります
1	1306	1306	ビタビアルゴリズムとは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	1306	0510	ビタビアルゴリズムとは何ですか	特徴空間上でクラスを分割する面
0	1306	0402	ビタビアルゴリズムとは何ですか	最大事後確率則
0	1306	0810	ビタビアルゴリズムとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1306	0114	ビタビアルゴリズムとは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1306	1116	ビタビアルゴリズムとは何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1306	1505	ビタビアルゴリズムとは何ですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1306	1509	ビタビアルゴリズムとは何ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1306	0701	ビタビアルゴリズムとは何ですか	サポートベクトルマシン
0	1306	0105	ビタビアルゴリズムとは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1306	0612	ビタビアルゴリズムとは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	1306	1110	ビタビアルゴリズムとは何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1306	0509	ビタビアルゴリズムとは何ですか	確率的最急勾配法
0	1306	1207	ビタビアルゴリズムとは何ですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1306	1214	ビタビアルゴリズムとは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1306	0602	ビタビアルゴリズムとは何ですか	数値型の正解情報のこと
0	1306	0105	ビタビアルゴリズムとは何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	1306	0503	ビタビアルゴリズムとは何ですか	様々な数値データに対して多く用いられる統計モデル
0	1306	0911	ビタビアルゴリズムとは何ですか	ランダムに一定割合のユニットを消して学習を行う
0	1306	0803	ビタビアルゴリズムとは何ですか	非線形識別面
0	1306	1406	ビタビアルゴリズムとは何ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1306	0901	ビタビアルゴリズムとは何ですか	深層学習に用いるニューラルネットワーク
0	1306	0204	ビタビアルゴリズムとは何ですか	特徴ベクトルの次元数を減らすこと
0	1306	0917	ビタビアルゴリズムとは何ですか	LSTM
0	1306	0404	ビタビアルゴリズムとは何ですか	事前確率
1	1306	1306	CRFとは何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1306	0113	CRFとは何ですか	入力データに潜む規則性を学習すること
0	1306	0614	CRFとは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1306	0715	CRFとは何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1306	1408	CRFとは何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1306	0916	CRFとは何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	1306	1203	CRFとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1306	1110	CRFとは何ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	1306	1502	CRFとは何ですか	将棋や囲碁などを行うプログラム
0	1306	0114	CRFとは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1306	0410	CRFとは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1306	1509	CRFとは何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1306	1302	CRFとは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1306	0710	CRFとは何ですか	もとの空間におけるデータ間の距離関係を保存
0	1306	0111	CRFとは何ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	1306	1114	CRFとは何ですか	クラスタリング結果のデータ数の分布
0	1306	0811	CRFとは何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1306	0505	CRFとは何ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1306	1209	CRFとは何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1306	0711	CRFとは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1306	1301	CRFとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1306	0901	CRFとは何ですか	深層学習に用いるニューラルネットワーク
0	1306	1103	CRFとは何ですか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	1306	0901	CRFとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1306	0417	CRFとは何ですか	ネットワークの構造とアークの条件付き確率
1	1306	1306	CRFは何の略ですか	条件付き確率場（Conditional Random Field: CRF）
0	1306	0901	CRFは何の略ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1306	0906	CRFは何の略ですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	1306	1406	CRFは何の略ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1306	0912	CRFは何の略ですか	畳み込みニューラルネットワーク
0	1306	1209	CRFは何の略ですか	規則の条件部が起こったときに結論部が起こる割合
0	1306	0906	CRFは何の略ですか	多階層構造でもそのまま適用できます
0	1306	0206	CRFは何の略ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1306	0907	CRFは何の略ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1306	0505	CRFは何の略ですか	パーセプトロン
0	1306	0701	CRFは何の略ですか	サポートベクトルマシン
0	1306	1506	CRFは何の略ですか	後に得られる報酬ほど割り引いて計算するための係数
0	1306	0802	CRFは何の略ですか	識別対象のクラス数
0	1306	1106	CRFは何の略ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1306	0717	CRFは何の略ですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	1306	0113	CRFは何の略ですか	入力データに潜む規則性を学習すること
0	1306	0701	CRFは何の略ですか	線形で識別できないデータに対応するため
0	1306	0302	CRFは何の略ですか	カテゴリ形式の正解情報のこと
0	1306	0112	CRFは何の略ですか	線形回帰，回帰木，モデル木など
0	1306	0508	CRFは何の略ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1306	0402	CRFは何の略ですか	入力を観測した後で計算される確率
0	1306	0114	CRFは何の略ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1306	0304	CRFは何の略ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1306	0606	CRFは何の略ですか	山の尾根という意味
0	1306	0710	CRFは何の略ですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
1	1306	1306	CRFの性質は何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1306	0514	CRFの性質は何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1306	0115	CRFの性質は何ですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	1306	0115	CRFの性質は何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1306	0404	CRFの性質は何ですか	事前確率
0	1306	0306	CRFの性質は何ですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	1306	0416	CRFの性質は何ですか	アークを無向とみなした結合を考えたとき
0	1306	0302	CRFの性質は何ですか	カテゴリ形式の正解情報のこと
0	1306	0906	CRFの性質は何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1306	0114	CRFの性質は何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1306	0710	CRFの性質は何ですか	もとの空間におけるデータ間の距離関係を保存
0	1306	0405	CRFの性質は何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1306	0508	CRFの性質は何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	1306	0502	CRFの性質は何ですか	一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です
0	1306	1015	CRFの性質は何ですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	1306	0917	CRFの性質は何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	1306	1001	CRFの性質は何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1306	1508	CRFの性質は何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1306	0505	CRFの性質は何ですか	パーセプトロン
0	1306	0710	CRFの性質は何ですか	もとの空間におけるデータ間の距離関係を保存
0	1306	0304	CRFの性質は何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1306	0906	CRFの性質は何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1306	1209	CRFの性質は何ですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1306	0114	CRFの性質は何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1306	0802	CRFの性質は何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
1	1309	1309	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1309	0906	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	十分多くの層
0	1309	1110	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1309	0707	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1309	0407	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1309	0902	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1309	0205	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1309	0507	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	1309	0514	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	ランダムに学習データを一つ
0	1309	0112	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	線形回帰，回帰木，モデル木など
0	1309	0715	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	識別面
0	1309	0513	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1309	0513	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1309	0606	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	山の尾根という意味
0	1309	0209	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	正例がどれだけ正しく判定されているかという指標
0	1309	0711	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	カーネル関数
0	1309	1502	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	将棋や囲碁などを行うプログラム
0	1309	0917	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	1309	0708	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	制約を弱める変数
0	1309	0402	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1309	1104	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1309	1507	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1309	0507	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	全ての誤りがなくなることが学習の終了条件なので
0	1309	1506	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	政策
0	1309	0906	入力の系列長に関わらず出力の系列長が1である問題の難しさは何ですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
1	1310	1310	HMMは何の略ですか	Hidden Marcov Model: 隠れマルコフモデル
0	1310	0103	HMMは何の略ですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	1310	1301	HMMは何の略ですか	動画像の分類や音声で入力された単語の識別などの問題
0	1310	1408	HMMは何の略ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1310	1106	HMMは何の略ですか	最も近い事例対の距離を類似度とする
0	1310	1007	HMMは何の略ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1310	1303	HMMは何の略ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	1310	0702	HMMは何の略ですか	識別面は平面を仮定する
0	1310	0114	HMMは何の略ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1310	0906	HMMは何の略ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	1310	1110	HMMは何の略ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1310	1508	HMMは何の略ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1310	0917	HMMは何の略ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	1310	0811	HMMは何の略ですか	ユニットの活性化関数を工夫する方法
0	1310	1506	HMMは何の略ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1310	1507	HMMは何の略ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	1310	0605	HMMは何の略ですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	1310	1412	HMMは何の略ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1310	0416	HMMは何の略ですか	値が真となる確率を知りたいノードが表す変数
0	1310	0211	HMMは何の略ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1310	0808	HMMは何の略ですか	入力の重み付き和の微分
0	1310	0906	HMMは何の略ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1310	0410	HMMは何の略ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1310	0803	HMMは何の略ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1310	0614	HMMは何の略ですか	回帰木と線形回帰の双方のよいところを取った方法
1	1313	1313	なぜHMMで最尤状態系列を求めるのですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1313	1116	なぜHMMで最尤状態系列を求めるのですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1313	0911	なぜHMMで最尤状態系列を求めるのですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	1313	0514	なぜHMMで最尤状態系列を求めるのですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1313	0711	なぜHMMで最尤状態系列を求めるのですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1313	1015	なぜHMMで最尤状態系列を求めるのですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1313	0607	なぜHMMで最尤状態系列を求めるのですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1313	0611	なぜHMMで最尤状態系列を求めるのですか	識別における決定木の考え方を回帰問題に適用する方法
0	1313	0912	なぜHMMで最尤状態系列を求めるのですか	畳み込みニューラルネットワーク
0	1313	0710	なぜHMMで最尤状態系列を求めるのですか	低次元の特徴ベクトルを高次元に写像
0	1313	1201	なぜHMMで最尤状態系列を求めるのですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1313	0111	なぜHMMで最尤状態系列を求めるのですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1313	0701	なぜHMMで最尤状態系列を求めるのですか	識別境界線と最も近いデータとの距離
0	1313	0912	なぜHMMで最尤状態系列を求めるのですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1313	0209	なぜHMMで最尤状態系列を求めるのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1313	0505	なぜHMMで最尤状態系列を求めるのですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1313	0802	なぜHMMで最尤状態系列を求めるのですか	隠れ層
0	1313	0701	なぜHMMで最尤状態系列を求めるのですか	識別境界線と最も近いデータとの距離
0	1313	0810	なぜHMMで最尤状態系列を求めるのですか	勾配消失問題
0	1313	0707	なぜHMMで最尤状態系列を求めるのですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1313	0616	なぜHMMで最尤状態系列を求めるのですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1313	0509	なぜHMMで最尤状態系列を求めるのですか	確率的最急勾配法
0	1313	0907	なぜHMMで最尤状態系列を求めるのですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1313	1106	なぜHMMで最尤状態系列を求めるのですか	最も遠い事例対の距離を類似度とする
0	1313	0801	なぜHMMで最尤状態系列を求めるのですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	1401	1401	半教師あり学習は実用上でなぜ必要なのですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	1401	1214	半教師あり学習は実用上でなぜ必要なのですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1401	0902	半教師あり学習は実用上でなぜ必要なのですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1401	0502	半教師あり学習は実用上でなぜ必要なのですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1401	0805	半教師あり学習は実用上でなぜ必要なのですか	誤差逆伝播法
0	1401	1004	半教師あり学習は実用上でなぜ必要なのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1401	0810	半教師あり学習は実用上でなぜ必要なのですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1401	1015	半教師あり学習は実用上でなぜ必要なのですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	1401	0502	半教師あり学習は実用上でなぜ必要なのですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	1401	1409	半教師あり学習は実用上でなぜ必要なのですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1401	0105	半教師あり学習は実用上でなぜ必要なのですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	1401	0803	半教師あり学習は実用上でなぜ必要なのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1401	0302	半教師あり学習は実用上でなぜ必要なのですか	カテゴリ形式の正解情報のこと
0	1401	0708	半教師あり学習は実用上でなぜ必要なのですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1401	0512	半教師あり学習は実用上でなぜ必要なのですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1401	0512	半教師あり学習は実用上でなぜ必要なのですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1401	1214	半教師あり学習は実用上でなぜ必要なのですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1401	0206	半教師あり学習は実用上でなぜ必要なのですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1401	0413	半教師あり学習は実用上でなぜ必要なのですか	変数間の独立性を表現できること
0	1401	0402	半教師あり学習は実用上でなぜ必要なのですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1401	0514	半教師あり学習は実用上でなぜ必要なのですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	1401	1207	半教師あり学習は実用上でなぜ必要なのですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1401	1313	半教師あり学習は実用上でなぜ必要なのですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1401	0717	半教師あり学習は実用上でなぜ必要なのですか	Grid search
0	1401	0901	半教師あり学習は実用上でなぜ必要なのですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
1	1402	1402	半教師あり学習が成立する条件は何ですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1402	0710	半教師あり学習が成立する条件は何ですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	1402	1001	半教師あり学習が成立する条件は何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1402	0901	半教師あり学習が成立する条件は何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1402	0811	半教師あり学習が成立する条件は何ですか	誤差が消失しません
0	1402	0304	半教師あり学習が成立する条件は何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	1402	0406	半教師あり学習が成立する条件は何ですか	各クラスから生じる特徴の尤もらしさを表す
0	1402	1510	半教師あり学習が成立する条件は何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1402	0105	半教師あり学習が成立する条件は何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	1402	0701	半教師あり学習が成立する条件は何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1402	0602	半教師あり学習が成立する条件は何ですか	数値型の正解情報のこと
0	1402	0810	半教師あり学習が成立する条件は何ですか	勾配消失問題
0	1402	0911	半教師あり学習が成立する条件は何ですか	学習時の自由度を意図的に下げていること
0	1402	1110	半教師あり学習が成立する条件は何ですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	1402	0810	半教師あり学習が成立する条件は何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1402	1012	半教師あり学習が成立する条件は何ですか	各データに重みを付け，そのもとで識別器を作成します
0	1402	0508	半教師あり学習が成立する条件は何ですか	最小二乗法
0	1402	0313	半教師あり学習が成立する条件は何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1402	1304	半教師あり学習が成立する条件は何ですか	入力と対応させる素性
0	1402	0109	半教師あり学習が成立する条件は何ですか	学習データに正解が付いている場合の学習
0	1402	0901	半教師あり学習が成立する条件は何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1402	0610	半教師あり学習が成立する条件は何ですか	学習結果の散らばり具合
0	1402	0305	半教師あり学習が成立する条件は何ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1402	1304	半教師あり学習が成立する条件は何ですか	出力系列を参照する素性
0	1402	0917	半教師あり学習が成立する条件は何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
1	1403	1403	多様体仮定とはどういうことですか	多次元でも「次元の呪い」にかかっていない，ということ
0	1403	0614	多様体仮定とはどういうことですか	回帰木と線形回帰の双方のよいところを取った方法
0	1403	0901	多様体仮定とはどういうことですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1403	0407	多様体仮定とはどういうことですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1403	0701	多様体仮定とはどういうことですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1403	1209	多様体仮定とはどういうことですか	この値が高いほど，得られる情報の多い規則であること
0	1403	1506	多様体仮定とはどういうことですか	最適政策$\pi^*$を獲得すること
0	1403	0302	多様体仮定とはどういうことですか	カテゴリ形式の正解情報のこと
0	1403	1106	多様体仮定とはどういうことですか	最も近い事例対の距離を類似度とする
0	1403	0715	多様体仮定とはどういうことですか	カーネルトリック
0	1403	0509	多様体仮定とはどういうことですか	確率的最急勾配法
0	1403	1409	多様体仮定とはどういうことですか	自分が出した誤りを指摘してくれる他人がいない
0	1403	0302	多様体仮定とはどういうことですか	カテゴリ形式の正解情報のこと
0	1403	1111	多様体仮定とはどういうことですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	1403	0512	多様体仮定とはどういうことですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1403	0113	多様体仮定とはどういうことですか	入力データに潜む規則性を学習すること
0	1403	1506	多様体仮定とはどういうことですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1403	0611	多様体仮定とはどういうことですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1403	1409	多様体仮定とはどういうことですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1403	0610	多様体仮定とはどういうことですか	真のモデルとの距離
0	1403	0701	多様体仮定とはどういうことですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1403	0412	多様体仮定とはどういうことですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	1403	0112	多様体仮定とはどういうことですか	線形回帰，回帰木，モデル木など
0	1403	0109	多様体仮定とはどういうことですか	正解が付いていない場合の学習
0	1403	0104	多様体仮定とはどういうことですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
1	1404	1404	オーバーラップとは何ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	1404	0305	オーバーラップとは何ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1404	0416	オーバーラップとは何ですか	値が真となる確率を知りたいノードが表す変数
0	1404	0710	オーバーラップとは何ですか	低次元の特徴ベクトルを高次元に写像
0	1404	1505	オーバーラップとは何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1404	1116	オーバーラップとは何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1404	0708	オーバーラップとは何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1404	1506	オーバーラップとは何ですか	その政策に従って行動したときの累積報酬の期待値で評価
0	1404	0209	オーバーラップとは何ですか	正例がどれだけ正しく判定されているかという指標
0	1404	0906	オーバーラップとは何ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	1404	1004	オーバーラップとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1404	0307	オーバーラップとは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1404	1508	オーバーラップとは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1404	0906	オーバーラップとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1404	0917	オーバーラップとは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1404	0311	オーバーラップとは何ですか	集合の乱雑さ
0	1404	0611	オーバーラップとは何ですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1404	0811	オーバーラップとは何ですか	ReLu
0	1404	0901	オーバーラップとは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1404	0906	オーバーラップとは何ですか	十分多くの層
0	1404	0717	オーバーラップとは何ですか	Grid search
0	1404	0110	オーバーラップとは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1404	1203	オーバーラップとは何ですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	1404	0115	オーバーラップとは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1404	1112	オーバーラップとは何ですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
1	1404	1404	オーバーラップが多いとどうなりますか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	1404	0204	オーバーラップが多いとどうなりますか	学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い
0	1404	1015	オーバーラップが多いとどうなりますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1404	1106	オーバーラップが多いとどうなりますか	最も遠い事例対の距離を類似度とする
0	1404	0614	オーバーラップが多いとどうなりますか	線形回帰式
0	1404	1509	オーバーラップが多いとどうなりますか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1404	1411	オーバーラップが多いとどうなりますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1404	0402	オーバーラップが多いとどうなりますか	事後確率が最大となるクラスを識別結果とする方法
0	1404	1301	オーバーラップが多いとどうなりますか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1404	1309	オーバーラップが多いとどうなりますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1404	0304	オーバーラップが多いとどうなりますか	個々の事例から，あるクラスについて共通点を見つけること
0	1404	0508	オーバーラップが多いとどうなりますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1404	1220	オーバーラップが多いとどうなりますか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1404	0416	オーバーラップが多いとどうなりますか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	1404	0612	オーバーラップが多いとどうなりますか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	1404	1012	オーバーラップが多いとどうなりますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1404	1209	オーバーラップが多いとどうなりますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	1404	0808	オーバーラップが多いとどうなりますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1404	0606	オーバーラップが多いとどうなりますか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1404	0614	オーバーラップが多いとどうなりますか	モデル木
0	1404	0902	オーバーラップが多いとどうなりますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1404	0811	オーバーラップが多いとどうなりますか	半分の領域で勾配が1になるので
0	1404	1405	オーバーラップが多いとどうなりますか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	1404	0903	オーバーラップが多いとどうなりますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1404	0111	オーバーラップが多いとどうなりますか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
1	1405	1405	特徴の伝搬とは何ですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	1405	0810	特徴の伝搬とは何ですか	2006 年頃に考案された事前学習法
0	1405	0313	特徴の伝搬とは何ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1405	1214	特徴の伝搬とは何ですか	計算量が膨大であること
0	1405	0906	特徴の伝搬とは何ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	1405	1502	特徴の伝搬とは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1405	0111	特徴の伝搬とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1405	0808	特徴の伝搬とは何ですか	シグモイド関数の微分
0	1405	1106	特徴の伝搬とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1405	1409	特徴の伝搬とは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1405	0801	特徴の伝搬とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1405	0611	特徴の伝搬とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1405	0602	特徴の伝搬とは何ですか	数値型の正解情報のこと
0	1405	1313	特徴の伝搬とは何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1405	0514	特徴の伝搬とは何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	1405	0412	特徴の伝搬とは何ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1405	0810	特徴の伝搬とは何ですか	誤差が小さくなって消失してしまう
0	1405	0906	特徴の伝搬とは何ですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	1405	1112	特徴の伝搬とは何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	1405	0405	特徴の伝搬とは何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1405	0209	特徴の伝搬とは何ですか	正例がどれだけ正しく判定されているかという指標
0	1405	1106	特徴の伝搬とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1405	0502	特徴の伝搬とは何ですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1405	1214	特徴の伝搬とは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1405	0802	特徴の伝搬とは何ですか	入力層・出力層の数に応じた適当な数
1	1406	1406	半教師あり学習の識別器には何が適切ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1406	0315	半教師あり学習の識別器には何が適切ですか	分割後のデータの分散
0	1406	0901	半教師あり学習の識別器には何が適切ですか	深層学習に用いるニューラルネットワーク
0	1406	0711	半教師あり学習の識別器には何が適切ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1406	0901	半教師あり学習の識別器には何が適切ですか	深層学習に用いるニューラルネットワーク
0	1406	0908	半教師あり学習の識別器には何が適切ですか	ユークリッド距離
0	1406	0810	半教師あり学習の識別器には何が適切ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1406	0410	半教師あり学習の識別器には何が適切ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1406	0402	半教師あり学習の識別器には何が適切ですか	事後確率が最大となるクラスを識別結果とする方法
0	1406	1219	半教師あり学習の識別器には何が適切ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1406	0611	半教師あり学習の識別器には何が適切ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1406	1502	半教師あり学習の識別器には何が適切ですか	将棋や囲碁などを行うプログラム
0	1406	0208	半教師あり学習の識別器には何が適切ですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	1406	0907	半教師あり学習の識別器には何が適切ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1406	0402	半教師あり学習の識別器には何が適切ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1406	1303	半教師あり学習の識別器には何が適切ですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1406	1304	半教師あり学習の識別器には何が適切ですか	出力系列を参照する素性
0	1406	0110	半教師あり学習の識別器には何が適切ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	1406	0717	半教師あり学習の識別器には何が適切ですか	連続値
0	1406	0917	半教師あり学習の識別器には何が適切ですか	LSTMセル
0	1406	0103	半教師あり学習の識別器には何が適切ですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	1406	1510	半教師あり学習の識別器には何が適切ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1406	0708	半教師あり学習の識別器には何が適切ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1406	0404	半教師あり学習の識別器には何が適切ですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1406	0710	半教師あり学習の識別器には何が適切ですか	低次元の特徴ベクトルを高次元に写像
1	1407	1407	自己学習とはなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1407	0803	自己学習とはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	1407	0906	自己学習とはなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1407	0717	自己学習とはなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1407	0411	自己学習とはなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1407	0311	自己学習とはなんですか	集合の乱雑さ
0	1407	0906	自己学習とはなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	1407	0901	自己学習とはなんですか	深層学習に用いるニューラルネットワーク
0	1407	1502	自己学習とはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1407	1201	自己学習とはなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1407	0614	自己学習とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	1407	0810	自己学習とはなんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1407	0505	自己学習とはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	1407	0702	自己学習とはなんですか	識別面は平面を仮定する
0	1407	0204	自己学習とはなんですか	特徴ベクトルの次元数を減らすこと
0	1407	1409	自己学習とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1407	1104	自己学習とはなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1407	0911	自己学習とはなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	1407	0611	自己学習とはなんですか	回帰
0	1407	0105	自己学習とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1407	0209	自己学習とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1407	0410	自己学習とはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1407	0907	自己学習とはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1407	0411	自己学習とはなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1407	0801	自己学習とはなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	1407	1407	自己学習の狙いは何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1407	0901	自己学習の狙いは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1407	1106	自己学習の狙いは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1407	0114	自己学習の狙いは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1407	0701	自己学習の狙いは何ですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1407	0810	自己学習の狙いは何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1407	0711	自己学習の狙いは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1407	0209	自己学習の狙いは何ですか	正例がどれだけ正しく判定されているかという指標
0	1407	0607	自己学習の狙いは何ですか	「投げ縄」という意味
0	1407	0209	自己学習の狙いは何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1407	0512	自己学習の狙いは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1407	1214	自己学習の狙いは何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1407	0505	自己学習の狙いは何ですか	パーセプトロン
0	1407	1104	自己学習の狙いは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1407	0701	自己学習の狙いは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1407	0802	自己学習の狙いは何ですか	特徴ベクトルの次元数
0	1407	1506	自己学習の狙いは何ですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	1407	1409	自己学習の狙いは何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1407	0305	自己学習の狙いは何ですか	仮説に対して課す制約
0	1407	0803	自己学習の狙いは何ですか	非線形識別面
0	1407	0513	自己学習の狙いは何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1407	0711	自己学習の狙いは何ですか	カーネル関数
0	1407	0805	自己学習の狙いは何ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1407	1301	自己学習の狙いは何ですか	動画像の分類や音声で入力された単語の識別などの問題
0	1407	0701	自己学習の狙いは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	1409	1409	自己学習の問題点は何がありますか	自分が出した誤りを指摘してくれる他人がいない
0	1409	0611	自己学習の問題点は何がありますか	識別における決定木の考え方を回帰問題に適用する方法
0	1409	0917	自己学習の問題点は何がありますか	LSTMセル
0	1409	0717	自己学習の問題点は何がありますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1409	0808	自己学習の問題点は何がありますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	1409	0901	自己学習の問題点は何がありますか	音声認識・画像認識・自然言語処理など
0	1409	1201	自己学習の問題点は何がありますか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1409	0611	自己学習の問題点は何がありますか	識別における決定木の考え方を回帰問題に適用する方法
0	1409	0901	自己学習の問題点は何がありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1409	0810	自己学習の問題点は何がありますか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1409	1012	自己学習の問題点は何がありますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1409	1207	自己学習の問題点は何がありますか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1409	1103	自己学習の問題点は何がありますか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1409	0710	自己学習の問題点は何がありますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1409	0911	自己学習の問題点は何がありますか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	1409	0402	自己学習の問題点は何がありますか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	1409	0508	自己学習の問題点は何がありますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1409	0802	自己学習の問題点は何がありますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1409	0710	自己学習の問題点は何がありますか	もとの空間におけるデータ間の距離関係を保存
0	1409	1209	自己学習の問題点は何がありますか	規則の条件部が起こったときに結論部が起こる割合
0	1409	0602	自己学習の問題点は何がありますか	正解情報$y$が数値であるということ
0	1409	0901	自己学習の問題点は何がありますか	Deep Neural Network (DNN) 
0	1409	0409	自己学習の問題点は何がありますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	1409	1214	自己学習の問題点は何がありますか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	1409	0103	自己学習の問題点は何がありますか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
1	1409	1409	共訓練ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1409	0708	共訓練ってなんですか	制約を弱める変数
0	1409	0405	共訓練ってなんですか	尤度と事前確率の積を最大とするクラス
0	1409	1402	共訓練ってなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	1409	0711	共訓練ってなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1409	0811	共訓練ってなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	1409	0405	共訓練ってなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	1409	0906	共訓練ってなんですか	十分多くの層
0	1409	0410	共訓練ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1409	0701	共訓練ってなんですか	サポートベクトルマシン
0	1409	0508	共訓練ってなんですか	最小二乗法
0	1409	0514	共訓練ってなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1409	0707	共訓練ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1409	0117	共訓練ってなんですか	学習データの一部にだけ正解が与えられている場合
0	1409	0611	共訓練ってなんですか	回帰
0	1409	0707	共訓練ってなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1409	0906	共訓練ってなんですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	1409	0901	共訓練ってなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1409	0211	共訓練ってなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1409	0802	共訓練ってなんですか	入力層・出力層の数に応じた適当な数
0	1409	1001	共訓練ってなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1409	0906	共訓練ってなんですか	入力に近い側の処理
0	1409	0402	共訓練ってなんですか	統計的識別手法
0	1409	0505	共訓練ってなんですか	ニューラルネットワーク
0	1409	0505	共訓練ってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
1	1411	1411	YATSIとは何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1411	0906	YATSIとは何ですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1411	0505	YATSIとは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1411	1103	YATSIとは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1411	0611	YATSIとは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1411	0701	YATSIとは何ですか	識別境界線と最も近いデータとの距離
0	1411	0704	YATSIとは何ですか	ラグランジュの未定乗数法
0	1411	1412	YATSIとは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1411	1004	YATSIとは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1411	1406	YATSIとは何ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	1411	1001	YATSIとは何ですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1411	0811	YATSIとは何ですか	ユニットの活性化関数を工夫する方法があります
0	1411	0402	YATSIとは何ですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1411	1205	YATSIとは何ですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1411	0810	YATSIとは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1411	1401	YATSIとは何ですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	1411	1012	YATSIとは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1411	1309	YATSIとは何ですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1411	0504	YATSIとは何ですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	1411	0305	YATSIとは何ですか	仮説に対して課す制約
0	1411	0508	YATSIとは何ですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	1411	1104	YATSIとは何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1411	0508	YATSIとは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	1411	1301	YATSIとは何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	1411	1106	YATSIとは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
1	1501	1501	強化学習はなぜ中間的学習なのですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1501	0112	強化学習はなぜ中間的学習なのですか	線形回帰，回帰木，モデル木など
0	1501	0906	強化学習はなぜ中間的学習なのですか	多階層構造でもそのまま適用できます
0	1501	0514	強化学習はなぜ中間的学習なのですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1501	1214	強化学習はなぜ中間的学習なのですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	1501	0311	強化学習はなぜ中間的学習なのですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1501	1007	強化学習はなぜ中間的学習なのですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1501	0803	強化学習はなぜ中間的学習なのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1501	1409	強化学習はなぜ中間的学習なのですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1501	0917	強化学習はなぜ中間的学習なのですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1501	0411	強化学習はなぜ中間的学習なのですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	1501	1001	強化学習はなぜ中間的学習なのですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1501	0109	強化学習はなぜ中間的学習なのですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	1501	0901	強化学習はなぜ中間的学習なのですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1501	0505	強化学習はなぜ中間的学習なのですか	パーセプトロン
0	1501	0105	強化学習はなぜ中間的学習なのですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1501	0307	強化学習はなぜ中間的学習なのですか	仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します
0	1501	0505	強化学習はなぜ中間的学習なのですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	1501	1503	強化学習はなぜ中間的学習なのですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	1501	0209	強化学習はなぜ中間的学習なのですか	正例がどれだけ正しく判定されているかという指標
0	1501	0508	強化学習はなぜ中間的学習なのですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1501	0811	強化学習はなぜ中間的学習なのですか	引数が負のときは0，0以上のときはその値を出力
0	1501	0311	強化学習はなぜ中間的学習なのですか	集合の乱雑さ
0	1501	0710	強化学習はなぜ中間的学習なのですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1501	0805	強化学習はなぜ中間的学習なのですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
1	1502	1502	意思決定エージェントの例は何ですか	将棋や囲碁などを行うプログラム
0	1502	0514	意思決定エージェントの例は何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	1502	1108	意思決定エージェントの例は何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1502	0204	意思決定エージェントの例は何ですか	特徴ベクトルの次元数を減らすこと
0	1502	0708	意思決定エージェントの例は何ですか	制約を弱める変数
0	1502	0805	意思決定エージェントの例は何ですか	誤差逆伝播法
0	1502	0710	意思決定エージェントの例は何ですか	低次元の特徴ベクトルを高次元に写像
0	1502	0707	意思決定エージェントの例は何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1502	1219	意思決定エージェントの例は何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1502	1201	意思決定エージェントの例は何ですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1502	0105	意思決定エージェントの例は何ですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	1502	0810	意思決定エージェントの例は何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1502	0315	意思決定エージェントの例は何ですか	分割後のデータの分散
0	1502	1106	意思決定エージェントの例は何ですか	最も近い事例対の距離を類似度とする
0	1502	0802	意思決定エージェントの例は何ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	1502	0505	意思決定エージェントの例は何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1502	1004	意思決定エージェントの例は何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1502	1509	意思決定エージェントの例は何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	1502	0810	意思決定エージェントの例は何ですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	1502	0109	意思決定エージェントの例は何ですか	正解が付いていない場合の学習
0	1502	0509	意思決定エージェントの例は何ですか	確率的最急勾配法
0	1502	1103	意思決定エージェントの例は何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	1502	0902	意思決定エージェントの例は何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1502	1411	意思決定エージェントの例は何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1502	1106	意思決定エージェントの例は何ですか	クラスタの重心間の距離を類似度とする
1	1503	1503	K-armed banditの学習結果は何ですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	1503	1104	K-armed banditの学習結果は何ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	1503	1313	K-armed banditの学習結果は何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1503	0406	K-armed banditの学習結果は何ですか	各クラスから生じる特徴の尤もらしさを表す
0	1503	0313	K-armed banditの学習結果は何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1503	0908	K-armed banditの学習結果は何ですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	1503	0810	K-armed banditの学習結果は何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1503	0912	K-armed banditの学習結果は何ですか	畳み込みニューラルネットワーク
0	1503	0611	K-armed banditの学習結果は何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1503	0512	K-armed banditの学習結果は何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1503	1110	K-armed banditの学習結果は何ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	1503	0211	K-armed banditの学習結果は何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	1503	0115	K-armed banditの学習結果は何ですか	パターンマイニング
0	1503	1007	K-armed banditの学習結果は何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	1503	1406	K-armed banditの学習結果は何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1503	0611	K-armed banditの学習結果は何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1503	1106	K-armed banditの学習結果は何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1503	0514	K-armed banditの学習結果は何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1503	0711	K-armed banditの学習結果は何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1503	1306	K-armed banditの学習結果は何ですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	1503	0917	K-armed banditの学習結果は何ですか	LSTM
0	1503	0606	K-armed banditの学習結果は何ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1503	0715	K-armed banditの学習結果は何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1503	1408	K-armed banditの学習結果は何ですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1503	0614	K-armed banditの学習結果は何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
1	1505	1505	マルコフ決定過程ってなんですか	「マルコフ性」を持つ確率過程における意思決定問題
0	1505	0504	マルコフ決定過程ってなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	1505	0711	マルコフ決定過程ってなんですか	カーネル関数
0	1505	0410	マルコフ決定過程ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1505	0803	マルコフ決定過程ってなんですか	非線形識別面
0	1505	0805	マルコフ決定過程ってなんですか	誤差逆伝播法
0	1505	0610	マルコフ決定過程ってなんですか	学習結果の散らばり具合
0	1505	0114	マルコフ決定過程ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1505	0505	マルコフ決定過程ってなんですか	生物の神経細胞の仕組みをモデル化したもの
0	1505	0704	マルコフ決定過程ってなんですか	ラグランジュの未定乗数法を不等式制約条件
0	1505	1508	マルコフ決定過程ってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1505	1401	マルコフ決定過程ってなんですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	1505	0416	マルコフ決定過程ってなんですか	値が真となる確率を知りたいノードが表す変数
0	1505	1304	マルコフ決定過程ってなんですか	入力と対応させる素性
0	1505	0412	マルコフ決定過程ってなんですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1505	0708	マルコフ決定過程ってなんですか	制約を満たさない程度を表すので，小さい方が望ましい
0	1505	0410	マルコフ決定過程ってなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1505	0313	マルコフ決定過程ってなんですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	1505	0610	マルコフ決定過程ってなんですか	片方を減らせば片方が増える
0	1505	0306	マルコフ決定過程ってなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	1505	0109	マルコフ決定過程ってなんですか	正解が付いていない場合の学習
0	1505	0404	マルコフ決定過程ってなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1505	0405	マルコフ決定過程ってなんですか	尤度と事前確率の積を最大とするクラス
0	1505	1304	マルコフ決定過程ってなんですか	出力系列を参照する素性
0	1505	0801	マルコフ決定過程ってなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	1505	1505	マルコフ性とは何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	1505	0410	マルコフ性とは何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1505	0114	マルコフ性とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1505	1106	マルコフ性とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1505	1004	マルコフ性とは何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1505	0707	マルコフ性とは何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1505	1220	マルコフ性とは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	1505	0801	マルコフ性とは何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1505	0810	マルコフ性とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1505	1108	マルコフ性とは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1505	0313	マルコフ性とは何ですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	1505	0810	マルコフ性とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1505	0912	マルコフ性とは何ですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1505	0612	マルコフ性とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1505	0505	マルコフ性とは何ですか	生物の神経細胞の仕組みをモデル化したもの
0	1505	1301	マルコフ性とは何ですか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	1505	0701	マルコフ性とは何ですか	線形で識別できないデータに対応するため
0	1505	0611	マルコフ性とは何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	1505	0413	マルコフ性とは何ですか	変数間の独立性を表現できること
0	1505	0708	マルコフ性とは何ですか	制約を弱める変数
0	1505	0906	マルコフ性とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1505	0811	マルコフ性とは何ですか	ユニットの活性化関数を工夫する方法があります
0	1505	0810	マルコフ性とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1505	0701	マルコフ性とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1505	0810	マルコフ性とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
1	1506	1506	割引率とは何ですか	後に得られる報酬ほど割り引いて計算するための係数
0	1506	0901	割引率とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1506	1106	割引率とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	1506	0508	割引率とは何ですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1506	0612	割引率とは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1506	0305	割引率とは何ですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1506	0704	割引率とは何ですか	以下の関数$L$の最小値を求めるという問題
0	1506	1301	割引率とは何ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	1506	0708	割引率とは何ですか	制約を弱める変数
0	1506	0906	割引率とは何ですか	十分多くの層を持つニューラルネットワーク
0	1506	0717	割引率とは何ですか	グリッド
0	1506	1114	割引率とは何ですか	クラスタリング結果のデータ数の分布
0	1506	1404	割引率とは何ですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	1506	0508	割引率とは何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1506	0105	割引率とは何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	1506	0402	割引率とは何ですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1506	0916	割引率とは何ですか	リカレントニューラルネットワーク
0	1506	1407	割引率とは何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1506	0901	割引率とは何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1506	0514	割引率とは何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1506	1106	割引率とは何ですか	最も遠い事例対の距離を類似度とする
0	1506	1302	割引率とは何ですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	1506	1407	割引率とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	1506	0115	割引率とは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	1506	0111	割引率とは何ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
1	1506	1506	なぜ割引率が必要なのですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1506	0610	なぜ割引率が必要なのですか	真のモデルとの距離
0	1506	0104	なぜ割引率が必要なのですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	1506	0902	なぜ割引率が必要なのですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1506	0911	なぜ割引率が必要なのですか	ドロップアウト
0	1506	0808	なぜ割引率が必要なのですか	入力の重み付き和の微分
0	1506	0307	なぜ割引率が必要なのですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	1506	0606	なぜ割引率が必要なのですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	1506	0611	なぜ割引率が必要なのですか	回帰
0	1506	0606	なぜ割引率が必要なのですか	入力が少し変化したときに，出力も少し変化する
0	1506	0802	なぜ割引率が必要なのですか	特徴ベクトルの次元数
0	1506	0803	なぜ割引率が必要なのですか	非線形識別面
0	1506	0701	なぜ割引率が必要なのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1506	1110	なぜ割引率が必要なのですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1506	0114	なぜ割引率が必要なのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1506	0114	なぜ割引率が必要なのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1506	0701	なぜ割引率が必要なのですか	サポートベクトルマシン
0	1506	0704	なぜ割引率が必要なのですか	ラグランジュの未定乗数法
0	1506	1012	なぜ割引率が必要なのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1506	1508	なぜ割引率が必要なのですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1506	1106	なぜ割引率が必要なのですか	クラスタの重心間の距離を類似度とする
0	1506	1001	なぜ割引率が必要なのですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	1506	1004	なぜ割引率が必要なのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1506	1509	なぜ割引率が必要なのですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	1506	0810	なぜ割引率が必要なのですか	誤差が小さくなって消失してしまう
1	1508	1508	ベルマン方程式ってなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1508	1510	ベルマン方程式ってなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	1508	0510	ベルマン方程式ってなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1508	0114	ベルマン方程式ってなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	1508	0902	ベルマン方程式ってなんですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	1508	0906	ベルマン方程式ってなんですか	十分多くの層
0	1508	1409	ベルマン方程式ってなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	1508	0916	ベルマン方程式ってなんですか	リカレントニューラルネットワーク
0	1508	0708	ベルマン方程式ってなんですか	制約を弱める変数
0	1508	0906	ベルマン方程式ってなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1508	0811	ベルマン方程式ってなんですか	ユニットの活性化関数を工夫する方法があります
0	1508	0715	ベルマン方程式ってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1508	0305	ベルマン方程式ってなんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	1508	0702	ベルマン方程式ってなんですか	識別面は平面を仮定する
0	1508	0906	ベルマン方程式ってなんですか	多階層構造でもそのまま適用できます
0	1508	0209	ベルマン方程式ってなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1508	1106	ベルマン方程式ってなんですか	最も遠い事例対の距離を類似度とする
0	1508	0503	ベルマン方程式ってなんですか	様々な数値データに対して多く用いられる統計モデル
0	1508	1313	ベルマン方程式ってなんですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	1508	0701	ベルマン方程式ってなんですか	サポートベクトルマシン
0	1508	0610	ベルマン方程式ってなんですか	真のモデルとの距離
0	1508	0917	ベルマン方程式ってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	1508	1411	ベルマン方程式ってなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	1508	0611	ベルマン方程式ってなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	1508	1205	ベルマン方程式ってなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
1	1510	1510	なぜTD学習において温度を学習が進むにつれて小さくするのですか	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	1510	0114	なぜTD学習において温度を学習が進むにつれて小さくするのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1510	0611	なぜTD学習において温度を学習が進むにつれて小さくするのですか	識別における決定木の考え方を回帰問題に適用する方法
0	1510	0802	なぜTD学習において温度を学習が進むにつれて小さくするのですか	多層パーセプトロンあるいはニューラルネットワーク
0	1510	0512	なぜTD学習において温度を学習が進むにつれて小さくするのですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1510	0717	なぜTD学習において温度を学習が進むにつれて小さくするのですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	1510	0503	なぜTD学習において温度を学習が進むにつれて小さくするのですか	様々な数値データに対して多く用いられる統計モデル
0	1510	1309	なぜTD学習において温度を学習が進むにつれて小さくするのですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1510	0405	なぜTD学習において温度を学習が進むにつれて小さくするのですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	1510	0704	なぜTD学習において温度を学習が進むにつれて小さくするのですか	以下の関数$L$の最小値を求めるという問題
0	1510	0701	なぜTD学習において温度を学習が進むにつれて小さくするのですか	サポートベクトルマシン
0	1510	1408	なぜTD学習において温度を学習が進むにつれて小さくするのですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1510	1108	なぜTD学習において温度を学習が進むにつれて小さくするのですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1510	0802	なぜTD学習において温度を学習が進むにつれて小さくするのですか	隠れ層
0	1510	1209	なぜTD学習において温度を学習が進むにつれて小さくするのですか	この値が高いほど，得られる情報の多い規則であること
0	1510	0105	なぜTD学習において温度を学習が進むにつれて小さくするのですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	1510	0406	なぜTD学習において温度を学習が進むにつれて小さくするのですか	各クラスから生じる特徴の尤もらしさを表す
0	1510	1215	なぜTD学習において温度を学習が進むにつれて小さくするのですか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	1510	0701	なぜTD学習において温度を学習が進むにつれて小さくするのですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1510	1012	なぜTD学習において温度を学習が進むにつれて小さくするのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1510	0404	なぜTD学習において温度を学習が進むにつれて小さくするのですか	事前確率
0	1510	1010	なぜTD学習において温度を学習が進むにつれて小さくするのですか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	1510	0204	なぜTD学習において温度を学習が進むにつれて小さくするのですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	1510	0616	なぜTD学習において温度を学習が進むにつれて小さくするのですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	1510	0508	なぜTD学習において温度を学習が進むにつれて小さくするのですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
1	0402	0402	最大事後確率則って何ですか	事後確率が最大となるクラスを識別結果とする方法
0	0402	0717	最大事後確率則って何ですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0402	0416	最大事後確率則って何ですか	値が真となる確率を知りたいノードが表す変数
0	0402	0810	最大事後確率則って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0402	1506	最大事後確率則って何ですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0402	0607	最大事後確率則って何ですか	「投げ縄」という意味
0	0402	0702	最大事後確率則って何ですか	識別面は平面を仮定する
0	0402	0411	最大事後確率則って何ですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0402	0701	最大事後確率則って何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0402	1103	最大事後確率則って何ですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0402	0717	最大事後確率則って何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0402	1508	最大事後確率則って何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0402	0102	最大事後確率則って何ですか	現在，人が行っている知的な判断を代わりに行う技術
0	0402	0103	最大事後確率則って何ですか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0402	0711	最大事後確率則って何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0402	0907	最大事後確率則って何ですか	事前学習法
0	0402	0906	最大事後確率則って何ですか	十分多くの層
0	0402	0409	最大事後確率則って何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0402	0205	最大事後確率則って何ですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0402	1510	最大事後確率則って何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0402	0114	最大事後確率則って何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0402	0304	最大事後確率則って何ですか	個々の事例から，あるクラスについて共通点を見つけること
0	0402	0810	最大事後確率則って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0402	0811	最大事後確率則って何ですか	ReLu
0	0402	0514	最大事後確率則って何ですか	ランダムに学習データを一つ
1	0410	0410	ナイーブベイズ識別って何ですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0410	0117	ナイーブベイズ識別って何ですか	学習データの一部にだけ正解が与えられている場合
0	0410	0614	ナイーブベイズ識別って何ですか	モデル木
0	0410	1505	ナイーブベイズ識別って何ですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0410	0911	ナイーブベイズ識別って何ですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0410	0114	ナイーブベイズ識別って何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0410	0703	ナイーブベイズ識別って何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0410	0512	ナイーブベイズ識別って何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0410	0717	ナイーブベイズ識別って何ですか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0410	0907	ナイーブベイズ識別って何ですか	事前学習法
0	0410	1407	ナイーブベイズ識別って何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0410	1001	ナイーブベイズ識別って何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0410	0802	ナイーブベイズ識別って何ですか	識別対象のクラス数
0	0410	1004	ナイーブベイズ識別って何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0410	0602	ナイーブベイズ識別って何ですか	正解情報$y$が数値であるということ
0	0410	0708	ナイーブベイズ識別って何ですか	制約を弱める変数
0	0410	1116	ナイーブベイズ識別って何ですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0410	1220	ナイーブベイズ識別って何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0410	0416	ナイーブベイズ識別って何ですか	値が真となる確率を知りたいノードが表す変数
0	0410	0701	ナイーブベイズ識別って何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0410	0509	ナイーブベイズ識別って何ですか	確率的最急勾配法
0	0410	1402	ナイーブベイズ識別って何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	0410	0507	ナイーブベイズ識別って何ですか	全ての誤りがなくなることが学習の終了条件なので
0	0410	0907	ナイーブベイズ識別って何ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0410	0506	ナイーブベイズ識別って何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
1	0416	0416	どういうときに確率伝播を使うんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0416	0917	どういうときに確率伝播を使うんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0416	0502	どういうときに確率伝播を使うんですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0416	0410	どういうときに確率伝播を使うんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0416	1502	どういうときに確率伝播を使うんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0416	0504	どういうときに確率伝播を使うんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0416	1506	どういうときに確率伝播を使うんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0416	0502	どういうときに確率伝播を使うんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0416	0508	どういうときに確率伝播を使うんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0416	1508	どういうときに確率伝播を使うんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0416	1001	どういうときに確率伝播を使うんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0416	0811	どういうときに確率伝播を使うんですか	半分の領域で勾配が1になるので
0	0416	1403	どういうときに確率伝播を使うんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0416	1209	どういうときに確率伝播を使うんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0416	1114	どういうときに確率伝播を使うんですか	クラスタリング結果のデータ数の分布から
0	0416	0704	どういうときに確率伝播を使うんですか	ラグランジュの未定乗数法を不等式制約条件
0	0416	0605	どういうときに確率伝播を使うんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0416	0902	どういうときに確率伝播を使うんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0416	0801	どういうときに確率伝播を使うんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0416	0115	どういうときに確率伝播を使うんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0416	0715	どういうときに確率伝播を使うんですか	識別面
0	0416	0406	どういうときに確率伝播を使うんですか	各クラスから生じる特徴の尤もらしさを表す
0	0416	1106	どういうときに確率伝播を使うんですか	最も近い事例対の距離を類似度とする
0	0416	0901	どういうときに確率伝播を使うんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0416	1008	どういうときに確率伝播を使うんですか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
1	0416	0416	どういうときに確率的シミュレーションを使うんですか	アークを無向とみなした結合を考えたとき
0	0416	0508	どういうときに確率的シミュレーションを使うんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0416	0917	どういうときに確率的シミュレーションを使うんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0416	0103	どういうときに確率的シミュレーションを使うんですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0416	0508	どういうときに確率的シミュレーションを使うんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0416	0105	どういうときに確率的シミュレーションを使うんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0416	0711	どういうときに確率的シミュレーションを使うんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0416	1001	どういうときに確率的シミュレーションを使うんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0416	0406	どういうときに確率的シミュレーションを使うんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0416	0404	どういうときに確率的シミュレーションを使うんですか	事前確率
0	0416	0116	どういうときに確率的シミュレーションを使うんですか	学習データが教師あり／教師なしの混在となっているもの
0	0416	0901	どういうときに確率的シミュレーションを使うんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0416	0402	どういうときに確率的シミュレーションを使うんですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0416	1015	どういうときに確率的シミュレーションを使うんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0416	0906	どういうときに確率的シミュレーションを使うんですか	多階層構造でもそのまま適用できます
0	0416	0601	どういうときに確率的シミュレーションを使うんですか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0416	1114	どういうときに確率的シミュレーションを使うんですか	クラスタリング結果のデータ数の分布から
0	0416	0209	どういうときに確率的シミュレーションを使うんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0416	0506	どういうときに確率的シミュレーションを使うんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0416	0514	どういうときに確率的シミュレーションを使うんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0416	0701	どういうときに確率的シミュレーションを使うんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0416	0111	どういうときに確率的シミュレーションを使うんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0416	1303	どういうときに確率的シミュレーションを使うんですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0416	0901	どういうときに確率的シミュレーションを使うんですか	深層学習に用いるニューラルネットワーク
0	0416	0902	どういうときに確率的シミュレーションを使うんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
1	0502	0502	数値特徴はカテゴリ特徴とどう違うんですか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0502	0602	数値特徴はカテゴリ特徴とどう違うんですか	数値型の正解情報のこと
0	0502	0105	数値特徴はカテゴリ特徴とどう違うんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0502	0411	数値特徴はカテゴリ特徴とどう違うんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0502	0114	数値特徴はカテゴリ特徴とどう違うんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0502	0505	数値特徴はカテゴリ特徴とどう違うんですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0502	0204	数値特徴はカテゴリ特徴とどう違うんですか	特徴ベクトルの次元数を減らすこと
0	0502	0802	数値特徴はカテゴリ特徴とどう違うんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0502	0701	数値特徴はカテゴリ特徴とどう違うんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0502	0508	数値特徴はカテゴリ特徴とどう違うんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0502	0606	数値特徴はカテゴリ特徴とどう違うんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0502	0411	数値特徴はカテゴリ特徴とどう違うんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0502	0508	数値特徴はカテゴリ特徴とどう違うんですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0502	0614	数値特徴はカテゴリ特徴とどう違うんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0502	0508	数値特徴はカテゴリ特徴とどう違うんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0502	0715	数値特徴はカテゴリ特徴とどう違うんですか	カーネルトリック
0	0502	0509	数値特徴はカテゴリ特徴とどう違うんですか	確率的最急勾配法
0	0502	0407	数値特徴はカテゴリ特徴とどう違うんですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0502	0510	数値特徴はカテゴリ特徴とどう違うんですか	特徴空間上でクラスを分割する面
0	0502	0803	数値特徴はカテゴリ特徴とどう違うんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0502	0917	数値特徴はカテゴリ特徴とどう違うんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0502	0504	数値特徴はカテゴリ特徴とどう違うんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0502	0116	数値特徴はカテゴリ特徴とどう違うんですか	学習データが教師あり／教師なしの混在となっているもの
0	0502	0417	数値特徴はカテゴリ特徴とどう違うんですか	ネットワークの構造とアークの条件付き確率表
0	0502	0514	数値特徴はカテゴリ特徴とどう違うんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
1	0505	0505	識別モデルとはどういうものですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0505	1112	識別モデルとはどういうものですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0505	1012	識別モデルとはどういうものですか	すべてのデータの重みは平等
0	0505	0713	識別モデルとはどういうものですか	文書分類やバイオインフォマティックスなど
0	0505	1301	識別モデルとはどういうものですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0505	0802	識別モデルとはどういうものですか	識別対象のクラス数
0	0505	0911	識別モデルとはどういうものですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0505	0902	識別モデルとはどういうものですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0505	1104	識別モデルとはどういうものですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0505	0805	識別モデルとはどういうものですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0505	0509	識別モデルとはどういうものですか	確率的最急勾配法
0	0505	1310	識別モデルとはどういうものですか	Hidden Marcov Model: 隠れマルコフモデル
0	0505	0717	識別モデルとはどういうものですか	連続値
0	0505	0701	識別モデルとはどういうものですか	識別境界線と最も近いデータとの距離
0	0505	0508	識別モデルとはどういうものですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0505	0104	識別モデルとはどういうものですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0505	0110	識別モデルとはどういうものですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0505	0713	識別モデルとはどういうものですか	文書分類やバイオインフォマティックスなど
0	0505	0514	識別モデルとはどういうものですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0505	1406	識別モデルとはどういうものですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0505	1009	識別モデルとはどういうものですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0505	0701	識別モデルとはどういうものですか	サポートベクトルマシン
0	0505	1001	識別モデルとはどういうものですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0505	0801	識別モデルとはどういうものですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0505	1001	識別モデルとはどういうものですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0507	0507	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	全ての誤りがなくなることが学習の終了条件なので
0	0507	0701	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	サポートベクトルマシン
0	0507	0614	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0507	0111	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0507	0109	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	正解が付いていない場合の学習
0	0507	0410	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0507	0417	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	ネットワークの構造とアークの条件付き確率
0	0507	0708	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	制約を弱める変数
0	0507	1220	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0507	1004	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0507	0802	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	隠れ層
0	0507	1506	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0507	1214	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0507	0307	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0507	0901	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0507	1304	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	入力と対応させる素性
0	0507	0715	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	複雑な非線形変換を求めるという操作を避ける方法
0	0507	1306	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0507	0313	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0507	0802	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	特徴ベクトルの次元数
0	0507	0810	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0507	0302	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	カテゴリ形式の正解情報のこと
0	0507	0803	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0507	1103	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	一つのまとまりと認められるデータは相互の距離がなるべく近くなるように
0	0507	1001	なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0508	0508	なぜ最小二乗法では正解との誤差を二乗するんですか	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0508	0507	なぜ最小二乗法では正解との誤差を二乗するんですか	全ての誤りがなくなることが学習の終了条件なので
0	0508	0707	なぜ最小二乗法では正解との誤差を二乗するんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0508	0610	なぜ最小二乗法では正解との誤差を二乗するんですか	片方を減らせば片方が増える
0	0508	0315	なぜ最小二乗法では正解との誤差を二乗するんですか	分割後のデータの分散
0	0508	1401	なぜ最小二乗法では正解との誤差を二乗するんですか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0508	0701	なぜ最小二乗法では正解との誤差を二乗するんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0508	0708	なぜ最小二乗法では正解との誤差を二乗するんですか	制約を弱める変数
0	0508	0717	なぜ最小二乗法では正解との誤差を二乗するんですか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0508	1106	なぜ最小二乗法では正解との誤差を二乗するんですか	最も近い事例対の距離を類似度とする
0	0508	1004	なぜ最小二乗法では正解との誤差を二乗するんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0508	0710	なぜ最小二乗法では正解との誤差を二乗するんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0508	0614	なぜ最小二乗法では正解との誤差を二乗するんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0508	1310	なぜ最小二乗法では正解との誤差を二乗するんですか	確率的非決定性オートマトンの一種
0	0508	0803	なぜ最小二乗法では正解との誤差を二乗するんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0508	1501	なぜ最小二乗法では正解との誤差を二乗するんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0508	1203	なぜ最小二乗法では正解との誤差を二乗するんですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0508	0711	なぜ最小二乗法では正解との誤差を二乗するんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0508	0115	なぜ最小二乗法では正解との誤差を二乗するんですか	Apriori アルゴリズムやその高速化版である FP-Growth 
0	0508	0917	なぜ最小二乗法では正解との誤差を二乗するんですか	入力ゲート・出力ゲート・忘却ゲート
0	0508	1106	なぜ最小二乗法では正解との誤差を二乗するんですか	最も遠い事例対の距離を類似度とする
0	0508	1502	なぜ最小二乗法では正解との誤差を二乗するんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0508	1103	なぜ最小二乗法では正解との誤差を二乗するんですか	異なったまとまり間の距離はなるべく遠くなるように
0	0508	1412	なぜ最小二乗法では正解との誤差を二乗するんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0508	1301	なぜ最小二乗法では正解との誤差を二乗するんですか	個々の要素の間に i.i.d. の関係が成立しないもの
1	0512	0512	最急勾配法って何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0512	0901	最急勾配法って何ですか	表現学習
0	0512	0707	最急勾配法って何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0512	1001	最急勾配法って何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0512	0411	最急勾配法って何ですか	確率のm推定という考え方を用います
0	0512	0316	最急勾配法って何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0512	1409	最急勾配法って何ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0512	1301	最急勾配法って何ですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0512	0810	最急勾配法って何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0512	0402	最急勾配法って何ですか	最大事後確率則
0	0512	0513	最急勾配法って何ですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0512	0801	最急勾配法って何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0512	0113	最急勾配法って何ですか	入力データに潜む規則性を学習すること
0	0512	0811	最急勾配法って何ですか	ReLu
0	0512	1303	最急勾配法って何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0512	0901	最急勾配法って何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0512	0906	最急勾配法って何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0512	0506	最急勾配法って何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0512	0402	最急勾配法って何ですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0512	0111	最急勾配法って何ですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0512	0614	最急勾配法って何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0512	0601	最急勾配法って何ですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0512	0901	最急勾配法って何ですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0512	0911	最急勾配法って何ですか	ドロップアウト
0	0512	1507	最急勾配法って何ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
1	0512	0512	ロジスティック識別器って何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0512	0901	ロジスティック識別器って何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0512	0102	ロジスティック識別器って何ですか	現在，人が行っている知的な判断を代わりに行う技術
0	0512	0912	ロジスティック識別器って何ですか	画像認識
0	0512	1219	ロジスティック識別器って何ですか	どの個人がどの商品を購入したかが記録されているデータ
0	0512	0911	ロジスティック識別器って何ですか	ランダムに一定割合のユニットを消して学習を行う
0	0512	1007	ロジスティック識別器って何ですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0512	0514	ロジスティック識別器って何ですか	ランダムに学習データを一つ
0	0512	0717	ロジスティック識別器って何ですか	グリッド
0	0512	0206	ロジスティック識別器って何ですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0512	0717	ロジスティック識別器って何ですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0512	0908	ロジスティック識別器って何ですか	シグモイド関数
0	0512	0611	ロジスティック識別器って何ですか	回帰
0	0512	0902	ロジスティック識別器って何ですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0512	0417	ロジスティック識別器って何ですか	ネットワークの構造とアークの条件付き確率
0	0512	1412	ロジスティック識別器って何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0512	1108	ロジスティック識別器って何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0512	0701	ロジスティック識別器って何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0512	0506	ロジスティック識別器って何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0512	0711	ロジスティック識別器って何ですか	カーネル関数
0	0512	0612	ロジスティック識別器って何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0512	0917	ロジスティック識別器って何ですか	LSTMセル
0	0512	0113	ロジスティック識別器って何ですか	入力データに潜む規則性を学習すること
0	0512	0701	ロジスティック識別器って何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0512	0916	ロジスティック識別器って何ですか	リカレントニューラルネットワーク
1	0513	0513	最急勾配法はいつ止まるんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0513	1505	最急勾配法はいつ止まるんですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0513	0111	最急勾配法はいつ止まるんですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0513	0907	最急勾配法はいつ止まるんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0513	0710	最急勾配法はいつ止まるんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0513	0109	最急勾配法はいつ止まるんですか	正解が付いていない場合の学習
0	0513	1012	最急勾配法はいつ止まるんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0513	0413	最急勾配法はいつ止まるんですか	変数間の独立性を表現できること
0	0513	0109	最急勾配法はいつ止まるんですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0513	0906	最急勾配法はいつ止まるんですか	十分多くの層を持つニューラルネットワーク
0	0513	0704	最急勾配法はいつ止まるんですか	以下の関数$L$の最小値を求めるという問題
0	0513	1506	最急勾配法はいつ止まるんですか	政策
0	0513	0916	最急勾配法はいつ止まるんですか	リカレントニューラルネットワーク
0	0513	0715	最急勾配法はいつ止まるんですか	カーネルトリック
0	0513	0811	最急勾配法はいつ止まるんですか	誤差が消失しません
0	0513	0510	最急勾配法はいつ止まるんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0513	0114	最急勾配法はいつ止まるんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0513	1110	最急勾配法はいつ止まるんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0513	0111	最急勾配法はいつ止まるんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0513	1219	最急勾配法はいつ止まるんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0513	0505	最急勾配法はいつ止まるんですか	生物の神経細胞の仕組みをモデル化したもの
0	0513	1219	最急勾配法はいつ止まるんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0513	0209	最急勾配法はいつ止まるんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0513	1114	最急勾配法はいつ止まるんですか	クラスタリング結果のデータ数の分布
0	0513	0802	最急勾配法はいつ止まるんですか	特徴ベクトルの次元数
1	0514	0514	確率的最急勾配法はどうやってデータを選びますか	ランダムに学習データを一つ
0	0514	0209	確率的最急勾配法はどうやってデータを選びますか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0514	0717	確率的最急勾配法はどうやってデータを選びますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0514	1110	確率的最急勾配法はどうやってデータを選びますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0514	0911	確率的最急勾配法はどうやってデータを選びますか	学習時の自由度を意図的に下げていること
0	0514	0917	確率的最急勾配法はどうやってデータを選びますか	LSTM
0	0514	1301	確率的最急勾配法はどうやってデータを選びますか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0514	1502	確率的最急勾配法はどうやってデータを選びますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0514	0912	確率的最急勾配法はどうやってデータを選びますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0514	0802	確率的最急勾配法はどうやってデータを選びますか	多層パーセプトロンあるいはニューラルネットワーク
0	0514	1108	確率的最急勾配法はどうやってデータを選びますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0514	0417	確率的最急勾配法はどうやってデータを選びますか	ネットワークの構造とアークの条件付き確率表
0	0514	1405	確率的最急勾配法はどうやってデータを選びますか	半教師あり学習は文書分類問題によく適用されます
0	0514	0211	確率的最急勾配法はどうやってデータを選びますか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0514	0204	確率的最急勾配法はどうやってデータを選びますか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0514	1503	確率的最急勾配法はどうやってデータを選びますか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0514	0717	確率的最急勾配法はどうやってデータを選びますか	連続値
0	0514	1505	確率的最急勾配法はどうやってデータを選びますか	「マルコフ性」を持つ確率過程における意思決定問題
0	0514	0209	確率的最急勾配法はどうやってデータを選びますか	正例がどれだけ正しく判定されているかという指標
0	0514	0901	確率的最急勾配法はどうやってデータを選びますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0514	0114	確率的最急勾配法はどうやってデータを選びますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0514	0404	確率的最急勾配法はどうやってデータを選びますか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0514	0313	確率的最急勾配法はどうやってデータを選びますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0514	0901	確率的最急勾配法はどうやってデータを選びますか	深層学習に用いるニューラルネットワーク
0	0514	0911	確率的最急勾配法はどうやってデータを選びますか	ドロップアウト
1	0602	0602	回帰問題は識別問題とどう違うんですか	正解情報$y$が数値であるということ
0	0602	0506	回帰問題は識別問題とどう違うんですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0602	0114	回帰問題は識別問題とどう違うんですか	階層的クラスタリングや k-means 法
0	0602	1001	回帰問題は識別問題とどう違うんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0602	0504	回帰問題は識別問題とどう違うんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0602	1302	回帰問題は識別問題とどう違うんですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0602	0305	回帰問題は識別問題とどう違うんですか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0602	0307	回帰問題は識別問題とどう違うんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	0602	0612	回帰問題は識別問題とどう違うんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0602	1106	回帰問題は識別問題とどう違うんですか	クラスタの重心間の距離を類似度とする
0	0602	1501	回帰問題は識別問題とどう違うんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0602	0901	回帰問題は識別問題とどう違うんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0602	1508	回帰問題は識別問題とどう違うんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0602	0510	回帰問題は識別問題とどう違うんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0602	0614	回帰問題は識別問題とどう違うんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0602	1214	回帰問題は識別問題とどう違うんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0602	0907	回帰問題は識別問題とどう違うんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0602	0307	回帰問題は識別問題とどう違うんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0602	0104	回帰問題は識別問題とどう違うんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0602	0313	回帰問題は識別問題とどう違うんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0602	0607	回帰問題は識別問題とどう違うんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0602	0514	回帰問題は識別問題とどう違うんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0602	1106	回帰問題は識別問題とどう違うんですか	最も遠い事例対の距離を類似度とする
0	0602	0413	回帰問題は識別問題とどう違うんですか	変数間の独立性を表現できること
0	0602	1106	回帰問題は識別問題とどう違うんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
1	0606	0606	なぜパラメータを0に近づけるんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0606	0701	なぜパラメータを0に近づけるんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0606	0917	なぜパラメータを0に近づけるんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0606	0404	なぜパラメータを0に近づけるんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0606	0802	なぜパラメータを0に近づけるんですか	識別対象のクラス数
0	0606	1301	なぜパラメータを0に近づけるんですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0606	1104	なぜパラメータを0に近づけるんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0606	0715	なぜパラメータを0に近づけるんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0606	0502	なぜパラメータを0に近づけるんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0606	0111	なぜパラメータを0に近づけるんですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	0606	0614	なぜパラメータを0に近づけるんですか	回帰木と線形回帰の双方のよいところを取った
0	0606	0912	なぜパラメータを0に近づけるんですか	畳み込みニューラルネットワーク
0	0606	0906	なぜパラメータを0に近づけるんですか	多階層構造でもそのまま適用できます
0	0606	0704	なぜパラメータを0に近づけるんですか	ラグランジュの未定乗数法
0	0606	0601	なぜパラメータを0に近づけるんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0606	0512	なぜパラメータを0に近づけるんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0606	0109	なぜパラメータを0に近づけるんですか	学習データに正解が付いている場合の学習
0	0606	0602	なぜパラメータを0に近づけるんですか	数値型の正解情報のこと
0	0606	0707	なぜパラメータを0に近づけるんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0606	0614	なぜパラメータを0に近づけるんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0606	1506	なぜパラメータを0に近づけるんですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0606	0801	なぜパラメータを0に近づけるんですか	このモデルは生物の神経細胞のモデルであると考えられています
0	0606	0901	なぜパラメータを0に近づけるんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0606	0710	なぜパラメータを0に近づけるんですか	低次元の特徴ベクトルを高次元に写像
0	0606	0710	なぜパラメータを0に近づけるんですか	もとの空間におけるデータ間の距離関係を保存
1	0611	0611	回帰木って何ですか	識別における決定木の考え方を回帰問題に適用する方法
0	0611	1004	回帰木って何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0611	0306	回帰木って何ですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0611	0708	回帰木って何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0611	0811	回帰木って何ですか	半分の領域で勾配が1になるので
0	0611	0906	回帰木って何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0611	0409	回帰木って何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0611	1509	回帰木って何ですか	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0611	1301	回帰木って何ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0611	0116	回帰木って何ですか	学習データが教師あり／教師なしの混在となっているもの
0	0611	0606	回帰木って何ですか	山の尾根という意味
0	0611	0105	回帰木って何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0611	0811	回帰木って何ですか	引数が負のときは0，0以上のときはその値を出力
0	0611	1108	回帰木って何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0611	0708	回帰木って何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0611	1112	回帰木って何ですか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0611	1214	回帰木って何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0611	0512	回帰木って何ですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0611	0416	回帰木って何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0611	0105	回帰木って何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0611	0811	回帰木って何ですか	半分の領域で勾配が1になるので
0	0611	0612	回帰木って何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0611	1103	回帰木って何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	0611	0701	回帰木って何ですか	サポートベクトルマシン
0	0611	0508	回帰木って何ですか	二乗誤差を最小にするように識別関数を調整する方法
1	0612	0612	CARTは回帰木とどう違いますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0612	0811	CARTは回帰木とどう違いますか	引数が負のときは0，0以上のときはその値を出力
0	0612	0906	CARTは回帰木とどう違いますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0612	0916	CARTは回帰木とどう違いますか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0612	0911	CARTは回帰木とどう違いますか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0612	0811	CARTは回帰木とどう違いますか	ReLu
0	0612	0114	CARTは回帰木とどう違いますか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0612	0701	CARTは回帰木とどう違いますか	線形で識別できないデータに対応するため
0	0612	0802	CARTは回帰木とどう違いますか	隠れ層
0	0612	0912	CARTは回帰木とどう違いますか	畳み込みニューラルネットワーク
0	0612	0304	CARTは回帰木とどう違いますか	個々の事例から，あるクラスについて共通点を見つけること
0	0612	0205	CARTは回帰木とどう違いますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0612	1407	CARTは回帰木とどう違いますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0612	0412	CARTは回帰木とどう違いますか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0612	1108	CARTは回帰木とどう違いますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0612	0701	CARTは回帰木とどう違いますか	サポートベクトルマシン
0	0612	1108	CARTは回帰木とどう違いますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0612	0109	CARTは回帰木とどう違いますか	正解が付いていない場合の学習
0	0612	0114	CARTは回帰木とどう違いますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0612	0402	CARTは回帰木とどう違いますか	最大事後確率則
0	0612	1411	CARTは回帰木とどう違いますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0612	0508	CARTは回帰木とどう違いますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0612	1214	CARTは回帰木とどう違いますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0612	0917	CARTは回帰木とどう違いますか	LSTM
0	0612	0715	CARTは回帰木とどう違いますか	複雑な非線形変換を求めるという操作を避ける方法
1	0612	0612	CARTって何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0612	0405	CARTって何ですか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0612	0105	CARTって何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0612	0801	CARTって何ですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0612	1001	CARTって何ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0612	0805	CARTって何ですか	誤差逆伝播法
0	0612	0701	CARTって何ですか	サポートベクトルマシン
0	0612	0917	CARTって何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0612	0209	CARTって何ですか	正例がどれだけ正しく判定されているかという指標
0	0612	1004	CARTって何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0612	0906	CARTって何ですか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0612	1214	CARTって何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0612	1106	CARTって何ですか	最も近い事例対の距離を類似度とする
0	0612	0701	CARTって何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0612	1306	CARTって何ですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0612	1103	CARTって何ですか	異なったまとまり間の距離はなるべく遠くなるように
0	0612	0811	CARTって何ですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0612	1209	CARTって何ですか	この値が高いほど，得られる情報の多い規則であること
0	0612	0514	CARTって何ですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0612	1303	CARTって何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0612	1411	CARTって何ですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0612	1304	CARTって何ですか	出力系列を参照する素性
0	0612	1104	CARTって何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0612	0402	CARTって何ですか	入力を観測した後で計算される確率
0	0612	0302	CARTって何ですか	カテゴリ形式の正解情報のこと
1	0614	0614	モデル木は回帰木とどう違いますか	回帰木と線形回帰の双方のよいところを取った
0	0614	0205	モデル木は回帰木とどう違いますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0614	0802	モデル木は回帰木とどう違いますか	隠れ層
0	0614	0411	モデル木は回帰木とどう違いますか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0614	0114	モデル木は回帰木とどう違いますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0614	0901	モデル木は回帰木とどう違いますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0614	0908	モデル木は回帰木とどう違いますか	シグモイド関数
0	0614	0802	モデル木は回帰木とどう違いますか	識別対象のクラス数
0	0614	1506	モデル木は回帰木とどう違いますか	最適政策$\pi^*$を獲得すること
0	0614	0708	モデル木は回帰木とどう違いますか	制約を弱める変数
0	0614	0311	モデル木は回帰木とどう違いますか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	0614	0405	モデル木は回帰木とどう違いますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0614	1404	モデル木は回帰木とどう違いますか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0614	0907	モデル木は回帰木とどう違いますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0614	1408	モデル木は回帰木とどう違いますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0614	0402	モデル木は回帰木とどう違いますか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0614	0805	モデル木は回帰木とどう違いますか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	0614	0811	モデル木は回帰木とどう違いますか	引数が負のときは0，0以上のときはその値を出力
0	0614	0417	モデル木は回帰木とどう違いますか	ネットワークの構造とアークの条件付き確率表
0	0614	0715	モデル木は回帰木とどう違いますか	カーネルトリック
0	0614	0911	モデル木は回帰木とどう違いますか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0614	1209	モデル木は回帰木とどう違いますか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0614	0114	モデル木は回帰木とどう違いますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0614	0502	モデル木は回帰木とどう違いますか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0614	0507	モデル木は回帰木とどう違いますか	パーセプトロンの収束定理
1	0614	0614	モデル木はCARTとどう違いますか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0614	0204	モデル木はCARTとどう違いますか	一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0614	0510	モデル木はCARTとどう違いますか	特徴空間上でクラスを分割する面
0	0614	0612	モデル木はCARTとどう違いますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0614	0717	モデル木はCARTとどう違いますか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0614	0701	モデル木はCARTとどう違いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0614	1104	モデル木はCARTとどう違いますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0614	0701	モデル木はCARTとどう違いますか	マージン
0	0614	0808	モデル木はCARTとどう違いますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0614	0116	モデル木はCARTとどう違いますか	学習データが教師あり／教師なしの混在となっているもの
0	0614	1505	モデル木はCARTとどう違いますか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0614	0711	モデル木はCARTとどう違いますか	カーネル関数
0	0614	0906	モデル木はCARTとどう違いますか	十分多くの層を持つニューラルネットワーク
0	0614	1301	モデル木はCARTとどう違いますか	形態素解析処理が典型的な問題
0	0614	0805	モデル木はCARTとどう違いますか	誤差逆伝播法
0	0614	1409	モデル木はCARTとどう違いますか	自分が出した誤りを指摘してくれる他人がいない
0	0614	0803	モデル木はCARTとどう違いますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0614	0901	モデル木はCARTとどう違いますか	深層学習に用いるニューラルネットワーク
0	0614	1012	モデル木はCARTとどう違いますか	すべてのデータの重みは平等
0	0614	0508	モデル木はCARTとどう違いますか	二乗誤差を最小にするように識別関数を調整する方法
0	0614	0803	モデル木はCARTとどう違いますか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0614	0917	モデル木はCARTとどう違いますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0614	0105	モデル木はCARTとどう違いますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0614	0901	モデル木はCARTとどう違いますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0614	0708	モデル木はCARTとどう違いますか	制約を弱める変数
1	0708	0708	スラック変数とはなんですか	制約を弱める変数
0	0708	0917	スラック変数とはなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0708	0701	スラック変数とはなんですか	サポートベクトルマシン
0	0708	1009	スラック変数とはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0708	0701	スラック変数とはなんですか	サポートベクトルマシン
0	0708	0413	スラック変数とはなんですか	変数間の独立性を表現できること
0	0708	0707	スラック変数とはなんですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0708	0611	スラック変数とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0708	0803	スラック変数とはなんですか	重みパラメータに対しては線形で，入力を非線形変換する
0	0708	1501	スラック変数とはなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0708	1505	スラック変数とはなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0708	0209	スラック変数とはなんですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0708	1001	スラック変数とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0708	1106	スラック変数とはなんですか	最も近い事例対の距離を類似度とする
0	0708	0611	スラック変数とはなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0708	0416	スラック変数とはなんですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0708	0105	スラック変数とはなんですか	観測対象から問題設定に適した情報を選んでデータ化する処理
0	0708	1303	スラック変数とはなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	0708	0916	スラック変数とはなんですか	リカレントニューラルネットワーク
0	0708	0908	スラック変数とはなんですか	シグモイド関数
0	0708	0504	スラック変数とはなんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0708	1412	スラック変数とはなんですか	近くのノードは同じクラスになりやすいという仮定
0	0708	0901	スラック変数とはなんですか	Deep Neural Network (DNN) 
0	0708	1304	スラック変数とはなんですか	入力と対応させる素性
0	0708	0513	スラック変数とはなんですか	重みの更新量があらかじめ定めた一定値以下になれば終了
1	0711	0711	カーネル関数とはどういうものですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0711	0710	カーネル関数とはどういうものですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0711	1303	カーネル関数とはどういうものですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0711	0409	カーネル関数とはどういうものですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0711	0412	カーネル関数とはどういうものですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0711	0802	カーネル関数とはどういうものですか	入力層・出力層の数に応じた適当な数
0	0711	0916	カーネル関数とはどういうものですか	リカレントニューラルネットワーク
0	0711	0302	カーネル関数とはどういうものですか	カテゴリ形式の正解情報のこと
0	0711	0614	カーネル関数とはどういうものですか	モデル木
0	0711	0710	カーネル関数とはどういうものですか	低次元の特徴ベクトルを高次元に写像
0	0711	0205	カーネル関数とはどういうものですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0711	0313	カーネル関数とはどういうものですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0711	1110	カーネル関数とはどういうものですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0711	1209	カーネル関数とはどういうものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0711	0701	カーネル関数とはどういうものですか	識別境界線と最も近いデータとの距離
0	0711	1007	カーネル関数とはどういうものですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0711	0611	カーネル関数とはどういうものですか	識別における決定木の考え方を回帰問題に適用する方法
0	0711	0906	カーネル関数とはどういうものですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0711	0402	カーネル関数とはどういうものですか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0711	1201	カーネル関数とはどういうものですか	データ集合中に一定頻度以上で現れるパターンを抽出する手法
0	0711	0605	カーネル関数とはどういうものですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0711	0616	カーネル関数とはどういうものですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0711	0811	カーネル関数とはどういうものですか	誤差が消失しません
0	0711	0708	カーネル関数とはどういうものですか	制約を弱める変数
0	0711	0805	カーネル関数とはどういうものですか	誤差逆伝播法
1	0715	0715	カーネルトリックとはなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0715	0204	カーネルトリックとはなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0715	0810	カーネルトリックとはなんですか	勾配消失問題
0	0715	0912	カーネルトリックとはなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0715	1409	カーネルトリックとはなんですか	自分が出した誤りを指摘してくれる他人がいない
0	0715	0611	カーネルトリックとはなんですか	回帰
0	0715	0901	カーネルトリックとはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0715	0917	カーネルトリックとはなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0715	1301	カーネルトリックとはなんですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0715	0810	カーネルトリックとはなんですか	誤差が小さくなって消失してしまう
0	0715	1306	カーネルトリックとはなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0715	0803	カーネルトリックとはなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0715	0304	カーネルトリックとはなんですか	個々の事例から，あるクラスについて共通点を見つけること
0	0715	1205	カーネルトリックとはなんですか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0715	0511	カーネルトリックとはなんですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0715	0614	カーネルトリックとはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0715	0802	カーネルトリックとはなんですか	特徴空間上では線形識別面を設定すること
0	0715	1106	カーネルトリックとはなんですか	最も近い事例対の距離を類似度とする
0	0715	0406	カーネルトリックとはなんですか	各クラスから生じる特徴の尤もらしさを表す
0	0715	0114	カーネルトリックとはなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0715	1001	カーネルトリックとはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0715	0903	カーネルトリックとはなんですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0715	0114	カーネルトリックとはなんですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0715	1110	カーネルトリックとはなんですか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0715	0110	カーネルトリックとはなんですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
1	0701	0701	識別境界線と最も近いデータとの距離のことをなんといいますか	マージン
0	0701	1207	識別境界線と最も近いデータとの距離のことをなんといいますか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0701	0316	識別境界線と最も近いデータとの距離のことをなんといいますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0701	1306	識別境界線と最も近いデータとの距離のことをなんといいますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0701	1214	識別境界線と最も近いデータとの距離のことをなんといいますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0701	0104	識別境界線と最も近いデータとの距離のことをなんといいますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0701	0315	識別境界線と最も近いデータとの距離のことをなんといいますか	分割後のデータの分散
0	0701	1412	識別境界線と最も近いデータとの距離のことをなんといいますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0701	0805	識別境界線と最も近いデータとの距離のことをなんといいますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0701	1106	識別境界線と最も近いデータとの距離のことをなんといいますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0701	0306	識別境界線と最も近いデータとの距離のことをなんといいますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0701	0113	識別境界線と最も近いデータとの距離のことをなんといいますか	入力データに潜む規則性
0	0701	0912	識別境界線と最も近いデータとの距離のことをなんといいますか	画像認識
0	0701	0209	識別境界線と最も近いデータとの距離のことをなんといいますか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0701	1407	識別境界線と最も近いデータとの距離のことをなんといいますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0701	0803	識別境界線と最も近いデータとの距離のことをなんといいますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0701	1001	識別境界線と最も近いデータとの距離のことをなんといいますか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0701	1209	識別境界線と最も近いデータとの距離のことをなんといいますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0701	0313	識別境界線と最も近いデータとの距離のことをなんといいますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0701	1219	識別境界線と最も近いデータとの距離のことをなんといいますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0701	0508	識別境界線と最も近いデータとの距離のことをなんといいますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0701	0514	識別境界線と最も近いデータとの距離のことをなんといいますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0701	0616	識別境界線と最も近いデータとの距離のことをなんといいますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0701	1301	識別境界線と最も近いデータとの距離のことをなんといいますか	動画像の分類や音声で入力された単語の識別などの問題
0	0701	1007	識別境界線と最も近いデータとの距離のことをなんといいますか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
1	0701	0701	マージンの定義はなんですか	識別境界線と最も近いデータとの距離
0	0701	0315	マージンの定義はなんですか	分割後のデータの分散
0	0701	1309	マージンの定義はなんですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0701	0810	マージンの定義はなんですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0701	1116	マージンの定義はなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0701	0810	マージンの定義はなんですか	2006 年頃に考案された事前学習法
0	0701	0801	マージンの定義はなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0701	1505	マージンの定義はなんですか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0701	0611	マージンの定義はなんですか	同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方
0	0701	1506	マージンの定義はなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0701	0710	マージンの定義はなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0701	0502	マージンの定義はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0701	1110	マージンの定義はなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0701	0514	マージンの定義はなんですか	ランダムに学習データを一つ
0	0701	1310	マージンの定義はなんですか	Hidden Marcov Model: 隠れマルコフモデル
0	0701	0810	マージンの定義はなんですか	勾配消失問題
0	0701	0710	マージンの定義はなんですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0701	0906	マージンの定義はなんですか	入力に近い側の処理で，特徴抽出をおこなおうとするものです．
0	0701	1207	マージンの定義はなんですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0701	0316	マージンの定義はなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0701	0715	マージンの定義はなんですか	識別面
0	0701	0614	マージンの定義はなんですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0701	1411	マージンの定義はなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0701	1306	マージンの定義はなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0701	1104	マージンの定義はなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
1	0710	0710	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	もとの空間におけるデータ間の距離関係を保存
0	0710	1409	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	自分が出した誤りを指摘してくれる他人がいない
0	0710	0512	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0710	1219	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0710	1302	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0710	1306	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0710	1509	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合
0	0710	1004	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0710	0811	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	ReLu
0	0710	0105	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする
0	0710	0502	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0710	1214	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0710	0901	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	音声認識・画像認識・自然言語処理など
0	0710	0912	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	画像認識
0	0710	0602	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	正解情報$y$が数値であるということ
0	0710	0912	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	畳み込みニューラルネットワーク
0	0710	0206	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0710	0802	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	識別対象のクラス数
0	0710	1406	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0710	1310	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	Hidden Marcov Model: 隠れマルコフモデル
0	0710	0402	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	入力を観測した後で計算される確率
0	0710	0508	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます
0	0710	1510	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．
0	0710	1301	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0710	1214	サポートベクトルマシンで高次元に非線形変換する際の条件はなに	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
1	0710	0710	特徴空間を高次元に変換する際に重要な条件はなんですか	もとの空間におけるデータ間の距離関係を保存
0	0710	0912	特徴空間を高次元に変換する際に重要な条件はなんですか	画像認識
0	0710	1506	特徴空間を高次元に変換する際に重要な条件はなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0710	1506	特徴空間を高次元に変換する際に重要な条件はなんですか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0710	1106	特徴空間を高次元に変換する際に重要な条件はなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0710	0514	特徴空間を高次元に変換する際に重要な条件はなんですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0710	0901	特徴空間を高次元に変換する際に重要な条件はなんですか	深層学習に用いるニューラルネットワーク
0	0710	0611	特徴空間を高次元に変換する際に重要な条件はなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0710	1219	特徴空間を高次元に変換する際に重要な条件はなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0710	0901	特徴空間を高次元に変換する際に重要な条件はなんですか	深層学習に用いるニューラルネットワーク
0	0710	1403	特徴空間を高次元に変換する際に重要な条件はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0710	0901	特徴空間を高次元に変換する際に重要な条件はなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0710	0708	特徴空間を高次元に変換する際に重要な条件はなんですか	制約を弱める変数
0	0710	1411	特徴空間を高次元に変換する際に重要な条件はなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0710	1410	特徴空間を高次元に変換する際に重要な条件はなんですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0710	1209	特徴空間を高次元に変換する際に重要な条件はなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0710	1110	特徴空間を高次元に変換する際に重要な条件はなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0710	0803	特徴空間を高次元に変換する際に重要な条件はなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0710	0411	特徴空間を高次元に変換する際に重要な条件はなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0710	0908	特徴空間を高次元に変換する際に重要な条件はなんですか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0710	1012	特徴空間を高次元に変換する際に重要な条件はなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0710	0906	特徴空間を高次元に変換する際に重要な条件はなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0710	0606	特徴空間を高次元に変換する際に重要な条件はなんですか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0710	0701	特徴空間を高次元に変換する際に重要な条件はなんですか	識別境界線と最も近いデータとの距離
0	0710	0911	特徴空間を高次元に変換する際に重要な条件はなんですか	学習時の自由度を意図的に下げていること
1	0710	0710	線形分離の可能性を高める方法はなにか	低次元の特徴ベクトルを高次元に写像
0	0710	1001	線形分離の可能性を高める方法はなにか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0710	0701	線形分離の可能性を高める方法はなにか	サポートベクトルマシン
0	0710	0801	線形分離の可能性を高める方法はなにか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0710	0209	線形分離の可能性を高める方法はなにか	正例がどれだけ正しく判定されているかという指標
0	0710	0612	線形分離の可能性を高める方法はなにか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0710	1106	線形分離の可能性を高める方法はなにか	最も近い事例対の距離を類似度とする
0	0710	0209	線形分離の可能性を高める方法はなにか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	0710	0105	線形分離の可能性を高める方法はなにか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0710	0104	線形分離の可能性を高める方法はなにか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0710	1503	線形分離の可能性を高める方法はなにか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0710	0901	線形分離の可能性を高める方法はなにか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0710	1301	線形分離の可能性を高める方法はなにか	形態素解析処理が典型的な問題
0	0710	0407	線形分離の可能性を高める方法はなにか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0710	1209	線形分離の可能性を高める方法はなにか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0710	0810	線形分離の可能性を高める方法はなにか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0710	0316	線形分離の可能性を高める方法はなにか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0710	0802	線形分離の可能性を高める方法はなにか	多層パーセプトロンあるいはニューラルネットワーク
0	0710	1306	線形分離の可能性を高める方法はなにか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0710	1411	線形分離の可能性を高める方法はなにか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0710	0916	線形分離の可能性を高める方法はなにか	リカレントニューラルネットワーク
0	0710	0508	線形分離の可能性を高める方法はなにか	二乗誤差を最小にするように識別関数を調整する方法
0	0710	1506	線形分離の可能性を高める方法はなにか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0710	1110	線形分離の可能性を高める方法はなにか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0710	0911	線形分離の可能性を高める方法はなにか	ランダムに一定割合のユニットを消して学習を行う
1	0711	0711	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	カーネル関数
0	0711	1205	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない
0	0711	0405	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	尤度と事前確率の積を最大とするクラスを求めることによって得られます
0	0711	0701	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	識別境界線と最も近いデータとの距離
0	0711	0901	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	深層学習に用いるニューラルネットワーク
0	0711	1407	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0711	1004	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0711	1303	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	0711	0906	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0711	0906	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0711	0701	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	識別境界線と最も近いデータとの距離
0	0711	1209	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	規則の条件部が起こったときに結論部が起こる割合
0	0711	1012	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0711	1407	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0711	1506	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	最適政策$\pi^*$を獲得すること
0	0711	0811	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	誤差が消失しません
0	0711	0601	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0711	0306	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0711	0808	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	シグモイド関数の微分
0	0711	0204	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	特徴ベクトルの次元数を減らすこと
0	0711	0912	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0711	0715	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	識別面
0	0711	0906	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	十分多くの層を持つニューラルネットワーク
0	0711	0512	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0711	1010	もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
1	0713	0713	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	文書分類やバイオインフォマティックスなど
0	0713	0805	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0713	1116	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0713	0502	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0713	1506	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	その政策に従って行動したときの累積報酬の期待値で評価
0	0713	1219	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0713	0306	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0713	0701	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0713	0115	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0713	0313	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0713	1220	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0713	0715	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	カーネルトリック
0	0713	0109	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	学習データに正解が付いている場合の学習
0	0713	0508	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0713	1106	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0713	0307	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0713	0808	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	入力の重み付き和の微分
0	0713	1010	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0713	0611	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	識別における決定木の考え方を回帰問題に適用する方法
0	0713	0906	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	十分多くの層
0	0713	1508	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0713	0402	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	入力を観測した後で計算される確率
0	0713	1004	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0713	0315	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	分割後のデータの分散
0	0713	0607	高次元にしてSVMを使って識別面を求める方法はどのような事に使われていますか	「投げ縄」という意味
1	0715	0715	カーネル関数が定まれば何が得られますか	識別面
0	0715	1103	カーネル関数が定まれば何が得られますか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0715	1301	カーネル関数が定まれば何が得られますか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0715	1304	カーネル関数が定まれば何が得られますか	出力系列を参照する素性
0	0715	0607	カーネル関数が定まれば何が得られますか	Lasso回帰
0	0715	1012	カーネル関数が定まれば何が得られますか	各データに重みを付け，そのもとで識別器を作成します
0	0715	0606	カーネル関数が定まれば何が得られますか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0715	0810	カーネル関数が定まれば何が得られますか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0715	1001	カーネル関数が定まれば何が得られますか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0715	1207	カーネル関数が定まれば何が得られますか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0715	0901	カーネル関数が定まれば何が得られますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0715	0801	カーネル関数が定まれば何が得られますか	このモデルは生物の神経細胞のモデルであると考えられています
0	0715	1214	カーネル関数が定まれば何が得られますか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0715	0911	カーネル関数が定まれば何が得られますか	ランダムに一定割合のユニットを消して学習を行う
0	0715	0114	カーネル関数が定まれば何が得られますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0715	0111	カーネル関数が定まれば何が得られますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0715	0306	カーネル関数が定まれば何が得られますか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0715	0614	カーネル関数が定まれば何が得られますか	モデル木
0	0715	0305	カーネル関数が定まれば何が得られますか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0715	0901	カーネル関数が定まれば何が得られますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0715	0908	カーネル関数が定まれば何が得られますか	AutoencoderとRestricted Bolzmann Machine(RBM)
0	0715	0614	カーネル関数が定まれば何が得られますか	回帰木と線形回帰の双方のよいところを取った方法
0	0715	0505	カーネル関数が定まれば何が得られますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0715	0607	カーネル関数が定まれば何が得られますか	「投げ縄」という意味
0	0715	0701	カーネル関数が定まれば何が得られますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0715	0715	カーネルトリックってなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	0715	0701	カーネルトリックってなんですか	識別境界線と最も近いデータとの距離
0	0715	0901	カーネルトリックってなんですか	Deep Neural Network (DNN) 
0	0715	0708	カーネルトリックってなんですか	制約を弱める変数
0	0715	0704	カーネルトリックってなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0715	0505	カーネルトリックってなんですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0715	0405	カーネルトリックってなんですか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0715	0602	カーネルトリックってなんですか	数値型の正解情報のこと
0	0715	0803	カーネルトリックってなんですか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0715	1106	カーネルトリックってなんですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0715	0112	カーネルトリックってなんですか	線形回帰，回帰木，モデル木など
0	0715	1501	カーネルトリックってなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0715	0917	カーネルトリックってなんですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0715	0505	カーネルトリックってなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0715	0611	カーネルトリックってなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0715	0803	カーネルトリックってなんですか	非線形識別面
0	0715	1411	カーネルトリックってなんですか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0715	1219	カーネルトリックってなんですか	どの個人がどの商品を購入したかが記録されているデータ
0	0715	0205	カーネルトリックってなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0715	0413	カーネルトリックってなんですか	変数間の独立性を表現できること
0	0715	0406	カーネルトリックってなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0715	0411	カーネルトリックってなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0715	0701	カーネルトリックってなんですか	サポートベクトルマシン
0	0715	0607	カーネルトリックってなんですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0715	0810	カーネルトリックってなんですか	勾配消失問題
1	0717	0717	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	Grid search
0	0717	1509	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0717	0304	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	個々の事例から，あるクラスについて共通点を見つけること
0	0717	1506	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	後に得られる報酬ほど割り引いて計算するための係数
0	0717	1506	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	その政策に従って行動したときの累積報酬の期待値で評価
0	0717	0316	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0717	0602	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	数値型の正解情報のこと
0	0717	0701	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0717	0410	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0717	1205	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0717	0209	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0717	1508	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0717	0907	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0717	0508	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	最小二乗法
0	0717	0208	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0717	0105	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0717	0906	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	多階層構造でもそのまま適用できます
0	0717	0701	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	識別境界線と最も近いデータとの距離
0	0717	1505	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0717	1214	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0717	1110	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0717	1506	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	0717	0504	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0717	1508	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0717	0711	パラメータの可能な値をリストアップし、そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法を何と言いますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
1	0717	0717	複数のパラメータを組合わせる空間を何と言いますか	グリッド
0	0717	0502	複数のパラメータを組合わせる空間を何と言いますか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0717	0901	複数のパラメータを組合わせる空間を何と言いますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0717	1010	複数のパラメータを組合わせる空間を何と言いますか	誤りを減らすことに特化した識別器を，次々に加えてゆくという方法
0	0717	0901	複数のパラメータを組合わせる空間を何と言いますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0717	0204	複数のパラメータを組合わせる空間を何と言いますか	特徴ベクトルの次元数を減らすこと
0	0717	1001	複数のパラメータを組合わせる空間を何と言いますか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0717	1209	複数のパラメータを組合わせる空間を何と言いますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0717	1304	複数のパラメータを組合わせる空間を何と言いますか	入力と対応させる素性
0	0717	0506	複数のパラメータを組合わせる空間を何と言いますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0717	0907	複数のパラメータを組合わせる空間を何と言いますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0717	0602	複数のパラメータを組合わせる空間を何と言いますか	正解情報$y$が数値であるということ
0	0717	1111	複数のパラメータを組合わせる空間を何と言いますか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0717	0315	複数のパラメータを組合わせる空間を何と言いますか	分割後のデータの分散
0	0717	1209	複数のパラメータを組合わせる空間を何と言いますか	この値が高いほど，得られる情報の多い規則であること
0	0717	1219	複数のパラメータを組合わせる空間を何と言いますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0717	1502	複数のパラメータを組合わせる空間を何と言いますか	将棋や囲碁などを行うプログラム
0	0717	0703	複数のパラメータを組合わせる空間を何と言いますか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0717	1301	複数のパラメータを組合わせる空間を何と言いますか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0717	0708	複数のパラメータを組合わせる空間を何と言いますか	制約を満たさない程度を表すので，小さい方が望ましい
0	0717	1108	複数のパラメータを組合わせる空間を何と言いますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0717	0715	複数のパラメータを組合わせる空間を何と言いますか	複雑な非線形変換を求めるという操作を避ける方法
0	0717	0311	複数のパラメータを組合わせる空間を何と言いますか	集合の乱雑さ
0	0717	0606	複数のパラメータを組合わせる空間を何と言いますか	重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．
0	0717	0917	複数のパラメータを組合わせる空間を何と言いますか	LSTM
1	0717	0717	SVMのハイパーパラメータの設定はどのように行いますか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0717	0313	SVMのハイパーパラメータの設定はどのように行いますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0717	0710	SVMのハイパーパラメータの設定はどのように行いますか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0717	1404	SVMのハイパーパラメータの設定はどのように行いますか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0717	1506	SVMのハイパーパラメータの設定はどのように行いますか	その政策に従って行動したときの累積報酬の期待値で評価
0	0717	0105	SVMのハイパーパラメータの設定はどのように行いますか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0717	0912	SVMのハイパーパラメータの設定はどのように行いますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したもの
0	0717	0505	SVMのハイパーパラメータの設定はどのように行いますか	生物の神経細胞の仕組みをモデル化したもの
0	0717	0402	SVMのハイパーパラメータの設定はどのように行いますか	入力を観測した後で計算される確率
0	0717	0402	SVMのハイパーパラメータの設定はどのように行いますか	統計的識別手法
0	0717	0505	SVMのハイパーパラメータの設定はどのように行いますか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0717	1220	SVMのハイパーパラメータの設定はどのように行いますか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0717	1203	SVMのハイパーパラメータの設定はどのように行いますか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0717	0917	SVMのハイパーパラメータの設定はどのように行いますか	LSTM
0	0717	0901	SVMのハイパーパラメータの設定はどのように行いますか	深層学習に用いるニューラルネットワーク
0	0717	0514	SVMのハイパーパラメータの設定はどのように行いますか	ランダムに学習データを一つ
0	0717	1401	SVMのハイパーパラメータの設定はどのように行いますか	識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません
0	0717	0514	SVMのハイパーパラメータの設定はどのように行いますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0717	0313	SVMのハイパーパラメータの設定はどのように行いますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0717	1004	SVMのハイパーパラメータの設定はどのように行いますか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0717	1505	SVMのハイパーパラメータの設定はどのように行いますか	「マルコフ性」を持つ確率過程における意思決定問題
0	0717	0512	SVMのハイパーパラメータの設定はどのように行いますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0717	0601	SVMのハイパーパラメータの設定はどのように行いますか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0717	0514	SVMのハイパーパラメータの設定はどのように行いますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0717	0607	SVMのハイパーパラメータの設定はどのように行いますか	Lasso回帰
1	0717	0717	スラック変数の重みは離散値ですか、連続値ですか	連続値
0	0717	0701	スラック変数の重みは離散値ですか、連続値ですか	線形で識別できないデータに対応するため
0	0717	1214	スラック変数の重みは離散値ですか、連続値ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0717	0114	スラック変数の重みは離散値ですか、連続値ですか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0717	1503	スラック変数の重みは離散値ですか、連続値ですか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0717	0406	スラック変数の重みは離散値ですか、連続値ですか	各クラスから生じる特徴の尤もらしさを表す
0	0717	0802	スラック変数の重みは離散値ですか、連続値ですか	特徴ベクトルの次元数
0	0717	0606	スラック変数の重みは離散値ですか、連続値ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0717	0502	スラック変数の重みは離散値ですか、連続値ですか	SVM
0	0717	0906	スラック変数の重みは離散値ですか、連続値ですか	多階層構造でもそのまま適用できます
0	0717	0715	スラック変数の重みは離散値ですか、連続値ですか	カーネルトリック
0	0717	0802	スラック変数の重みは離散値ですか、連続値ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0717	0917	スラック変数の重みは離散値ですか、連続値ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0717	1412	スラック変数の重みは離散値ですか、連続値ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0717	0803	スラック変数の重みは離散値ですか、連続値ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0717	1404	スラック変数の重みは離散値ですか、連続値ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0717	1001	スラック変数の重みは離散値ですか、連続値ですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0717	0602	スラック変数の重みは離散値ですか、連続値ですか	正解情報$y$が数値であるということ
0	0717	1509	スラック変数の重みは離散値ですか、連続値ですか	環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合
0	0717	0802	スラック変数の重みは離散値ですか、連続値ですか	多層パーセプトロンあるいはニューラルネットワーク
0	0717	1306	スラック変数の重みは離散値ですか、連続値ですか	条件付き確率場（Conditional Random Field: CRF）
0	0717	0110	スラック変数の重みは離散値ですか、連続値ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0717	0614	スラック変数の重みは離散値ですか、連続値ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0717	1410	スラック変数の重みは離散値ですか、連続値ですか	学習初期の誤りに強いということ
0	0717	0901	スラック変数の重みは離散値ですか、連続値ですか	深層学習に用いるニューラルネットワーク
1	0802	0802	誤り訂正学習は特徴空間上では何に相当しますか	特徴空間上では線形識別面を設定すること
0	0802	1103	誤り訂正学習は特徴空間上では何に相当しますか	異なったまとまり間の距離はなるべく遠くなるように
0	0802	0610	誤り訂正学習は特徴空間上では何に相当しますか	真のモデルとの距離
0	0802	1302	誤り訂正学習は特徴空間上では何に相当しますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0802	1209	誤り訂正学習は特徴空間上では何に相当しますか	この値が高いほど，得られる情報の多い規則であること
0	0802	0411	誤り訂正学習は特徴空間上では何に相当しますか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	0802	0803	誤り訂正学習は特徴空間上では何に相当しますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0802	0710	誤り訂正学習は特徴空間上では何に相当しますか	もとの空間におけるデータ間の距離関係を保存
0	0802	0410	誤り訂正学習は特徴空間上では何に相当しますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0802	0204	誤り訂正学習は特徴空間上では何に相当しますか	特徴ベクトルの次元数を減らすこと
0	0802	1505	誤り訂正学習は特徴空間上では何に相当しますか	「マルコフ性」を持つ確率過程における意思決定問題
0	0802	0911	誤り訂正学習は特徴空間上では何に相当しますか	ランダムに一定割合のユニットを消して学習を行う
0	0802	0907	誤り訂正学習は特徴空間上では何に相当しますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0802	0316	誤り訂正学習は特徴空間上では何に相当しますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0802	0104	誤り訂正学習は特徴空間上では何に相当しますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0802	0502	誤り訂正学習は特徴空間上では何に相当しますか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0802	0707	誤り訂正学習は特徴空間上では何に相当しますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0802	0514	誤り訂正学習は特徴空間上では何に相当しますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0802	0805	誤り訂正学習は特徴空間上では何に相当しますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0802	0701	誤り訂正学習は特徴空間上では何に相当しますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0802	0111	誤り訂正学習は特徴空間上では何に相当しますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0802	1110	誤り訂正学習は特徴空間上では何に相当しますか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0802	0114	誤り訂正学習は特徴空間上では何に相当しますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0802	0811	誤り訂正学習は特徴空間上では何に相当しますか	ユニットの活性化関数を工夫する方法があります
0	0802	0701	誤り訂正学習は特徴空間上では何に相当しますか	サポートベクトルマシン
1	0803	0803	ノードを階層的に組むとどのような識別面ができるか	非線形識別面
0	0803	1214	ノードを階層的に組むとどのような識別面ができるか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0803	0916	ノードを階層的に組むとどのような識別面ができるか	リカレントニューラルネットワーク
0	0803	1411	ノードを階層的に組むとどのような識別面ができるか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0803	0413	ノードを階層的に組むとどのような識別面ができるか	変数間の独立性を表現できること
0	0803	0801	ノードを階層的に組むとどのような識別面ができるか	このモデルは生物の神経細胞のモデルであると考えられています
0	0803	0417	ノードを階層的に組むとどのような識別面ができるか	ネットワークの構造とアークの条件付き確率
0	0803	0211	ノードを階層的に組むとどのような識別面ができるか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0803	0810	ノードを階層的に組むとどのような識別面ができるか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0803	0602	ノードを階層的に組むとどのような識別面ができるか	正解情報$y$が数値であるということ
0	0803	0614	ノードを階層的に組むとどのような識別面ができるか	回帰木と線形回帰の双方のよいところを取った方法
0	0803	0901	ノードを階層的に組むとどのような識別面ができるか	音声認識・画像認識・自然言語処理など
0	0803	1301	ノードを階層的に組むとどのような識別面ができるか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0803	0114	ノードを階層的に組むとどのような識別面ができるか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0803	0111	ノードを階層的に組むとどのような識別面ができるか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0803	1505	ノードを階層的に組むとどのような識別面ができるか	「マルコフ性」を持つ確率過程における意思決定問題
0	0803	1114	ノードを階層的に組むとどのような識別面ができるか	クラスタリング結果のデータ数の分布から
0	0803	1407	ノードを階層的に組むとどのような識別面ができるか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0803	0514	ノードを階層的に組むとどのような識別面ができるか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0803	0610	ノードを階層的に組むとどのような識別面ができるか	学習結果の散らばり具合
0	0803	0902	ノードを階層的に組むとどのような識別面ができるか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0803	0717	ノードを階層的に組むとどのような識別面ができるか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0803	1301	ノードを階層的に組むとどのような識別面ができるか	連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります
0	0803	0402	ノードを階層的に組むとどのような識別面ができるか	条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます
0	0803	0205	ノードを階層的に組むとどのような識別面ができるか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
1	0803	0803	なぜノードを階層的に組むと非線形識別面を実現できるんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0803	0410	なぜノードを階層的に組むと非線形識別面を実現できるんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0803	1219	なぜノードを階層的に組むと非線形識別面を実現できるんですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0803	0802	なぜノードを階層的に組むと非線形識別面を実現できるんですか	特徴空間上では線形識別面を設定すること
0	0803	0810	なぜノードを階層的に組むと非線形識別面を実現できるんですか	2006 年頃に考案された事前学習法
0	0803	1207	なぜノードを階層的に組むと非線形識別面を実現できるんですか	小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0803	0810	なぜノードを階層的に組むと非線形識別面を実現できるんですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0803	0111	なぜノードを階層的に組むと非線形識別面を実現できるんですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0803	0917	なぜノードを階層的に組むと非線形識別面を実現できるんですか	入力ゲート・出力ゲート・忘却ゲート
0	0803	0505	なぜノードを階層的に組むと非線形識別面を実現できるんですか	ニューラルネットワーク
0	0803	0109	なぜノードを階層的に組むと非線形識別面を実現できるんですか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0803	0602	なぜノードを階層的に組むと非線形識別面を実現できるんですか	数値型の正解情報のこと
0	0803	0113	なぜノードを階層的に組むと非線形識別面を実現できるんですか	入力データに潜む規則性
0	0803	0305	なぜノードを階層的に組むと非線形識別面を実現できるんですか	仮説に対して課す制約
0	0803	0306	なぜノードを階層的に組むと非線形識別面を実現できるんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0803	0616	なぜノードを階層的に組むと非線形識別面を実現できるんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0803	1103	なぜノードを階層的に組むと非線形識別面を実現できるんですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0803	0908	なぜノードを階層的に組むと非線形識別面を実現できるんですか	ユークリッド距離
0	0803	1209	なぜノードを階層的に組むと非線形識別面を実現できるんですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0803	0610	なぜノードを階層的に組むと非線形識別面を実現できるんですか	真のモデルとの距離
0	0803	0502	なぜノードを階層的に組むと非線形識別面を実現できるんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0803	0701	なぜノードを階層的に組むと非線形識別面を実現できるんですか	サポートベクトルマシン
0	0803	0610	なぜノードを階層的に組むと非線形識別面を実現できるんですか	学習結果の散らばり具合
0	0803	1112	なぜノードを階層的に組むと非線形識別面を実現できるんですか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0803	0605	なぜノードを階層的に組むと非線形識別面を実現できるんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
1	0810	0810	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	勾配消失問題
0	0810	0402	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0810	0305	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0810	0417	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	ネットワークの構造とアークの条件付き確率表
0	0810	0502	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	SVM
0	0810	0104	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0810	1301	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	形態素解析処理が典型的な問題
0	0810	0906	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	入力に近い側の処理
0	0810	1402	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0810	1407	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0810	0402	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0810	0803	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0810	0612	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0810	0912	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	畳み込みニューラルネットワーク
0	0810	0612	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0810	1405	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0810	0116	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	学習データが教師あり／教師なしの混在となっているもの
0	0810	0916	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	リカレントニューラルネットワーク
0	0810	0704	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	ラグランジュの未定乗数法
0	0810	0605	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0810	1302	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	0810	0110	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします
0	0810	0505	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	生物の神経細胞の仕組みをモデル化したもの
0	0810	1409	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	自分が出した誤りを指摘してくれる他人がいない
0	0810	0717	多層ニューラルネットワークの誤差逆伝播法で段々と誤差が小さくなっていく問題をなんといいますか	グリッド
1	0810	0810	ディープニューラルネットワークの性能が向上しない原因はなんですか	勾配消失問題
0	0810	0306	ディープニューラルネットワークの性能が向上しない原因はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0810	0605	ディープニューラルネットワークの性能が向上しない原因はなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
0	0810	0704	ディープニューラルネットワークの性能が向上しない原因はなんですか	以下の関数$L$の最小値を求めるという問題
0	0810	0413	ディープニューラルネットワークの性能が向上しない原因はなんですか	変数間の独立性を表現できること
0	0810	0105	ディープニューラルネットワークの性能が向上しない原因はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0810	1214	ディープニューラルネットワークの性能が向上しない原因はなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0810	1502	ディープニューラルネットワークの性能が向上しない原因はなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0810	0508	ディープニューラルネットワークの性能が向上しない原因はなんですか	最小二乗法
0	0810	1004	ディープニューラルネットワークの性能が向上しない原因はなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0810	0610	ディープニューラルネットワークの性能が向上しない原因はなんですか	真のモデルとの距離
0	0810	0901	ディープニューラルネットワークの性能が向上しない原因はなんですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する
0	0810	1508	ディープニューラルネットワークの性能が向上しない原因はなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0810	1110	ディープニューラルネットワークの性能が向上しない原因はなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0810	0911	ディープニューラルネットワークの性能が向上しない原因はなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0810	0211	ディープニューラルネットワークの性能が向上しない原因はなんですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0810	0114	ディープニューラルネットワークの性能が向上しない原因はなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0810	1407	ディープニューラルネットワークの性能が向上しない原因はなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0810	0104	ディープニューラルネットワークの性能が向上しない原因はなんですか	音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題
0	0810	0715	ディープニューラルネットワークの性能が向上しない原因はなんですか	識別面
0	0810	0614	ディープニューラルネットワークの性能が向上しない原因はなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0810	1402	ディープニューラルネットワークの性能が向上しない原因はなんですか	正解なしデータから得られる$p(\bm{x})$に関する情報が，$P(y|\bm{x})$の推定に役立つこと
0	0810	1108	ディープニューラルネットワークの性能が向上しない原因はなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0810	0109	ディープニューラルネットワークの性能が向上しない原因はなんですか	学習データに正解が付いている場合の学習
0	0810	1403	ディープニューラルネットワークの性能が向上しない原因はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
1	0811	0811	勾配消失問題への取組みにはどのような方法がありますか	ユニットの活性化関数を工夫する方法
0	0811	0702	勾配消失問題への取組みにはどのような方法がありますか	識別面は平面を仮定する
0	0811	1506	勾配消失問題への取組みにはどのような方法がありますか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0811	1506	勾配消失問題への取組みにはどのような方法がありますか	各状態でどの行為を取ればよいのかという意思決定規則
0	0811	1503	勾配消失問題への取組みにはどのような方法がありますか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0811	0808	勾配消失問題への取組みにはどのような方法がありますか	シグモイド関数の微分
0	0811	0803	勾配消失問題への取組みにはどのような方法がありますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0811	1407	勾配消失問題への取組みにはどのような方法がありますか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0811	1313	勾配消失問題への取組みにはどのような方法がありますか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0811	0204	勾配消失問題への取組みにはどのような方法がありますか	特徴ベクトルの次元数を減らすこと
0	0811	0703	勾配消失問題への取組みにはどのような方法がありますか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0811	0911	勾配消失問題への取組みにはどのような方法がありますか	ランダムに一定割合のユニットを消して学習を行う
0	0811	0708	勾配消失問題への取組みにはどのような方法がありますか	制約を弱める変数
0	0811	0708	勾配消失問題への取組みにはどのような方法がありますか	制約を満たさない程度を表すので，小さい方が望ましい
0	0811	0810	勾配消失問題への取組みにはどのような方法がありますか	勾配消失問題
0	0811	0614	勾配消失問題への取組みにはどのような方法がありますか	線形回帰式
0	0811	0514	勾配消失問題への取組みにはどのような方法がありますか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります
0	0811	1408	勾配消失問題への取組みにはどのような方法がありますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0811	0114	勾配消失問題への取組みにはどのような方法がありますか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0811	0701	勾配消失問題への取組みにはどのような方法がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0811	0116	勾配消失問題への取組みにはどのような方法がありますか	学習データが教師あり／教師なしの混在となっているもの
0	0811	0111	勾配消失問題への取組みにはどのような方法がありますか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0811	0616	勾配消失問題への取組みにはどのような方法がありますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0811	0717	勾配消失問題への取組みにはどのような方法がありますか	グリッドを設定して，一通りの組み合わせで評価実験をおこないます
0	0811	0701	勾配消失問題への取組みにはどのような方法がありますか	学習データからのマージンが最大となる識別境界線
1	0811	0811	ReLUを用いる利点はなんですか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0811	0114	ReLUを用いる利点はなんですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0811	0711	ReLUを用いる利点はなんですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0811	0912	ReLUを用いる利点はなんですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0811	1104	ReLUを用いる利点はなんですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0811	0701	ReLUを用いる利点はなんですか	サポートベクトルマシン
0	0811	0611	ReLUを用いる利点はなんですか	識別における決定木の考え方を回帰問題に適用する方法
0	0811	0311	ReLUを用いる利点はなんですか	集合の乱雑さ
0	0811	0611	ReLUを用いる利点はなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0811	0606	ReLUを用いる利点はなんですか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0811	1106	ReLUを用いる利点はなんですか	最も近い事例対の距離を類似度とする
0	0811	0416	ReLUを用いる利点はなんですか	値が真となる確率を知りたいノードが表す変数
0	0811	1412	ReLUを用いる利点はなんですか	近くのノードは同じクラスになりやすいという仮定
0	0811	0717	ReLUを用いる利点はなんですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0811	0111	ReLUを用いる利点はなんですか	決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなど
0	0811	0209	ReLUを用いる利点はなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0811	0911	ReLUを用いる利点はなんですか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0811	1507	ReLUを用いる利点はなんですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0811	0704	ReLUを用いる利点はなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0811	0507	ReLUを用いる利点はなんですか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0811	0802	ReLUを用いる利点はなんですか	隠れ層
0	0811	1214	ReLUを用いる利点はなんですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0811	0505	ReLUを用いる利点はなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0811	1407	ReLUを用いる利点はなんですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0811	0204	ReLUを用いる利点はなんですか	特徴ベクトルの次元数を減らすこと
1	0811	0811	ReLUを用いると誤差はどうなりますか	誤差が消失しません
0	0811	0206	ReLUを用いると誤差はどうなりますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0811	1510	ReLUを用いると誤差はどうなりますか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0811	0712	ReLUを用いると誤差はどうなりますか	カーネル関数が正定値関数という条件を満たすとき
0	0811	0901	ReLUを用いると誤差はどうなりますか	音声認識・画像認識・自然言語処理など
0	0811	0512	ReLUを用いると誤差はどうなりますか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0811	0507	ReLUを用いると誤差はどうなりますか	学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します
0	0811	0810	ReLUを用いると誤差はどうなりますか	2006 年頃に考案された事前学習法
0	0811	0109	ReLUを用いると誤差はどうなりますか	正解が付いていない場合の学習
0	0811	1219	ReLUを用いると誤差はどうなりますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0811	0606	ReLUを用いると誤差はどうなりますか	回帰式中の係数$\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫
0	0811	1409	ReLUを用いると誤差はどうなりますか	自分が出した誤りを指摘してくれる他人がいない
0	0811	0808	ReLUを用いると誤差はどうなりますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0811	1408	ReLUを用いると誤差はどうなりますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0811	0610	ReLUを用いると誤差はどうなりますか	片方を減らせば片方が増える
0	0811	1503	ReLUを用いると誤差はどうなりますか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0811	1116	ReLUを用いると誤差はどうなりますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0811	0614	ReLUを用いると誤差はどうなりますか	回帰木と線形回帰の双方のよいところを取った方法
0	0811	0205	ReLUを用いると誤差はどうなりますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0811	0701	ReLUを用いると誤差はどうなりますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0811	0708	ReLUを用いると誤差はどうなりますか	制約を弱める変数
0	0811	0502	ReLUを用いると誤差はどうなりますか	特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点
0	0811	0611	ReLUを用いると誤差はどうなりますか	識別における決定木の考え方を回帰問題に適用する方法
0	0811	0510	ReLUを用いると誤差はどうなりますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0811	0801	ReLUを用いると誤差はどうなりますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
1	0901	0901	深層学習の定義はなにか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0901	0611	深層学習の定義はなにか	識別における決定木の考え方を回帰問題に適用する方法
0	0901	0710	深層学習の定義はなにか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
0	0901	0810	深層学習の定義はなにか	重みの修正量が層を戻るにつれて小さくなってゆく
0	0901	0711	深層学習の定義はなにか	カーネル関数
0	0901	1004	深層学習の定義はなにか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0901	1106	深層学習の定義はなにか	クラスタの重心間の距離を類似度とする
0	0901	0711	深層学習の定義はなにか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0901	0916	深層学習の定義はなにか	リカレントニューラルネットワーク
0	0901	0802	深層学習の定義はなにか	識別対象のクラス数
0	0901	0114	深層学習の定義はなにか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0901	0103	深層学習の定義はなにか	簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるとき
0	0901	0902	深層学習の定義はなにか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0901	0907	深層学習の定義はなにか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0901	0205	深層学習の定義はなにか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0901	0802	深層学習の定義はなにか	特徴空間上では線形識別面を設定すること
0	0901	0906	深層学習の定義はなにか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0901	0906	深層学習の定義はなにか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0901	0304	深層学習の定義はなにか	個々の事例から，あるクラスについて共通点を見つけること
0	0901	0507	深層学習の定義はなにか	全ての誤りがなくなることが学習の終了条件なので
0	0901	0708	深層学習の定義はなにか	制約を弱める変数
0	0901	0111	深層学習の定義はなにか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0901	0701	深層学習の定義はなにか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0901	0607	深層学習の定義はなにか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0901	1306	深層学習の定義はなにか	条件付き確率場（Conditional Random Field: CRF）
1	0902	0902	深層学習の特徴はなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0902	0801	深層学習の特徴はなんですか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0902	0811	深層学習の特徴はなんですか	ユニットの活性化関数を工夫する方法があります
0	0902	0510	深層学習の特徴はなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0902	1001	深層学習の特徴はなんですか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0902	0115	深層学習の特徴はなんですか	パターンマイニング
0	0902	1406	深層学習の特徴はなんですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0902	0114	深層学習の特徴はなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0902	0105	深層学習の特徴はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0902	0508	深層学習の特徴はなんですか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0902	0507	深層学習の特徴はなんですか	パーセプトロンの収束定理
0	0902	0701	深層学習の特徴はなんですか	識別境界線と最も近いデータとの距離
0	0902	0409	深層学習の特徴はなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0902	0409	深層学習の特徴はなんですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0902	0715	深層学習の特徴はなんですか	カーネルトリック
0	0902	1404	深層学習の特徴はなんですか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0902	0614	深層学習の特徴はなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0902	0204	深層学習の特徴はなんですか	特徴ベクトルの次元数を減らすこと
0	0902	0702	深層学習の特徴はなんですか	識別面は平面を仮定する
0	0902	1110	深層学習の特徴はなんですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0902	0502	深層学習の特徴はなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	0902	0906	深層学習の特徴はなんですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0902	0512	深層学習の特徴はなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0902	1510	深層学習の特徴はなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0902	0105	深層学習の特徴はなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
1	0906	0906	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	入力に近い側の処理
0	0906	1501	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0906	1004	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	0906	0115	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	パターンマイニング
0	0906	0810	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0906	0701	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．
0	0906	0912	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0906	1505	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0906	1510	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0906	0512	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	0906	0601	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0906	1001	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0906	1219	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0906	1306	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0906	0802	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	入力層・出力層の数に応じた適当な数
0	0906	0208	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0906	0717	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0906	1301	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	動画像の分類や音声で入力された単語の識別などの問題
0	0906	0508	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	二乗誤差を最小にするように識別関数を調整する方法
0	0906	1104	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0906	0808	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	シグモイド関数の微分
0	0906	1015	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0906	1009	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0906	1012	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0906	0205	多階層ニューラルネットワークでは特徴抽出をどのあたりで行うのですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
1	0906	0906	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	十分多くの層
0	0906	1214	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	計算量が膨大であること
0	0906	0701	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	サポートベクトルマシン
0	0906	1506	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	政策
0	0906	0902	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0906	1309	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0906	1404	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう
0	0906	0912	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	畳み込みニューラルネットワーク
0	0906	1110	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0906	0808	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0906	0908	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	シグモイド関数
0	0906	0109	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの
0	0906	0601	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題
0	0906	0803	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0906	0612	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0906	0502	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0906	1408	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0906	0412	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0906	0411	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	確率のm推定という考え方を用います
0	0906	0901	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	深層学習に用いるニューラルネットワーク
0	0906	1404	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0906	1412	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0906	0805	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0906	0802	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	特徴ベクトルの次元数
0	0906	0701	特徴抽出を学習するにはどれくらいのニューラルネットワークが必要になりますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0704	0704	制約条件ではどのような式を用いますか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0704	0113	制約条件ではどのような式を用いますか	入力データに潜む規則性
0	0704	0606	制約条件ではどのような式を用いますか	山の尾根という意味
0	0704	0601	制約条件ではどのような式を用いますか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0704	0703	制約条件ではどのような式を用いますか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	0704	0209	制約条件ではどのような式を用いますか	正例がどれだけ正しく判定されているかという指標
0	0704	0711	制約条件ではどのような式を用いますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0704	0508	制約条件ではどのような式を用いますか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	0704	0616	制約条件ではどのような式を用いますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0704	0407	制約条件ではどのような式を用いますか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0704	0805	制約条件ではどのような式を用いますか	誤差逆伝播法
0	0704	1110	制約条件ではどのような式を用いますか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0704	1220	制約条件ではどのような式を用いますか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0704	0402	制約条件ではどのような式を用いますか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	0704	1106	制約条件ではどのような式を用いますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0704	1306	制約条件ではどのような式を用いますか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0704	0610	制約条件ではどのような式を用いますか	片方を減らせば片方が増える
0	0704	0505	制約条件ではどのような式を用いますか	生物の神経細胞の仕組みをモデル化したもの
0	0704	0612	制約条件ではどのような式を用いますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0704	1104	制約条件ではどのような式を用いますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0704	0801	制約条件ではどのような式を用いますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0704	0711	制約条件ではどのような式を用いますか	カーネル関数
0	0704	0115	制約条件ではどのような式を用いますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0704	0802	制約条件ではどのような式を用いますか	識別対象のクラス数
0	0704	1106	制約条件ではどのような式を用いますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
1	0704	0704	αについての2次計画問題とはなんですか	以下の関数$L$の最小値を求めるという問題
0	0704	0711	αについての2次計画問題とはなんですか	カーネル関数
0	0704	0209	αについての2次計画問題とはなんですか	正例がどれだけ正しく判定されているかという指標
0	0704	1508	αについての2次計画問題とはなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0704	0906	αについての2次計画問題とはなんですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0704	0113	αについての2次計画問題とはなんですか	入力データに潜む規則性
0	0704	0611	αについての2次計画問題とはなんですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0704	0602	αについての2次計画問題とはなんですか	数値型の正解情報のこと
0	0704	0105	αについての2次計画問題とはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0704	0305	αについての2次計画問題とはなんですか	仮説に対して課す制約
0	0704	0805	αについての2次計画問題とはなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0704	0906	αについての2次計画問題とはなんですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0704	1301	αについての2次計画問題とはなんですか	形態素解析処理が典型的な問題
0	0704	1506	αについての2次計画問題とはなんですか	各状態でどの行為を取ればよいのかという意思決定規則
0	0704	0504	αについての2次計画問題とはなんですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0704	1301	αについての2次計画問題とはなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0704	1009	αについての2次計画問題とはなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0704	0606	αについての2次計画問題とはなんですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0704	0701	αについての2次計画問題とはなんですか	識別境界線と最も近いデータとの距離
0	0704	0109	αについての2次計画問題とはなんですか	正解が付いていない場合の学習
0	0704	1409	αについての2次計画問題とはなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0704	1205	αについての2次計画問題とはなんですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0704	0811	αについての2次計画問題とはなんですか	半分の領域で勾配が1になるので
0	0704	1306	αについての2次計画問題とはなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0704	0901	αについての2次計画問題とはなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
1	0707	0707	線形分離可能でない場合はどうすれば良いですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0707	1001	線形分離可能でない場合はどうすれば良いですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0707	1306	線形分離可能でない場合はどうすれば良いですか	条件付き確率場（Conditional Random Field: CRF）
0	0707	1506	線形分離可能でない場合はどうすれば良いですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0707	0610	線形分離可能でない場合はどうすれば良いですか	トレードオフの関係
0	0707	0305	線形分離可能でない場合はどうすれば良いですか	仮説に対して課す制約
0	0707	0209	線形分離可能でない場合はどうすれば良いですか	正例がどれだけ正しく判定されているかという指標
0	0707	1502	線形分離可能でない場合はどうすれば良いですか	将棋や囲碁などを行うプログラム
0	0707	0805	線形分離可能でない場合はどうすれば良いですか	誤差逆伝播法
0	0707	0917	線形分離可能でない場合はどうすれば良いですか	入力ゲート・出力ゲート・忘却ゲート
0	0707	0114	線形分離可能でない場合はどうすれば良いですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0707	0402	線形分離可能でない場合はどうすれば良いですか	入力を観測した後で計算される確率
0	0707	1104	線形分離可能でない場合はどうすれば良いですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	0707	0513	線形分離可能でない場合はどうすれば良いですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0707	0404	線形分離可能でない場合はどうすれば良いですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0707	1506	線形分離可能でない場合はどうすれば良いですか	最適政策$\pi^*$を獲得すること
0	0707	0708	線形分離可能でない場合はどうすれば良いですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0707	0901	線形分離可能でない場合はどうすれば良いですか	表現学習
0	0707	1501	線形分離可能でない場合はどうすれば良いですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0707	0606	線形分離可能でない場合はどうすれば良いですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0707	0906	線形分離可能でない場合はどうすれば良いですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0707	0410	線形分離可能でない場合はどうすれば良いですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	0707	0701	線形分離可能でない場合はどうすれば良いですか	サポートベクトルマシン
0	0707	0901	線形分離可能でない場合はどうすれば良いですか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0707	0503	線形分離可能でない場合はどうすれば良いですか	様々な数値データに対して多く用いられる統計モデル
1	0711	0711	カーネル関数とは何ですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0711	0901	カーネル関数とは何ですか	表現学習
0	0711	0505	カーネル関数とは何ですか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	0711	0105	カーネル関数とは何ですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	0711	1220	カーネル関数とは何ですか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0711	1313	カーネル関数とは何ですか	最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる
0	0711	1510	カーネル関数とは何ですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0711	0712	カーネル関数とは何ですか	カーネル関数が正定値関数という条件を満たすとき
0	0711	1407	カーネル関数とは何ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0711	0206	カーネル関数とは何ですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0711	1310	カーネル関数とは何ですか	確率的非決定性オートマトンの一種
0	0711	0701	カーネル関数とは何ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0711	1015	カーネル関数とは何ですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0711	0117	カーネル関数とは何ですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0711	0204	カーネル関数とは何ですか	特徴ベクトルの次元数を減らすこと
0	0711	0111	カーネル関数とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0711	0508	カーネル関数とは何ですか	二乗誤差を最小にするように識別関数を調整する方法
0	0711	0906	カーネル関数とは何ですか	多階層構造でもそのまま適用できます
0	0711	1103	カーネル関数とは何ですか	個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る
0	0711	1301	カーネル関数とは何ですか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0711	0917	カーネル関数とは何ですか	入力ゲート・出力ゲート・忘却ゲート
0	0711	0912	カーネル関数とは何ですか	画像認識
0	0711	1106	カーネル関数とは何ですか	最も遠い事例対の距離を類似度とする
0	0711	1010	カーネル関数とは何ですか	バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります
0	0711	1106	カーネル関数とは何ですか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
1	0710	0710	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0710	0208	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます
0	0710	0315	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	分割後のデータの分散
0	0710	0607	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	0710	1110	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0710	0611	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0710	0511	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0710	0402	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	事後確率が最大となるクラスを識別結果とする方法
0	0710	1116	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0710	0209	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	0710	1110	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した
0	0710	0504	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0710	1015	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0710	0917	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	入力ゲート・出力ゲート・忘却ゲート
0	0710	0413	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	変数間の独立性を表現できること
0	0710	0409	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0710	1203	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録
0	0710	0407	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0710	1007	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0710	0917	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0710	0802	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	入力層・出力層の数に応じた適当な数
0	0710	1306	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	0710	1505	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	「マルコフ性」を持つ確率過程における意思決定問題
0	0710	0316	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0710	0614	なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか	回帰木と線形回帰の双方のよいところを取った方法
1	0802	0802	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0802	0901	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	音声認識・画像認識・自然言語処理など
0	0802	0411	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0802	0701	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0802	0707	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0802	1114	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	クラスタリング結果のデータ数の分布から
0	0802	0503	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	様々な数値データに対して多く用いられる統計モデル
0	0802	1407	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの
0	0802	1409	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0802	0103	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0802	1501	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0802	1104	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0802	1110	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの
0	0802	0713	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	文書分類やバイオインフォマティックスなど
0	0802	0412	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	0802	1012	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0802	0715	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	カーネルトリック
0	0802	1410	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0802	1310	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0802	0911	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	0802	0109	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	学習データに正解が付いている場合の学習
0	0802	0917	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0802	1301	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	動画像の分類や音声で入力された単語の識別などの問題
0	0802	0109	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	正解が付いていない場合の学習
0	0802	1409	フィードフォワード型ニューラルネットワークを構成するには何が必要ですか	自分が出した誤りを指摘してくれる他人がいない
1	0808	0808	ニューラルネットワークの最適化手法には何が一番多く用いられますか	通常，ニューラルネットワークの学習は確率的最急勾配法を用いる
0	0808	1409	ニューラルネットワークの最適化手法には何が一番多く用いられますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0808	1112	ニューラルネットワークの最適化手法には何が一番多く用いられますか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0808	1404	ニューラルネットワークの最適化手法には何が一番多く用いられますか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0808	0802	ニューラルネットワークの最適化手法には何が一番多く用いられますか	多層パーセプトロンあるいはニューラルネットワーク
0	0808	1505	ニューラルネットワークの最適化手法には何が一番多く用いられますか	「マルコフ性」を持つ確率過程における意思決定問題
0	0808	0710	ニューラルネットワークの最適化手法には何が一番多く用いられますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0808	0901	ニューラルネットワークの最適化手法には何が一番多く用いられますか	深層学習に用いるニューラルネットワーク
0	0808	1306	ニューラルネットワークの最適化手法には何が一番多く用いられますか	条件付き確率場（Conditional Random Field: CRF）
0	0808	1306	ニューラルネットワークの最適化手法には何が一番多く用いられますか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0808	0114	ニューラルネットワークの最適化手法には何が一番多く用いられますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0808	0612	ニューラルネットワークの最適化手法には何が一番多く用いられますか	識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します
0	0808	0505	ニューラルネットワークの最適化手法には何が一番多く用いられますか	パーセプトロン
0	0808	0406	ニューラルネットワークの最適化手法には何が一番多く用いられますか	各クラスから生じる特徴の尤もらしさを表す
0	0808	0802	ニューラルネットワークの最適化手法には何が一番多く用いられますか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0808	0801	ニューラルネットワークの最適化手法には何が一番多く用いられますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	0808	1508	ニューラルネットワークの最適化手法には何が一番多く用いられますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0808	0102	ニューラルネットワークの最適化手法には何が一番多く用いられますか	現在，人が行っている知的な判断を代わりに行う技術
0	0808	1215	ニューラルネットワークの最適化手法には何が一番多く用いられますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0808	0717	ニューラルネットワークの最適化手法には何が一番多く用いられますか	連続値
0	0808	1219	ニューラルネットワークの最適化手法には何が一番多く用いられますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0808	0811	ニューラルネットワークの最適化手法には何が一番多く用いられますか	誤差が消失しません
0	0808	0611	ニューラルネットワークの最適化手法には何が一番多く用いられますか	識別における決定木の考え方を回帰問題に適用する方法
0	0808	0204	ニューラルネットワークの最適化手法には何が一番多く用いられますか	特徴ベクトルの次元数を減らすこと
0	0808	0912	ニューラルネットワークの最適化手法には何が一番多く用いられますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
1	0417	0417	ベイジアンネットワークで重要な項目は何ですか	ネットワークの構造とアークの条件付き確率表
0	0417	1214	ベイジアンネットワークで重要な項目は何ですか	計算量が膨大であること
0	0417	0407	ベイジアンネットワークで重要な項目は何ですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0417	1409	ベイジアンネットワークで重要な項目は何ですか	自分が出した誤りを指摘してくれる他人がいない
0	0417	0606	ベイジアンネットワークで重要な項目は何ですか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0417	0708	ベイジアンネットワークで重要な項目は何ですか	制約を満たさない程度を表すので，小さい方が望ましい
0	0417	0614	ベイジアンネットワークで重要な項目は何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	0417	0313	ベイジアンネットワークで重要な項目は何ですか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0417	0416	ベイジアンネットワークで重要な項目は何ですか	値が真となる確率を知りたいノードが表す変数
0	0417	0911	ベイジアンネットワークで重要な項目は何ですか	ドロップアウト
0	0417	1506	ベイジアンネットワークで重要な項目は何ですか	最適政策$\pi^*$を獲得すること
0	0417	0901	ベイジアンネットワークで重要な項目は何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0417	0912	ベイジアンネットワークで重要な項目は何ですか	画像認識
0	0417	0901	ベイジアンネットワークで重要な項目は何ですか	音声認識・画像認識・自然言語処理など
0	0417	0916	ベイジアンネットワークで重要な項目は何ですか	時系列信号や自然言語などの系列パターンを扱うことができます．
0	0417	1104	ベイジアンネットワークで重要な項目は何ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0417	0707	ベイジアンネットワークで重要な項目は何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	0417	1214	ベイジアンネットワークで重要な項目は何ですか	一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので
0	0417	0906	ベイジアンネットワークで重要な項目は何ですか	十分多くの層を持つニューラルネットワーク
0	0417	0614	ベイジアンネットワークで重要な項目は何ですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0417	0204	ベイジアンネットワークで重要な項目は何ですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0417	0211	ベイジアンネットワークで重要な項目は何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0417	1406	ベイジアンネットワークで重要な項目は何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0417	0117	ベイジアンネットワークで重要な項目は何ですか	学習データの一部にだけ正解が与えられている場合
0	0417	0715	ベイジアンネットワークで重要な項目は何ですか	複雑な非線形変換を求めるという操作を避ける方法
1	1116	1116	EMアルゴリズムとはなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	1116	0114	EMアルゴリズムとはなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	1116	0105	EMアルゴリズムとはなんですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1116	0907	EMアルゴリズムとはなんですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	1116	0410	EMアルゴリズムとはなんですか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1116	1502	EMアルゴリズムとはなんですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1116	0112	EMアルゴリズムとはなんですか	線形回帰，回帰木，モデル木など
0	1116	1301	EMアルゴリズムとはなんですか	形態素解析処理が典型的な問題
0	1116	0502	EMアルゴリズムとはなんですか	非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法
0	1116	1004	EMアルゴリズムとはなんですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1116	0402	EMアルゴリズムとはなんですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1116	0512	EMアルゴリズムとはなんですか	重み$\bm{w}$（これ以降は，説明を簡潔にするために$\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル
0	1116	0205	EMアルゴリズムとはなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1116	0404	EMアルゴリズムとはなんですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	1116	0908	EMアルゴリズムとはなんですか	シグモイド関数
0	1116	0906	EMアルゴリズムとはなんですか	多階層構造でもそのまま適用できます
0	1116	0307	EMアルゴリズムとはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造
0	1116	0912	EMアルゴリズムとはなんですか	画像認識
0	1116	1015	EMアルゴリズムとはなんですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	1116	0505	EMアルゴリズムとはなんですか	生物の神経細胞の仕組みをモデル化したもの
0	1116	0405	EMアルゴリズムとはなんですか	尤度と事前確率の積を最大とするクラス
0	1116	1106	EMアルゴリズムとはなんですか	最も遠い事例対の距離を類似度とする
0	1116	0407	EMアルゴリズムとはなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	1116	0911	EMアルゴリズムとはなんですか	学習時の自由度を意図的に下げていること
0	1116	1406	EMアルゴリズムとはなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
1	0102	0102	人工知能の定義はなんですか	現在，人が行っている知的な判断を代わりに行う技術
0	0102	0701	人工知能の定義はなんですか	マージン
0	0102	0701	人工知能の定義はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0102	0811	人工知能の定義はなんですか	ユニットの活性化関数を工夫する方法があります
0	0102	0402	人工知能の定義はなんですか	統計的識別手法
0	0102	0306	人工知能の定義はなんですか	仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないこと
0	0102	0504	人工知能の定義はなんですか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0102	0704	人工知能の定義はなんですか	ラグランジュの未定乗数法を不等式制約条件
0	0102	0710	人工知能の定義はなんですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0102	1405	人工知能の定義はなんですか	正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段
0	0102	0313	人工知能の定義はなんですか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0102	1409	人工知能の定義はなんですか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0102	0802	人工知能の定義はなんですか	多層パーセプトロンあるいはニューラルネットワーク
0	0102	0505	人工知能の定義はなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0102	0702	人工知能の定義はなんですか	識別面は平面を仮定する
0	0102	1306	人工知能の定義はなんですか	条件付き確率場（Conditional Random Field: CRF）
0	0102	1114	人工知能の定義はなんですか	クラスタリング結果のデータ数の分布
0	0102	0306	人工知能の定義はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0102	1110	人工知能の定義はなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0102	0402	人工知能の定義はなんですか	入力を観測した後で計算される確率
0	0102	0316	人工知能の定義はなんですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0102	0606	人工知能の定義はなんですか	入力が少し変化したときに，出力も少し変化する
0	0102	1106	人工知能の定義はなんですか	クラスタの重心間の距離を類似度とする
0	0102	0109	人工知能の定義はなんですか	学習データに正解が付いている場合の学習
0	0102	0808	人工知能の定義はなんですか	入力の重み付き和の微分
1	0102	0102	機械学習と人工知能の意味の差はありますか	「人工知能=機械学習」ではありません．知的に振る舞うシステムを作る方法は，機械学習だけとは限りません．開発者が，その振る舞いの規則をプログラムとして作成することによっても，それなりに知的に振る舞うシステムは作製できます
0	0102	0917	機械学習と人工知能の意味の差はありますか	LSTM
0	0102	0708	機械学習と人工知能の意味の差はありますか	制約を弱める変数
0	0102	0901	機械学習と人工知能の意味の差はありますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	0102	0316	機械学習と人工知能の意味の差はありますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0102	0917	機械学習と人工知能の意味の差はありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0102	0901	機械学習と人工知能の意味の差はありますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0102	0610	機械学習と人工知能の意味の差はありますか	真のモデルとの距離
0	0102	0211	機械学習と人工知能の意味の差はありますか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0102	0901	機械学習と人工知能の意味の差はありますか	深層学習に用いるニューラルネットワーク
0	0102	0305	機械学習と人工知能の意味の差はありますか	仮説に対して課す制約
0	0102	1215	機械学習と人工知能の意味の差はありますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．
0	0102	0911	機械学習と人工知能の意味の差はありますか	過学習が起きにくくなり，汎用性が高まることが報告されています
0	0102	0611	機械学習と人工知能の意味の差はありますか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	0102	1502	機械学習と人工知能の意味の差はありますか	将棋や囲碁などを行うプログラム
0	0102	0805	機械学習と人工知能の意味の差はありますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0102	0114	機械学習と人工知能の意味の差はありますか	もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法
0	0102	0402	機械学習と人工知能の意味の差はありますか	入力を観測した後で計算される確率
0	0102	0409	機械学習と人工知能の意味の差はありますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0102	1001	機械学習と人工知能の意味の差はありますか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0102	0701	機械学習と人工知能の意味の差はありますか	識別境界線と最も近いデータとの距離
0	0102	1012	機械学習と人工知能の意味の差はありますか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	0102	1112	機械学習と人工知能の意味の差はありますか	近くにデータがないか，あるは極端に少ないものを外れ値とみなす
0	0102	0908	機械学習と人工知能の意味の差はありますか	ユークリッド距離
0	0102	0605	機械学習と人工知能の意味の差はありますか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
1	0406	0406	尤度とはどういう意味ですか	各クラスから生じる特徴の尤もらしさを表す
0	0406	1404	尤度とはどういう意味ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0406	0901	尤度とはどういう意味ですか	深層学習に用いるニューラルネットワーク
0	0406	0114	尤度とはどういう意味ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0406	0917	尤度とはどういう意味ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0406	0316	尤度とはどういう意味ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0406	0715	尤度とはどういう意味ですか	複雑な非線形変換を求めるという操作を避ける方法
0	0406	0701	尤度とはどういう意味ですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0406	0908	尤度とはどういう意味ですか	シグモイド関数
0	0406	0505	尤度とはどういう意味ですか	関数$\hat{c}(\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法
0	0406	0612	尤度とはどういう意味ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0406	0509	尤度とはどういう意味ですか	確率的最急勾配法
0	0406	0805	尤度とはどういう意味ですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0406	0612	尤度とはどういう意味ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0406	1507	尤度とはどういう意味ですか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0406	1406	尤度とはどういう意味ですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0406	0907	尤度とはどういう意味ですか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0406	1104	尤度とはどういう意味ですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0406	0311	尤度とはどういう意味ですか	集合の乱雑さ
0	0406	1306	尤度とはどういう意味ですか	条件付き確率場（Conditional Random Field: CRF）
0	0406	0710	尤度とはどういう意味ですか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0406	0502	尤度とはどういう意味ですか	境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合
0	0406	0901	尤度とはどういう意味ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0406	0715	尤度とはどういう意味ですか	識別面
0	0406	0710	尤度とはどういう意味ですか	もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということ
1	0503	0503	正規分布とはどのようなものですか	様々な数値データに対して多く用いられる統計モデル
0	0503	0610	正規分布とはどのようなものですか	真のモデルとの距離
0	0503	1412	正規分布とはどのようなものですか	近くのノードは同じクラスになりやすいという仮定
0	0503	0504	正規分布とはどのようなものですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0503	0610	正規分布とはどのようなものですか	学習結果の散らばり具合
0	0503	1116	正規分布とはどのようなものですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0503	0109	正規分布とはどのようなものですか	正解が付いていない場合の学習
0	0503	0715	正規分布とはどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	0503	0711	正規分布とはどのようなものですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0503	0901	正規分布とはどのようなものですか	深層学習に用いるニューラルネットワーク
0	0503	0614	正規分布とはどのようなものですか	回帰木と線形回帰の双方のよいところを取った
0	0503	0704	正規分布とはどのようなものですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	0503	1303	正規分布とはどのようなものですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0503	0906	正規分布とはどのようなものですか	多階層構造でもそのまま適用できます
0	0503	1301	正規分布とはどのようなものですか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0503	1406	正規分布とはどのようなものですか	特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく
0	0503	0901	正規分布とはどのようなものですか	表現学習
0	0503	0404	正規分布とはどのようなものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
0	0503	0911	正規分布とはどのようなものですか	ランダムに一定割合のユニットを消して学習を行う
0	0503	1209	正規分布とはどのようなものですか	規則の条件部が起こったときに結論部が起こる割合
0	0503	0513	正規分布とはどのようなものですか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0503	0704	正規分布とはどのようなものですか	ラグランジュの未定乗数法を不等式制約条件
0	0503	0715	正規分布とはどのようなものですか	複雑な非線形変換を求めるという操作を避ける方法
0	0503	0802	正規分布とはどのようなものですか	特徴ベクトルの次元数
0	0503	0404	正規分布とはどのようなものですか	入力を観測する前にもっているそれぞれのクラスの起こりやすさ
1	0616	0616	回帰にカーネル法を取り入れる理由はなんですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0616	1009	回帰にカーネル法を取り入れる理由はなんですか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0616	0105	回帰にカーネル法を取り入れる理由はなんですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0616	1001	回帰にカーネル法を取り入れる理由はなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	0616	1408	回帰にカーネル法を取り入れる理由はなんですか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	0616	0810	回帰にカーネル法を取り入れる理由はなんですか	誤差が小さくなって消失してしまう
0	0616	0114	回帰にカーネル法を取り入れる理由はなんですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0616	0306	回帰にカーネル法を取り入れる理由はなんですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0616	0204	回帰にカーネル法を取り入れる理由はなんですか	多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています
0	0616	0411	回帰にカーネル法を取り入れる理由はなんですか	これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます
0	0616	1403	回帰にカーネル法を取り入れる理由はなんですか	多次元でも「次元の呪い」にかかっていない，ということ
0	0616	0912	回帰にカーネル法を取り入れる理由はなんですか	畳み込みニューラルネットワーク
0	0616	0205	回帰にカーネル法を取り入れる理由はなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0616	1108	回帰にカーネル法を取り入れる理由はなんですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0616	1110	回帰にカーネル法を取り入れる理由はなんですか	事前にクラスタ数$k$を固定しなければいけないという問題点
0	0616	0512	回帰にカーネル法を取り入れる理由はなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0616	0508	回帰にカーネル法を取り入れる理由はなんですか	最小二乗法
0	0616	0510	回帰にカーネル法を取り入れる理由はなんですか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	0616	1015	回帰にカーネル法を取り入れる理由はなんですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0616	1306	回帰にカーネル法を取り入れる理由はなんですか	制限を設け，対数線型モデルを系列識別問題に適用したもの
0	0616	1301	回帰にカーネル法を取り入れる理由はなんですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0616	0115	回帰にカーネル法を取り入れる理由はなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0616	0802	回帰にカーネル法を取り入れる理由はなんですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
0	0616	0114	回帰にカーネル法を取り入れる理由はなんですか	階層的クラスタリングや k-means 法
0	0616	0911	回帰にカーネル法を取り入れる理由はなんですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
1	0616	0616	カーネル回帰とはなんですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	0616	0908	カーネル回帰とはなんですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0616	0510	カーネル回帰とはなんですか	特徴空間上でクラスを分割する面
0	0616	1209	カーネル回帰とはなんですか	この値が高いほど，得られる情報の多い規則であること
0	0616	0406	カーネル回帰とはなんですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	0616	0114	カーネル回帰とはなんですか	顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用
0	0616	0614	カーネル回帰とはなんですか	回帰木と線形回帰の双方のよいところを取った方法
0	0616	0311	カーネル回帰とはなんですか	集合の乱雑さ
0	0616	0407	カーネル回帰とはなんですか	モデルのパラメータが与えられたときの，学習データ全体が生成される尤度
0	0616	1111	カーネル回帰とはなんですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	0616	0307	カーネル回帰とはなんですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	0616	0114	カーネル回帰とはなんですか	階層的クラスタリングや k-means 法
0	0616	0612	カーネル回帰とはなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0616	0701	カーネル回帰とはなんですか	サポートベクトルマシン
0	0616	0508	カーネル回帰とはなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	0616	0614	カーネル回帰とはなんですか	データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます
0	0616	0206	カーネル回帰とはなんですか	学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法
0	0616	0116	カーネル回帰とはなんですか	学習データが教師あり／教師なしの混在となっているもの
0	0616	0713	カーネル回帰とはなんですか	文書分類やバイオインフォマティックスなど
0	0616	1110	カーネル回帰とはなんですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0616	0713	カーネル回帰とはなんですか	文書分類やバイオインフォマティックスなど
0	0616	0413	カーネル回帰とはなんですか	変数間の独立性を表現できること
0	0616	0512	カーネル回帰とはなんですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0616	1304	カーネル回帰とはなんですか	出力系列を参照する素性
0	0616	1001	カーネル回帰とはなんですか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
1	0903	0903	深層学習の中で特化した構造にはどのようなものがありますか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	0903	1007	深層学習の中で特化した構造にはどのようなものがありますか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0903	0906	深層学習の中で特化した構造にはどのようなものがありますか	入力に近い側の処理
0	0903	1219	深層学習の中で特化した構造にはどのようなものがありますか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0903	0917	深層学習の中で特化した構造にはどのようなものがありますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換える
0	0903	0601	深層学習の中で特化した構造にはどのようなものがありますか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0903	0416	深層学習の中で特化した構造にはどのようなものがありますか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0903	1410	深層学習の中で特化した構造にはどのようなものがありますか	それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること
0	0903	0409	深層学習の中で特化した構造にはどのようなものがありますか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0903	0902	深層学習の中で特化した構造にはどのようなものがありますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0903	0206	深層学習の中で特化した構造にはどのようなものがありますか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	0903	0906	深層学習の中で特化した構造にはどのようなものがありますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0903	1106	深層学習の中で特化した構造にはどのようなものがありますか	最も遠い事例対の距離を類似度とする
0	0903	1112	深層学習の中で特化した構造にはどのようなものがありますか	単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．
0	0903	1201	深層学習の中で特化した構造にはどのようなものがありますか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	0903	0606	深層学習の中で特化した構造にはどのようなものがありますか	山の尾根という意味
0	0903	1304	深層学習の中で特化した構造にはどのようなものがありますか	出力系列を参照する素性
0	0903	0606	深層学習の中で特化した構造にはどのようなものがありますか	パラメータ$\bm{w}$の二乗を正則化項とするもの
0	0903	0912	深層学習の中で特化した構造にはどのようなものがありますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0903	0607	深層学習の中で特化した構造にはどのようなものがありますか	「投げ縄」という意味
0	0903	1008	深層学習の中で特化した構造にはどのようなものがありますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0903	0417	深層学習の中で特化した構造にはどのようなものがありますか	ネットワークの構造とアークの条件付き確率
0	0903	1506	深層学習の中で特化した構造にはどのようなものがありますか	最適政策$\pi^*$を獲得すること
0	0903	1503	深層学習の中で特化した構造にはどのようなものがありますか	全ての行為を順に試みて最も報酬の高い行為を選べばよい
0	0903	0205	深層学習の中で特化した構造にはどのようなものがありますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
1	1501	1501	強化学習が中間的学習という位置づけにある理由はなんですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	1501	1012	強化学習が中間的学習という位置づけにある理由はなんですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	1501	1303	強化学習が中間的学習という位置づけにある理由はなんですか	形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなど
0	1501	0811	強化学習が中間的学習という位置づけにある理由はなんですか	ユニットの活性化関数を工夫する方法
0	1501	0701	強化学習が中間的学習という位置づけにある理由はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1501	0205	強化学習が中間的学習という位置づけにある理由はなんですか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1501	0911	強化学習が中間的学習という位置づけにある理由はなんですか	学習時の自由度を意図的に下げていること
0	1501	0209	強化学習が中間的学習という位置づけにある理由はなんですか	正例がどれだけ正しく判定されているかという指標
0	1501	0803	強化学習が中間的学習という位置づけにある理由はなんですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1501	0404	強化学習が中間的学習という位置づけにある理由はなんですか	事前確率
0	1501	0805	強化学習が中間的学習という位置づけにある理由はなんですか	誤差逆伝播法
0	1501	0302	強化学習が中間的学習という位置づけにある理由はなんですか	カテゴリ形式の正解情報のこと
0	1501	0715	強化学習が中間的学習という位置づけにある理由はなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1501	1012	強化学習が中間的学習という位置づけにある理由はなんですか	各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します
0	1501	0209	強化学習が中間的学習という位置づけにある理由はなんですか	識別器が正例と判断したときに，それがどれだけ信頼できるかという指標
0	1501	0704	強化学習が中間的学習という位置づけにある理由はなんですか	ここでは，ラグランジュの未定乗数法を不等式制約条件で用います
0	1501	0715	強化学習が中間的学習という位置づけにある理由はなんですか	複雑な非線形変換を求めるという操作を避ける方法
0	1501	0508	強化学習が中間的学習という位置づけにある理由はなんですか	二乗誤差を最小にするように識別関数を調整する方法
0	1501	0901	強化学習が中間的学習という位置づけにある理由はなんですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	1501	1303	強化学習が中間的学習という位置づけにある理由はなんですか	単語の系列を入力として，それぞれの単語に品詞を付けるという問題
0	1501	0811	強化学習が中間的学習という位置づけにある理由はなんですか	引数が負のときは0，0以上のときはその値を出力
0	1501	1201	強化学習が中間的学習という位置づけにある理由はなんですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1501	0411	強化学習が中間的学習という位置づけにある理由はなんですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1501	0416	強化学習が中間的学習という位置づけにある理由はなんですか	値が真となる確率を知りたいノードが表す変数
0	1501	0605	強化学習が中間的学習という位置づけにある理由はなんですか	「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの
1	0504	0504	生成モデルアプローチが有効なのはどのようなときですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0504	0416	生成モデルアプローチが有効なのはどのようなときですか	アークを無向とみなした結合を考えたとき
0	0504	1007	生成モデルアプローチが有効なのはどのようなときですか	識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法
0	0504	0109	生成モデルアプローチが有効なのはどのようなときですか	学習データに正解が付いている場合の学習
0	0504	0115	生成モデルアプローチが有効なのはどのようなときですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0504	0612	生成モデルアプローチが有効なのはどのようなときですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0504	0711	生成モデルアプローチが有効なのはどのようなときですか	カーネル関数
0	0504	0114	生成モデルアプローチが有効なのはどのようなときですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0504	0912	生成モデルアプローチが有効なのはどのようなときですか	畳み込みニューラルネットワーク
0	0504	0917	生成モデルアプローチが有効なのはどのようなときですか	LSTM
0	0504	0701	生成モデルアプローチが有効なのはどのようなときですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0504	1302	生成モデルアプローチが有効なのはどのようなときですか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題
0	0504	1219	生成モデルアプローチが有効なのはどのようなときですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	0504	0412	生成モデルアプローチが有効なのはどのようなときですか	「特徴の部分集合が，あるクラスのもとで独立である」と仮定
0	0504	1301	生成モデルアプローチが有効なのはどのようなときですか	個々の要素の間に i.i.d. の関係が成立しないもの
0	0504	1501	生成モデルアプローチが有効なのはどのようなときですか	教師信号ではないがそれに準ずる情報が，一部の学習データにのみ与えられている状況での学習とみることができるので
0	0504	0111	生成モデルアプローチが有効なのはどのようなときですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	0504	0902	生成モデルアプローチが有効なのはどのようなときですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	0504	1106	生成モデルアプローチが有効なのはどのようなときですか	クラスタの重心間の距離を類似度とする
0	0504	1506	生成モデルアプローチが有効なのはどのようなときですか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0504	0601	生成モデルアプローチが有効なのはどのようなときですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0504	0104	生成モデルアプローチが有効なのはどのようなときですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0504	0917	生成モデルアプローチが有効なのはどのようなときですか	LSTMセル
0	0504	0901	生成モデルアプローチが有効なのはどのようなときですか	深層学習に用いるニューラルネットワーク
0	0504	0802	生成モデルアプローチが有効なのはどのようなときですか	入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します
1	1309	1309	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1309	0715	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	複雑な非線形変換を求めるという操作を避ける方法
0	1309	0717	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1309	0114	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1309	0811	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	引数が負のときは0，0以上のときはその値を出力
0	1309	0306	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1309	1219	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1309	1506	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	最適政策$\pi^*$を獲得すること
0	1309	0805	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです
0	1309	0612	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	1309	0105	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術
0	1309	1201	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．
0	1309	0602	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	正解情報$y$が数値であるということ
0	1309	0416	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	アークを無向とみなした結合を考えたとき
0	1309	0109	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	正解が付いていない場合の学習
0	1309	0906	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	1309	1004	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1309	1301	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	動画像の分類や音声で入力された単語の識別などの問題
0	1309	0402	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法
0	1309	0710	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	1309	0717	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	Grid search
0	1309	0611	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1309	0411	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる
0	1309	1412	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1309	0305	入力の系列長に関わらず出力の系列長が1である問題は何が難しいのですか	仮説に対して課す制約
1	1412	1412	ラベル伝搬法の考え方は何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1412	0707	ラベル伝搬法の考え方は何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1412	0508	ラベル伝搬法の考え方は何ですか	統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する
0	1412	0903	ラベル伝搬法の考え方は何ですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1412	0417	ラベル伝搬法の考え方は何ですか	ネットワークの構造とアークの条件付き確率
0	1412	0717	ラベル伝搬法の考え方は何ですか	Grid search
0	1412	0701	ラベル伝搬法の考え方は何ですか	サポートベクトルマシン
0	1412	0204	ラベル伝搬法の考え方は何ですか	特徴ベクトルの次元数を減らすこと
0	1412	1004	ラベル伝搬法の考え方は何ですか	学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの
0	1412	0715	ラベル伝搬法の考え方は何ですか	複雑な非線形変換を求めるという操作を避ける方法
0	1412	0611	ラベル伝搬法の考え方は何ですか	回帰
0	1412	0311	ラベル伝搬法の考え方は何ですか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1412	0810	ラベル伝搬法の考え方は何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1412	0606	ラベル伝搬法の考え方は何ですか	入力が少し変化したときに，出力も少し変化する
0	1412	0417	ラベル伝搬法の考え方は何ですか	ネットワークの構造とアークの条件付き確率表
0	1412	0512	ラベル伝搬法の考え方は何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1412	0614	ラベル伝搬法の考え方は何ですか	CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします
0	1412	0707	ラベル伝搬法の考え方は何ですか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1412	1110	ラベル伝搬法の考え方は何ですか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1412	0512	ラベル伝搬法の考え方は何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	1412	0616	ラベル伝搬法の考え方は何ですか	カーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチ
0	1412	0209	ラベル伝搬法の考え方は何ですか	精度と再現率を総合的に判断するために，その調和平均をとったもの
0	1412	0510	ラベル伝搬法の考え方は何ですか	特徴空間上でクラスを分割する面
0	1412	0803	ラベル伝搬法の考え方は何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1412	0406	ラベル伝搬法の考え方は何ですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
1	1412	1412	ラベル伝搬法の考え方にある仮定はどのようなものですか	近くのノードは同じクラスになりやすいという仮定
0	1412	0811	ラベル伝搬法の考え方にある仮定はどのようなものですか	半分の領域で勾配が1になるので
0	1412	0803	ラベル伝搬法の考え方にある仮定はどのようなものですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1412	0903	ラベル伝搬法の考え方にある仮定はどのようなものですか	多階層ニューラルネットワーク，畳み込みネットワーク，リカレントネットワーク
0	1412	0810	ラベル伝搬法の考え方にある仮定はどのようなものですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1412	1503	ラベル伝搬法の考え方にある仮定はどのようなものですか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	1412	0206	ラベル伝搬法の考え方にある仮定はどのようなものですか	一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します
0	1412	0911	ラベル伝搬法の考え方にある仮定はどのようなものですか	学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります
0	1412	0902	ラベル伝搬法の考え方にある仮定はどのようなものですか	どのような特徴を抽出するのかもデータから機械学習しようとするものです
0	1412	0717	ラベル伝搬法の考え方にある仮定はどのようなものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	1412	1309	ラベル伝搬法の考え方にある仮定はどのようなものですか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	1412	0111	ラベル伝搬法の考え方にある仮定はどのようなものですか	たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題
0	1412	0901	ラベル伝搬法の考え方にある仮定はどのようなものですか	音声認識・画像認識・自然言語処理など
0	1412	0402	ラベル伝搬法の考え方にある仮定はどのようなものですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	1412	1306	ラベル伝搬法の考え方にある仮定はどのようなものですか	素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質
0	1412	0906	ラベル伝搬法の考え方にある仮定はどのようなものですか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1412	0116	ラベル伝搬法の考え方にある仮定はどのようなものですか	学習データが教師あり／教師なしの混在となっているもの
0	1412	0507	ラベル伝搬法の考え方にある仮定はどのようなものですか	パーセプトロンの収束定理
0	1412	1506	ラベル伝搬法の考え方にある仮定はどのようなものですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	1412	1015	ラベル伝搬法の考え方にある仮定はどのようなものですか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	1412	0406	ラベル伝搬法の考え方にある仮定はどのようなものですか	それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します
0	1412	1111	ラベル伝搬法の考え方にある仮定はどのようなものですか	入力$\{\bm{x}_i\}$に含まれる異常値を，教師信号なしで見つけることです
0	1412	1104	ラベル伝搬法の考え方にある仮定はどのようなものですか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1412	0311	ラベル伝搬法の考え方にある仮定はどのようなものですか	集合の乱雑さ
0	1412	0614	ラベル伝搬法の考え方にある仮定はどのようなものですか	回帰木と線形回帰の双方のよいところを取った方法
1	0506	0506	パーセプトロンの出力はどのようになっていますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0506	0710	パーセプトロンの出力はどのようになっていますか	もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています
0	0506	1411	パーセプトロンの出力はどのようになっていますか	繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム
0	0506	0112	パーセプトロンの出力はどのようになっていますか	線形回帰，回帰木，モデル木など
0	0506	0413	パーセプトロンの出力はどのようになっていますか	変数間の独立性を表現できること
0	0506	1220	パーセプトロンの出力はどのようになっていますか	まばらなデータを低次元行列の積に分解する方法の一つ
0	0506	0417	パーセプトロンの出力はどのようになっていますか	ネットワークの構造とアークの条件付き確率
0	0506	0917	パーセプトロンの出力はどのようになっていますか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0506	0803	パーセプトロンの出力はどのようになっていますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0506	0304	パーセプトロンの出力はどのようになっていますか	個々の事例から，あるクラスについて共通点を見つけること
0	0506	1104	パーセプトロンの出力はどのようになっていますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	0506	0908	パーセプトロンの出力はどのようになっていますか	シグモイド関数
0	0506	1009	パーセプトロンの出力はどのようになっていますか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします．
0	0506	1505	パーセプトロンの出力はどのようになっていますか	「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質
0	0506	1506	パーセプトロンの出力はどのようになっていますか	その政策に従って行動したときの累積報酬の期待値で評価
0	0506	0901	パーセプトロンの出力はどのようになっていますか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0506	0114	パーセプトロンの出力はどのようになっていますか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	0506	0906	パーセプトロンの出力はどのようになっていますか	誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0506	0717	パーセプトロンの出力はどのようになっていますか	グリッド
0	0506	0916	パーセプトロンの出力はどのようになっていますか	リカレントニューラルネットワーク
0	0506	1015	パーセプトロンの出力はどのようになっていますか	損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます
0	0506	1502	パーセプトロンの出力はどのようになっていますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0506	0511	パーセプトロンの出力はどのようになっていますか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0506	0313	パーセプトロンの出力はどのようになっていますか	モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象
0	0506	0917	パーセプトロンの出力はどのようになっていますか	入力ゲート・出力ゲート・忘却ゲート
1	1302	1302	系列ラベリングでの問題は何がありますか	単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点
0	1302	1108	系列ラベリングでの問題は何がありますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1302	0315	系列ラベリングでの問題は何がありますか	分割後のデータの分散
0	1302	1104	系列ラベリングでの問題は何がありますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すもの
0	1302	0205	系列ラベリングでの問題は何がありますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	1302	0906	系列ラベリングでの問題は何がありますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1302	1001	系列ラベリングでの問題は何がありますか	識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法
0	1302	0611	系列ラベリングでの問題は何がありますか	このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる
0	1302	0406	系列ラベリングでの問題は何がありますか	各クラスから生じる特徴の尤もらしさを表す
0	1302	0803	系列ラベリングでの問題は何がありますか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	1302	0713	系列ラベリングでの問題は何がありますか	文書分類やバイオインフォマティックスなど
0	1302	0906	系列ラベリングでの問題は何がありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1302	0901	系列ラベリングでの問題は何がありますか	深層学習に用いるニューラルネットワーク
0	1302	0505	系列ラベリングでの問題は何がありますか	ニューラルネットワーク
0	1302	0602	系列ラベリングでの問題は何がありますか	正解情報$y$が数値であるということ
0	1302	0510	系列ラベリングでの問題は何がありますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1302	1506	系列ラベリングでの問題は何がありますか	最適政策$\pi^*$を獲得すること
0	1302	0311	系列ラベリングでの問題は何がありますか	「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということ
0	1302	1104	系列ラベリングでの問題は何がありますか	近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです
0	1302	0111	系列ラベリングでの問題は何がありますか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1302	1301	系列ラベリングでの問題は何がありますか	連続音声認識
0	1302	1214	系列ラベリングでの問題は何がありますか	計算量が膨大であること
0	1302	0710	系列ラベリングでの問題は何がありますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	1302	1114	系列ラベリングでの問題は何がありますか	クラスタリング結果のデータ数の分布
0	1302	0508	系列ラベリングでの問題は何がありますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
1	1205	1205	a prioriな原理とは何ですか	ある項目集合が頻出ならば，その部分集合も頻出である
0	1205	1219	a prioriな原理とは何ですか	新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する
0	1205	1402	a prioriな原理とは何ですか	データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない
0	1205	0810	a prioriな原理とは何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
0	1205	0917	a prioriな原理とは何ですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	1205	1508	a prioriな原理とは何ですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	1205	0614	a prioriな原理とは何ですか	回帰木と線形回帰の双方のよいところを取った方法
0	1205	1412	a prioriな原理とは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	1205	0703	a prioriな原理とは何ですか	微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます
0	1205	1306	a prioriな原理とは何ですか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	1205	0514	a prioriな原理とは何ですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	1205	1012	a prioriな原理とは何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	1205	0316	a prioriな原理とは何ですか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	1205	0111	a prioriな原理とは何ですか	識別の目的は，学習データに対して100\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした
0	1205	0701	a prioriな原理とは何ですか	識別境界線と最も近いデータとの距離
0	1205	1406	a prioriな原理とは何ですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	1205	0607	a prioriな原理とは何ですか	パラメータ$\bm{w}$の絶対値を正則化項とするもの
0	1205	0111	a prioriな原理とは何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	1205	0906	a prioriな原理とは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	1205	1303	a prioriな原理とは何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出
0	1205	0908	a prioriな原理とは何ですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	1205	0307	a prioriな原理とは何ですか	データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現
0	1205	0114	a prioriな原理とは何ですか	入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの
0	1205	1108	a prioriな原理とは何ですか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	1205	0506	a prioriな原理とは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
1	0116	0116	半教師あり学習とはどのようなものですか	学習データが教師あり／教師なしの混在となっているもの
0	0116	0916	半教師あり学習とはどのようなものですか	リカレントニューラルネットワーク
0	0116	1209	半教師あり学習とはどのようなものですか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0116	0906	半教師あり学習とはどのようなものですか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	0116	1502	半教師あり学習とはどのようなものですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0116	0717	半教師あり学習とはどのようなものですか	パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます
0	0116	1106	半教師あり学習とはどのようなものですか	最も近い事例対の距離を類似度とする
0	0116	0917	半教師あり学習とはどのようなものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします
0	0116	0711	半教師あり学習とはどのようなものですか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	0116	1012	半教師あり学習とはどのようなものですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成
0	0116	0912	半教師あり学習とはどのようなものですか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	0116	1310	半教師あり学習とはどのようなものですか	確率的非決定性オートマトンの一種
0	0116	0701	半教師あり学習とはどのようなものですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0116	0917	半教師あり学習とはどのようなものですか	中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫
0	0116	1104	半教師あり学習とはどのようなものですか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0116	0616	半教師あり学習とはどのようなものですか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0116	1015	半教師あり学習とはどのようなものですか	式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法
0	0116	0710	半教師あり学習とはどのようなものですか	もとの空間におけるデータ間の距離関係を保存
0	0116	0610	半教師あり学習とはどのようなものですか	学習結果の散らばり具合
0	0116	0402	半教師あり学習とはどのようなものですか	代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法
0	0116	1506	半教師あり学習とはどのようなものですか	各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス
0	0116	0112	半教師あり学習とはどのようなものですか	線形回帰，回帰木，モデル木など
0	0116	0612	半教師あり学習とはどのようなものですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0116	0117	半教師あり学習とはどのようなものですか	たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります
0	0116	1406	半教師あり学習とはどのようなものですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
1	0513	0513	最急勾配法の重みの更新はどうなったら止まりますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0513	1309	最急勾配法の重みの更新はどうなったら止まりますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0513	1009	最急勾配法の重みの更新はどうなったら止まりますか	通常の決定木の学習は，リーフのデータ数が一定値以下になるとそれ以上の成長を止めるか，十分に伸ばした後に枝刈りをするかして，過学習をしないように工夫します．しかし，ランダムフォレストでは意図的に過学習をさせ，できるだけ異なった木を作るようにします
0	0513	0907	最急勾配法の重みの更新はどうなったら止まりますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0513	0114	最急勾配法の重みの更新はどうなったら止まりますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0513	1108	最急勾配法の重みの更新はどうなったら止まりますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0513	0906	最急勾配法の重みの更新はどうなったら止まりますか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0513	1220	最急勾配法の重みの更新はどうなったら止まりますか	購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから
0	0513	0505	最急勾配法の重みの更新はどうなったら止まりますか	生物の神経細胞の仕組みをモデル化したもの
0	0513	0205	最急勾配法の重みの更新はどうなったら止まりますか	相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作
0	0513	0315	最急勾配法の重みの更新はどうなったら止まりますか	分割後のデータの分散
0	0513	0701	最急勾配法の重みの更新はどうなったら止まりますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0513	0508	最急勾配法の重みの更新はどうなったら止まりますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	0513	0601	最急勾配法の重みの更新はどうなったら止まりますか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0513	0305	最急勾配法の重みの更新はどうなったら止まりますか	仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限
0	0513	0313	最急勾配法の重みの更新はどうなったら止まりますか	適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法
0	0513	1406	最急勾配法の重みの更新はどうなったら止まりますか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0513	1001	最急勾配法の重みの更新はどうなったら止まりますか	評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということ
0	0513	0508	最急勾配法の重みの更新はどうなったら止まりますか	二乗誤差を最小にするように識別関数を調整する方法
0	0513	0802	最急勾配法の重みの更新はどうなったら止まりますか	識別対象のクラス数
0	0513	0906	最急勾配法の重みの更新はどうなったら止まりますか	識別に有効な特徴が入力の線形結合で表される，という保証はないからです
0	0513	0902	最急勾配法の重みの更新はどうなったら止まりますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0513	1306	最急勾配法の重みの更新はどうなったら止まりますか	先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すこと
0	0513	1506	最急勾配法の重みの更新はどうなったら止まりますか	その政策に従って行動したときの累積報酬の期待値で評価します
0	0513	1116	最急勾配法の重みの更新はどうなったら止まりますか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
1	0112	0112	回帰にはどのような種類がありますか	線形回帰，回帰木，モデル木など
0	0112	0803	回帰にはどのような種類がありますか	重みパラメータに対しては線形で，入力を非線形変換することによって実現
0	0112	1205	回帰にはどのような種類がありますか	ある項目集合が頻出ならば，その部分集合も頻出である
0	0112	1309	回帰にはどのような種類がありますか	学習の際に観測されない隠れ変数の値を用いなければならない，という点
0	0112	0912	回帰にはどのような種類がありますか	畳み込みニューラルネットワーク
0	0112	0810	回帰にはどのような種類がありますか	多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点
0	0112	0417	回帰にはどのような種類がありますか	ネットワークの構造とアークの条件付き確率
0	0112	0901	回帰にはどのような種類がありますか	深層学習に用いるニューラルネットワーク
0	0112	0710	回帰にはどのような種類がありますか	データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります
0	0112	0701	回帰にはどのような種類がありますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	0112	0306	回帰にはどのような種類がありますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	0112	0503	回帰にはどのような種類がありますか	様々な数値データに対して多く用いられる統計モデル
0	0112	0204	回帰にはどのような種類がありますか	特徴ベクトルの次元数を減らすこと
0	0112	1104	回帰にはどのような種類がありますか	一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了
0	0112	0917	回帰にはどのような種類がありますか	LSTMセル
0	0112	1409	回帰にはどのような種類がありますか	判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法
0	0112	0701	回帰にはどのような種類がありますか	識別境界線と最も近いデータとの距離
0	0112	0204	回帰にはどのような種類がありますか	特徴ベクトルの次元数を減らすこと
0	0112	0115	回帰にはどのような種類がありますか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0112	0507	回帰にはどのような種類がありますか	全ての誤りがなくなることが学習の終了条件なので
0	0112	0513	回帰にはどのような種類がありますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	0112	1301	回帰にはどのような種類がありますか	これまでに学んだ識別器を入力に対して逐次適用してゆくというもの
0	0112	0506	回帰にはどのような種類がありますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0112	1508	回帰にはどのような種類がありますか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0112	0405	回帰にはどのような種類がありますか	尤度と事前確率の積を最大とするクラス
1	1114	1114	事前確率はどのように求めますか	クラスタリング結果のデータ数の分布から
0	1114	0802	事前確率はどのように求めますか	識別対象のクラス数
0	1114	0711	事前確率はどのように求めますか	もとの特徴空間上の2点$\bm{x}, \bm{x}'$の距離に基づいて定義されるある関数$K(\bm{x}, \bm{x}')$
0	1114	0801	事前確率はどのように求めますか	計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム
0	1114	0906	事前確率はどのように求めますか	十分多くの層を持つニューラルネットワーク
0	1114	0416	事前確率はどのように求めますか	値が真となる確率を知りたいノードが表す変数
0	1114	1502	事前確率はどのように求めますか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	1114	0912	事前確率はどのように求めますか	畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです
0	1114	1215	事前確率はどのように求めますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	1114	0410	事前確率はどのように求めますか	各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法
0	1114	1207	事前確率はどのように求めますか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1114	1403	事前確率はどのように求めますか	多次元でも「次元の呪い」にかかっていない，ということ
0	1114	0510	事前確率はどのように求めますか	事後確率を特徴値の組み合わせから求めるようにモデルを作ります
0	1114	0412	事前確率はどのように求めますか	確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル
0	1114	0416	事前確率はどのように求めますか	アークを無向とみなした結合を考えたとき
0	1114	1505	事前確率はどのように求めますか	「マルコフ性」を持つ確率過程における意思決定問題
0	1114	0713	事前確率はどのように求めますか	文書分類やバイオインフォマティックスなど
0	1114	0110	事前確率はどのように求めますか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1114	0508	事前確率はどのように求めますか	識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの
0	1114	1407	事前確率はどのように求めますか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	1114	0503	事前確率はどのように求めますか	様々な数値データに対して多く用いられる統計モデル
0	1114	0707	事前確率はどのように求めますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1114	0805	事前確率はどのように求めますか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	1114	1506	事前確率はどのように求めますか	同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる
0	1114	0906	事前確率はどのように求めますか	多階層構造でもそのまま適用できます
1	0702	0702	SVMの識別面はどのようなものを仮定しますか	識別面は平面を仮定する
0	0702	0506	SVMの識別面はどのようなものを仮定しますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0702	0810	SVMの識別面はどのようなものを仮定しますか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	0702	0612	SVMの識別面はどのようなものを仮定しますか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた
0	0702	1008	SVMの識別面はどのようなものを仮定しますか	学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．
0	0702	1301	SVMの識別面はどのようなものを仮定しますか	ひとまとまりの系列データを特定のクラスに識別する問題
0	0702	0611	SVMの識別面はどのようなものを仮定しますか	識別における決定木の考え方を回帰問題に適用する方法
0	0702	0907	SVMの識別面はどのようなものを仮定しますか	誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア
0	0702	0504	SVMの識別面はどのようなものを仮定しますか	同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる
0	0702	0416	SVMの識別面はどのようなものを仮定しますか	値が真となる確率を知りたいノードが表す変数
0	0702	0507	SVMの識別面はどのようなものを仮定しますか	パーセプトロンの収束定理
0	0702	0701	SVMの識別面はどのようなものを仮定しますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0702	1507	SVMの識別面はどのようなものを仮定しますか	各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの
0	0702	1108	SVMの識別面はどのようなものを仮定しますか	クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します
0	0702	0616	SVMの識別面はどのようなものを仮定しますか	一般に非線形式ではデータにフィットしすぎてしまうため
0	0702	0114	SVMの識別面はどのようなものを仮定しますか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0702	0802	SVMの識別面はどのようなものを仮定しますか	隠れ層
0	0702	0811	SVMの識別面はどのようなものを仮定しますか	ユニットの活性化関数を工夫する方法があります
0	0702	0407	SVMの識別面はどのようなものを仮定しますか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
0	0702	0402	SVMの識別面はどのようなものを仮定しますか	事後確率
0	0702	0906	SVMの識別面はどのようなものを仮定しますか	隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので
0	0702	0505	SVMの識別面はどのようなものを仮定しますか	生物の神経細胞の仕組みをモデル化したもの
0	0702	0405	SVMの識別面はどのようなものを仮定しますか	尤度と事前確率の積を最大とするクラス
0	0702	0405	SVMの識別面はどのようなものを仮定しますか	あるクラス$\omega_i$から特徴ベクトル$\bm{x}$が出現する\ruby{尤}{もっと}もらしさ
0	0702	0112	SVMの識別面はどのようなものを仮定しますか	線形回帰，回帰木，モデル木など
1	1408	1408	自己学習にはどんな性質がありますか	低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質
0	1408	0306	自己学習にはどんな性質がありますか	FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法
0	1408	0506	自己学習にはどんな性質がありますか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	1408	0110	自己学習にはどんな性質がありますか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	1408	0610	自己学習にはどんな性質がありますか	学習結果の散らばり具合
0	1408	0505	自己学習にはどんな性質がありますか	$P(\omega_i|\bm{x})$をデータから直接推定するアプローチ
0	1408	0305	自己学習にはどんな性質がありますか	仮説に対して課す制約
0	1408	0305	自己学習にはどんな性質がありますか	仮説に対して課す制約
0	1408	0406	自己学習にはどんな性質がありますか	各クラスから生じる特徴の尤もらしさを表す
0	1408	0902	自己学習にはどんな性質がありますか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	1408	0810	自己学習にはどんな性質がありますか	重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．
0	1408	0514	自己学習にはどんな性質がありますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	1408	0802	自己学習にはどんな性質がありますか	隠れ層
0	1408	0811	自己学習にはどんな性質がありますか	引数が負のときは0，0以上のときはその値を出力
0	1408	0707	自己学習にはどんな性質がありますか	間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ
0	1408	0710	自己学習にはどんな性質がありますか	もとの空間におけるデータ間の距離関係を保存
0	1408	0513	自己学習にはどんな性質がありますか	重みの更新量があらかじめ定めた一定値以下になれば終了
0	1408	1110	自己学習にはどんな性質がありますか	最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	1408	0901	自己学習にはどんな性質がありますか	特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところ
0	1408	0701	自己学習にはどんな性質がありますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	1408	1207	自己学習にはどんな性質がありますか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	1408	0917	自己学習にはどんな性質がありますか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	1408	0701	自己学習にはどんな性質がありますか	学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります
0	1408	0906	自己学習にはどんな性質がありますか	入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない
0	1408	1106	自己学習にはどんな性質がありますか	最も近い事例対の距離を類似度とする
1	0316	0316	特徴が数値であるときの決定木の学習はどのように行いますか	連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなう
0	0316	0402	特徴が数値であるときの決定木の学習はどのように行いますか	事後確率が最大となるクラスを識別結果とする方法
0	0316	0211	特徴が数値であるときの決定木の学習はどのように行いますか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0316	1110	特徴が数値であるときの決定木の学習はどのように行いますか	2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの
0	0316	0402	特徴が数値であるときの決定木の学習はどのように行いますか	統計的識別手法
0	0316	1106	特徴が数値であるときの決定木の学習はどのように行いますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0316	1106	特徴が数値であるときの決定木の学習はどのように行いますか	与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする
0	0316	0701	特徴が数値であるときの決定木の学習はどのように行いますか	識別境界線と最も近いデータとの距離
0	0316	1209	特徴が数値であるときの決定木の学習はどのように行いますか	規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比
0	0316	0717	特徴が数値であるときの決定木の学習はどのように行いますか	SVMでは，たとえばRBFカーネルの範囲を制御する$\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．
0	0316	0708	特徴が数値であるときの決定木の学習はどのように行いますか	制約を満たさない程度を表すので，小さい方が望ましい
0	0316	1503	特徴が数値であるときの決定木の学習はどのように行いますか	スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）
0	0316	0811	特徴が数値であるときの決定木の学習はどのように行いますか	引数が負のときは0，0以上のときはその値を出力
0	0316	0917	特徴が数値であるときの決定木の学習はどのように行いますか	LSTM
0	0316	0701	特徴が数値であるときの決定木の学習はどのように行いますか	線形で識別できないデータに対応するため
0	0316	1103	特徴が数値であるときの決定木の学習はどのように行いますか	全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく
0	0316	0204	特徴が数値であるときの決定木の学習はどのように行いますか	特徴ベクトルの次元数を減らすこと
0	0316	0916	特徴が数値であるときの決定木の学習はどのように行いますか	リカレントニューラルネットワーク
0	0316	0911	特徴が数値であるときの決定木の学習はどのように行いますか	ランダムに一定割合のユニットを消して学習を行う
0	0316	0811	特徴が数値であるときの決定木の学習はどのように行いますか	半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える
0	0316	0802	特徴が数値であるときの決定木の学習はどのように行いますか	識別対象のクラス数
0	0316	0906	特徴が数値であるときの決定木の学習はどのように行いますか	多階層構造でもそのまま適用できます
0	0316	1215	特徴が数値であるときの決定木の学習はどのように行いますか	最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます
0	0316	0514	特徴が数値であるときの決定木の学習はどのように行いますか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0316	0701	特徴が数値であるときの決定木の学習はどのように行いますか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
1	0509	0509	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	確率的最急勾配法
0	0509	0505	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	ニューラルネットワーク
0	0509	0701	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法
0	0509	0612	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0509	0115	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0509	1209	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	規則の条件部が起こったときに結論部が起こる割合
0	0509	0315	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	分割後のデータの分散
0	0509	1116	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	ある時点での分布を使って各データがそのクラスタに属する確率を求め，その確率をデータの重みとみなして分布のパラメータの再計算を行うアルゴリズム
0	0509	1209	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます
0	0509	0514	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります
0	0509	0505	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	生物の神経細胞の仕組みをモデル化したもの
0	0509	1508	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したもの
0	0509	0514	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています
0	0509	0902	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	どのような特徴を抽出するのかもデータから機械学習しようとする
0	0509	1406	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	ナイーブベイズ識別器のような，その識別結果に確信度を伴うもの
0	0509	0402	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	統計的識別手法
0	0509	0805	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法
0	0509	1510	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	確率$1-\epsilon$で最適な行為，確率$\epsilon$でそれ以外の行為を実行する探索手法の総称
0	0509	0810	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	勾配消失問題
0	0509	0610	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	真のモデルとの距離
0	0509	0311	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	集合の乱雑さ
0	0509	0601	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題
0	0509	1412	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0509	1506	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	その政策に従って行動したときの累積報酬の期待値で評価
0	0509	0407	学習データが多いときや特徴の次元が大きいときに使う勾配法はなんですか	確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので
1	0110	0110	識別と回帰の意味の違いは何ですか	正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます
0	0110	0803	識別と回帰の意味の違いは何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0110	1502	識別と回帰の意味の違いは何ですか	報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習
0	0110	0601	識別と回帰の意味の違いは何ですか	過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること
0	0110	1506	識別と回帰の意味の違いは何ですか	政策
0	0110	0204	識別と回帰の意味の違いは何ですか	特徴ベクトルの次元数を減らすこと
0	0110	0114	識別と回帰の意味の違いは何ですか	入力データ集合から適切なまとまりを作ることでクラスを推定する手法
0	0110	0504	識別と回帰の意味の違いは何ですか	（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合
0	0110	0416	識別と回帰の意味の違いは何ですか	効率を求める場合や，一部のノードの値しか観測されなかった場合
0	0110	0409	識別と回帰の意味の違いは何ですか	推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます
0	0110	1207	識別と回帰の意味の違いは何ですか	a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法
0	0110	0612	識別と回帰の意味の違いは何ですか	木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木
0	0110	0305	識別と回帰の意味の違いは何ですか	仮説に対して課す制約
0	0110	0512	識別と回帰の意味の違いは何ですか	最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法
0	0110	0810	識別と回帰の意味の違いは何ですか	2006 年頃に考案された事前学習法
0	0110	0115	識別と回帰の意味の違いは何ですか	データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法
0	0110	0901	識別と回帰の意味の違いは何ですか	深層学習に用いるニューラルネットワーク
0	0110	0906	識別と回帰の意味の違いは何ですか	ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの
0	0110	0506	識別と回帰の意味の違いは何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0110	0311	識別と回帰の意味の違いは何ですか	集合の乱雑さ
0	0110	1412	識別と回帰の意味の違いは何ですか	特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する
0	0110	0109	識別と回帰の意味の違いは何ですか	学習データに正解が付いている場合の学習
0	0110	0105	識別と回帰の意味の違いは何ですか	アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること
0	0110	1001	識別と回帰の意味の違いは何ですか	この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \epsilon, L)$となります
0	0110	0715	識別と回帰の意味の違いは何ですか	カーネルトリック
1	0302	0302	クラスって何ですか	カテゴリ形式の正解情報のこと
0	0302	0803	クラスって何ですか	個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから
0	0302	1015	クラスって何ですか	二乗誤差，誤差の絶対値，フーバー損失（$|x| \leq \epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）など
0	0302	1111	クラスって何ですか	学習データに含まれるデータの中で，ほかと大きく異なるデータ
0	0302	0111	クラスって何ですか	音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など
0	0302	0810	クラスって何ですか	2006 年頃に考案された事前学習法
0	0302	1012	クラスって何ですか	最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします
0	0302	0104	クラスって何ですか	深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところ
0	0302	0506	クラスって何ですか	入力の重み付き和を計算して，その値と閾値を比べて出力を決める
0	0302	1404	クラスって何ですか	教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる
0	0302	1214	クラスって何ですか	高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法
0	0302	0511	クラスって何ですか	$p(\ominus|\bm{x}) = 1 - p(\oplus|\bm{x})$（ただし，$\ominus$は負のクラス）
0	0302	0514	クラスって何ですか	ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法
0	0302	1310	クラスって何ですか	Hidden Marcov Model: 隠れマルコフモデル
0	0302	0908	クラスって何ですか	深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段
0	0302	0901	クラスって何ですか	特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習
0	0302	0805	クラスって何ですか	誤差逆伝播法
0	0302	1106	クラスって何ですか	最も近い事例対の距離を類似度とする
0	0302	0906	クラスって何ですか	十分多くの層
0	0302	0917	クラスって何ですか	内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．
0	0302	1407	クラスって何ですか	繰り返しによって学習データが増加し，より信頼性の高い識別器ができること
0	0302	0103	クラスって何ですか	ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです
0	0302	1303	クラスって何ですか	地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す
0	0302	0211	クラスって何ですか	横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたもの
0	0302	0810	クラスって何ですか	重みの修正量が層を戻るにつれて小さくなってゆく
