{
    "data": [
        {
            "paragraphs": [
                {
                    "context": "まず，最も基本的な識別関数法である誤り訂正学習から説明を始めます．1943年に，McCullochとPittsは神経細胞の数理モデル（図5.5）を組み合わせて，任意の論理関数が計算可能であることを示しました．図5.5に基づいた計算モデルを単層パーセプトロンとよびます．このモデルを単独で考えると，入力の重み付き和を計算して，その値と閾値を比べて出力を決めるということをしています．閾値との比較をしている部分は，$x_0 = 1$という固定した入力を仮定し，この入力に対する重みを$w_0 = - \\theta$とすることで，その他の入力の重み付き和に組み込むことができます．これは$d$次元の特徴空間上で，$g(\\bm{x}) = w_0+w_1 x_1+\\dots+w_d x_d = 0$という識別超平面を設定し，入力がこの識別超平面のどちら側にあるのかを計算していることと等価になります．もし，与えられた学習データが特徴空間上で線形分離可能ならば（超平面で区切ることができるならば），以下に示すパーセプトロンの学習アルゴリズムで，線形分離面を見つけることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 148,
                                    "text": "入力の重み付き和を計算して，その値と閾値を比べて出力を決める"
                                }
                            ],
                            "id": "010ac791-4337-4f18-86a5-b8641d69edb8",
                            "question": "このモデルはどうやって出力を決めるんですか"
                        }
                    ]
                }
            ],
            "title": "0506"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象"
                                }
                            ],
                            "id": "2f3ea4bd-adf9-47aa-bd2c-27012c59f3cc",
                            "question": "過学習とはどのような状態ですか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "また，パラメータ$\\bm{w}$の絶対値を正則化項とするものをLasso回帰とよびます．一般に，Lasso回帰は値を0とするパラメータが多くなるように正則化されます．英単語のlassoは「投げ縄」という意味で，投げ縄回帰と訳されることがあります．多くの特徴がひしめき合っている中に投げ縄を投げて，小数のものを捕まえるというイメージをもってこのように呼ばれているのかもしれませんが，Lassoのオリジナルの論文では，Lasso は least absolute shrinkage and selection operator の意味だと書かれています．Lasso回帰に用いる誤差評価式を，式(6.9)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ値を0とする重みが多くなります．Lasso回帰の解は，原点で微分不可能な絶対値を含むため，最小二乗法のように解析的に求めることはできません．そこで，正則化項の上限を微分可能な二次関数で押さえ，その二次関数のパラメータを誤差が小さくなるように繰り返し更新する方法などが提案されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 93,
                                    "text": "「投げ縄」という意味で"
                                }
                            ],
                            "id": "6dd2e0e9-831a-4ee1-80f0-858a61453ed2",
                            "question": "lassoってどういう意味ですか"
                        }
                    ]
                }
            ],
            "title": "0607"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する"
                                }
                            ],
                            "id": "eee8a9ce-f5b7-4310-a8e4-9b2c7a92e2ff",
                            "question": "オートエンコーダとは何か"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "ac38c35a-3770-4779-a027-8089a0d66ea8",
                            "question": "サポートベクトルマシンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ただし，このままでは，$g(\\bm{x})$は$\\bm{x}$の値次第で，極端に大きな（あるいは小さな）値となる可能性があり，確率と対応づけることが難しくなります．$g(\\bm{x})$の望ましいふるまいは，出力範囲が0以上1以下ので，正例に属する$\\bm{x}$には1に近い値を，負例に属する$\\bm{x}$には0に近い値を出力することです．このようなふるまいは，変換$1/(1+e^{-g(\\bm{x})})$を行い，これを式(5.10)に示すように事後確率$p(\\oplus|\\bm{x})$（ただし，$\\oplus$は正のクラス）と対応付けることで実現できます．この場合，$\\bm{x}$が負のクラスになる確率は$p(\\ominus|\\bm{x}) = 1 - p(\\oplus|\\bm{x})$（ただし，$\\ominus$は負のクラス）で求められます．式(5.10)はシグモイド関数（図5.6）とよばれるもので，$g(\\bm{x}) = w_0+\\bm{w}\\cdot\\bm{x}$がどのような値をとっても，シグモイド関数の値は0から1の間となります．また，$g(\\bm{x})=0$のとき，式(5.10)の値は0.5となり，これは確率を表現するのに適しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 310,
                                    "text": "$p(\\ominus|\\bm{x}) = 1 - p(\\oplus|\\bm{x})$（ただし，$\\ominus$は負のクラス）"
                                }
                            ],
                            "id": "2a43eca9-b028-4e15-a5c2-44cd22ab2375",
                            "question": "入力が負例のときは、確率はどう求められますか"
                        }
                    ]
                }
            ],
            "title": "0511"
        },
        {
            "paragraphs": [
                {
                    "context": "協調フィルタリングの前提は，どの個人がどの商品を購入したかが記録されているデータがあることです．そして，新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦するというのが，基本的な考え方です．しかし，この個人別の購入データは，前節までのトランザクションデータとどうように，まばらに値が入っているデータです．購入パターンが似ているユーザを探す際に，データをベクトルとみなして，コサイン類似度による計算をおこなっても，ほとんど一致する項目数を数えているに過ぎないような状況になってしまいます．そこで，購入データをもっと低次元の行列に分解し，ユーザ・商品の特徴を低次元のベクトルで抽出する方法が考えられました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "どの個人がどの商品を購入したかが記録されているデータ"
                                }
                            ],
                            "id": "6faf3468-578a-4298-8e27-8e160714a512",
                            "question": "協調フィルタリングではどのようなデータが必要ですか"
                        }
                    ]
                }
            ],
            "title": "1219"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 426,
                                    "text": "入力を観測した後で計算される確率"
                                }
                            ],
                            "id": "dd2c4546-a957-4c4b-8a59-367b86a125b7",
                            "question": "事後確率とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 80,
                                    "text": "低次元の特徴ベクトルを高次元に写像"
                                }
                            ],
                            "id": "501537bf-69e7-4529-8889-798b5f74eb6f",
                            "question": "線形分離の可能性を高める方法はなにか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "与えられたデータをまとまりに分ける操作"
                                }
                            ],
                            "id": "a6399099-aa3e-43e1-bef4-99d13bc218b1",
                            "question": "クラスタリングってなんですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 428,
                                    "text": "学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります"
                                }
                            ],
                            "id": "e73bfcbf-b969-47aa-abc3-cb087e3510a1",
                            "question": "なぜドロップアウトによって過学習が回避できるのですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "最初の問題設定として，ラベル特徴の系列を入力として，それと同じ長さのラベル系列を出力する識別問題を扱います．では，このような系列ラベリング問題を，機械学習によって解決する識別器の構成を考えてゆきましょう．単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点があります．学習データは，入力系列と出力ラベルのペアとして与えられますが，形態素解析や固有表現抽出の例でみたように，出力系列には並びによる依存関係があるので，個々の識別問題として扱うのは不適当だということです（問題点1）．それでは出力もまとめてしまって，出力系列を一つのクラスとするということも考えられますが，通常，そのクラス数は膨大な数になってしまいます．たとえば，品詞が10種類で，20単語からなる文にラベル付けする問題では$10^{20}$種類の出力が可能になり，これらを個別のクラスとして扱うのはほとんど不可能です（問題点2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "単純に一つの入力に対して一つのラベルを出力する識別器を順次適用するという方法では，系列としての性質を捨ててしまっているという問題点"
                                }
                            ],
                            "id": "414f2b87-8a5f-46a9-98bc-d43c5cff715c",
                            "question": "系列ラベリング問題とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1302"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく"
                                }
                            ],
                            "id": "f06452f3-7164-4b28-b0a9-bac2d7438f1d",
                            "question": "勾配消失問題とはどのような問題ですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 44,
                                    "text": "畳み込みニューラルネットワーク"
                                }
                            ],
                            "id": "621ab403-58ab-468e-bc37-2c465b3cfb9b",
                            "question": "画像認識でよく用いられるタスクに特化したディープニューラルネットワークは何ですか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 84,
                                    "text": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫"
                                }
                            ],
                            "id": "375d6112-57ae-496a-bd18-329093daaea0",
                            "question": "リカレントニューラルネットワークはどのようにして勾配消失問題を解決しているか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 426,
                                    "text": "これまでに学んだ識別器を入力に対して逐次適用してゆくというもの"
                                }
                            ],
                            "id": "14d13761-a505-4d83-aa66-9420989282b9",
                            "question": "系列データの入力の系列長と出力の系列長が等しい問題の解決法はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 453,
                                    "text": "この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります"
                                }
                            ],
                            "id": "1cbbbf9e-2e0d-424b-8ade-c6620ae2ecb7",
                            "question": "なぜ仮定として，識別器の誤り率はすべて等しく，その誤りは独立であるとするのですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化する"
                                }
                            ],
                            "id": "558f4e52-9aa3-4c93-9e35-a3fcdf96b6f5",
                            "question": "線形分離不可能なデータにはどのように対処しますか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる"
                                }
                            ],
                            "id": "0c86b1b5-a7c4-4733-909d-481eef8a16c6",
                            "question": "ゼロ頻度問題はどのような問題ですか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 98,
                                    "text": "サポートベクトルマシン"
                                }
                            ],
                            "id": "8377138a-fc58-43ce-8b28-8b3a158604b9",
                            "question": "SVMとは何の略称ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムの中で，詳しい説明のない「特徴集合A中で最も分類能力の高い特徴」を決定する方法について説明します．分類能力が高いとは，「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということです．いいかえると，乱雑さが少なくなるように分類を行うということですね．乱雑さの尺度として，エントロピーを用います．学習データ集合$D$の乱雑さを計算するために，まず正例の割合: $P_+$,  負例の割合: $P_-$を計算し，それを元に式(3.1)によって，その集合の乱雑さ（エントロピー）$E(D)$を求めます．エントロピーの値は$P_+=1$または$P_-=1$のとき最小値0となり，$P_+=P_-=0.5$のとき，最大値1となるので，エントロピーの値が小さいほど，集合が乱雑でない，すなわち整っている（同じクラスのものが大半を占めている）ということになります．このエントロピーの減り具合を計算したいのですが，単純に引き算はできません．エントロピーは集合に対して定義できるものです．分類前は1つの集合で，分類後はその特徴値の種類数だけ集合ができます．そこで，分類後の集合の要素数の割合で重みを付けて計算します．この値を情報獲得量と定義します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 246,
                                    "text": "集合の乱雑さ"
                                }
                            ],
                            "id": "d9a81117-23fe-4160-a689-a4a729866675",
                            "question": "エントロピーとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0311"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 129,
                                    "text": "このモデルは生物の神経細胞のモデルであると考えられています"
                                }
                            ],
                            "id": "713fd3fe-c505-4652-9af5-e3c2ed066f54",
                            "question": "ニューラルとはどういう意味か"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく，というのが半教師あり学習の基本的な進め方です（この章の最後に紹介するYATSIアルゴリズムは例外です）．識別器を作成するアルゴリズムはこれまで紹介してきたものを問題に応じて用いればよいのですが，信用できる出力をする教師なしデータを次回の識別器作成に取り込むためには，ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切です．一方，繰り返しアルゴリズムに関して，単純に終了のための閾値チェックをするだけなのか，識別器のパラメータを繰り返しの度に変化させるか，識別器で使う特徴に制限をかけるか，など様々な設定が可能です．以下では，繰り返しアルゴリズムの違いによって生じる，様々な半教師あり学習手法について説明してゆきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 209,
                                    "text": "ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切"
                                }
                            ],
                            "id": "1a0901b5-e992-47d2-9dea-b773fd884653",
                            "question": "半教師あり学習の識別器には何が適切ですか"
                        }
                    ]
                }
            ],
            "title": "1406"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 465,
                                    "text": "二乗誤差を最小にするように識別関数を調整する方法"
                                }
                            ],
                            "id": "67d83c1a-2937-47f6-982f-39f0c4d3ab70",
                            "question": "最小二乗法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでもう一度，表12.1の小規模データに戻って，項目集合を絞り込む方法を考えましょう．今度は図示しやすいように，項目に通し番号を付けて，$\\{0,1,2,3\\}$と表します．すべての可能な項目の組合せは$2^4-1=15$で，図12.2に丸で示すもの（ただし，最上段の空集合$\\emptyset$は除く）になります．ここで，a prioriな原理として，ある項目集合が頻出ならば，その部分集合も頻出であるを考えます．そうすると，上で述べたa prioriな原理から，ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でないが成り立ちます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 234,
                                    "text": "ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない"
                                }
                            ],
                            "id": "6924c945-f11d-4ebd-bf90-28506cf320d5",
                            "question": "a priori な原理の対偶とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1205"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\bm{w}$は$d+1$次元の重みベクトルとします．また，$\\eta$は学習係数で，適当な小さい値を設定します．このアルゴリズムは，学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します．これをパーセプトロンの収束定理とよびます．一方，学習データが線形分離不可能な場合にはこのアルゴリズムを適用することができません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合はこのアルゴリズムは停止しません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します"
                                }
                            ],
                            "id": "a1c670f2-28e0-4070-b445-0c8ee1bff2f9",
                            "question": "パーセプトロンの収束定理ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0507"
        },
        {
            "paragraphs": [
                {
                    "context": "もうひとつのタスクに特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつリカレントニューラルネットワーク（図9.10(a)）があります．リカレントニューラルネットワーは時系列信号や自然言語などの系列パターンを扱うことができます．このリカレントニューラルネットワークへの入力は，特徴ベクトルの系列$\\bm{x}_1,\\bm{x}_2,\\dots, \\bm{x}_T $という形式になります．たとえば，動画像を入力して異常検知を行ったり，ベクトル化された単語系列を入力して品詞列を出力するようなタスクが具体的に考えられます．これらに共通していることは，単純に各時点の入力からだけでは出力を決めることが難しく，それまでの入力系列の情報が何らかの役に立つという点です．リカレントニューラルネットワークの中間層は，入力層からの情報に加えて，一つ前の中間層の活性化状態を入力とします．この振舞いを時間方向に展開したものが，図9.10(b)です．時刻$t$における出力は，時刻$t-1$以前のすべての入力を元に計算されるので，これが深い構造をもっていることがわかります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "リカレントニューラルネットワーク"
                                }
                            ],
                            "id": "b9671333-5b9d-4a7a-9013-70273ed950f3",
                            "question": "時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか"
                        }
                    ]
                }
            ],
            "title": "0916"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています"
                                }
                            ],
                            "id": "e9bd8ae1-9e76-4a6b-af63-8397ef147274",
                            "question": "なぜ次元を削減するんですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象"
                                }
                            ],
                            "id": "c9a42f2a-12b0-45d2-8fd0-5e8d0b2fd7fc",
                            "question": "過学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1276,
                                    "text": "決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります"
                                }
                            ],
                            "id": "071f98ed-e4a6-47b3-84b2-2c447b5a90af",
                            "question": "識別の代表的な手法には何がありますか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "条件付き確率場（Conditional Random Field: CRF）"
                                }
                            ],
                            "id": "233cea1e-05b3-4178-8850-81642fd54b3b",
                            "question": "CRFとは何の略称ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングは，データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法です．スーパーマーケットなどで同時に購入される商品の組み合わせを発見するバスケット分析が代表的な応用例です．図1.10にパターンマイニングの考え方を示します．パターンマイニングの敵は膨大な計算量です．まさに，大量のデータの中から，貴重な知見をマイニング（＝発掘）する作業です．図1.10に示した例では，発見された規則の条件部も結論部も要素数が一つなので，すべての商品の組み合わせに対してその出現頻度を計算することは，それほど膨大な計算量にはみえません．しかし，一般的なパターンマイニングでは，条件部・結論部のいずれも要素の集合となります．それらのあらゆる組み合わせに対して，マイニングの対象となる大きなデータ集合から出現数を数えあげなければならないので，単純な方法では気の遠くなるような計算量になってしまいます．そこで効率よく頻出パターンを見つけ出す手法が必要になります．パターンマイニングの代表的な手法としては Apriori アルゴリズムやその高速化版である FP-Growth があります．これらを第12章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "データ中に何度も出現するパターンや，そのパターンに基づいた規則を発見する手法"
                                }
                            ],
                            "id": "fbee9f85-f97a-4cf9-99ea-02f7643ca977",
                            "question": "パターンマイニングって何ですか"
                        }
                    ]
                }
            ],
            "title": "0115"
        },
        {
            "paragraphs": [
                {
                    "context": "そこで，利用できる素性を図13.3に示す組合せに限定します．つまり，出力系列で参照できる情報は一つ前のみ，入力系列は自由な範囲で参照できるとします．出力系列を参照する素性を遷移素性，入力と対応させる素性を観測素性と呼びます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 74,
                                    "text": "出力系列を参照する素性を遷移素性"
                                }
                            ],
                            "id": "180e98e2-e229-4d54-8cd3-81663601b8d4",
                            "question": "遷移素性とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1304"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，簡単なカーネルについてその非線形変換$\\phi$を求めてみましょう．特徴ベクトルを2次元として多項式カーネル(p=2)を展開します．したがって，$\\bm{x}=(x_1, x_2)$のとき，$\\phi(\\bm{x})=(x_1^2, x_2^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1) )$となります．この変換の第3項に注目してください．特徴の積の項が加わっています．積をとるということは，2つの特徴が同時に現れるときに大きな値になります．すなわち，共起の情報が加わったことになります．このような，非線形変換で線形分離可能な高次元にデータを飛ばしてしまい，マージン最大化基準で信頼できる識別面を求めるというSVMの方法は非常に強力で，文書分類やバイオインフォマティックスなど様々な分野で利用されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "文書分類やバイオインフォマティックスなど様々な分野で利用されています"
                                }
                            ],
                            "id": "0eb0cc1b-554f-44d1-a11c-74a5fbb03075",
                            "question": "SVMの方法は例えば何に使われていますか"
                        }
                    ]
                }
            ],
            "title": "0713"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 149,
                                    "text": "誤差が小さくなって消失してしまう"
                                }
                            ],
                            "id": "1675e96d-a132-4266-b490-b6399aecb324",
                            "question": "多段階に誤差逆伝播法を適用すると問題はありますか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "ab728a3e-26e4-4e29-b424-1d034fad7e8d",
                            "question": "ニューラルネットワークってどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 34,
                                    "text": "このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる"
                                }
                            ],
                            "id": "33355507-e1ba-467b-a808-b7e5048b2764",
                            "question": "回帰木の特徴はなんですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで$C$は制約を満たさないデータをどの程度の重みで組み込むかを決める定数で，$C$が大きければ影響が大きく，$C$が小さければほとんど無視するような振る舞いになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 7,
                                    "text": "制約を満たさないデータをどの程度の重みで組み込むかを決める定数"
                                }
                            ],
                            "id": "011b015d-e495-447f-9884-3d8ab7fba304",
                            "question": "$C$はなんですか"
                        }
                    ]
                }
            ],
            "title": "0709"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは回帰モデルの評価について考えます．教師付き学習においては，未知データに対する誤差が問題となります．この回帰式は未知データに対してもうまく値を予測してくれるのでしょうか．回帰問題の評価は，交差確認法との相性はあまりよくありません．識別問題では，交差確認に用いるデータの部分集合は，そこに含まれるクラスの割合が，全体の割合と整合するように分割すれば，一回ごとの評価値がそれほど極端にはぶれず，ある程度適切な評価が行えます．しかし，回帰では何をもって部分集合の構成が近いかを定義することが難しくなります．したがって，計算能力に余裕があれば，一つ抜き法で評価することをお勧めします．そこでの評価指標は，学習の基準に合わせると平均二乗誤差ということになります．しかし，この値はデータが異なれば，スケールがまったく異なるので，結果がよいものかどうか直観的にはわかりにくいものです．そこで，回帰の場合は，正解と予測とがどの程度似ているかを表す相関係数や，式(6.6)で計算できる決定係数で評価します．決定係数は，「正解との離れ具合」と「平均との離れ具合」の比を1から引いたものですが，式変形により相関係数の二乗と一致するので，$R^2$とも表記されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 453,
                                    "text": "「正解との離れ具合」と「平均との離れ具合」の比を1から引いたものです"
                                }
                            ],
                            "id": "e26a4232-f0e9-4575-9672-2954d90e2541",
                            "question": "決定係数ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0605"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 962,
                                    "text": "識別器が正例と判断したときに，それがどれだけ信頼できるかという指標"
                                }
                            ],
                            "id": "281767f1-b2b7-428b-90ef-909453169077",
                            "question": "精度って何ですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "引数が負のときは0，0以上のときはその値を出力"
                                }
                            ],
                            "id": "851a15df-e47b-4fb8-aeb9-64ea7c4c4626",
                            "question": "ReLuってなんですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 324,
                                    "text": "各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの"
                                }
                            ],
                            "id": "a2ada396-f0fe-4fd8-a306-e13962a929b2",
                            "question": "モデルの対数尤度とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，これまでに説明した教師あり学習／教師なし学習に当てはまらない問題について説明します．学習データが教師あり／教師なしの混在となっているものが半教師あり学習です．また，与えられる正解が間接的で，教師あり／教師なしの中間的な状況となっているものが強化学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 47,
                                    "text": "学習データが教師あり／教師なしの混在となっているものが半教師あり学習です"
                                }
                            ],
                            "id": "82381efe-6606-4a1a-b488-0688fe19a213",
                            "question": "半教師あり学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0116"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 154,
                                    "text": "連続値"
                                }
                            ],
                            "id": "07eb09b8-d776-481d-81bd-24e8d9cd4aa6",
                            "question": "スラック変数の重みは離散値ですか、連続値ですか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 173,
                                    "text": "地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す"
                                }
                            ],
                            "id": "b105cd50-e01c-4d25-90f6-d4ab1e58905e",
                            "question": "固定表現抽出とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 4,
                                    "text": "もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$"
                                }
                            ],
                            "id": "dd6f8cc7-b663-4e3a-9865-8f843659a47c",
                            "question": "カーネル関数とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 899,
                                    "text": "連続音声認識"
                                }
                            ],
                            "id": "ac662bc7-7ffa-4453-99c6-88eaee942fbc",
                            "question": "系列データの入力の系列長と出力の系列長の間に，明確な対応関係がない問題の例は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題です過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習することが目標です．回帰問題は，正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題と定義できます(図6.1)．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 133,
                                    "text": "正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題"
                                }
                            ],
                            "id": "8d3be88e-7723-4313-90cd-9d9b0fecd615",
                            "question": "回帰問題とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0601"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 49,
                                    "text": "判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法"
                                }
                            ],
                            "id": "5e678994-4095-4f84-82b3-47f712495136",
                            "question": "共訓練とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，特徴数削減の手法として，主成分分析 (principal component analysis: PCA) を紹介します．図2.5に，2次元から1次元への削減を例として，主成分分析の考え方を示します．主成分分析とは，相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作です．次元削減の対象である高次元特徴空間上にデータがどのように散らばっているかという情報は，もとのデータの統計的性質をあらわす共分散行列によって表現することができるので，この共分散行列の情報を基にして，低次元空間への写像をおこなう行列を作ってゆきます．学習データ$\\{\\bm{x} | \\bm{x} \\in D\\}$の共分散行列$\\Sigma$は式(2.1)を用いて計算されます．ここで，$\\bm{\\mu}$は$D$の平均ベクトル，$N$は$D$の要素数です．平均ベクトル$\\bm{\\mu}$は式(2.2)を用いて計算されます．図2.5左上に示すような2次元データの場合，平均ベクトルを$\\bm{\\mu}=(\\bar{x_1}, \\bar{x_2})^T$とすると，共分散行列$\\Sigma$は式(2.3)のようになります．対角成分は，次元ごとの散らばり具合を表す分散に対応し，非対角成分は次元間の相関を表します．次に，この共分散行列の固有値と固有ベクトルを求めると，固有値の大きい順にその対応する固有ベクトルの方向が，データの散らばりが大きい（すなわち，識別するにあたって情報が多い）方向となります．固有ベクトルどうしは直交するので，固有値の大きい順に軸として採用し，特徴空間を構成すると，たとえば上位$n$位までなら$n$次元空間が構成でき，これらはもとの多次元特徴空間のデータの散らばりを最もよく保存した$n$次元空間ということになります．特徴空間の次元数が下がれば下がるほど，学習において推定するべきパラメータ数が少なくなるので，学習結果の信頼性が高まります．もっとも，もとのデータの情報が大きく損なわれるほどに次元を削減してしまっては意味がないので，そのあたりの調整は難しいところです．主成分分析によって構成した軸では，対応する固有値が分散になるので，「すべての軸の固有値の和」に対する「採用した軸の固有値の和」の比（累積寄与率）を計算することで，次元削減後の空間が，もとのデータの情報をどの程度保存しているのか，見当をつけることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 113,
                                    "text": "相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作"
                                }
                            ],
                            "id": "eee62711-9969-48c7-9c3a-df933165cfe1",
                            "question": "主成分分析とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0205"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "特徴ベクトルの次元数を減らすことです"
                                }
                            ],
                            "id": "3d05ea91-bab8-4bc7-a739-439378bcb190",
                            "question": "次元削減とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 121,
                                    "text": "将棋や囲碁などを行うプログラム"
                                }
                            ],
                            "id": "bf95c351-dd8d-44c0-9ea6-45c7a4b44a09",
                            "question": "強化学習は例えば何に使われていますか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "9d0de104-a2b7-4146-b7c8-7a1b6f677b94",
                            "question": "ニューラルネットワークとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 21,
                                    "text": "単語の系列を入力として，それぞれの単語に品詞を付けるという問題"
                                }
                            ],
                            "id": "bede334c-d008-4a86-9348-1032148acdad",
                            "question": "形態素解析とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 84,
                                    "text": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします"
                                }
                            ],
                            "id": "432f8a7d-6924-49cb-bf3a-645817e49ec3",
                            "question": "リカレントニューラルネットワークにおいて勾配消失問題にどのように対処するか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．図11.4のように，データまたはクラスタを融合する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作でクラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 139,
                                    "text": "一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了"
                                }
                            ],
                            "id": "7ce766b9-3ab6-455e-94bf-1c952886fa88",
                            "question": "階層的クラスタリングは、どのようなアルゴリズムですか"
                        }
                    ]
                }
            ],
            "title": "1104"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 403,
                                    "text": "境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は"
                                }
                            ],
                            "id": "1d83a83f-7171-4eb2-9a98-389b232166e8",
                            "question": "統計モデルによるアプローチはどのようなときに有効ですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "この与えられた系列を$\\bm{x}$として，クラス$y$（ただし，$y= B (初心者) or S (熟練者)$）の事後確率$P(y|\\bm{x})$を何らかのモデルを使って計算することを考えます．ここで式(13.4)のような$\\bm{x}, y$の同時確率を考える生成モデルアプローチをとるのが\\textbf{HMM}(Hidden Marcov Model: 隠れマルコフモデル)の考え方です．HMMは，式(13.4)の$P(\\bm{x}|y)$の値を与える確率的非決定性オートマトンの一種です．各状態であるシンボルをある確率で出力し，ある確率で他の状態(あるいは自分自身)に遷移します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 161,
                                    "text": "Hidden Marcov Model: 隠れマルコフモデル"
                                }
                            ],
                            "id": "4588564d-aace-4706-a5ea-b6828872ad18",
                            "question": "HMMは何の略ですか"
                        }
                    ]
                }
            ],
            "title": "1310"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "与えられたデータをまとまりに分ける操作"
                                }
                            ],
                            "id": "a79e8219-a883-416b-ab16-6e1cf4f75e95",
                            "question": "クラスタリングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "f945f1dd-1c10-4063-af2e-4285f42fbef6",
                            "question": "Deep Neural Network とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが移動するという状況です．ゴールGに着けば，報酬が得られます．単純なケースでは報酬は決定的（ゴールに着けば必ずもらえる）で，部屋の移動にあたる状態遷移も決定的（必ず意図した部屋に移動できる）です．問題を一般化して，報酬や遷移が確率的である場合も想定できます．これらが確率的になる原因として，例えばロボットのゴールを探知するセンサーがノイズで誤動作をしたり，路面状況でスリップが生じるなどの不確定な要因で行為が成功しない状況が考えられます．これらは，非決定的であるとはいえ，学習中に状況が変化してしまうとどうしようもないので，この非決定性が確率的であるとし，確率分布は学習期間中を通じて一定であるとします．このような問題は，以下のようなマルコフ決定過程として定式化することができます．\\begin{itemize}\\item 時刻$t$における状態$s_t \\in S$\\item 時刻$t$における行為$a_t \\in A(s_t)$\\item 報酬 $r_{t+1} \\in R$，確率分布$p(r_{t+1}|s_t, a_t)$\\item 次状態$s_{t+1} \\in S$，確率分布$P(s_{t+1}|s_t, a_t)$\\end{itemize}マルコフ決定過程は，「マルコフ性」を持つ確率過程における意思決定問題で，「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質です．ここでは，報酬と次状態への遷移の確率が現在の状態と行為のみに依存しているという定式化になっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 579,
                                    "text": "「マルコフ性」を持つ確率過程における意思決定問題"
                                }
                            ],
                            "id": "68ba38c3-f9e5-4a11-87d1-2ea71c90fa96",
                            "question": "マルコフ決定過程とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1505"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 107,
                                    "text": "最も近い事例対の距離を類似度とする"
                                }
                            ],
                            "id": "cd067a87-627d-4d22-93bf-1d3d98439d43",
                            "question": "単連結法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1037,
                                    "text": "正例がどれだけ正しく判定されているかという指標"
                                }
                            ],
                            "id": "10f04ee5-791b-479a-8640-1e912cf7b5b7",
                            "question": "再現率って何ですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 24,
                                    "text": "ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの"
                                }
                            ],
                            "id": "2da7236a-c29e-452a-9b72-f956e826a57b",
                            "question": "多階層ニューラルネットワークとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 63,
                                    "text": "先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返す"
                                }
                            ],
                            "id": "e814e10b-dcfe-46a8-9d82-5419faa20719",
                            "question": "ビタビアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 267,
                                    "text": "素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質"
                                }
                            ],
                            "id": "6716d3eb-5093-4fd4-804f-acb8953c51ec",
                            "question": "CRFの性質は何ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 232,
                                    "text": "クラスタの重心間の距離を類似度とする"
                                }
                            ],
                            "id": "4cabb113-f9ce-4443-9f75-c6e37d1ee89f",
                            "question": "重心法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 34,
                                    "text": "このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなる"
                                }
                            ],
                            "id": "32d51fe1-07d7-4e8b-8e69-73120856cd78",
                            "question": "回帰木の特徴はどんものですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 398,
                                    "text": "識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを"
                                }
                            ],
                            "id": "8fa514c7-1523-42f2-9fa2-84c9c8c6a331",
                            "question": "二乗誤差とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "重みパラメータに対しては線形で，入力を非線形変換することによって実現"
                                }
                            ],
                            "id": "f5eece8a-ff16-477c-9239-15d4d1c81e8c",
                            "question": "基底関数ベクトルによる非線形識別面はどういう風に実現されているのか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習では，学習に用いられるデータに正解情報が付いていません．入力ベクトル$\\bm{x}_i$の次元数に関しては，教師あり学習の場合と同様に，$d$次元の固定長ベクトルで，各要素は数値あるいはカテゴリのいずれかの値をとると考えておきます．教師なし学習は，入力データに潜む規則性を学習することを目的とします．ここで着目すべき規則性としては，2通り考えられます．一つめは，入力データ全体を支配する規則性で，これを学習によって推定するの問題がモデル推定 (model estimation)です．もう一つは，入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです．これを発見する問題がパターンマイニング (pattern mining) です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 254,
                                    "text": "入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです"
                                }
                            ],
                            "id": "9ffa2d85-c5e6-456f-b87b-44dd5060cf01",
                            "question": "パターンマイニングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0113"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは学習です，とゆきたいところですが，その前に学習結果の評価基準を設定します．ここで扱っているデータはirisデータなので，教師あり・識別の場合の評価基準を考えます．この場合，学習データに対して正解率100\\%でも意味がありません．未知データに対してどれだけの正解率が期待できるかが評価のポイントですが，どうやって未知データで評価すればよいのでしょうか．学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法が考えられます．この方法を分割学習法とよびます．評価用に半分というのは，多すぎるように見えるかもしれませんが，評価用データがあまりに少ないと，未知データの分布と全く異なる可能性が高くなり，評価そのものが信頼できなくなります．また，学習パラメータの調整をおこなうような場合では，データを学習用・調整用・評価用と分けるケースもあります．しかし，irisデータは150事例しかないので，分割学習法で評価するのは難しそうです．このような場合，一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します(図2.7)．この方法では学習データを$m$個の集合に分割し，そのうちの$m-1$個で学習を行い，除外した残りの一つで評価を行います．そして，その除外するデータを順に交換することで，合計$m$回の学習と評価を行います．これで，全データがひととおり評価に使われ，かつその評価時に用いられる識別器は評価用データを除いて構築されたものとなっています．$m$を交差数とよび，技術論文では交差数$m$を10とするケース (10-fold CV) や，データの個数とするケースがよく見られます．$m$がデータの個数の場合を一つ抜き法(leave-one-out method)とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 434,
                                    "text": "一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します"
                                }
                            ],
                            "id": "96ec9b25-f26c-4552-9d02-7913f0b5eba4",
                            "question": "分割学習法の際学習データが足りない場合どうしたらいいですか"
                        }
                    ]
                }
            ],
            "title": "0206"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます"
                                }
                            ],
                            "id": "c0929bba-48ab-4a50-bf85-a67e26a27089",
                            "question": "グリッドサーチとはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 300,
                                    "text": "与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする"
                                }
                            ],
                            "id": "fee5263a-ddcb-414b-b0ba-bdfde6c326f2",
                            "question": "Ward法はどういうものですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 784,
                                    "text": "SVM"
                                }
                            ],
                            "id": "700aa717-4904-4a35-8358-b4a4603234ab",
                            "question": "線形のモデルで学習データに特化しすぎないような識別面を求めるという方法はなに"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 507,
                                    "text": "適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法"
                                }
                            ],
                            "id": "3a17d215-0797-42ff-a87b-13035583fc60",
                            "question": "過学習を避けるためにはどのような方法がありますか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木"
                                }
                            ],
                            "id": "22825978-e693-4e42-b9ff-83d3f70840e6",
                            "question": "CARTって何ですか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1096,
                                    "text": "精度と再現率を総合的に判断するために，その調和平均をとったものをF値"
                                }
                            ],
                            "id": "9e94fb6e-af9e-4389-aa00-c2dca71b1559",
                            "question": "F値って何ですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "8e762dda-19ee-4736-b8ad-697c6bce35d3",
                            "question": "k-平均法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 390,
                                    "text": "これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます"
                                }
                            ],
                            "id": "acd05c4b-631f-4436-8632-642be283160d",
                            "question": "確率のm推定ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 354,
                                    "text": "入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します"
                                }
                            ],
                            "id": "efba2c16-5299-47a0-ba00-883c166561d8",
                            "question": "フィードフォワード型ニューラルネットワークを構成するには何が必要ですか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 300,
                                    "text": "与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする"
                                }
                            ],
                            "id": "6b34c5ab-7778-4dfe-8b0b-ebea9e673113",
                            "question": "Ward法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 368,
                                    "text": "規則の条件部が起こったときに結論部が起こる割合"
                                }
                            ],
                            "id": "4de57246-9338-471e-8a65-d556c15014b6",
                            "question": "確信度とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意する，ということです．ここまでの教師あり学習の説明からもわかるように，学習データが異なれば，たいていその学習結果も異なります．バギング(Bagging)はこのアイディアに基づいて，学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです（図10.3}）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 136,
                                    "text": "学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです"
                                }
                            ],
                            "id": "4da07aa9-09cb-4cca-9fd2-50f0bd5c8609",
                            "question": "バギングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1004"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 388,
                                    "text": "正解が付いていない場合の学習"
                                }
                            ],
                            "id": "df5ee794-4dee-4964-8c25-e58208800bcb",
                            "question": "教師なし学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 54,
                                    "text": "個々の要素の間に i.i.d. の関係が成立しないもの"
                                }
                            ],
                            "id": "bf5fafd6-9ae0-4cf0-a671-cb62c9055f61",
                            "question": "系列データとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法"
                                }
                            ],
                            "id": "4ade5341-1003-4334-b09e-977488147061",
                            "question": "統計的識別とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 255,
                                    "text": "パーセプトロン"
                                }
                            ],
                            "id": "f1c59072-1676-4d94-8089-31577843c71b",
                            "question": "最も古典的な識別関数法の手法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので"
                                }
                            ],
                            "id": "4e52ecf4-8273-4071-aa39-3771178f1549",
                            "question": "ReLUだと勾配消失が起こらないのはなぜですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシグモイド関数の微分です．第2項は入力の重み付き和の微分です．従って，出力層の重みの更新式は以下のようになります．通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．また，中間層は，この修正量を重み付きで足し合わせます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "通常，ニューラルネットワークの学習は確率的最急勾配法を用いる"
                                }
                            ],
                            "id": "c44aa353-8dde-48ef-acbc-2064a716347e",
                            "question": "ニューラルネットワークの最適化手法には何が一番多く用いられますか"
                        }
                    ]
                }
            ],
            "title": "0808"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1096,
                                    "text": "精度と再現率を総合的に判断するために，その調和平均をとったものをF値"
                                }
                            ],
                            "id": "73552290-1ad5-4822-8983-0f028b77e230",
                            "question": "F値とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "Q値を推定する方法は，モデルの関する知識の前提によって大きく2つに分類されます．環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合，Q値は，動的計画法の考え方を用いて求めることができます．この方法をモデルベースの手法 と呼びます．一方，環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合，試行錯誤を通じて環境と相互作用をした結果を使って学習を行います．この方法をモデルフリーの手法 と呼びます．本節ではモデルベースの手法を，次節ではモデルフリーの手法を説明します．モデルベースの手法では，状態遷移確率$P(s_{t+1}|s_t, a_t)$と，報酬の確率分布$p(r_{t+1}|s_t, a_t)$が与えられているものとします．その前提で，アルゴリズム15.1に示すValue iterationアルゴリズムを実行すると，状態価値関数$V(s)$の最適値を求めることができ，それぞれの状態でQ値を最大とする行為が求まりますので，これが最適政策ということになります．アルゴリズム15.1 中の報酬の期待値$E(r|s,a)$は報酬の確率分布$p(r_{t+1}|s_t, a_t)$から求めます．このアルゴリズムは，迷路中で報酬がもらえる状態（ゴール）が1つだけある場合，まずそのゴール状態の1つ手前での最適行為が得られ，次にその1つ手前，さらにその1つ手前と，繰り返しを回る毎に正しい最適値が得られている状態がゴールを中心に広がってゆくイメージをしていただけると，わかりやすいと思います．また，モデルベースの手法には，Value iterationアルゴリズムの他にも，適当な政策を初期値として，そのもとでの状態価値関数$V(s)$を計算し，各状態で現在の知識から得られる最適行為を選び直すことを繰り返すPolicy iterationアルゴリズムもあります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 134,
                                    "text": "環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合"
                                }
                            ],
                            "id": "9e1415fa-699a-4834-804a-98cb30797405",
                            "question": "モデルフリーの手法とはどのような場合ですか"
                        }
                    ]
                }
            ],
            "title": "1509"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 19,
                                    "text": "ユニットの活性化関数を工夫する方法があります"
                                }
                            ],
                            "id": "3d4c96db-cc14-4c03-bf15-f18c497ea0cc",
                            "question": "どのようにして勾配消失問題を解決しますか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．ラグランジュの未定乗数法を用いると，$g(\\bm{x})=0$という条件の下で$f(\\bm{x})$の最小値（あるいは最大値）を求める問題は，$L(\\bm{x}, \\lambda)=f(\\bm{x})-\\lambda g(\\bm{x})$ （ただし$\\lambda$はラグランジュ乗数）という新しいラグランジュ関数を導入し，この関数の極値を求めるという問題に置き換えることができます．ラグランジュ関数の$\\bm{x}$に関する偏微分を0とすると，式(7.8)のようになります．ここでは，ラグランジュの未定乗数法を不等式制約条件で用います．そうすると，式(7.6)，式(7.7)の制約付きの最小化問題は，ラグランジュ乗数$\\alpha_i$を導入して，以下の関数$L$の最小値を求めるという問題に置き換えることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 370,
                                    "text": "以下の関数$L$の最小値を求めるという問題"
                                }
                            ],
                            "id": "d482edf9-2d27-49c1-ab53-c709d045a3fd",
                            "question": "αについての2次計画問題とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0704"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 109,
                                    "text": "音声認識・画像認識・自然言語処理など"
                                }
                            ],
                            "id": "6c5d7ca5-3a9a-4448-b938-1524929080d4",
                            "question": "深層学習はどのような場面で応用されますか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 109,
                                    "text": "一つのまとまりと認められるデータは相互の距離がなるべく近くなるように"
                                }
                            ],
                            "id": "4b34ba0e-afa6-43a4-91a8-762d9e080aaf",
                            "question": "内的結合とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 340,
                                    "text": "特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する"
                                }
                            ],
                            "id": "153ea770-687b-4d4e-aea4-839f26ac44f6",
                            "question": "表現学習ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 307,
                                    "text": "ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法"
                                }
                            ],
                            "id": "1afe6bc8-8be1-4681-8b53-7923aa5ec9ed",
                            "question": "確率的最急勾配法とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 58,
                                    "text": "ランダムに一定割合のユニットを消して学習を行う"
                                }
                            ],
                            "id": "795d58b9-5d79-46ce-b5c0-9f79ae3f64af",
                            "question": "ドロップアウトとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 444,
                                    "text": "隠れ層"
                                }
                            ],
                            "id": "0c55720c-cedb-4a7c-b601-39e0a61d0948",
                            "question": "中間層を言い換えるとなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．図11.4のように，データまたはクラスタを融合する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作でクラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです"
                                }
                            ],
                            "id": "03b7e2fe-0c12-4871-9d51-4e08856636e9",
                            "question": "階層的クラスタリングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1104"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．図11.4のように，データまたはクラスタを融合する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作でクラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです"
                                }
                            ],
                            "id": "c35765fe-3f7b-4b85-856a-0b53cbc74ef3",
                            "question": "階層的クラスタリングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1104"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 260,
                                    "text": "Deep Neural Network (DNN) "
                                }
                            ],
                            "id": "338f43d5-ae84-4569-840e-ee542b09db08",
                            "question": "深層学習に用いるニューラルネットワークをなんとよぶか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 792,
                                    "text": "学習データからのマージンが最大となる識別境界線"
                                }
                            ],
                            "id": "1b6c64ee-2331-435c-807c-5c75ee03c04a",
                            "question": "超平面とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "AutoencoderとRestricted Bolzmann Machine(RBM)"
                                }
                            ],
                            "id": "6057c7e1-d47b-4c1f-88fb-364a41dc2c43",
                            "question": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習でよく使われる手段はなに"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる"
                                }
                            ],
                            "id": "5f161484-0d75-4137-9665-f0b271e73931",
                            "question": "ゼロ頻度問題って何ですか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 271,
                                    "text": "生物の神経細胞の仕組みをモデル化したもの"
                                }
                            ],
                            "id": "9830fabc-d0ab-44ca-a826-bfaa7e5482a8",
                            "question": "パーセプトロンってなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，誤差$E(\\bm{w})$の勾配方向の計算は以下のようになります．$x_{ij}$は，$i$番目の学習データの$j$次元目の値です．したがって，重みの更新式は以下のようになります．最急勾配法は，重みの更新量があらかじめ定めた一定値以下になれば終了です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 101,
                                    "text": "重みの更新量があらかじめ定めた一定値以下になれば"
                                }
                            ],
                            "id": "f56811f1-f0a8-4834-a272-b99d479654c8",
                            "question": "最急勾配法はいつ止まるんですか"
                        }
                    ]
                }
            ],
            "title": "0513"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "引数が負のときは0，0以上のときはその値を出力"
                                }
                            ],
                            "id": "9b216489-54c5-425b-aebe-f2d93b144111",
                            "question": "ReLUとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 654,
                                    "text": "最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です"
                                }
                            ],
                            "id": "cbda98ee-d689-4899-9dda-997c6d5f7b82",
                            "question": "最急勾配法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 307,
                                    "text": "ランダムに学習データを一つ"
                                }
                            ],
                            "id": "e9aa4573-dd94-4ff1-9dcb-b0685aaca2d0",
                            "question": "確率的最急勾配法はどうやってデータを選びますか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題です過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習することが目標です．回帰問題は，正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題と定義できます(図6.1)．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題"
                                }
                            ],
                            "id": "1175a65f-1502-44d5-a2ac-bcb130aa42c1",
                            "question": "回帰問題ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0601"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 654,
                                    "text": "最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です"
                                }
                            ],
                            "id": "6ed29a36-08c6-4121-97f4-d160122bb967",
                            "question": "最急勾配法って何ですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "MDP設定下での強化学習では，エージェントは行為後の次状態を環境から受け取るという仮定を置いています．つまり，エージェントは今，どの状態にいるのか，確定した情報を持っているということです．しかし，現実世界でロボットを動かした場合，ロボットの入力はカメラやセンサから得る値ですので，これらの情報から確実に状態を特定できるとは限りません．エージェントは観測された情報(部分的な状態の情報)を受け取る，という設定の方が現実的です．つまり，エージェントは現在の状態が確定できない状況で，意思決定を行うということになります（図15.7）．このような状況は，部分観測マルコフ決定過程(POMDP; Partially Observable MDP)による定式化が適しています．部分観測マルコフ決定過程の要素は以下のものです．\\begin{itemize}\\item 状態$s_t$で行為$a_t$を行うと観測$o_{t+1}$が確率的に得られる\\item エージェントは状態の確率分布を信念状態$b_t$として持つ\\item エージェントは，信念状態$b_t$，行為$a_t$，観測$o_{t+1}$から次の信念状態$b_{t+1}$を推定する状態見積器(state estimator)を内部に持つ\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 216,
                                    "text": "エージェントは現在の状態が確定できない状況"
                                }
                            ],
                            "id": "3764419d-19f8-4d72-93fa-3fe543e7a8ea",
                            "question": "部分観測マルコフ決定過程が適する状況は何ですか"
                        }
                    ]
                }
            ],
            "title": "1514"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰は入力から予測される妥当な出力値を求める問題です．典型的な回帰問題には，消費電力の予測，中古車の価格算出，生産量計画などがあります．回帰の単純な例として，入力を気温，出力をビールの売上高とした架空のデータ（図1.8(a)）を考えてみます．未知データに対して妥当な出力値を求めるために，入力データがある関数に基づいてターゲットを出力していると考え，その関数を求める問題が回帰問題です．ただし，関数の形として1次関数，2次関数，3次関数だけでなく，三角関数や指数関数などとの組み合わせまで考えてもよいとなると手がつけられなくなるので，通常は関数の形を先に決めて，その係数を学習データから推定するという問題とみなします．1次関数を仮定して，学習データとの誤差が最も少なくなるように係数を求めると，図1.8(b)に示すような直線が得られます．ほとんどの点がこの直線を外れているので，あまりよい近似とはいえないようにみえるかもしれません．一方，複雑な高次の関数を前提とすれば，図1.8(c)に示すように，すべての学習データを通る関数を求めることができます．このどちらを採用すべきかについて，回帰問題でも識別問題と同様の立場をとります．すなわち，未知データに対する出力として，どちらが妥当かということを考えます．気温の少しの変化に対して売上が大きく変わるところがある図1.8(c)の関数は，やはり不自然な回帰に見えます．この例のように入力と出力を2次元で眺めることができれば，その妥当性をある程度直観的に議論できますが，通常の場合，入力は多次元なので，直観に頼らずに学習結果の妥当性を吟味する方法を考えなければなりません．回帰の代表的な手法には線形回帰，回帰木，モデル木などがあります．これらを第6章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 718,
                                    "text": "線形回帰，回帰木，モデル木などがあります"
                                }
                            ],
                            "id": "402c7f16-9ac8-4eb7-a8ef-aac73fcfc990",
                            "question": "回帰の代表的な手法には何がありますか"
                        }
                    ]
                }
            ],
            "title": "0112"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 84,
                                    "text": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫"
                                }
                            ],
                            "id": "f67f518d-0ba7-426c-9848-38a21ebe41b3",
                            "question": "LSTMとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "制約を弱める変数"
                                }
                            ],
                            "id": "81078458-ad50-4e73-aa4a-e6591150f143",
                            "question": "スラック変数とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 90,
                                    "text": "各データに重みを付け，そのもとで識別器を作成します"
                                }
                            ],
                            "id": "ecdc917a-33e9-41af-a543-3e3a6e69b140",
                            "question": "ブースティングのアイディアは何ですか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 121,
                                    "text": "将棋や囲碁などを行うプログラム"
                                }
                            ],
                            "id": "b2a0fb3b-67c0-48b8-bb74-d7b1e3568c61",
                            "question": "意思決定エージェントの例は何ですか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "Matrix Factorizationは，まばらなデータを低次元行列の積に分解する方法の一つです．一般に，行列分解にはSVD (Singular Value Decomposition) とよばれる方法がありますが，推薦システムにこの方法を適用しても，うまくいかないことが多いといわれています．その理由として，購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するからです．購入したものを1，しなかったものを0として行列で表現した購入データをそのまま推薦に利用すると，1が「好き」，0が「きらい」に対応するものとして扱ってしまいます．そのようなことを避けるために，値の入っているところのみで最適化をおこなう手法が，Matrix Factorizationです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 156,
                                    "text": "購入データに値が入っていないところは，「購入しない」と判断したものはごく少数で，大半は，「検討しなかった」ということに対応するから"
                                }
                            ],
                            "id": "8af238ad-b034-4245-9d3c-887dc2955afc",
                            "question": "推薦システムを作るとき行列分解でSVDを用いるとなぜうまくいかないことが多いのですか"
                        }
                    ]
                }
            ],
            "title": "1220"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "ea7e2bd2-3372-48e8-8a0c-31f379ab1b8d",
                            "question": "カーネルトリックとは何か"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 479,
                                    "text": "アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります"
                                }
                            ],
                            "id": "2969cc2c-9b9e-4008-a1a5-9eb48a39c01d",
                            "question": "機械学習の基本的な定義はなんですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "共訓練の特徴は，学習初期の誤りに強いということが挙げられます．欠点としては，それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること，などです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 38,
                                    "text": "それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること"
                                }
                            ],
                            "id": "4a5ca9c1-f116-494e-aad8-9349724d0ac9",
                            "question": "共訓練の問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "1410"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，このデータからFP-木 (Frequent Pattern Tree) を作成します．FP-木は，最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．挿入アルゴリズムは以下のようになります．具体的な FP-木の作成手順は図12.7のようになります．まず，$\\{z,r\\}$がnull($\\emptyset$)を根とするFP-木に挿入されます．次に，$\\{z,x,y,s,t\\}$の挿入です．まず$z$はFP-木にあるのでカウントを1増やして2として，この$z:2$をルートとするFP-木に対して，残りの$\\{x,y,s,t\\}$を挿入します．次の$x$はFP-木にないので，新たにノードを作って$z:2$につなぎます．そして，この新しい$x:1$をルートとするFP-木に対して残りの要素を挿入する作業を再帰的に繰り返します．できあがったFP-木に対して，特徴を見出しとするヘッダテーブルを作成し，その頻度を記録しておくとともにFP-木に出現する同じ要素をリンクで結んでおきます（図12.8上）．特定の特徴は，自分より頻度の高い特徴の出現の有無に応じて，複数の枝に分かれて出現します．このリンクをたどって集めた出現数は，全体のトランザクション集合での出現数に一致します．% figure 12.8",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 53,
                                    "text": "最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます"
                                }
                            ],
                            "id": "ed6c48c1-7353-4740-aada-840111c804c7",
                            "question": "FP-木とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1215"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ここまでに説明してきたクラスタリングの結果を用いて，新たなデータが観測されたときにそのデータが属するクラスタを決める，という問題を考えます．つまり，教師なし学習で識別器を作りたい，ということと同じです．k-meansアルゴリズムの結果は，各クラスタの平均ベクトルなので，新しく観測したデータと，各クラスタの平均ベクトルとの距離を求めて，最も近いクラスタに分類するという手法は，簡単に思いつきます．しかしこの距離計算は，全クラスタの分散が等しいことを前提にしており，クラスタ毎にデータの広がり方が異なる場合は，適切な結果になりません．また，クラスタの事前確率も考慮されていません．第4章で説明したように，事後確率最大となる識別結果は，事前確率と尤度（各クラス毎の確率密度関数が観測されたデータを生成する確率）の積を最大とするクラスとなるので，よい識別器を作るためにはこれらの確率が必要です．事前確率はクラスタリング結果のデータ数の分布から求まります．そこで，尤度を計算するための確率モデルを，与えられた教師なし学習データから求めるという問題設定で，その解決法を考えてゆきましょう．k-means法を代表とする分割最適化クラスタリングは，平均値のみを推定していたので，この考え方を一般化します．各クラスタの確率分布の形を仮定して，そのパラメータを学習データから推定するという問題を設定します．確率分布として式(11.6)のような正規分布（ガウス分布）を仮定すると，教師なし学習データから，クラスタ$c_m$の平均$\\bm{\\mu}_m$と分散$\\bm{\\mu}_m$を推定する問題になります．k-means法では，各データはいずれかのクラスタに属していました．一方，この方法の場合は，どの$\\bm{x}$に対しても，すべてのクラスタが式(11.6)に基づいて，そのデータがそのクラスタから生成された確率を出力します．すなわち，個々のデータはどのクラスタに属するかを一意に決めることができず，クラスタ1に属する確率が0.2，クラスタ2に属する確率が0.8，といった表し方になります（図11.16）．このような表現を混合分布による表現といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 404,
                                    "text": "クラスタリング結果のデータ数の分布"
                                }
                            ],
                            "id": "0bd9766b-962a-45bf-85fd-f5063cf9a554",
                            "question": "事前確率を求めるには何が必要ですか"
                        }
                    ]
                }
            ],
            "title": "1114"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 155,
                                    "text": "制限を設け，対数線型モデルを系列識別問題に適用したもの"
                                }
                            ],
                            "id": "ef354fc1-e92f-47c4-bcbd-478fc6177fb9",
                            "question": "CRFとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木"
                                }
                            ],
                            "id": "fbf5040e-354a-4c61-9e72-039d72cb0ad7",
                            "question": "CARTとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "e0cf6be6-f8c7-4d99-ab4a-4e67848f91bc",
                            "question": "DNNとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "特徴空間上では線形識別面を設定すること"
                                }
                            ],
                            "id": "965db2a6-4f32-416f-8f7b-c494c68ab29b",
                            "question": "誤り訂正学習は特徴空間上では何に相当しますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 503,
                                    "text": "誤差逆伝播法"
                                }
                            ],
                            "id": "51e6c98d-2391-4962-8265-786b02bb4616",
                            "question": "出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが移動するという状況です．ゴールGに着けば，報酬が得られます．単純なケースでは報酬は決定的（ゴールに着けば必ずもらえる）で，部屋の移動にあたる状態遷移も決定的（必ず意図した部屋に移動できる）です．問題を一般化して，報酬や遷移が確率的である場合も想定できます．これらが確率的になる原因として，例えばロボットのゴールを探知するセンサーがノイズで誤動作をしたり，路面状況でスリップが生じるなどの不確定な要因で行為が成功しない状況が考えられます．これらは，非決定的であるとはいえ，学習中に状況が変化してしまうとどうしようもないので，この非決定性が確率的であるとし，確率分布は学習期間中を通じて一定であるとします．このような問題は，以下のようなマルコフ決定過程として定式化することができます．\\begin{itemize}\\item 時刻$t$における状態$s_t \\in S$\\item 時刻$t$における行為$a_t \\in A(s_t)$\\item 報酬 $r_{t+1} \\in R$，確率分布$p(r_{t+1}|s_t, a_t)$\\item 次状態$s_{t+1} \\in S$，確率分布$P(s_{t+1}|s_t, a_t)$\\end{itemize}マルコフ決定過程は，「マルコフ性」を持つ確率過程における意思決定問題で，「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質です．ここでは，報酬と次状態への遷移の確率が現在の状態と行為のみに依存しているという定式化になっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 579,
                                    "text": "「マルコフ性」を持つ確率過程における意思決定問題"
                                }
                            ],
                            "id": "394bfafb-2f91-4d32-b239-15e070931e48",
                            "question": "マルコフ決定過程ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1505"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです(図14.5)自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 21,
                                    "text": "最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すもの"
                                }
                            ],
                            "id": "0debe64b-a61e-4640-b2da-7c1338684f3c",
                            "question": "自己学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1407"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります"
                                }
                            ],
                            "id": "ec8a6b34-772c-4397-8652-f7f56034d365",
                            "question": "特徴次元が多いとどうなるのか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 678,
                                    "text": "一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です"
                                }
                            ],
                            "id": "e0cf8218-30c3-44ca-9d0b-4062b2a629c3",
                            "question": "非線形性を持つデータに対して識別を行うにはどんな方法がありますか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木"
                                }
                            ],
                            "id": "ddaa9530-fd15-47b6-8495-91c3bcd2b18d",
                            "question": "CARTはどのような決定木ですか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 499,
                                    "text": "事後確率が最大となるクラスを識別結果とする方法で"
                                }
                            ],
                            "id": "31681296-5db7-49fb-b47c-900edba6a24b",
                            "question": "最大事後確率則ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "b2095532-2b33-43cb-8659-01822e702126",
                            "question": "カーネルトリックってなんですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 317,
                                    "text": "データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます"
                                }
                            ],
                            "id": "260ad4fe-0cbf-4265-a09b-6519897db8d3",
                            "question": "なぜ最小二乗法では正解との誤差を二乗するんですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "また，パラメータ$\\bm{w}$の絶対値を正則化項とするものをLasso回帰とよびます．一般に，Lasso回帰は値を0とするパラメータが多くなるように正則化されます．英単語のlassoは「投げ縄」という意味で，投げ縄回帰と訳されることがあります．多くの特徴がひしめき合っている中に投げ縄を投げて，小数のものを捕まえるというイメージをもってこのように呼ばれているのかもしれませんが，Lassoのオリジナルの論文では，Lasso は least absolute shrinkage and selection operator の意味だと書かれています．Lasso回帰に用いる誤差評価式を，式(6.9)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ値を0とする重みが多くなります．Lasso回帰の解は，原点で微分不可能な絶対値を含むため，最小二乗法のように解析的に求めることはできません．そこで，正則化項の上限を微分可能な二次関数で押さえ，その二次関数のパラメータを誤差が小さくなるように繰り返し更新する方法などが提案されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "Lasso回帰"
                                }
                            ],
                            "id": "e6d58588-c940-4f01-89c0-426bf297276a",
                            "question": "パラメータ$\\bm{w}$の絶対値を正則化項とするものをなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0607"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです．ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．外れ値は，学習データに含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れたデータや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に除去しておくのが望ましいデータです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 39,
                                    "text": "入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです"
                                }
                            ],
                            "id": "d5a81c67-9d90-4895-a882-fb63493af2ec",
                            "question": "異常検出とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1111"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った"
                                }
                            ],
                            "id": "41453840-3164-45fa-abfa-0925e874d7f3",
                            "question": "モデル木は回帰木とどう違いますか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 170,
                                    "text": "最も遠い事例対の距離を類似度とする"
                                }
                            ],
                            "id": "0582edff-7d82-4553-bdb3-be19bcdfe0bc",
                            "question": "完全連結法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 181,
                                    "text": "LSTMセル"
                                }
                            ],
                            "id": "561a5c44-076b-471b-bcb1-90296019a8cb",
                            "question": "LSTMで置き換えられるメモリユニットの名前はなんですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 298,
                                    "text": "表現学習"
                                }
                            ],
                            "id": "04f0cf59-8c3b-4f2b-9129-299b437cecb5",
                            "question": "深層学習を言い換えると何になりますか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 613,
                                    "text": "入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するもの"
                                }
                            ],
                            "id": "1c243f68-ed20-47e7-8ffa-9dcdf5baa004",
                            "question": "パターンマイニングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 219,
                                    "text": "もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています"
                                }
                            ],
                            "id": "8e9801e1-273a-41b2-ae96-74f875fa594d",
                            "question": "なぜ元の空間でのデータ間の距離関係を保持する方が良いのですか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "制約を満たさない程度を表すので，小さい方が望ましい"
                                }
                            ],
                            "id": "668c4fcd-f3e9-4571-bbd7-d9964272d5d5",
                            "question": "スラック変数が小さいほうがいいのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 8,
                                    "text": "報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習"
                                }
                            ],
                            "id": "7cd41c80-829b-47f4-967d-6a3965a6660d",
                            "question": "強化学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的手法の代表的手法である階層的クラスタリングは，近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです（図11.3）．アルゴリズムとしては，一つのデータからなるクラスタをデータ個数分作成するところから始まって，最も近いクラスタを融合して新しいクラスタを作る操作を繰り返し，最終的にデータが一つのクラスタになれば終了です．図11.4のように，データまたはクラスタを融合する操作を木構造で記録しておけば，全データ数$N$から始まって，1回の操作でクラスタが一つずつ減ってゆき，最後は一つになるので，任意のクラスタ数からなる結果を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "近くのデータをまとめて小さいクラスタを作り，その小さいクラスタに近くのデータを取り込むか，あるいは小さいクラスタ同士をまとめて，少し大きめの新しいクラスタを作るという手順を繰り返すものです"
                                }
                            ],
                            "id": "95d2f624-c46f-44b6-8687-f10fd3d0f710",
                            "question": "階層的クラスタリングってなんですか"
                        }
                    ]
                }
            ],
            "title": "1104"
        },
        {
            "paragraphs": [
                {
                    "context": "精度あるいは再現率のどちらかを重視する場合に，閾値を変えたときの精度と再現率の関係を見ることができれば，タスクで要求される適切な設定にすることができます．このためには，ROC曲線(ROC curve)（図2.11）を用います．ROC曲線は，横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたものです．ROC曲線は必ず原点から始まり，必ず(1,1)の点で終わります．図2.11の例では，識別器のパラメータを$\\theta < 4.3$で正例と判定するように設定すれば，すべてのテストデータが負と判定され，TPR=FPR=0となるので，この識別器はROC曲線の原点に対応します．一方，識別器のパラメータを$\\theta < 8.0$で正例と判定するように設定すれば，すべてのテストデータが正と判定され，TPR=FPR=1となるので，この識別器はROC曲線の(1,1)の点に対応します．このパラメータを4.3から8.0まで小刻みに変化させてゆくと，原点から始まり，(1,1)で終わる図2.12のようなROC曲線を描くことができます．このように，機械学習の結果は様々な評価指標やグラフを使って評価することになります．ここまで説明してきた2クラスの評価手法を多クラスに適用する場合は，クラス毎の精度や再現率を求め，そのクラスのデータ数に応じた割合を掛けることで，総合的な評価を行います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 120,
                                    "text": "横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたものです"
                                }
                            ],
                            "id": "6899a3a0-8220-43af-918a-da4914fe4842",
                            "question": "ROC曲線って何ですか"
                        }
                    ]
                }
            ],
            "title": "0211"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 82,
                                    "text": "最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの"
                                }
                            ],
                            "id": "827f3975-88a8-49f9-9fb4-6d48efb85f0d",
                            "question": "X-meansアルゴリズムは、どのよなアルゴリズムなのですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 82,
                                    "text": "最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの"
                                }
                            ],
                            "id": "1312b438-c094-4439-9a60-3e00436d3cba",
                            "question": "x-meansアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから"
                                }
                            ],
                            "id": "4df75283-79a3-4792-a933-c9537f2534c1",
                            "question": "ノードを階層的に組むと非線形識別面が実現できるのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った方法です"
                                }
                            ],
                            "id": "47e4f055-168b-4b6b-8ae6-999e37cc46af",
                            "question": "モデル木って何ですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "環境モデルが未知の場合，TD(Temporal Difference)学習と呼ばれる方法を使います．環境の探索が必要なので，探索戦略としてε-greedy法を使います．ε-greedy法は確率$1-\\epsilon$で最適な行為，確率$\\epsilon$でそれ以外の行為を実行する探索手法の総称で，実際にはQ値を確率に変換したものを基準に行為を選択します．ただし，探索の初期はいろいろな行為を試し，落ち着いてくると最適な行為を多く選ぶようにするように，温度の概念を導入します．温度を$T$として，式(15.8)で表される確率に従って行為を選びます．$T$をアニーリング(焼き鈍し)における温度と呼び，高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "確率$1-\\epsilon$で最適な行為，確率$\\epsilon$でそれ以外の行為を実行する探索手法の総称"
                                }
                            ],
                            "id": "e141c07c-aa87-4db3-83d4-e1898e4aa834",
                            "question": "ε-greedy法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1510"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 391,
                                    "text": "識別対象のクラス数"
                                }
                            ],
                            "id": "5242ee69-19af-4f4e-b771-37a00127f506",
                            "question": "フィードフォワード型モデルの出力層のクラスの数はいくつ"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 741,
                                    "text": "山の尾根という意味で"
                                }
                            ],
                            "id": "8c54b619-ab8b-4122-bf17-369124c62416",
                            "question": "Ridgeってどういう意味ですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "まず，学習データが線形識別可能な状況で，マージンが最大となる識別面を求める方法を考えてゆきましょう．使用するデータは，数値特徴に対して正解情報の付いたデータですが，ここでは，2値分類問題に限定し，正解情報の値を正例$1$，負例$-1$とします．識別面は平面を仮定するので，特徴空間上では式(7.1)で表現されます．そうすると，$i$番目のデータ$\\bm{x}_i$と，この識別面との距離$Dist({\\bm{x}_i})$は，点と直線の距離の公式を用いて，式(7.2)のように計算できます．ここで，式(7.1)の左辺は正例に対しては正の値，負例に対しては負の値を出力するようにその重みを調整した式ですが，右辺の値が0なので，左辺を定数倍しても表す平面は変わりません．そこで，識別面に最も近いデータを識別面の式に代入したときに，その絶対値が1になるように重みを調整したとします．そうすると，式(7.2)，(7.3)より，学習パターンと識別面との最小距離は，以下のようになります．式(7.4)がマージンを表すので，マージンを最大にする識別面を求める問題は，$||\\bm{w}||$を最小化する問題になります．ここで，最小化しやすいように，この問題を$||\\bm{w}||^2$の最小化とします．この式の形だけを見ると，$\\bm{w}=\\bm{0}$が最小解ですが，これでは識別面になりません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 122,
                                    "text": "識別面は平面を仮定する"
                                }
                            ],
                            "id": "88ec9421-6812-4a86-91b6-f4975280731d",
                            "question": "サポートベクトルマシンの識別面はどのような形になりますか"
                        }
                    ]
                }
            ],
            "title": "0702"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル"
                                }
                            ],
                            "id": "3a3f637a-07a5-4ace-8c82-e784e997bb54",
                            "question": "ロジスティック識別器ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けないというものです．数式で表すと，確率変数の値$\\{z_1,\\dots,z_n\\}$の結合確率は，以下のように計算されます．ただし$\\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，この計算を順に行うことで，ネットワーク全体の確率計算が行えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル"
                                }
                            ],
                            "id": "d818f816-8759-45d1-8f3e-3050f0cbc240",
                            "question": "ベイジアンネットワークってなんですか"
                        }
                    ]
                }
            ],
            "title": "0412"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 462,
                                    "text": "もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法"
                                }
                            ],
                            "id": "9c27e20e-bf32-4d67-ba98-3e52a9e2cb0d",
                            "question": "密度推定とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 41,
                                    "text": "音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります"
                                }
                            ],
                            "id": "829e8d4a-01ae-46db-adf4-a16c98c786b3",
                            "question": "識別問題にはどんな種類がありますか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "真のモデルとの距離"
                                }
                            ],
                            "id": "5ad2efaa-056e-41e6-816c-f5b0a0c46cd0",
                            "question": "バイアスってなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 40,
                                    "text": "学習結果の散らばり具合"
                                }
                            ],
                            "id": "c42c82f9-10af-492e-98f5-d1f0343b68e7",
                            "question": "分散ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 391,
                                    "text": "識別対象のクラス数"
                                }
                            ],
                            "id": "83363a90-a34a-4f58-a566-c3190e198ef4",
                            "question": "3階層のフィードフォワード型モデルでは出力層をいくつ用意しますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，前章で学んだ統計モデルによる識別法で，数値を要素とする特徴ベクトルを識別する問題に取り組みます．数値を要素とする特徴ベクトルに対する識別問題は，一般にはパターン認識とよばれます．第3章と第4章では，カテゴリ特徴に対する識別問題を扱いました．続いて本章では，数値を要素とする特徴ベクトル$\\bm{x}$に対する識別問題を扱います（図5.1）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 54,
                                    "text": "数値を要素とする特徴ベクトルに対する識別問題は"
                                }
                            ],
                            "id": "be36061f-e048-45f4-9551-a1859b5a6b7f",
                            "question": "パターン認識とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0501"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 295,
                                    "text": "入力データ集合から適切なまとまりを作ることでクラスを推定する手法"
                                }
                            ],
                            "id": "a618b353-98a7-4fd6-bb94-00d3d6395236",
                            "question": "クラスタリングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 248,
                                    "text": "線形で識別できないデータに対応するため"
                                }
                            ],
                            "id": "bdffe594-ff20-45e4-a3a7-5ff64e7dcf95",
                            "question": "なぜペナルティを設定するのですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "b91caad5-ae84-4314-9dbd-69e677d84b6c",
                            "question": "回帰木って何ですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです．ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．外れ値は，学習データに含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れたデータや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に除去しておくのが望ましいデータです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 121,
                                    "text": "学習データに含まれるデータの中で，ほかと大きく異なるデータ"
                                }
                            ],
                            "id": "a44df977-53af-4cec-86b3-59c8a4442586",
                            "question": "外れ値とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "1111"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 19,
                                    "text": "ユニットの活性化関数を工夫する方法"
                                }
                            ],
                            "id": "1cbe20e4-804a-4057-8abb-cbdfb45690dd",
                            "question": "勾配消失問題への取組みにはどのような方法がありますか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "カテゴリ特徴の場合は，数値特徴の場合のような一般化は難しいのですが，カテゴリ特徴で大量に学習データが入手可能な状況というのは，ほぼ言語データの識別問題に絞られます．最も簡単なケースは図14.3のように，教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる場合です．このように識別に役立つ語のオーバーラップが多いデータは教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそうです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 101,
                                    "text": "教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる"
                                }
                            ],
                            "id": "5c3ab45e-e663-4638-a44d-1c7cf2b04dd9",
                            "question": "オーバーラップとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1404"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "1689ca1b-1633-4efd-abe0-1265e493c82b",
                            "question": "マージンとは何か"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 428,
                                    "text": "学習時の自由度を意図的に下げていること"
                                }
                            ],
                            "id": "2be949bf-1103-4e4e-b8e3-bd827a8e50cb",
                            "question": "ドロップアウトによって過学習が生じにくくなっている理由はなんですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習は図14.2左の図のような，半教師あり学習に適したデータの場合はよいのですが，低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "低密度分離が満たされていないデータに対しては，教師ありデータによって作成した初期識別器の誤りがずっと影響を及ぼし続ける性質"
                                }
                            ],
                            "id": "5b4bfb41-f26e-40b7-8e62-0510894fff16",
                            "question": "自己学習の性質にはどのようなものがありますか"
                        }
                    ]
                }
            ],
            "title": "1408"
        },
        {
            "paragraphs": [
                {
                    "context": "ここからは，教師なし学習の問題に取り組みます．この章では，特徴ベクトルの要素が数値である場合に，その特徴ベクトルが生じるもとになったクラスを推定するモデル推定の問題を扱います(図11.1)．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 29,
                                    "text": "特徴ベクトルの要素が数値である場合に，その特徴ベクトルが生じるもとになったクラスを推定するモデル推定"
                                }
                            ],
                            "id": "377466a0-db24-4a22-bff5-2194bf195572",
                            "question": "クラスモデルとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1101"
        },
        {
            "paragraphs": [
                {
                    "context": "式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法が，勾配ブースティングです．損失関数としては，二乗誤差，誤差の絶対値，フーバー損失（$|x| \\leq \\epsilon$ の範囲で2次関数，その外側の範囲で線形に増加）などが用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "式(10.4)で，差が最小になるものを求めている部分を，損失関数の勾配に置き換えた式(10.5)に従って，新しい識別器を構成する方法"
                                }
                            ],
                            "id": "baa689b7-47c5-44cb-87be-2647482722da",
                            "question": "勾配ブースティングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1015"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 962,
                                    "text": "識別器が正例と判断したときに，それがどれだけ信頼できるかという指標"
                                }
                            ],
                            "id": "6211ee3a-f109-4401-9ce0-8fcaf6bd543d",
                            "question": "精度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 401,
                                    "text": "シグモイド関数"
                                }
                            ],
                            "id": "cea0c67f-4da7-44b5-b4d2-063609037db7",
                            "question": "入力が0または1の2値であれば，出力層の活性化関数として使われるのはなんですか"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "制約を弱める変数"
                                }
                            ],
                            "id": "5aa11dc1-8cfe-4c7c-b3de-54418665d479",
                            "question": "スラック変数ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムの中で，詳しい説明のない「特徴集合A中で最も分類能力の高い特徴」を決定する方法について説明します．分類能力が高いとは，「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということです．いいかえると，乱雑さが少なくなるように分類を行うということですね．乱雑さの尺度として，エントロピーを用います．学習データ集合$D$の乱雑さを計算するために，まず正例の割合: $P_+$,  負例の割合: $P_-$を計算し，それを元に式(3.1)によって，その集合の乱雑さ（エントロピー）$E(D)$を求めます．エントロピーの値は$P_+=1$または$P_-=1$のとき最小値0となり，$P_+=P_-=0.5$のとき，最大値1となるので，エントロピーの値が小さいほど，集合が乱雑でない，すなわち整っている（同じクラスのものが大半を占めている）ということになります．このエントロピーの減り具合を計算したいのですが，単純に引き算はできません．エントロピーは集合に対して定義できるものです．分類前は1つの集合で，分類後はその特徴値の種類数だけ集合ができます．そこで，分類後の集合の要素数の割合で重みを付けて計算します．この値を情報獲得量と定義します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 68,
                                    "text": "「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということです"
                                }
                            ],
                            "id": "60b2ec7f-f0ee-4b64-840f-4fa5b80eec04",
                            "question": "分類能力が高いとはどういうことですか"
                        }
                    ]
                }
            ],
            "title": "0311"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "e0d3abac-8d7d-4b8d-8139-d926cd8df602",
                            "question": "カーネルトリックとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\bm{w}$は$d+1$次元の重みベクトルとします．また，$\\eta$は学習係数で，適当な小さい値を設定します．このアルゴリズムは，学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します．これをパーセプトロンの収束定理とよびます．一方，学習データが線形分離不可能な場合にはこのアルゴリズムを適用することができません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合はこのアルゴリズムは停止しません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 149,
                                    "text": "パーセプトロンの収束定理"
                                }
                            ],
                            "id": "4c0b8fe7-3696-4ac7-8ad9-a26fb0c33fa2",
                            "question": "学習データが線形分離可能なとき必ず識別境界面を見つけて停止する定理はなんですか"
                        }
                    ]
                }
            ],
            "title": "0507"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 7,
                                    "text": "入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの"
                                }
                            ],
                            "id": "2c7195dd-eea9-4afa-b95a-9b0972294571",
                            "question": "モデル推定とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 158,
                                    "text": "たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題"
                                }
                            ],
                            "id": "0e9c2f71-ba16-4a0e-b6ad-34f1fb9ebfea",
                            "question": "2値分類とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 100,
                                    "text": "過学習が起きにくくなり，汎用性が高まることが報告されています"
                                }
                            ],
                            "id": "66c11b3d-cc9d-49b1-a3e6-39be07ec6204",
                            "question": "多階層学習においてドロップアウトを用いるとどうなるか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 673,
                                    "text": "階層的クラスタリングや k-means 法"
                                }
                            ],
                            "id": "5aa0c856-6810-4640-a1b5-b18ac32e111f",
                            "question": "クラスタリングの代表的な手法には何がありますか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1109,
                                    "text": "式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので"
                                }
                            ],
                            "id": "c500798f-af83-4eb3-a98c-f110cd2521ad",
                            "question": "なぜベイズの定理の分母は必要ないのですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です"
                                }
                            ],
                            "id": "1c6f6184-7dc1-4d51-8e7f-ef83ef114dce",
                            "question": "アンサンブル学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 26,
                                    "text": "重みパラメータに対しては線形で，入力を非線形変換することによって実現"
                                }
                            ],
                            "id": "8e790818-9135-4599-9ccb-837bd82d4b65",
                            "question": "基底関数ベクトルによる非線形識別面はどのように実現されていますか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "ランダムフォレストの学習手順は，基本的にバギングアルゴリズムと同様，学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．図10.4に，ランダムフォレストによる個々の木の作成の様子を示します．たとえば，学習データの特徴が$\\{A,B,C,D,E\\}$と5種類あるとして，ルートノードの分岐特徴を決める際は，ここから予め決められた数（たとえば3種類）をランダムに選択します．ここで，$\\{A,B,E\\}$が選択されたとすると，それぞれの分類能力を計算し，最も分類能力の高い属性を選んで，その値によってデータを分割します．分けられたデータ集合に対しても，同様にランダムに属性集合を選択して，その中で最も分類能力の高い属性を選んで木を成長させます．このような手順で，識別器の方を強制的に異なったものにするのがランダムフォレストの手法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 34,
                                    "text": "学習データから復元抽出して，同サイズのデータ集合を複数作るところから始まります．次に，各データ集合に対して，識別器として決定木を作成するのですが，ノードでの分岐特徴を選ぶ際に，全特徴からあらかじめ決められた数だけ特徴をランダムに選びだし，その中から最も分類能力の高い特徴（たとえば，式(3.2)で定義したGain値の高いもの）を選びます．そして，その操作をリーフが単一クラスの集合になるまで再帰的に行います．"
                                }
                            ],
                            "id": "c747547b-bab1-41fb-9e44-4c2c479fe5fb",
                            "question": "ランダムフォレストの学習手順は"
                        }
                    ]
                }
            ],
            "title": "1008"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 7,
                                    "text": "入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの"
                                }
                            ],
                            "id": "24ffc9fe-e4d3-4d14-b101-0d1d4c3f72dc",
                            "question": "モデル推定とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 355,
                                    "text": "顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用"
                                }
                            ],
                            "id": "dbe41a51-f217-47fc-be17-db0b4a9b2f9b",
                            "question": "クラスタリングはどのようなことに使われますか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "この連結されたHMMを用いて，入力系列に対して最も確率が高くなる遷移系列をビタビアルゴリズムによって求めます．最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まるということになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 55,
                                    "text": "最も確率が高くなる遷移系列が定まるということは，全ての隠れ変数の値が定まるということに等しくなりますので，入力の各部分系列に対して，出力が定まる"
                                }
                            ],
                            "id": "a5256e88-ef01-4ecf-a4fb-7d7effaa710b",
                            "question": "なぜHMMで最尤状態系列を求めるのですか"
                        }
                    ]
                }
            ],
            "title": "1313"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で"
                                }
                            ],
                            "id": "94ab3ce2-4b93-49fa-b0cd-ea61a6acd7aa",
                            "question": "ブースティングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1010"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "64651c02-5558-48f5-9971-15f48d17e1ae",
                            "question": "サポートベクトルマシンはどういう手法ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "最適政策$\\pi^*$に従ったときの累積報酬の期待値$V^{\\pi^*} (s_t)$は，見やすさのため，以後$V^* (s_t)$と表記します．この最適政策を求めるための考え方は，K-armed bandit問題と同じで，各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というものです．$Q^*(s_t, a_t)$を状態$s_t$ で行為$a_t$ を行い，その後最適政策に従ったときの期待累積報酬の見積もりとすると，$V^*$と$Q^*$の関係から，以下の式が導けます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 112,
                                    "text": "各状態において$Q(s_t, a_t)$（状態$s_t$ で行為$a_t$ を行うときの価値）の値を，問題の状況設定に従って求めてゆく，というもの"
                                }
                            ],
                            "id": "b1cfd2e4-277d-4f48-97b5-f5896e87aad2",
                            "question": "マルコフ決定過程における学習での最適政策を求めるための考え方とはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1507"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 64,
                                    "text": "全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります"
                                }
                            ],
                            "id": "0639ad09-686e-4e03-9da0-0248b4d73ea3",
                            "question": "最急勾配法の欠点はなんですか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 88,
                                    "text": "同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方"
                                }
                            ],
                            "id": "a60c329c-a4c0-48be-b39d-a28ec8058a2e",
                            "question": "決定木はどのような考えですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 446,
                                    "text": "事後確率"
                                }
                            ],
                            "id": "c4616a82-dbda-4c99-9b21-83bd0cc5223e",
                            "question": "入力を観測した後で計算される確率は何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 525,
                                    "text": "パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰"
                                }
                            ],
                            "id": "b85743e6-8378-4622-bc84-39bda3c2b66e",
                            "question": "Ridge回帰ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 372,
                                    "text": "確率のm推定という考え方を用います"
                                }
                            ],
                            "id": "4a4cc677-af5a-4e01-be66-5979dffda456",
                            "question": "ゼロ頻度問題はどうすれば回避できますか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシグモイド関数の微分です．第2項は入力の重み付き和の微分です．従って，出力層の重みの更新式は以下のようになります．通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．また，中間層は，この修正量を重み付きで足し合わせます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 62,
                                    "text": "入力の重み付き和の微分"
                                }
                            ],
                            "id": "c29bf630-fa61-4b83-b073-b19e69e9c608",
                            "question": "1つ目の式の右辺の第2項は何ですか"
                        }
                    ]
                }
            ],
            "title": "0808"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 7,
                                    "text": "入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するもの"
                                }
                            ],
                            "id": "fe66ff40-082c-45a8-8ad4-90f60d1ad5f6",
                            "question": "モデル推定ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "61974264-d926-4d86-8fc7-800f8a0881a8",
                            "question": "ニューラルネットワークはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ"
                                }
                            ],
                            "id": "63c407fd-59b0-4132-b6bc-3a992a4246b6",
                            "question": "ソフトマージンによる際の識別面の設定の仕方"
                        }
                    ]
                }
            ],
            "title": "0707"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 128,
                                    "text": "学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低い"
                                }
                            ],
                            "id": "221d9ee8-9896-4fff-a733-8b7b978af0cd",
                            "question": "特徴ベクトルの次元数が増えるとどんな問題がありますか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けないというものです．数式で表すと，確率変数の値$\\{z_1,\\dots,z_n\\}$の結合確率は，以下のように計算されます．ただし$\\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，この計算を順に行うことで，ネットワーク全体の確率計算が行えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル"
                                }
                            ],
                            "id": "2fcb6af2-b40b-462c-b609-18ce15583e53",
                            "question": "ベイジアンネットワークって何ですか"
                        }
                    ]
                }
            ],
            "title": "0412"
        },
        {
            "paragraphs": [
                {
                    "context": "環境モデルが未知の場合，TD(Temporal Difference)学習と呼ばれる方法を使います．環境の探索が必要なので，探索戦略としてε-greedy法を使います．ε-greedy法は確率$1-\\epsilon$で最適な行為，確率$\\epsilon$でそれ以外の行為を実行する探索手法の総称で，実際にはQ値を確率に変換したものを基準に行為を選択します．ただし，探索の初期はいろいろな行為を試し，落ち着いてくると最適な行為を多く選ぶようにするように，温度の概念を導入します．温度を$T$として，式(15.8)で表される確率に従って行為を選びます．$T$をアニーリング(焼き鈍し)における温度と呼び，高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 300,
                                    "text": "高ければすべての行為を等確率に近い確率で選択し，低ければ最適なものに偏ります．学習が進むにつれて，$T$の値を小さくすることで，学習結果が安定します．"
                                }
                            ],
                            "id": "1ec18bc1-775f-451c-acb5-d258fdcc4d82",
                            "question": "なぜTD学習において温度を学習が進むにつれて小さくするのですか"
                        }
                    ]
                }
            ],
            "title": "1510"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 107,
                                    "text": "最も近い事例対の距離を類似度とする"
                                }
                            ],
                            "id": "1e02a11d-7ee7-43a3-97b8-d6bf6c0611d3",
                            "question": "単連結法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 81,
                                    "text": "ドロップアウト"
                                }
                            ],
                            "id": "a1aea3d9-2277-4597-af8e-92d4b2a4ee65",
                            "question": "ニューラルネットワークで，ランダムに一定割合のユニットを消して学習を行う方法をなんという"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です"
                                }
                            ],
                            "id": "3a5763a5-c73c-45e7-b08b-1a562c809350",
                            "question": "アンサンブル学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．ラグランジュの未定乗数法を用いると，$g(\\bm{x})=0$という条件の下で$f(\\bm{x})$の最小値（あるいは最大値）を求める問題は，$L(\\bm{x}, \\lambda)=f(\\bm{x})-\\lambda g(\\bm{x})$ （ただし$\\lambda$はラグランジュ乗数）という新しいラグランジュ関数を導入し，この関数の極値を求めるという問題に置き換えることができます．ラグランジュ関数の$\\bm{x}$に関する偏微分を0とすると，式(7.8)のようになります．ここでは，ラグランジュの未定乗数法を不等式制約条件で用います．そうすると，式(7.6)，式(7.7)の制約付きの最小化問題は，ラグランジュ乗数$\\alpha_i$を導入して，以下の関数$L$の最小値を求めるという問題に置き換えることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 16,
                                    "text": "ラグランジュの未定乗数法"
                                }
                            ],
                            "id": "657ff8bd-e54f-49ed-8a56-3d8fda4da977",
                            "question": "マージンを最大とする識別面の計算方法はどうするの"
                        }
                    ]
                }
            ],
            "title": "0704"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 542,
                                    "text": "入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの"
                                }
                            ],
                            "id": "8b0ec46d-ea8e-4fb3-8190-aef4da8f518e",
                            "question": "回帰とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています．ここで，確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので，式(4.6)の対数をとって計算します．式(4.7)で計算される値を対数尤度$\\mathcal{L}(D)$とよびます．この対数尤度の値は，大きければ大きいほど学習データがそのモデルから生成された確率が高い、ということがいえます．そして，学習データが，真のモデルから偏りなく生成されたものであると仮定すると，この方法で求めたモデルは真の分布に近い，と考えることができます．したがって，式(4.7)を最大にするパラメータが求まればよいわけです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 114,
                                    "text": "式(4.7)で計算される値"
                                }
                            ],
                            "id": "831e5691-818b-46c7-a9e8-58d461b9e3ec",
                            "question": "対数尤度って何ですか"
                        }
                    ]
                }
            ],
            "title": "0407"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 653,
                                    "text": "マージン"
                                }
                            ],
                            "id": "2d2ec02a-e93b-4a6e-be21-03772461eac6",
                            "question": "識別境界線と最も近いデータとの距離のことをなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ"
                                }
                            ],
                            "id": "54998518-9b38-4e2a-9c67-f53686d19ceb",
                            "question": "線形分離可能でない場合はどうすれば良いですか"
                        }
                    ]
                }
            ],
            "title": "0707"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています"
                                }
                            ],
                            "id": "c6867ce0-a93f-4bca-bdde-36a220e8a706",
                            "question": "なぜ前処理で次元削減を行うのですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく"
                                }
                            ],
                            "id": "93721179-27b7-48f9-a64e-965c68f5ae45",
                            "question": "勾配消失問題とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 31,
                                    "text": "計算量が膨大であること"
                                }
                            ],
                            "id": "7294e395-16e2-4107-af6a-bed87730be65",
                            "question": "Aprioriアルゴリズムの問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワークの利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの確率計算の例を示します．最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)がHead-to-tail connectionでつながっている例を示します．これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．何も情報がない状態での「芝生が濡れている」確率は以下のようになります．まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って「芝生が濡れている」確率$P(W)$を求めます．ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，以下のようになります．つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していないことになります．なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，その日が「曇っている」確率は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 57,
                                    "text": "変数間の独立性を表現できること"
                                }
                            ],
                            "id": "362f7ffe-b646-4cf2-b06f-21b00f5cd1df",
                            "question": "ベイジアンネットワークのメリットは何ですか"
                        }
                    ]
                }
            ],
            "title": "0413"
        },
        {
            "paragraphs": [
                {
                    "context": "異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意する，ということです．ここまでの教師あり学習の説明からもわかるように，学習データが異なれば，たいていその学習結果も異なります．バギング(Bagging)はこのアイディアに基づいて，学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです（図10.3}）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 136,
                                    "text": "学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです"
                                }
                            ],
                            "id": "1a2fc1b9-151c-43c1-930f-08a807c3917c",
                            "question": "バギングとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1004"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 365,
                                    "text": "形態素解析処理が典型的な問題"
                                }
                            ],
                            "id": "2f0362a1-f719-4aae-a4de-7098f4b9352b",
                            "question": "系列データの入力の系列長と出力の系列長が等しい問題の例は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる概念の仮説を少なくし，その仮説の空間を探索することで概念を求める手法が開発されました．そのような手法として，FIND-Sアルゴリズムや，候補削除アルゴリズムがあります（図3.2）．FIND-Sアルゴリズムは，仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限します．このように，仮説に対して課す制約をバイアスとよびます．最初は，最も特殊な仮説（いかなる事例も正ではない）からスタートし，正例を一つずつ読み込んで，その事例の値を受け入れるように仮説を最低限一般化します．たとえば，表3.1のデータにおいて，最初の正例である1番のデータから，論理式「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ astigmatism=no $\\wedge$  tear-prod = reduced」が得られます．次の正例である3番のデータは，age, spectacle-prescrip, tear-prodの値はこの論理式に当てはまりますが，astigmatismの値が異なります．1番と3番のデータの両方が当てはまるようにするために，この論理式から，astigmatismの条件を取り除き，新たな仮説を「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ tear-prod = reduced」とします．これを続けると，5番のデータでspectacle-prescripの条件が落ち，9番のデータでageの条件が落ち，最後は16番のデータでtear-prodの条件まで落ちて，条件が何もなくなってしまいます．これでは，すべての入力が正例であるという概念になり，明らかにおかしな結果になってしまいました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 194,
                                    "text": "仮説に対して課す制約"
                                }
                            ],
                            "id": "21baac37-eea1-4014-91b1-8714ac87d32d",
                            "question": "バイアスって何ですか"
                        }
                    ]
                }
            ],
            "title": "0305"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "モデル木"
                                }
                            ],
                            "id": "c2f17385-1942-48df-b1e6-de0c6f52e32f",
                            "question": "回帰木と線形回帰の双方のよいところを取った方法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "もうひとつのタスクに特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつリカレントニューラルネットワーク（図9.10(a)）があります．リカレントニューラルネットワーは時系列信号や自然言語などの系列パターンを扱うことができます．このリカレントニューラルネットワークへの入力は，特徴ベクトルの系列$\\bm{x}_1,\\bm{x}_2,\\dots, \\bm{x}_T $という形式になります．たとえば，動画像を入力して異常検知を行ったり，ベクトル化された単語系列を入力して品詞列を出力するようなタスクが具体的に考えられます．これらに共通していることは，単純に各時点の入力からだけでは出力を決めることが難しく，それまでの入力系列の情報が何らかの役に立つという点です．リカレントニューラルネットワークの中間層は，入力層からの情報に加えて，一つ前の中間層の活性化状態を入力とします．この振舞いを時間方向に展開したものが，図9.10(b)です．時刻$t$における出力は，時刻$t-1$以前のすべての入力を元に計算されるので，これが深い構造をもっていることがわかります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "リカレントニューラルネットワーク"
                                }
                            ],
                            "id": "cef3c0c8-36d0-4003-8256-73bcc5873c0f",
                            "question": "中間層の出力が時間遅れで自分自身に戻ってくる構造をもつタスクに特化した構造をもつニューラルネットワークは何ですか"
                        }
                    ]
                }
            ],
            "title": "0916"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "ce483940-f7cf-4e41-a766-402b2d63e901",
                            "question": "深層学習はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，特徴ベクトルのなかで数値を値とする特徴がある場合の決定木学習について説明します．基本的なアイディアとして，連続値である数値特徴を，いくつかのグループに分ける離散化という処理をおこなうことで，決定木での学習を可能にします．具体的には，数値特徴$a_i$に対して$a_i < \\theta$という条件式を値とするノードを作成します．この条件式を満たすデータと満たさないデータに分割することで，ラベルを値として持つ特徴に対する決定木学習と同じアルゴリズムが使えます．問題は，閾値$\\theta$をどうやって決めるかということです．具体的にデータを見ながら，考えてゆきましょう．Weka付属のweather.numericデータ（表3.5）は，weather.nominalデータの特徴temperatureと特徴humidityを数値にしたものです．たとえば，特徴humidityの値でデータを分割する場合を考えてみましょう．最もエントロピーが低くなるような切り方を見つけたいので，同じクラスの中で切ることは意味がありません．クラスの境目を探すと，図3.6に点線で示す箇所になります．この境目の値はその前後の値の平均値をしておきましょう．これらの中で最も情報獲得量の多い場所を選びます．カテゴリ特徴のときと同様の計算を行うと，$c=82.5$となるときに（すなわち，82.5未満の値はyes，82.5以上はnoとなるカテゴリ特徴に変換したときに）最も情報獲得量が多い分割になります．この操作をすべての数値特徴についておこなった後は，通常のID3アルゴリズムと同じです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "いくつかのグループに分ける離散化という処理をおこなう"
                                }
                            ],
                            "id": "0163e134-745c-48fa-afcd-6fc46bc79306",
                            "question": "数値の特徴を持つ場合どうすれば決定木による学習が可能になりますか"
                        }
                    ]
                }
            ],
            "title": "0316"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．ラグランジュの未定乗数法を用いると，$g(\\bm{x})=0$という条件の下で$f(\\bm{x})$の最小値（あるいは最大値）を求める問題は，$L(\\bm{x}, \\lambda)=f(\\bm{x})-\\lambda g(\\bm{x})$ （ただし$\\lambda$はラグランジュ乗数）という新しいラグランジュ関数を導入し，この関数の極値を求めるという問題に置き換えることができます．ラグランジュ関数の$\\bm{x}$に関する偏微分を0とすると，式(7.8)のようになります．ここでは，ラグランジュの未定乗数法を不等式制約条件で用います．そうすると，式(7.6)，式(7.7)の制約付きの最小化問題は，ラグランジュ乗数$\\alpha_i$を導入して，以下の関数$L$の最小値を求めるという問題に置き換えることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 283,
                                    "text": "ここでは，ラグランジュの未定乗数法を不等式制約条件で用います"
                                }
                            ],
                            "id": "62e2fed3-af21-4ba0-98ab-977948774b1e",
                            "question": "制約条件ではどのような式を用いますか"
                        }
                    ]
                }
            ],
            "title": "0704"
        },
        {
            "paragraphs": [
                {
                    "context": "さて，いよいよ学習です．ここでは学習アルゴリズムとして，入力されたデータに近い学習データを近い順に$k$個選び，多数決などで所属するクラスを決定するk-NN法 (k-nearest neighbor method) を使います（図2.9）．k-NN法は，いわば学習データを集めるだけの学習法です．$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます．このk-NN法で調整するべきパラメータは，近傍としていくつまでの学習データを考えるか（すなわち$k$の値）になります．また，学習データが多い場合，効率よく近傍を探索するアルゴリズムを組み合わせることもあります．これから機械学習を学ぼうと意気込んでいるみなさんに，最初に紹介するのが「学習しない」学習法なので，がっかりされたかもしれません．しかし，データが大量に入手・記録可能で，かつ並列で高速に近傍計算ができる現在では，k-NN法は驚くほどの性能を示すこともあります．たとえば，スマートフォンの音声対話アプリで実現されている発話理解手法の一部は，k-NN法の考え方に近いものです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 148,
                                    "text": "$k=1$の場合，識別したいデータと最も近い学習データを探して，その学習データが属するクラスを答えとします．$k>1$の場合は，多数決を取るか，距離の重み付き投票で識別結果を決めます．"
                                }
                            ],
                            "id": "97451312-1e15-4f02-bb02-bccf006c75c3",
                            "question": "k-NN法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0208"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワークの構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合にも対応できる方法として，確率伝播による計算法があります．このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，適用することができません．そのような場合は，確率的シミュレーションも用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 247,
                                    "text": "値が真となる確率を知りたいノードが表す変数"
                                }
                            ],
                            "id": "699e034d-eab8-464b-a60a-c61a3a06c454",
                            "question": "ここでいう目的変数って何ですか"
                        }
                    ]
                }
            ],
            "title": "0416"
        },
        {
            "paragraphs": [
                {
                    "context": "データから規則や知見を得る機械学習技術のなかでも，特に深層学習 (deep learning)は，高い性能を実現する方法として近年注目を集めています．深層学習は，一般に隠れ層を多くもつニューラルネットワーク（図1.3）によって実装されています．深層学習が他の機械学習手法と異なるのは，深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです．近年の深層学習の流行を見ると，他の機械学習技術はもう不要に見えるかもしれません．しかし，深層学習がその強さを発揮しているのは，音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題であるという傾向があります．さまざまな問題に対して機械学習アルゴリズムの性能を競うサイトでは，深層学習と並んで勾配ブースティングなどの手法が上位を占めることがあります．また一方で，性能は多少低くてもよいので判定結果に至るプロセスがわかりやすい手法や，運用後のチューニングが容易な手法が好まれる場合もあり，さまざまな状況でさまざまな問題に取り組むためには，深層学習だけではなく機械学習手法全般に関して理解しておくことが必要であるといえます．本書では機械学習全般に関して，設定した問題に対する基本的な手法の概要と，フリーソフトを用いた例題の解法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 142,
                                    "text": "深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです"
                                }
                            ],
                            "id": "d836733a-9c4a-4483-8fe1-cb0999c9192d",
                            "question": "深層学習と機械学習の違いはなんですか"
                        }
                    ]
                }
            ],
            "title": "0104"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 491,
                                    "text": "最小二乗法"
                                }
                            ],
                            "id": "33106766-23c0-4478-a1c3-fa66c911470d",
                            "question": "二乗誤差を最小にするように識別関数を調整する方法を何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "まず，最も単純な，入力も出力もスカラーである場合の回帰問題（図6.2）を考えましょう．この学習データから，入力$x$を出力$y$に写像する関数$\\hat{c}(x)$を推定します．図6.2のデータからは，入力$x$が大きくなると，出力$y$も大きい値になる傾向が見えます．そこで，この傾向を直線であらわして，入力$x$と出力$y$を関係付けることを試みます．もし学習データのすべての点が，その上にあるように直線を決めることができれば，これで問題は終わりなのですが，通常そのようなことはありません．そこで，図6.3のようになるべく誤差の少ない直線を求めることとします．そうすると，ここでの定式化は5.3.2項で説明した最小二乗法による学習と等しくなります．違いは，識別問題における教師信号$y_i$が1または0であったのに対して，回帰問題の教師信号$y_i$が連続値であるということですが，学習アルゴリズム自体は変更なく適用することができます．回帰式を式（6.2)とする．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 45,
                                    "text": "学習データから，入力$x$を出力$y$に写像する関数$\\hat{c}(x)$を推定します．図6.2のデータからは，入力$x$が大きくなると，出力$y$も大きい値になる傾向が見えます．そこで，この傾向を直線であらわして，入力$x$と出力$y$を関係付けることを試みます．"
                                }
                            ],
                            "id": "ea665b63-1dfd-4305-9ace-c8e25ed9d86b",
                            "question": "線形回帰はどのような学習手法ですか"
                        }
                    ]
                }
            ],
            "title": "0603"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから"
                                }
                            ],
                            "id": "49875095-6280-4bad-8d88-173b33f61c64",
                            "question": "なぜノードを階層的に組むと非線形識別面を実現できるんですか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "求める関数を線形であると仮定すると，式(5.5)のように表現できます．この式の係数$\\bm{w}$を学習データから推定します．推定の基準として，式(5.5)で算出された出力と，教師信号との誤差がなるべく少なくなるようにします．誤差は式(5.5)の係数$\\bm{w}$の値によって決まるので，$E(\\bm{w})$と表現し，以下の式で求めます．ここで，扱いにくい総和演算を消すために，学習データを行列で，教師信号をベクトルであらわします．$d$次元列ベクトルの学習データ$\\bm{x}_i$を$N$個横に並べた行列を$\\bm{X}$と表し，教師信号$y_i$の値を並べた列ベクトルを$\\bm{y}$，係数を並べた列ベクトルを$\\bm{w}$とすると，誤差は以下のようになります．この値が最小になるのは，$\\bm{\\beta}$で微分した値が0となる極値をとるときなので，求める係数は以下のようになります．すなわち，二乗誤差を最小にする重み$\\bm{w}$は，学習データから解析的に求めることができます．また，学習データ数や特徴の次元数が大きく，逆行列を求めることが困難な場合は，5.3.5項で説明する確率的最急勾配法を用いて，重み$\\bm{w}$を学習します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 499,
                                    "text": "確率的最急勾配法"
                                }
                            ],
                            "id": "bc19108c-f1d5-4159-bff6-072258a04075",
                            "question": "学習データ数や特徴の次元数が大きく、逆行列を求めることが難しい場合はどのような方法を用いますか"
                        }
                    ]
                }
            ],
            "title": "0509"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 173,
                                    "text": "ReLu"
                                }
                            ],
                            "id": "ed957f7b-66c6-4e79-be0b-4800cb0bef1a",
                            "question": "勾配が消失しない関数ってなんだったっけ"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる概念の仮説を少なくし，その仮説の空間を探索することで概念を求める手法が開発されました．そのような手法として，FIND-Sアルゴリズムや，候補削除アルゴリズムがあります（図3.2）．FIND-Sアルゴリズムは，仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限します．このように，仮説に対して課す制約をバイアスとよびます．最初は，最も特殊な仮説（いかなる事例も正ではない）からスタートし，正例を一つずつ読み込んで，その事例の値を受け入れるように仮説を最低限一般化します．たとえば，表3.1のデータにおいて，最初の正例である1番のデータから，論理式「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ astigmatism=no $\\wedge$  tear-prod = reduced」が得られます．次の正例である3番のデータは，age, spectacle-prescrip, tear-prodの値はこの論理式に当てはまりますが，astigmatismの値が異なります．1番と3番のデータの両方が当てはまるようにするために，この論理式から，astigmatismの条件を取り除き，新たな仮説を「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ tear-prod = reduced」とします．これを続けると，5番のデータでspectacle-prescripの条件が落ち，9番のデータでageの条件が落ち，最後は16番のデータでtear-prodの条件まで落ちて，条件が何もなくなってしまいます．これでは，すべての入力が正例であるという概念になり，明らかにおかしな結果になってしまいました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 150,
                                    "text": "仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限"
                                }
                            ],
                            "id": "cd15d7dd-bd49-42ee-92a8-5a0b144679bb",
                            "question": "FIND-Sアルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0305"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 580,
                                    "text": "ひとまとまりの系列データを特定のクラスに識別する問題"
                                }
                            ],
                            "id": "cc1b17c6-4f15-458a-9f2f-cb8af7b039ec",
                            "question": "系列データの入力の系列長にかかわらず，出力の系列長が1である問題とはつまりどのような問題ですか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 531,
                                    "text": "最大事後確率則"
                                }
                            ],
                            "id": "37fde5af-3e77-446a-8f46-223673198c8e",
                            "question": "事後確率が最大となるクラスを識別結果とする方法を何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "式(4.4)右辺第1項の条件付き確率$P(\\bm{x} \\vert \\omega_i)$を{ゆう|ど} (likelihood) とよびます．あるクラス$\\omega_i$から特徴ベクトル$\\bm{x}$が出現する\\ruby{尤}{もっと}もらしさを表します．結論として，事後確率が最大となるクラスは，尤度と事前確率の積を最大とするクラスを求めることによって得られます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 71,
                                    "text": "あるクラス$\\omega_i$から特徴ベクトル$\\bm{x}$が出現する\\ruby{尤}{もっと}もらしさ"
                                }
                            ],
                            "id": "e7c44ed9-417c-41b4-badd-fbfbdc208695",
                            "question": "尤度とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0405"
        },
        {
            "paragraphs": [
                {
                    "context": "教師あり学習では，正解の付いた学習データを用います．このデータを訓練例とよぶこともあります．学習データは，入力データに対応するベクトル$\\bm{x}_i$と，正解情報$y_i$のペアからなります．ここで，$N$は学習データの総数，添字$i$は学習データ中の$i$番目の事例であることを示します．当面，入力ベクトル$\\bm{x}_i$は次元数$d$の固定長ベクトルであると考えておきます．図1.4の上部に示したような， (134.1, 34.6, 12.9) や， (女, 68, 165, 44, no) などが入力ベクトル$\\bm{x}_i$の例です．入力ベクトルの各要素 $x_{i1},\\dots,x_{id}$ を，特徴 (feature) あるいは 属性 (attribute)  とよびます．特徴は，数値データあるいはカテゴリカルデータのいずれかです．数値データは長さや温度などの連続値をとる場合もあれば，商品の購入個数や単語の出現回数などの離散値をとることもあります．また，カテゴリカルデータは一般に文字列として表記され，たとえば性別を表す「男・女」や天候を表す「晴・曇・雨」などのカテゴリを値とします．教師あり学習は，この学習データから，入力$\\bm{x}$を正解$y$に写像する関数$c$を学習することを目的とします．ここで$\\bm{x}$は，学習データ中の$\\bm{x}_i$に限らず，今後この関数に入力され得るすべてのデータを表しているので，関数$c(\\bm{x})$はあらゆる入力に対して正しい出力を与える理想的な写像ということになります．機械学習では，そのような理想的な写像を求める問題に対して，関数の形を扱いやすいものに仮定して，その関数のパラメータを学習データから推定するという問題に置き換えます．この推定する関数を$\\hat{c}(\\bm{x})$と記述します．関数$\\hat{c}(\\bm{x})$の実際の形は，入力ベクトル$\\bm{x}$と正解$y$の種類によって異なります．また，正解情報（あるいは関数$\\hat{c}(\\bm{x})$の出力）$y$も，数値あるいはカテゴリのいずれかになります．正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします．具体的な教師あり学習問題の説明に入る前に，性能測定基準について少し説明します．学習結果である関数$\\hat{c}(\\bm{x})$は，学習データに含まれていない未知のデータ$\\bm{x}$に対してなるべく正しい答えを出力するように一般化されなければなりません．学習データに対する正解率ではなく，未知のデータに対する正解率が重要なのです．学習データに対しては，その正解をすべて表形式で記録しておけば，間違いなく正解を出力することができます．しかし，未知データに対して正解を出力するには，「学習データの背後にある法則のようなもの」を獲得する必要があるのです．機械学習は，人間が解き方のわからない問題に対して適用するものであることを，前節で説明しました．「学習データの背後にある法則のようなもの」をいかにして獲得するか，ということが教師あり学習のテーマになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 917,
                                    "text": "正解$y$が数値の場合を回帰 (regression) 問題，カテゴリの場合を識別 (classification) 問題とよびます．回帰問題の正解をターゲット (target) ，識別問題の正解をクラス (class) とよぶこととします"
                                }
                            ],
                            "id": "08b747e7-efd1-41be-98f8-d84647fb6c27",
                            "question": "識別と回帰の違いはなんですか"
                        }
                    ]
                }
            ],
            "title": "0110"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 228,
                                    "text": "複雑な非線形変換を求めるという操作を避ける方法"
                                }
                            ],
                            "id": "5d62ea4b-73c8-4349-a289-149b2012be5a",
                            "question": "カーネルトリックとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "この与えられた系列を$\\bm{x}$として，クラス$y$（ただし，$y= B (初心者) or S (熟練者)$）の事後確率$P(y|\\bm{x})$を何らかのモデルを使って計算することを考えます．ここで式(13.4)のような$\\bm{x}, y$の同時確率を考える生成モデルアプローチをとるのが\\textbf{HMM}(Hidden Marcov Model: 隠れマルコフモデル)の考え方です．HMMは，式(13.4)の$P(\\bm{x}|y)$の値を与える確率的非決定性オートマトンの一種です．各状態であるシンボルをある確率で出力し，ある確率で他の状態(あるいは自分自身)に遷移します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 231,
                                    "text": "確率的非決定性オートマトンの一種"
                                }
                            ],
                            "id": "c44492d6-9484-423d-b1ca-e53a5a63d00f",
                            "question": "隠れマルコフモデルとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1310"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 88,
                                    "text": "データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます"
                                }
                            ],
                            "id": "f8671936-2f8d-491b-96c0-5cdafa8cee4f",
                            "question": "モデル木はどのような方法ですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 50,
                                    "text": "一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので"
                                }
                            ],
                            "id": "f7955fec-da99-4ef5-8fe2-2f80c0b132a9",
                            "question": "Aprioriアルゴリズムはなぜ計算量が膨大なのですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 495,
                                    "text": "入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない"
                                }
                            ],
                            "id": "cff3af92-345d-4e94-87c0-5b79b1a8e3a2",
                            "question": "多階層における誤差伝播法の問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "カテゴリ特徴の場合は，数値特徴の場合のような一般化は難しいのですが，カテゴリ特徴で大量に学習データが入手可能な状況というのは，ほぼ言語データの識別問題に絞られます．最も簡単なケースは図14.3のように，教師ありデータで抽出された特徴語の多くが教師なしデータに含まれる場合です．このように識別に役立つ語のオーバーラップが多いデータは教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそうです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "教師なしデータに対しても比較的高い精度でターゲットを付けることができ，口述する自己学習などを使えば，よい識別器ができそう"
                                }
                            ],
                            "id": "f6d3f42c-f815-4199-b763-8c6603065dcf",
                            "question": "オーバーラップが多いとどうなりますか"
                        }
                    ]
                }
            ],
            "title": "1404"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習では，学習に用いられるデータに正解情報が付いていません．入力ベクトル$\\bm{x}_i$の次元数に関しては，教師あり学習の場合と同様に，$d$次元の固定長ベクトルで，各要素は数値あるいはカテゴリのいずれかの値をとると考えておきます．教師なし学習は，入力データに潜む規則性を学習することを目的とします．ここで着目すべき規則性としては，2通り考えられます．一つめは，入力データ全体を支配する規則性で，これを学習によって推定するの問題がモデル推定 (model estimation)です．もう一つは，入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです．これを発見する問題がパターンマイニング (pattern mining) です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 130,
                                    "text": "入力データに潜む規則性を学習すること"
                                }
                            ],
                            "id": "abb75148-4e5b-42b8-ae02-d9d6b13f3069",
                            "question": "教師なし学習の目的はなんですか"
                        }
                    ]
                }
            ],
            "title": "0113"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 212,
                                    "text": "2006 年頃に考案された事前学習法"
                                }
                            ],
                            "id": "e126f335-d220-48cb-a140-e5f9c45cd19a",
                            "question": "階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 337,
                                    "text": "学習データに正解が付いている場合の学習"
                                }
                            ],
                            "id": "432d62d0-4f56-43a5-ae69-88e16f942520",
                            "question": "教師あり学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "その政策に従って行動したときの累積報酬の期待値で評価します"
                                }
                            ],
                            "id": "fd217e46-9f7a-4ee4-891b-1624f4aaed1a",
                            "question": "マルコフ決定過程における学習での意思決定規則である政策はどのように評価しますか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 128,
                                    "text": "多層パーセプトロンあるいはニューラルネットワーク"
                                }
                            ],
                            "id": "5210eb0b-d91e-421b-88e2-d6e958e0b81a",
                            "question": "パーセプトロンをノードとして，階層状に結合したものをなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 459,
                                    "text": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法"
                                }
                            ],
                            "id": "46c216b4-1def-4ea6-a457-e39a8ed5dc08",
                            "question": "誤差逆伝播法とは何か"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "協調フィルタリングの前提は，どの個人がどの商品を購入したかが記録されているデータがあることです．そして，新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦するというのが，基本的な考え方です．しかし，この個人別の購入データは，前節までのトランザクションデータとどうように，まばらに値が入っているデータです．購入パターンが似ているユーザを探す際に，データをベクトルとみなして，コサイン類似度による計算をおこなっても，ほとんど一致する項目数を数えているに過ぎないような状況になってしまいます．そこで，購入データをもっと低次元の行列に分解し，ユーザ・商品の特徴を低次元のベクトルで抽出する方法が考えられました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 52,
                                    "text": "新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する"
                                }
                            ],
                            "id": "b1a02159-46ed-4a67-bc40-202664b7a93f",
                            "question": "協調フィルタリングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1219"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです(図14.5)自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 171,
                                    "text": "繰り返しによって学習データが増加し，より信頼性の高い識別器ができる"
                                }
                            ],
                            "id": "3755cb32-d1ef-4a82-b739-36b22652f6a4",
                            "question": "自己学習の狙いは何ですか"
                        }
                    ]
                }
            ],
            "title": "1407"
        },
        {
            "paragraphs": [
                {
                    "context": "精度あるいは再現率のどちらかを重視する場合に，閾値を変えたときの精度と再現率の関係を見ることができれば，タスクで要求される適切な設定にすることができます．このためには，ROC曲線(ROC curve)（図2.11）を用います．ROC曲線は，横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたものです．ROC曲線は必ず原点から始まり，必ず(1,1)の点で終わります．図2.11の例では，識別器のパラメータを$\\theta < 4.3$で正例と判定するように設定すれば，すべてのテストデータが負と判定され，TPR=FPR=0となるので，この識別器はROC曲線の原点に対応します．一方，識別器のパラメータを$\\theta < 8.0$で正例と判定するように設定すれば，すべてのテストデータが正と判定され，TPR=FPR=1となるので，この識別器はROC曲線の(1,1)の点に対応します．このパラメータを4.3から8.0まで小刻みに変化させてゆくと，原点から始まり，(1,1)で終わる図2.12のようなROC曲線を描くことができます．このように，機械学習の結果は様々な評価指標やグラフを使って評価することになります．ここまで説明してきた2クラスの評価手法を多クラスに適用する場合は，クラス毎の精度や再現率を求め，そのクラスのデータ数に応じた割合を掛けることで，総合的な評価を行います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 120,
                                    "text": "横軸に false positive rate (FPR = FP/負例数), 縦軸に true positive rate (TPR = TP/正例数)をとって，閾値を変えていったときの値をプロットしたものです"
                                }
                            ],
                            "id": "bcf24d3a-c436-46a3-a350-02a3b7603db5",
                            "question": "ROC曲線とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0211"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "入力が少し変化したときに，出力も少し変化する"
                                }
                            ],
                            "id": "c92c8e91-b3f6-43a1-8f2b-9813068a345c",
                            "question": "汎化能力という点で望ましい線形回帰式の性質はなんですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "ロジスティック識別器は重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデルとみなすことができます．そして，このモデルに学習データ$D$中の$\\bm{x}_i$を入力したときの出力を$o_i$とします．望ましい出力は，正解情報$y_i$です．2値分類問題を仮定し，正例では$y_i=1$，負例では$y_i=0$とします．作成したモデルがどの程度うまく学習データを説明できているか，ということを評価する値として，尤度を式(5.11)のように定義します．正例のときは$o_i$がなるべく大きく，負例のときは$1-o_i$がなるべく大きく（すなわち$o_i$がなるべく小さく）なるようなモデルが，よいモデルだということを表現しています．尤度の最大値を求めるときは，計算をしやすいように対数尤度にして扱います．最適化問題をイメージしやすくするために，この節では，対数尤度の負号を反転させたものを誤差関数$E(\\bm{w})$と定義し，以後，誤差関数の最小化問題を考えます．これを微分して極値となる$\\bm{w}$を求めます．モデルはロジステック識別器なので，その出力である$o_i$はシグモイド関数で与えられます．シグモイド関数の微分は以下のようになります．モデルの出力は重み$\\bm{w}$の関数なので，$\\bm{w}$を変えると誤差の値も変化します（図5.7）．このような問題では，最急勾配法によって解を求めることができます．最急勾配法とは，最小化したい関数の勾配方向へ，パラメータを少しずつ動かすことを繰り返して，最適解へ収束させる方法です．この場合はパラメータ$\\bm{w}$を誤差の$E(\\bm{w})$の勾配方向へ少しずつ動かすことになります．この「少し」という量を，学習係数$\\eta$と表すことにすると，最急勾配法による重みの更新式は式(5.16)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "重み$\\bm{w}$（これ以降は，説明を簡潔にするために$\\bm{w}$は$w_0$を含みます）をパラメータとする確率モデル"
                                }
                            ],
                            "id": "2db63269-62ee-43c4-a18c-11db64864d3a",
                            "question": "ロジスティック識別器って何ですか"
                        }
                    ]
                }
            ],
            "title": "0512"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 150,
                                    "text": "高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法"
                                }
                            ],
                            "id": "80617073-9cf5-4af3-a97c-02b2b61c87cf",
                            "question": "FP-Grouthとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "ff33c63c-5ef2-4983-a1c9-ba97c39aac7d",
                            "question": "深層学習とは何か"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ラベル特徴の教師あり／なしの混合データに対する半教師あり学習のように，特徴的なラベルが同じクラスのデータで伝播するような性質がある場合は，自己学習や共訓練のような繰り返しアルゴリズムが効果を発揮します．しかし，数値特徴では繰り返しアルゴリズムがうまく働く場合と，初期の誤りがその後の結果に影響を及ぼし続けてあまりよい結果とならない場合があります．そこで，繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズムがYATSI(Yet Another Two-Stage Idea)です（図14.7）．教師ありデータを$D_l$，教師なしデータを$D_u$として，以下のアルゴリズムで重み付きk-NN識別器を作成します．ここで，$F$は教師なしデータの寄与分で，どれだけ教師なしデータの識別結果を信用するか，というパラメータです．このようにして作成した重み付きk-NN識別器でテストデータを識別します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 177,
                                    "text": "繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム"
                                }
                            ],
                            "id": "8b0ae8eb-4139-4958-8a3f-c3366b6c9d9f",
                            "question": "YATSIとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1411"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 119,
                                    "text": "形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向"
                                }
                            ],
                            "id": "f4c79101-af3f-44b1-96db-f9e93f6acda3",
                            "question": "日本語の形態素列の特徴は何ですか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える"
                                }
                            ],
                            "id": "b27bd0dc-4c98-4b5e-b5cc-efab1addae44",
                            "question": "ReLUを用いる利点はなんですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 18,
                                    "text": "非線形識別面"
                                }
                            ],
                            "id": "b166bef6-8c95-478a-9f18-faeb22533451",
                            "question": "ノードを階層的に組むとどのような識別面ができるか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 84,
                                    "text": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫"
                                }
                            ],
                            "id": "963df4c7-01c2-4f39-9946-5a71761eeec6",
                            "question": "リカレントニューラルネットワークでの勾配消失問題への対処法は何ですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとする"
                                }
                            ],
                            "id": "9fd0ef7e-e4f1-4c93-b5e6-a68d6771c602",
                            "question": "深層学習の特徴はなんですか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "53d21e29-e9e5-4d90-920f-3e9512e51fb4",
                            "question": "回帰木とはなに"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 354,
                                    "text": "入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します"
                                }
                            ],
                            "id": "3bc9c1eb-0c94-4738-ba5c-c654d0165c8e",
                            "question": "ニューラルネットワークのモデルとはどのようなものか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "式(4.4)右辺第1項の条件付き確率$P(\\bm{x} \\vert \\omega_i)$を{ゆう|ど} (likelihood) とよびます．あるクラス$\\omega_i$から特徴ベクトル$\\bm{x}$が出現する\\ruby{尤}{もっと}もらしさを表します．結論として，事後確率が最大となるクラスは，尤度と事前確率の積を最大とするクラスを求めることによって得られます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 151,
                                    "text": "尤度と事前確率の積を最大とするクラスを求めることによって得られます"
                                }
                            ],
                            "id": "1ef342d2-a840-4430-9c0b-c169c996dfd7",
                            "question": "事後確率が最大になるクラスはどうやって得られますか"
                        }
                    ]
                }
            ],
            "title": "0405"
        },
        {
            "paragraphs": [
                {
                    "context": "このような設定で，最も単純な例から始めましょう．対象とするものはK-armed banditと呼ばれる，$K$本のアームを持つスロットマシンです（図15.3）．$K$本のアームのうち，どのアームを引くかによって賞金が変わるものとします．これは，1状態，$K$種の行為，即時報酬の問題となります．学習結果は，このスロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）になります．もし，報酬が決定的であれば，学習は非常に簡単です．全ての行為を順に試みて最も報酬の高い行為を選べばよいのです．あまりにも単純ですが，今後のことを考えて学習過程を定式化しておきましょう．行為$a$の価値を$Q(a)$と定義し，学習過程によって正しい$Q(a)$の値(以後Q値といいます)が得られれば，Q値を最大とする行為が学習の結果になります．最初は，行為$a$を行ってどれだけの報酬が得られるのかわからないので，全ての$a$について$Q(a)$の値を0に初期化します．次に，可能な$a$を順番に行って($K$本のアームを順番に引いて)，そのときの報酬$r_a$を得ます．そして，各$a$について$Q(a)= r_a$として，Q値がいちばん高い$a$が求める行為になります．一方，報酬が非決定的な場合は，こんなに簡単にはゆきません．各行為$a$に対応する報酬$r$は，非決定的ですがまったくでたらめではなく，確率分布$p(r|a)$に従うと仮定します．つまり，決定的ではないが，確率的であると仮定します．ただし，この確率分布は未知だとします．そのような状況では，各アームを1回だけ引くのではなく，何度も引いて，平均的に多くの報酬が得られるアームを選ぶことになります．何回も試行することで，確率分布$p(r|a)$を推定するわけです．何度も試行して学習を行うので，定式化に時刻$t$を持ち込みます．扱いやすいように，$t$は離散的であるとして，時刻$t$で一回試行，時刻$t+1$で次の試行と続けてゆくと考えます．この場合，行為$a$の価値の時刻$t$における見積りを$Q_t(a)$とします．このQ値を時刻$t$以前での，行為$a$による報酬の平均値に一致させることを目指します．そうすると，その行為が平均的にどれぐらいうまくゆくか，ということがわかります．ただし，単純に平均値を求めるためには，それまでの行為$a$の試行回数を記憶しておかなければなりませんし，Q値はずっと変動し続けます．そこで，式(15.1)のように，時刻$t$の行為$a$による試行の報酬$r_{t+1}$と，現在のQ値との差を変動幅とし，学習係数$\\eta$をかけてQ値の更新を行います．学習率$\\eta$は最初は1以下の適当な値に設定し，時刻$t$の増加に従って減少するようにしておけば，試行を重ねてゆけばQ値が収束します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 236,
                                    "text": "全ての行為を順に試みて最も報酬の高い行為を選べばよい"
                                }
                            ],
                            "id": "42406b0f-ab9b-4a0b-bd12-8a07f77d1a54",
                            "question": "強化学習で、報酬が決定的な場合の学習はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1503"
        },
        {
            "paragraphs": [
                {
                    "context": "ラベル特徴の教師あり／なしの混合データに対する半教師あり学習のように，特徴的なラベルが同じクラスのデータで伝播するような性質がある場合は，自己学習や共訓練のような繰り返しアルゴリズムが効果を発揮します．しかし，数値特徴では繰り返しアルゴリズムがうまく働く場合と，初期の誤りがその後の結果に影響を及ぼし続けてあまりよい結果とならない場合があります．そこで，繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズムがYATSI(Yet Another Two-Stage Idea)です（図14.7）．教師ありデータを$D_l$，教師なしデータを$D_u$として，以下のアルゴリズムで重み付きk-NN識別器を作成します．ここで，$F$は教師なしデータの寄与分で，どれだけ教師なしデータの識別結果を信用するか，というパラメータです．このようにして作成した重み付きk-NN識別器でテストデータを識別します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 177,
                                    "text": "繰り返しによる誤りの増幅を避けるために，教師ありデータで一度だけ識別器を学習し，その識別器で全ての教師なしデータの識別してしまい，その結果を重み付きで利用してk-NN法による識別器を作る，という単純なアルゴリズム"
                                }
                            ],
                            "id": "b7ab83fa-1981-4cf2-b6fe-23867d66ff7f",
                            "question": "YATSIアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1411"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングはデータマイニングともよばれ，ビッグデータ活用の一つとして注目を集めているものです．パターンマイニングの応用例としては，ネットショッピングサイトなどでの「おすすめ商品」の提示や，データからの連想規則（あるいは相関規則）の抽出による新たな知見の獲得などが試みられています．この章では，まず，パターンマイニングの基本的な手法であるApriori（アプリオリ）アルゴリズムとその高速化版であるFP-Growthについて説明します．次に，問題設定を推薦システムに絞り，協調フィルタリングとMatrix Factorizationについて概説します．この章で扱う問題は，カテゴリからなる特徴ベクトルに対して，正解が付与されていない状況（すなわち「教師なし」の状況）で，そのデータに潜んでいる有用なパターンを見つけてくる，というものです（図12.1）．一般にパターンマイニングとよばれます．パターンマイニングの基本技術は頻出項目抽出です．これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．この頻出項目から，連想規則抽出をおこなうことができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 422,
                                    "text": "これは，データ集合中に一定頻度以上で現れるパターンを抽出する手法です．"
                                }
                            ],
                            "id": "099f7338-5dc5-4ab1-b9f2-83ff2fec502e",
                            "question": "頻出項目抽出とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1201"
        },
        {
            "paragraphs": [
                {
                    "context": "このようなことを考慮すると，入力が数値のベクトルである場合，半教師あり学習が可能なデータは以下の仮定を満たしていることが必要になります．\\begin{itemize}\\item 半教師あり平滑性仮定\\\\　もし二つの入力 $\\bm{x}_1$ と $\\bm{x}_2$ が高密度領域　　で近ければ，出力 $y_1$ と $y_2$ も関連している\\item クラスタ仮定\\\\　もし入力が同じクラスタに属するなら，それらは同じクラスになりやすい\\item 低密度分離\\\\　識別境界は低密度領域にある\\item 多様体仮定\\\\　高次元のデータは，低次元の多様体上に写像できる\\end{itemize}最後の仮定は，多次元でも「次元の呪い」にかかっていない，ということです．第7章で行った，多次元空間への写像の逆が成り立っているということになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 304,
                                    "text": "多次元でも「次元の呪い」にかかっていない，ということです"
                                }
                            ],
                            "id": "9a7c342a-363e-4892-87fc-e5688a5824e7",
                            "question": "多様体仮定とはどういうことですか"
                        }
                    ]
                }
            ],
            "title": "1403"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 287,
                                    "text": "内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．"
                                }
                            ],
                            "id": "b9e2f382-41c3-488b-a608-f115d59faf62",
                            "question": "LSTMと通常のユニットの違いは何ですか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 355,
                                    "text": "ユークリッド距離"
                                }
                            ],
                            "id": "e9014355-e651-4381-879e-dc43c31269e7",
                            "question": "自己写像の学習において使われる入力と出力の距離は何ですか"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 254,
                                    "text": "全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく"
                                }
                            ],
                            "id": "4656ab87-bdda-44b6-8a83-04d0d5962145",
                            "question": "分割最適化手法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．第4章で説明したように，データから直接に頻度を数えて事後確率$P(\\omega_i|\\bm{x})$を求めることはできません．そこで，識別モデルでは，この事後確率を特徴値の組み合わせから求めるようにモデルを作ります．つまり，特徴ベクトル$\\bm{x$}が与えられたときに，その$\\bm{x}$の値を用いて，何らかの方法で，出力$y$の確率分布を計算するメカニズムをモデル化します．いま，2値分類問題における特徴ベクトル$\\bm{x}=(x_1, \\dots, x_d)^T$に対して，各特徴の重み付き和$w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d$を考え，正例に関しては正の値，負例に関しては負の値を出力するように重みを調整することを考えます．ただし，これでは原点$\\bm{x} = \\bm{0}$に対して判定ができないので，定数$w_0$をパラメータとして加え，改めて$g(\\bm{x})=w_0 + w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d = w_0 + \\bm{w} \\cdot \\bm{x}$と定義します．ここで，$g(\\bm{x})=0$とおいたものは，式の形から$d$次元空間上の平面を表しています．もし，この平面が上記のようにふるまうように調整ができたとすると，この平面上にある点は，どちらのクラスとも判別がつかず，平面の正の側（$g(\\bm{x})>0$となる側）の空間に正例，平面の負の側（$g(\\bm{x})<0$となる側）の空間に負例の空間ができるはずです．このように，特徴空間上でクラスを分割する面を識別面とよびます．また，それぞれの点の判定の確からしさは，識別面からの距離に反映されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 722,
                                    "text": "特徴空間上でクラスを分割する面"
                                }
                            ],
                            "id": "13fd56f4-d7b7-45a1-9b80-a3f79754ebac",
                            "question": "識別面ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0510"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます"
                                }
                            ],
                            "id": "a092542d-bf0a-4a83-b3f5-bd7ac017c28a",
                            "question": "グリッドサーチとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 213,
                                    "text": "個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る"
                                }
                            ],
                            "id": "882b0381-1c3a-4f4c-a6d5-e5809e7306e1",
                            "question": "階層的手法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 489,
                                    "text": "代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で"
                                }
                            ],
                            "id": "dd4d7359-89b8-4942-8bcb-44362668c6e8",
                            "question": "統計的識別ってどうやってやるんですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 199,
                                    "text": "誤差が消失しません"
                                }
                            ],
                            "id": "77c077c5-db46-4c01-83ad-ca52ede7c07d",
                            "question": "ReLUを用いると誤差はどうなりますか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，通常はそう簡単にはゆきません．図14.3の例で示したような商品の評価を行う文書にしても，褒める言葉やけなす言葉は様々なバリエーションがあります．顔文字を使ったり，略語を使ったりもするでしょう．そのような場合，正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段が考えられます（図14.4）．つまり，ラベル特徴の場合，教師ありデータと教師なしデータにラベル値のオーバーラップが全く見られないデータでは，半教師あり学習は役に立ちませんが，教師なしデータの一部とでも適当なオーバーラップがあれば，その一部の教師なしデータが他の教師なしデータを徐々に巻き込んでゆく可能性があります．通常，自然言語で書かれたデータはこの後者の仮定を満たすことが多いので，半教師あり学習は文書分類問題によく適用されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 397,
                                    "text": "半教師あり学習は文書分類問題によく適用されます"
                                }
                            ],
                            "id": "10290e26-ed85-42a5-8b61-4e25118bfa64",
                            "question": "半教師あり学習は何によく使われますか"
                        }
                    ]
                }
            ],
            "title": "1405"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習とは，正解情報が付けられていないデータを対象に行う学習です．データの集合を，以下のように定義します．この章では，この特徴ベクトルの要素がすべて数値である場合を考えます．要素がすべて数値であるということは，特徴ベクトルを$d$次元空間上の点として考えることができます．そうすると，モデル推定は，特徴空間上にあるデータのまとまりを見つける問題ということになります．データがまとまっている，ということは，共通の性質をもつようにみえる，ということなので，図11.2のように，このような個々のデータを生じさせた共通の性質を持つクラスを見つけることが目標になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "正解情報が付けられていないデータを対象に行う学習"
                                }
                            ],
                            "id": "d4a2b685-9e11-4377-9762-448e080f3eab",
                            "question": "教師なし学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "1102"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 11,
                                    "text": "識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です"
                                }
                            ],
                            "id": "0bf42ae6-1a8d-4e50-b84c-41d4051fe521",
                            "question": "アンサンブル学習とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報であるクラス$y_i$のペアからなります．カテゴリ特徴との違いは，特徴ベクトルの要素が数値なので，各要素を軸とする空間を考えることができる点です．特徴の種類数を$d$個とすると，この空間は$d$次元空間になります．この空間を，特徴空間とよびます．学習データ中の各事例は，特徴空間上の点として表すことができます（図5.2）．もし，特徴抽出段階で適切な特徴が選ばれているならば，図5.3のように，学習データは特徴空間上で，クラスごとにある程度まとまりを構成していることが期待できます．そうでなければ，人間や動物が日常的に識別問題を解決できるはずがありません．このように考えると，数値特徴に対する識別問題は，クラスのまとまり間の境界をみつける，すなわち，特徴空間上でクラスを分離する境界面を探す問題として定義することができます．境界面が平面や2次曲面程度で，よく知られた統計モデルがデータの分布にうまく当てはまりそうな場合は，本章で説明する統計モデルによるアプローチが有効です．一方，学習データがまとまっているはずだといっても，それが比較的単純な識別面で区別できるほど，きれいには分かれていない場合もあります．その境界は，曲がりくねった非線形曲面になっているかもしれません．また，異なるクラスのデータが一部重なる部分がある可能性があります．そのような，非線形性を持ち，完全には分離できないかもしれないデータに対して識別を試みるには，次章以降で説明する二つのアプローチがあります．一つは，学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です．この代表的な手法が，第7章で説明するSVM（サポートベクトルマシン）です．もう一つは，非線形のモデルであらわした複雑な境界面のパラメータを，学習データとの誤差がなるべく少なくなるように調整する方法です．この代表的な手法が第8章と第9章で説明するニューラルネットワークです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 682,
                                    "text": "学習データを高次元の空間に写すことで，きれいに分離される可能性を高めておいて，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求めるという方法です"
                                }
                            ],
                            "id": "47272f24-2f3c-439c-aa64-412ae6088231",
                            "question": "SVMってなんですか"
                        }
                    ]
                }
            ],
            "title": "0502"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワークの構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合にも対応できる方法として，確率伝播による計算法があります．このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，適用することができません．そのような場合は，確率的シミュレーションも用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 247,
                                    "text": "値が真となる確率を知りたいノードが表す変数"
                                }
                            ],
                            "id": "0a3ce056-0401-4f5f-8395-304725ce2631",
                            "question": "目的変数ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0416"
        },
        {
            "paragraphs": [
                {
                    "context": "協調フィルタリングの前提は，どの個人がどの商品を購入したかが記録されているデータがあることです．そして，新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦するというのが，基本的な考え方です．しかし，この個人別の購入データは，前節までのトランザクションデータとどうように，まばらに値が入っているデータです．購入パターンが似ているユーザを探す際に，データをベクトルとみなして，コサイン類似度による計算をおこなっても，ほとんど一致する項目数を数えているに過ぎないような状況になってしまいます．そこで，購入データをもっと低次元の行列に分解し，ユーザ・商品の特徴を低次元のベクトルで抽出する方法が考えられました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 52,
                                    "text": "新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦する"
                                }
                            ],
                            "id": "aa8a9e9d-8db0-4032-bd01-6ed38d882fc8",
                            "question": "協調フィルタリングとはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1219"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 16,
                                    "text": "各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセス"
                                }
                            ],
                            "id": "13100c3f-fc54-47d1-bbc6-259502125640",
                            "question": "マルコフ決定過程における学習プロセスはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 109,
                                    "text": "音声認識・画像認識・自然言語処理など"
                                }
                            ],
                            "id": "89dda0bb-b16b-415c-8442-06baedf751d8",
                            "question": "深層学習はどのような分野で用いられていますか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 287,
                                    "text": "十分多くの層を持つニューラルネットワーク"
                                }
                            ],
                            "id": "496a0a0b-3009-428e-b4a6-ab5e971aa3ab",
                            "question": "特徴抽出を学習するには何が必要か"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "式(4.4)右辺第1項の条件付き確率$P(\\bm{x} \\vert \\omega_i)$を{ゆう|ど} (likelihood) とよびます．あるクラス$\\omega_i$から特徴ベクトル$\\bm{x}$が出現する\\ruby{尤}{もっと}もらしさを表します．結論として，事後確率が最大となるクラスは，尤度と事前確率の積を最大とするクラスを求めることによって得られます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 151,
                                    "text": "尤度と事前確率の積を最大とするクラス"
                                }
                            ],
                            "id": "f34830b2-dd73-4326-8173-9257c7fd927a",
                            "question": "事後確率が最大となるクラスは、何を求めることで得られますか"
                        }
                    ]
                }
            ],
            "title": "0405"
        },
        {
            "paragraphs": [
                {
                    "context": "関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの場合も，ベクトルの場合もあります）の対として与えられます．識別問題との違いは，正解情報$y$が数値であるということです．特に数値型の正解情報のことをターゲットとよびます．しかし，回帰と「数値特徴を入力としてクラスラベルを出力する」識別問題との境界はそれほど明確ではありません．例えば，クラスによって異なる値をとるクラス変数を導入し，入力からクラス変数の値を予測する問題と考えると，識別問題を回帰問題として考えることもできます．実際，カーネル法など共通して使われる手法も多くあり，混乱しそうになってしまうのですが，まずはここでは「数値特徴を入力として数値を出力する」手法の習得に集中し，その全体像が見えてから，他の問題との関係を考えてゆきましょう．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "正解情報$y$が数値であるということです"
                                }
                            ],
                            "id": "484831c1-c6f7-4275-a0f6-eb9e99e4e394",
                            "question": "回帰問題は識別問題とどう違うんですか"
                        }
                    ]
                }
            ],
            "title": "0602"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "2583efc0-be67-4d1d-b48b-3dedc098530d",
                            "question": "k-平均法って何ですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 688,
                                    "text": "学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります"
                                }
                            ],
                            "id": "be09222f-8ff8-491a-adf9-dcae55869cb4",
                            "question": "マージンは大きいほうがいいんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "そこで，利用できる素性を図13.3に示す組合せに限定します．つまり，出力系列で参照できる情報は一つ前のみ，入力系列は自由な範囲で参照できるとします．出力系列を参照する素性を遷移素性，入力と対応させる素性を観測素性と呼びます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "入力と対応させる素性を観測素性"
                                }
                            ],
                            "id": "c6ff4187-4823-4289-8a17-35392d71d1f7",
                            "question": "観測素性とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1304"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 499,
                                    "text": "事後確率が最大となるクラスを識別結果とする方法で"
                                }
                            ],
                            "id": "77a49a98-4345-4261-b2f0-51f92f578696",
                            "question": "最大事後確率則って何ですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 337,
                                    "text": "学習データに正解が付いている場合の学習"
                                }
                            ],
                            "id": "16117b93-02d3-4dff-9dfd-f445e5f19141",
                            "question": "教師あり学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "真のモデルとの距離"
                                }
                            ],
                            "id": "91db5a1c-855f-4170-96e0-386ff678226b",
                            "question": "バイアスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 106,
                                    "text": "SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．"
                                }
                            ],
                            "id": "8277c874-42b2-4067-a3a0-cd32688509e6",
                            "question": "グリッドサーチはなんのためにやるんですか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "cba43e23-24f7-45c1-ac7a-e3228944c9c5",
                            "question": "深層学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 19,
                                    "text": "ユニットの活性化関数を工夫する方法があります"
                                }
                            ],
                            "id": "2cee40d9-77d1-4241-b897-a726446592e1",
                            "question": "勾配消失問題を解決する手法はあるのか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "回帰"
                                }
                            ],
                            "id": "9f460f46-16b2-4107-8f59-7cc719317f92",
                            "question": "出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル推定は，入力データ中から何らかの共通点を持つデータをまとめることで，入力データを生じさせたクラスの存在や，そのパラメータを推定するものです．図1.9にモデル推定の考え方を示します．観測されたデータは，もともと何らかのクラスに属していたものが，揺らぎを伴って生成されたものと考えます．その逆のプロセスをたどることができれば，データを生成したもととなったと推定されるクラスを見つけることができます．発見されたクラスの性質は，そこから生成されたと推定されるデータを分析することでわかります．もしかしたら，その発見されたクラスは，誰も考えつかなかった性質をもつものかもしれません．このように，入力データ集合から適切なまとまりを作ることでクラスを推定する手法をクラスタリング (clustering) とよびます．顧客をクラスタリングした結果から特徴的な属性を見つけ出し，それぞれに適したマーケティングを行うような応用や，製品に対する口コミ文書をクラスタリングして，典型的な不満や要望を抽出する応用などが考えられています．一方，もともとのクラスは何らかのデータを生成する関数をもっていると仮定して，その関数のパラメータを入力データから推定する手法を密度推定  (density estimation) とよびます．密度推定はクラスタリングを発展させたものとみることができますが，その応用はクラスタリングだけでなく，不完全データを対象としたモデル推定の問題や，異常なデータの検出など，いろいろな場面で用いられています．クラスタリングの代表的な手法には，階層的クラスタリングや k-means 法があり，密度推定の手法としてはEMアルゴリズムがあります．これらを第11章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 295,
                                    "text": "入力データ集合から適切なまとまりを作ることでクラスを推定する手法"
                                }
                            ],
                            "id": "5c7ce917-cdc5-4ed3-b99a-1ae7f3e47d8e",
                            "question": "クラスタリングって何ですか"
                        }
                    ]
                }
            ],
            "title": "0114"
        },
        {
            "paragraphs": [
                {
                    "context": "関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの場合も，ベクトルの場合もあります）の対として与えられます．識別問題との違いは，正解情報$y$が数値であるということです．特に数値型の正解情報のことをターゲットとよびます．しかし，回帰と「数値特徴を入力としてクラスラベルを出力する」識別問題との境界はそれほど明確ではありません．例えば，クラスによって異なる値をとるクラス変数を導入し，入力からクラス変数の値を予測する問題と考えると，識別問題を回帰問題として考えることもできます．実際，カーネル法など共通して使われる手法も多くあり，混乱しそうになってしまうのですが，まずはここでは「数値特徴を入力として数値を出力する」手法の習得に集中し，その全体像が見えてから，他の問題との関係を考えてゆきましょう．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 108,
                                    "text": "数値型の正解情報"
                                }
                            ],
                            "id": "a8214a0c-dbd7-4180-9fcd-5f38e1b616e7",
                            "question": "ターゲットってなんですか"
                        }
                    ]
                }
            ],
            "title": "0602"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです．ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．外れ値は，学習データに含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れたデータや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に除去しておくのが望ましいデータです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 121,
                                    "text": "学習データに含まれるデータの中で，ほかと大きく異なるデータ"
                                }
                            ],
                            "id": "08a24a47-b5a6-46e4-8b3d-20e912b4a11d",
                            "question": "外れ値とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1111"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 426,
                                    "text": "入力を観測した後で計算される確率"
                                }
                            ],
                            "id": "910d1327-8c13-451b-8167-9f8308c4e690",
                            "question": "事後確率とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "$P(\\omega_i|\\bm{x})$をデータから直接推定するアプローチは，識別モデルとよばれます．識別モデルと近い考え方で，識別関数法というものがあります．これは，第1章で説明した関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法です．確率・統計的な手法が主流となる前の時代，すなわちコンピュータがそれほど高速でなく，大規模なデータを用意することが難しかった時代には，この識別関数法はパターン認識の主流の手法でした．最も古典的な識別関数法の手法は，パーセプトロンとよばれるもので，生物の神経細胞の仕組みをモデル化したものでした．以後，このパーセプトロンを多層に重ねたニューラルネットワーク（多層パーセプトロンともよばれます）について，理論的な研究が進められ，1980年代に誤差逆伝播法によって学習が可能であることが多くの研究者に認知されると，一時的にブームを向かえることになります．しかし，その後は，統計的手法の発展や，同じ識別関数法でもサポートベクトルマシンの優位性が強調されるにつれて，次第に過去の手法と見なされるようになっていました．ところが近年，第9章で紹介する深層学習が驚異的な成果を上げていることで，再度注目されるようになっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 92,
                                    "text": "関数$\\hat{c}(\\bm{x})$を，確率分布などの制約を一切考えずに，データだけに注目して構成する方法"
                                }
                            ],
                            "id": "840a57cb-09da-4242-aa01-072c78021da3",
                            "question": "識別関数法ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0505"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 173,
                                    "text": "地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出"
                                }
                            ],
                            "id": "fbe825f5-758f-4ee2-bfc5-faa784d37c71",
                            "question": "系列ラベリングの典型的な問題は何かありますか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "このような設定で，最も単純な例から始めましょう．対象とするものはK-armed banditと呼ばれる，$K$本のアームを持つスロットマシンです（図15.3）．$K$本のアームのうち，どのアームを引くかによって賞金が変わるものとします．これは，1状態，$K$種の行為，即時報酬の問題となります．学習結果は，このスロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）になります．もし，報酬が決定的であれば，学習は非常に簡単です．全ての行為を順に試みて最も報酬の高い行為を選べばよいのです．あまりにも単純ですが，今後のことを考えて学習過程を定式化しておきましょう．行為$a$の価値を$Q(a)$と定義し，学習過程によって正しい$Q(a)$の値(以後Q値といいます)が得られれば，Q値を最大とする行為が学習の結果になります．最初は，行為$a$を行ってどれだけの報酬が得られるのかわからないので，全ての$a$について$Q(a)$の値を0に初期化します．次に，可能な$a$を順番に行って($K$本のアームを順番に引いて)，そのときの報酬$r_a$を得ます．そして，各$a$について$Q(a)= r_a$として，Q値がいちばん高い$a$が求める行為になります．一方，報酬が非決定的な場合は，こんなに簡単にはゆきません．各行為$a$に対応する報酬$r$は，非決定的ですがまったくでたらめではなく，確率分布$p(r|a)$に従うと仮定します．つまり，決定的ではないが，確率的であると仮定します．ただし，この確率分布は未知だとします．そのような状況では，各アームを1回だけ引くのではなく，何度も引いて，平均的に多くの報酬が得られるアームを選ぶことになります．何回も試行することで，確率分布$p(r|a)$を推定するわけです．何度も試行して学習を行うので，定式化に時刻$t$を持ち込みます．扱いやすいように，$t$は離散的であるとして，時刻$t$で一回試行，時刻$t+1$で次の試行と続けてゆくと考えます．この場合，行為$a$の価値の時刻$t$における見積りを$Q_t(a)$とします．このQ値を時刻$t$以前での，行為$a$による報酬の平均値に一致させることを目指します．そうすると，その行為が平均的にどれぐらいうまくゆくか，ということがわかります．ただし，単純に平均値を求めるためには，それまでの行為$a$の試行回数を記憶しておかなければなりませんし，Q値はずっと変動し続けます．そこで，式(15.1)のように，時刻$t$の行為$a$による試行の報酬$r_{t+1}$と，現在のQ値との差を変動幅とし，学習係数$\\eta$をかけてQ値の更新を行います．学習率$\\eta$は最初は1以下の適当な値に設定し，時刻$t$の増加に従って減少するようにしておけば，試行を重ねてゆけばQ値が収束します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 155,
                                    "text": "スロットマシン（すなわちこの唯一の状態）で，最大の報酬を得る行為（$K$本のうちどのアームを引くか）"
                                }
                            ],
                            "id": "d5d6cf36-b4ff-4061-8ce2-4debef7d4df5",
                            "question": "K-armed banditの学習結果は何ですか"
                        }
                    ]
                }
            ],
            "title": "1503"
        },
        {
            "paragraphs": [
                {
                    "context": "こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています．ここで，確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので，式(4.6)の対数をとって計算します．式(4.7)で計算される値を対数尤度$\\mathcal{L}(D)$とよびます．この対数尤度の値は，大きければ大きいほど学習データがそのモデルから生成された確率が高い、ということがいえます．そして，学習データが，真のモデルから偏りなく生成されたものであると仮定すると，この方法で求めたモデルは真の分布に近い，と考えることができます．したがって，式(4.7)を最大にするパラメータが求まればよいわけです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので"
                                }
                            ],
                            "id": "5b4c5aa1-0eb4-4b44-82c4-b6e8874deda6",
                            "question": "何故尤度の対数をとって計算するんですか"
                        }
                    ]
                }
            ],
            "title": "0407"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "特徴ベクトルの次元数を減らすことです"
                                }
                            ],
                            "id": "d7aa637e-5a68-4372-8211-896560772a28",
                            "question": "次元削減って何ですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 80,
                                    "text": "カーネル関数"
                                }
                            ],
                            "id": "966e75e3-5593-4061-b508-23a688017c39",
                            "question": "もとの特徴空間上の2点の距離に基づいて定義される関数をなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワークの構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合にも対応できる方法として，確率伝播による計算法があります．このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，適用することができません．そのような場合は，確率的シミュレーションも用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 407,
                                    "text": "効率を求める場合や，一部のノードの値しか観測されなかった場合"
                                }
                            ],
                            "id": "8e1e917d-0036-4b32-b0fd-a4ed53a207c1",
                            "question": "どういうときに確率伝播を使うんですか"
                        }
                    ]
                }
            ],
            "title": "0416"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 495,
                                    "text": "入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない"
                                }
                            ],
                            "id": "0b838f52-766f-40cf-9d01-6e93ceded5af",
                            "question": "誤差逆伝播法の問題点は何がありますか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 358,
                                    "text": "特徴ベクトルの次元数"
                                }
                            ],
                            "id": "29d12aae-6095-4d4e-8da6-95fab7f7cbb2",
                            "question": "3階層のフィードフォワード型モデルでは，入力層をいくつ用意しますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木"
                                }
                            ],
                            "id": "040c6960-e1f8-41e9-a2ef-a829bd30adda",
                            "question": "CARTってなんですか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "識別面としてすべての学習データを識別できるという式(7.5)の条件を加えます．$y_i=1~or~-1$としていたので，正例・負例両方の制約を一つの式で表すことができました．ここまでで，マージンを最大にする識別面を求める問題の定式化が終わりました．式(7.5)の制約下での$||\\bm{w} ||^2$の最小化問題になりました．この後，微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 168,
                                    "text": "微分を利用して極値を求めて最小解を導くので，乗数$1/2$を付けておきます"
                                }
                            ],
                            "id": "7a1f2f7b-c830-4631-ad83-e82cb45f1afc",
                            "question": "目的関数と距離の式が違うのはなぜですか"
                        }
                    ]
                }
            ],
            "title": "0703"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはありません．仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します．決定木とは，データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現です．正例のリーフに到るノードの分岐の値をAND条件で結合し，それらをさらにOR条件で結合することで等価な論理式に変換できますが，木構造のままの方が，人間の目から見て学習結果がわかりやすいので，こちらの表現が好まれます．コンタクトレンズデータ (contact-lens.arff)（表3.1）から作成した決定木の例を図3.3に示します．図3.3の木では特徴tear-prod-rate（涙量）が最初の質問で，この値がreduced（減少）であると，コンタクトレンズは勧められない，という結論になります．この値がnormal（正常）であれば，次の特徴astigmatism（乱視）を調べる，という手順になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 171,
                                    "text": "データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現"
                                }
                            ],
                            "id": "da1c9f38-1bbc-4c9e-8753-8368ca7a6711",
                            "question": "決定木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0307"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "バギングやランダムフォレストでは，用いるデータ集合を変えたり，識別器を構成する条件を変えたりすることによって，異なる識別器を作ろうとしていました．これに対して，ブースティング (Boosting) では，誤りを減らすことに特化した識別器を，次々に加えてゆくという方法で，異なる振る舞いをする識別器の集合を作ります"
                                }
                            ],
                            "id": "7c1366ca-f2b2-41d1-9d1c-269ae83cceb2",
                            "question": "バギングやランダムフォレストとブースティングではどのような点で異なりますか"
                        }
                    ]
                }
            ],
            "title": "1010"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 16,
                                    "text": "各状態でどの行為を取ればよいのかという意思決定規則"
                                }
                            ],
                            "id": "252da6c7-aff7-4c38-9c49-aa5ee2eaec37",
                            "question": "政策とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，特徴数削減の手法として，主成分分析 (principal component analysis: PCA) を紹介します．図2.5に，2次元から1次元への削減を例として，主成分分析の考え方を示します．主成分分析とは，相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作です．次元削減の対象である高次元特徴空間上にデータがどのように散らばっているかという情報は，もとのデータの統計的性質をあらわす共分散行列によって表現することができるので，この共分散行列の情報を基にして，低次元空間への写像をおこなう行列を作ってゆきます．学習データ$\\{\\bm{x} | \\bm{x} \\in D\\}$の共分散行列$\\Sigma$は式(2.1)を用いて計算されます．ここで，$\\bm{\\mu}$は$D$の平均ベクトル，$N$は$D$の要素数です．平均ベクトル$\\bm{\\mu}$は式(2.2)を用いて計算されます．図2.5左上に示すような2次元データの場合，平均ベクトルを$\\bm{\\mu}=(\\bar{x_1}, \\bar{x_2})^T$とすると，共分散行列$\\Sigma$は式(2.3)のようになります．対角成分は，次元ごとの散らばり具合を表す分散に対応し，非対角成分は次元間の相関を表します．次に，この共分散行列の固有値と固有ベクトルを求めると，固有値の大きい順にその対応する固有ベクトルの方向が，データの散らばりが大きい（すなわち，識別するにあたって情報が多い）方向となります．固有ベクトルどうしは直交するので，固有値の大きい順に軸として採用し，特徴空間を構成すると，たとえば上位$n$位までなら$n$次元空間が構成でき，これらはもとの多次元特徴空間のデータの散らばりを最もよく保存した$n$次元空間ということになります．特徴空間の次元数が下がれば下がるほど，学習において推定するべきパラメータ数が少なくなるので，学習結果の信頼性が高まります．もっとも，もとのデータの情報が大きく損なわれるほどに次元を削減してしまっては意味がないので，そのあたりの調整は難しいところです．主成分分析によって構成した軸では，対応する固有値が分散になるので，「すべての軸の固有値の和」に対する「採用した軸の固有値の和」の比（累積寄与率）を計算することで，次元削減後の空間が，もとのデータの情報をどの程度保存しているのか，見当をつけることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 113,
                                    "text": "相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作"
                                }
                            ],
                            "id": "fa67046f-dc88-44e2-994b-c9105f64b99c",
                            "question": "主成分分析とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0205"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ"
                                }
                            ],
                            "id": "57cae501-c01e-4bc7-a891-e1b16c128333",
                            "question": "ソフトマージンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0707"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\bm{w}$は$d+1$次元の重みベクトルとします．また，$\\eta$は学習係数で，適当な小さい値を設定します．このアルゴリズムは，学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します．これをパーセプトロンの収束定理とよびます．一方，学習データが線形分離不可能な場合にはこのアルゴリズムを適用することができません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合はこのアルゴリズムは停止しません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します"
                                }
                            ],
                            "id": "b594fc32-f31a-4b47-9824-b7f248d4631c",
                            "question": "パーセプトロンの学習アルゴリズムはどのようなときに停止しますか"
                        }
                    ]
                }
            ],
            "title": "0507"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "Grid search"
                                }
                            ],
                            "id": "8dc220b5-ccdf-4f7a-b6e4-53bcdc185127",
                            "question": "パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求める方法はなに"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 507,
                                    "text": "適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法"
                                }
                            ],
                            "id": "b150c708-c6a1-4a8f-9089-b6f4b1fb5c4a",
                            "question": "決定木ではどうやって過学習を回避しますか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で説明したAprioriアルゴリズムの問題点として，やはり計算量が膨大であることが挙げられます．一般にパターンマイニングの対象であるトランザクションデータは膨大で，アルゴリズム中に(候補数)×(トランザクション数) のループがあるので，この部分をなんとか減らさなければ高速化はできません．そこで，高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法が考えられました．この手法をFP-Growthアルゴリズムとよびます．トランザクションデータをコンパクトにする手段として，まず特徴を出現頻度順に並べ替えます．そして頻度の高い特徴から順にその情報をまとめると，「商品Aの購入が100件，そのうち商品Bも同時に購入が40件，商品Cも同時に購入が30件，...」のように多数のトランザクションの情報を手短に表現できます．ただし，このような自然言語による表現はプログラムによる処理に向かないので，この情報を木構造で保持します．ここまでの手順を，例を用いて説明します．まず，以下のようなトランザクション集合を学習データとします．ここで出現する文字は特徴名で，出現しているときはその値がtとなっているものとします．次に，特徴をその出現頻度順にソートし，出現頻度が低い特徴（ここでは2回以下しか現れないもの）をフィルタにかけて消去します．出現頻度が低い特徴は，Aprioriアルゴリズムでの頻出項目抽出や連想規則抽出でも，余計な候補を生成しないために最初に除かれていました．ソート，フィルタリング後の結果は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 150,
                                    "text": "高速化のアイディアとして，トランザクションデータをコンパクトな情報に変換し，そのコンパクトな情報に対してパターンマイニングを行うという手法"
                                }
                            ],
                            "id": "0c4b046d-106e-4c7b-90af-a9f42bf4fdd3",
                            "question": "FP-Growthアルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1214"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "fde8444f-51a7-4e56-acdc-0e1d526a0d49",
                            "question": "深層学習とはどのようなものか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章の決定木は，ある事例がある概念に，当てはまるか否かだけを答えるものでした．しかし，たとえば病気の診断のように，その答えがどれだけ確からしいか，ということを知りたい場合も多くあります．学習データの統計的性質に基づいて，ある入力があるクラスに分類される確率を求め，最も確率が高いクラスを出力とする方法が，統計的識別手法です．本節と次節では，第3章でも取り上げたweather.nominalデータを例に，統計的識別の考え方を説明します．次に，入力$\\bm{x}$が観測されたとします．この観測によって，事前確率からのみ判断した結果とは異なる結果が導き出される場合があります．たとえば，$\\bm{x}$が悪天候を示唆しているならば，ゴルフをする確率は下がると考えられます．入力$\\bm{x}$が観測されたときに，結果がクラス$\\omega_i$である確率を，条件付き確率 $P(\\omega_i \\vert \\bm{x})$ で表現します．この確率は，入力を観測した後で計算される確率なので，事後確率 (posterior probability) とよびます．統計的識別手法の代表的な方法は，この事後確率が最大となるクラスを識別結果とする方法で，この決定規則を最大事後確率則(maximum a posteriori probability rule)とよびます．式(4.1)は，事後確率最大のクラス$C_{\\mbox{\\scriptsize{MAP}}}$を求める式です．それでは，この事後確率の値を学習データから求める方法を考えてゆきましょう．一般的な条件付き確率の値は，条件部（縦棒の右側）が一致するデータをできるだけ多く集め，結論部（縦棒の左側）の頻度を求めることによって推定できます．weather.nominalデータの場合では，たとえば (sunny, hot, high, FALSE) という特徴をとるデータを大量に集めてきて，その中でyesが何回，noが何回出現したという頻度を得て，その頻度から条件付き確率値を推定します．そして，この推定をあらゆる特徴値の組合せについておこないます．しかし，weather.nominalデータをみると，特徴値の組み合わせのうちのいくつかが1回ずつ出てきているだけで，可能な特徴値の組み合わせの大半は表の中に出てきません．これでは条件付き確率の推定はできません．そこで，この事後確率の値を直接求めるのではなく，式(4.2)に示すベイズの定理を使って，より求めやすい確率値から計算します．式(4.1)にベイズの定理を適用すると，式(4.3)が得られます．ここで，式(4.3)の右辺の分母は，クラス番号$i$を変化させても一定なので，右辺全体が最大となるクラス番号$i$を求める際には，その値を考慮する必要がありません．したがって，事後確率を最大とするクラス番号$i$を求める式は，式(4.4)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 153,
                                    "text": "統計的識別手法"
                                }
                            ],
                            "id": "764b8d02-43fb-4b08-8980-52cd24c45303",
                            "question": "最も確率が高いクラスを出力とする手法はなに"
                        }
                    ]
                }
            ],
            "title": "0402"
        },
        {
            "paragraphs": [
                {
                    "context": "最小値では$L$の勾配が0になるはずなので，以下の式が成り立ちます．これを式(7.9)に代入して，以下の式を得ます．次はこれを最小化するわけですが，これは$\\alpha_i$に関する2次計画問題と呼ばれるものです．したがって，Scilabなどの数値計算ソフトウェアを使って簡単に解くことができます．これを解くと，$\\alpha_i \\neq 0$となるのは，サポートベクトルに対応するもののみで，大半は$\\alpha_i = 0$となります．この$\\alpha_i$を式(7.11)に代入して，$\\bm{w}$を得ることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 113,
                                    "text": "Scilabなどの数値計算ソフトウェア"
                                }
                            ],
                            "id": "f49a271e-ed21-4df5-8ad5-0944ad5125ef",
                            "question": "マージンを最大とする識別面の方法はどのようなソフトで計算できるのか"
                        }
                    ]
                }
            ],
            "title": "0705"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 65,
                                    "text": "政策"
                                }
                            ],
                            "id": "2340051f-7fb6-464c-b611-86d4be18fc12",
                            "question": "マルコフ決定過程における学習での意思決定規則のことを何と呼びますか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 409,
                                    "text": "この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます"
                                }
                            ],
                            "id": "b8f1e20d-8469-4887-9985-bc6462b35921",
                            "question": "確信度の値からどのようなことがわかりますか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 188,
                                    "text": "グリッドを設定して，一通りの組み合わせで評価実験をおこないます"
                                }
                            ],
                            "id": "f5973966-0a5e-450b-b837-53a29692368e",
                            "question": "SVMのハイパーパラメータの設定はどのように行いますか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 4,
                                    "text": "もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．"
                                }
                            ],
                            "id": "ad680006-69ae-4421-9b46-2f6823d15a69",
                            "question": "カーネル関数ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "148d9b71-9768-4a21-9c9e-8762b516354d",
                            "question": "DNNってなんですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題です過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習することが目標です．回帰問題は，正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題と定義できます(図6.1)．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 56,
                                    "text": "過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習することが目標"
                                }
                            ],
                            "id": "45471b06-2a95-4eac-ad58-45e7abbedeb0",
                            "question": "回帰問題の学習目的はなんですか"
                        }
                    ]
                }
            ],
            "title": "0601"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 164,
                                    "text": "LSTM"
                                }
                            ],
                            "id": "dffc03d1-a9d5-4fd2-a560-98b11d019e52",
                            "question": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 191,
                                    "text": "隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので"
                                }
                            ],
                            "id": "d2017b2a-b327-430b-b455-94bfa78b6cbf",
                            "question": "多階層ニューラルネットワークにおいて、3階層ニューラルネットワークに1層加えて非線形にすることが十分でないのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行える"
                                }
                            ],
                            "id": "29537b4e-3b2f-46d4-94d7-efd9e55075e9",
                            "question": "ReLu関数の良さは何ですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 37,
                                    "text": "入力に近い側の処理で，特徴抽出をおこなおうとするものです．"
                                }
                            ],
                            "id": "fe8d40f8-ed34-4940-aba7-2b98096391e1",
                            "question": "多階層ニューラルネットワークにおける特徴抽出の場所はどのあたりか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムで用いた情報獲得量は，値の種類が多い特徴ほど大きな値になる傾向があります．一般に，その性質は悪いものではないのですが，値の種類が極端に多い場合には問題があります．例えば表3.3の特徴として，日付(date)があったとし，その値が全てのデータで異なっているとします．この場合，dateによって分割した集合は要素数が1となって，そのエントロピーは0となりますので，$\\mbox{Gain}(D, date)$の値は最大値である$E(D)$になって，この特徴が決定木のルートに選ばれることになります．こうして出来た決定木ではテスト例は分類できません．そこで，分割の程度を式(3.3)によって評価し，分割が少ない方が有利になるように式(3.4)で定義された獲得率を用いて特徴を選択することもあります．また，学習データの性質や学習の目的によって，データの乱雑さを評価する基準も変化することがあります．データの乱雑さを不純度(impurity)と定義すると，先述のエントロピー以外にいくつかの可能性を考えることができます．式(3.5)で計算されるGini Inpurityは，分割後のデータの分散を表します．この性質は回帰木の作成で用いますので，そこで再度，解説します．また，Gini Inpurityの平方根を取って，最大値がGini Inpurityと同じ0.5になるように係数を補正したものをRootGiniImpurityとして，式(3.6)で定義します．いずれも，分割前後の値の差によって選ぶ特徴を決めるのですが，獲得率やジニ不純度は，正例・負例の数に偏りがあると，多数派の性能の影響が大きくなってしまいます．一方，RootGiniImpurityは，分割前のGini Inpurityと，分割後の重み付きGini Inpurityの比を計算していることになり，正例・負例の数に偏りがあっても，分割基準としては影響を受けないようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 491,
                                    "text": "分割後のデータの分散"
                                }
                            ],
                            "id": "8a071891-10ff-420a-bee4-5ee43a76b6b4",
                            "question": "ジニ不純度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0315"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，複数の状態を持つ問題に拡張しましょう．図15.4のような迷路をロボットRが移動するという状況です．ゴールGに着けば，報酬が得られます．単純なケースでは報酬は決定的（ゴールに着けば必ずもらえる）で，部屋の移動にあたる状態遷移も決定的（必ず意図した部屋に移動できる）です．問題を一般化して，報酬や遷移が確率的である場合も想定できます．これらが確率的になる原因として，例えばロボットのゴールを探知するセンサーがノイズで誤動作をしたり，路面状況でスリップが生じるなどの不確定な要因で行為が成功しない状況が考えられます．これらは，非決定的であるとはいえ，学習中に状況が変化してしまうとどうしようもないので，この非決定性が確率的であるとし，確率分布は学習期間中を通じて一定であるとします．このような問題は，以下のようなマルコフ決定過程として定式化することができます．\\begin{itemize}\\item 時刻$t$における状態$s_t \\in S$\\item 時刻$t$における行為$a_t \\in A(s_t)$\\item 報酬 $r_{t+1} \\in R$，確率分布$p(r_{t+1}|s_t, a_t)$\\item 次状態$s_{t+1} \\in S$，確率分布$P(s_{t+1}|s_t, a_t)$\\end{itemize}マルコフ決定過程は，「マルコフ性」を持つ確率過程における意思決定問題で，「マルコフ性」とは次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質です．ここでは，報酬と次状態への遷移の確率が現在の状態と行為のみに依存しているという定式化になっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 614,
                                    "text": "次状態での事象の起こる確率は現在の状態だけから決まり，過去の状態には依存しないという性質"
                                }
                            ],
                            "id": "c4566b5f-d0b7-42c5-a8b5-8811e23152c4",
                            "question": "マルコフ性とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1505"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはありません．仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します．決定木とは，データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現です．正例のリーフに到るノードの分岐の値をAND条件で結合し，それらをさらにOR条件で結合することで等価な論理式に変換できますが，木構造のままの方が，人間の目から見て学習結果がわかりやすいので，こちらの表現が好まれます．コンタクトレンズデータ (contact-lens.arff)（表3.1）から作成した決定木の例を図3.3に示します．図3.3の木では特徴tear-prod-rate（涙量）が最初の質問で，この値がreduced（減少）であると，コンタクトレンズは勧められない，という結論になります．この値がnormal（正常）であれば，次の特徴astigmatism（乱視）を調べる，という手順になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 171,
                                    "text": "データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造"
                                }
                            ],
                            "id": "65634bf1-5d6a-4901-9ae2-36ace181508f",
                            "question": "決定木とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0307"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，あるいはまったく与えられていないかのいずれかでした．その中間的な設定として，学習データの一部にだけ正解が与えられている場合が考えられます．学習データに正解を与えるのは人間なので，正解付きのデータを作成するにはコスト（費用・時間）がかかります．一方，正解なしのデータならば，容易にかつ大量に入手可能であるという状況があります．たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります．このような状況で，正解付きデータから得られた識別器の性能を，正解なしデータを使って向上させる問題を半教師あり学習 (semi-supervised learning) といいます．半教師あり学習は主として識別問題に対して用いられます．半教師あり学習の代表的な手法のアイディアを図1.11に示します．図1.11左のように，全データの中で正解の付加されたデータを丸・バツで表し，正解のないデータを三角形で表します．最初は丸・バツが付いたデータだけから識別器を作り，たとえば，その中間あたりに境界直線を引いたものとします．これに従って三角形のデータを分類しますが，境界線近辺のデータはあまり信用せず，境界線から大きく離れたものを確信度が高いとみなして正解を付与します．今度は，これらの新しく正解を付与されたデータも加えて，再度識別境界を計算します．これを，新しい正解付きデータが増えなくなるまで繰り返します．この学習法は，識別するべきクラスがうまくまとまっているようなデータや，識別結果によって有効な特徴が増えてゆくような，やや特殊なデータに対して適用するときにうまくゆきます．この手法を第14章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 206,
                                    "text": "たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります"
                                }
                            ],
                            "id": "66efff8f-db07-4b74-abd0-56b969b5cb46",
                            "question": "半教師あり学習に適した状況はどのように作られますか"
                        }
                    ]
                }
            ],
            "title": "0117"
        },
        {
            "paragraphs": [
                {
                    "context": "また，パラメータ$\\bm{w}$の絶対値を正則化項とするものをLasso回帰とよびます．一般に，Lasso回帰は値を0とするパラメータが多くなるように正則化されます．英単語のlassoは「投げ縄」という意味で，投げ縄回帰と訳されることがあります．多くの特徴がひしめき合っている中に投げ縄を投げて，小数のものを捕まえるというイメージをもってこのように呼ばれているのかもしれませんが，Lassoのオリジナルの論文では，Lasso は least absolute shrinkage and selection operator の意味だと書かれています．Lasso回帰に用いる誤差評価式を，式(6.9)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ値を0とする重みが多くなります．Lasso回帰の解は，原点で微分不可能な絶対値を含むため，最小二乗法のように解析的に求めることはできません．そこで，正則化項の上限を微分可能な二次関数で押さえ，その二次関数のパラメータを誤差が小さくなるように繰り返し更新する方法などが提案されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 3,
                                    "text": "パラメータ$\\bm{w}$の絶対値を正則化項とするものをLasso回帰"
                                }
                            ],
                            "id": "fc06a5f0-d96b-4c79-9aa9-e5c39fa5686d",
                            "question": "Lasso回帰ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0607"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 106,
                                    "text": "識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します"
                                }
                            ],
                            "id": "24c028c0-c109-46c9-9f75-28b83e8d0a09",
                            "question": "Gini Impurityってなんですか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，事前確率 (prior probability) とよびます．クラス$\\omega_i$の事前確率は$P(\\omega_i) ~~ (i=1,\\dots,c)$ （ただし$c$はクラス数）と表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 303,
                                    "text": "入力を観測する前にもっているそれぞれのクラスの起こりやすさ"
                                }
                            ],
                            "id": "d67e9962-2c1a-4f59-b146-928a9bd562ae",
                            "question": "事前確率とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0404"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，写像後の空間での識別関数は以下のように書くことができます．ここで，SVMを適用すると，$\\bm{w}$は，式(7.11)のようになるので，以下の識別関数を得ることになります．同様に，式(7.12)より，学習の問題も以下の式を最大化するという問題になります．ここで注意すべきなのは，式(7.20), (7.21)のどちらの式からも$\\phi$が消えているということです．カーネル関数$K$さえ定まれば，識別面を得ることができるのです．このように，複雑な非線形変換を求めるという操作を避ける方法をカーネルトリックとよびます．これがSVMがいろいろな応用に使われてきた理由です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 206,
                                    "text": "識別面"
                                }
                            ],
                            "id": "61c7cb7f-10c2-4a73-8894-f658ec64738e",
                            "question": "カーネル関数が定まれば何が得られますか"
                        }
                    ]
                }
            ],
            "title": "0715"
        },
        {
            "paragraphs": [
                {
                    "context": "関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの場合も，ベクトルの場合もあります）の対として与えられます．識別問題との違いは，正解情報$y$が数値であるということです．特に数値型の正解情報のことをターゲットとよびます．しかし，回帰と「数値特徴を入力としてクラスラベルを出力する」識別問題との境界はそれほど明確ではありません．例えば，クラスによって異なる値をとるクラス変数を導入し，入力からクラス変数の値を予測する問題と考えると，識別問題を回帰問題として考えることもできます．実際，カーネル法など共通して使われる手法も多くあり，混乱しそうになってしまうのですが，まずはここでは「数値特徴を入力として数値を出力する」手法の習得に集中し，その全体像が見えてから，他の問題との関係を考えてゆきましょう．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "正解情報$y$が数値であるということです"
                                }
                            ],
                            "id": "0400627a-fbad-4b68-a13c-b707c9f62975",
                            "question": "回帰問題が識別問題と異なるのはどのような点ですか"
                        }
                    ]
                }
            ],
            "title": "0602"
        },
        {
            "paragraphs": [
                {
                    "context": "候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です"
                                }
                            ],
                            "id": "b01b3c96-46c8-41ff-88d8-ed5451d0c703",
                            "question": "候補削除アルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0306"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，もう一度教師あり学習の設定に戻って，系列データを識別する手法について説明します．系列データとは，個々の要素の間に i.i.d. の関係が成立しないものです．系列データの識別をおこなうモデルを学習するためには，一部教師なし学習の要素が入ってくる場合があるので，この順序で説明することとしました．入力の系列長と出力の系列長との関係を整理すると，系列データの識別問題は，以下の三つのパターンに分類されます．\\begin{enumerate}\\item 入力の系列長と出力の系列長が等しい問題\\item 入力の系列長にかかわらず，出力の系列長が1である問題\\item 入力の系列長と出力の系列長の間に，明確な対応関係がない問題\\end{enumerate}1.は，単語列を入力して，名詞や動詞などの品詞の列を出力する，いわゆる形態素解析処理が典型的な問題です．一つの入力（単語）に一つの出力（品詞）が対応します．この問題の最も単純な解決法としては，これまでに学んだ識別器を入力に対して逐次適用してゆくというものが考えられます．しかし，ある時点の出力がその前後の出力や，近辺の入力に依存している場合があるので，そのような情報をうまく使うことができれば，識別率を上げることができるかもしれません．この問題が本章の主題の一つである系列ラベリング問題です．2.は，ひとまとまりの系列データを特定のクラスに識別する問題です．たとえば動画像の分類や音声で入力された単語の識別などの問題が考えられます．最も単純には，入力をすべて並べて一つの大きなベクトルにしてしまうという方法が考えられますが，入力系列は一般に時系列信号でその長さは不定なので，そう簡単にはゆきません．また，典型的には入力系列は共通の性質を持ついくつかのセグメントに分割して扱われますが，入力系列のどこにそのセグメントの切れ目があるかという情報は，一般には得られません．そこで，このセグメントの区切りを隠れ変数の値として，その分布を教師なしで学習するという手法と，通常の教師あり学習を組み合わせることになります．これが系列認識問題です．3.は連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります．この問題は学習にも認識にも相当込み入った工夫が必要になるので，本章ではその概要のみ説明します．音声認識の詳細に関しては，拙著\\cite{araki15}をご覧ください．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 899,
                                    "text": "連続音声認識が典型的な例で，早口かゆっくりかによって単位時間当たりの出力単語数が異なります"
                                }
                            ],
                            "id": "80141250-fd55-4fc2-b1d5-4a9a866ca5d1",
                            "question": "入力の系列長と出力の系列長の間に明確な対応関係がないとはどういうことですか"
                        }
                    ]
                }
            ],
            "title": "1301"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 512,
                                    "text": "この値が高いほど，得られる情報の多い規則であること"
                                }
                            ],
                            "id": "ce16860a-6d80-40ac-a51b-345d849739ab",
                            "question": "リフト値の値からどのようなことがわかりますか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 44,
                                    "text": "畳み込みニューラルネットワーク"
                                }
                            ],
                            "id": "0cbf1ed3-c2c3-480a-93f9-955eff27bb13",
                            "question": "畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものは何ですか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 406,
                                    "text": "勾配消失問題"
                                }
                            ],
                            "id": "2e59f5db-d7a7-4a2d-ae7a-042f97a439fc",
                            "question": "ディープニューラルネットワークの性能が向上しない原因はなんですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "第14章と第15章は，教師あり学習と教師なし学習のどちらでもない学習手法について説明します（図14.1）．この「どちらでもない」ということが何を意味かを正確に定義するのは難しい問題です．今まで扱ってきた問題を単純化すると，手元の全ての入力例に対して望ましい出力が付けられている状況で，入力から出力へ写像する関数を獲得するという設定か，あるいはとにかく大量のデータがある状況で，それらに内在する性質を発見するという設定のいずれかで，この2つには当てはまらないけれどデータを活用したい，という状況は現実には様々な設定でありそうです．まず，典型的な「どちらでもない」状況は，教師信号が一部の学習データにのみ与えられている状況です．例えば，特定の製品について書かれたブログエントリやツイートなどのWeb文書に対して，肯定的／否定的の分類を行いたいという問題設定を考えます．クローラープログラムを使えば，その製品名を含むWeb文書を多数入手することができます．しかし，それらに識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません．すなわち，少量のターゲット付きデータと，大量のターゲット無しデータがある状況で識別器を構成するという設定になります．このような状況を半教師あり学習と呼び，本章でその手法を検討してゆきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 433,
                                    "text": "識別対象のターゲット（肯定的／否定的）を付けるのは手間の掛かる作業なので，現実的には集めた文書の一部にしかターゲットを与えることができません"
                                }
                            ],
                            "id": "820c3663-5e8e-4f91-92c6-246cefa5306c",
                            "question": "半教師あり学習は実用上でなぜ必要なのですか"
                        }
                    ]
                }
            ],
            "title": "1401"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから"
                                }
                            ],
                            "id": "9dd9f92f-0173-4f50-a1d4-93699f414189",
                            "question": "なぜノードを階層的に組むと非線形識別面が実現できるのか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．第4章で説明したように，データから直接に頻度を数えて事後確率$P(\\omega_i|\\bm{x})$を求めることはできません．そこで，識別モデルでは，この事後確率を特徴値の組み合わせから求めるようにモデルを作ります．つまり，特徴ベクトル$\\bm{x$}が与えられたときに，その$\\bm{x}$の値を用いて，何らかの方法で，出力$y$の確率分布を計算するメカニズムをモデル化します．いま，2値分類問題における特徴ベクトル$\\bm{x}=(x_1, \\dots, x_d)^T$に対して，各特徴の重み付き和$w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d$を考え，正例に関しては正の値，負例に関しては負の値を出力するように重みを調整することを考えます．ただし，これでは原点$\\bm{x} = \\bm{0}$に対して判定ができないので，定数$w_0$をパラメータとして加え，改めて$g(\\bm{x})=w_0 + w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d = w_0 + \\bm{w} \\cdot \\bm{x}$と定義します．ここで，$g(\\bm{x})=0$とおいたものは，式の形から$d$次元空間上の平面を表しています．もし，この平面が上記のようにふるまうように調整ができたとすると，この平面上にある点は，どちらのクラスとも判別がつかず，平面の正の側（$g(\\bm{x})>0$となる側）の空間に正例，平面の負の側（$g(\\bm{x})<0$となる側）の空間に負例の空間ができるはずです．このように，特徴空間上でクラスを分割する面を識別面とよびます．また，それぞれの点の判定の確からしさは，識別面からの距離に反映されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 722,
                                    "text": "特徴空間上でクラスを分割する面"
                                }
                            ],
                            "id": "1cc0dadb-13eb-4848-9929-82224314f233",
                            "question": "識別面とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0510"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 154,
                                    "text": "異なったまとまり間の距離はなるべく遠くなるように"
                                }
                            ],
                            "id": "93b6b2d4-b019-40d2-a1f8-92a92b1c4cb7",
                            "question": "外的分離とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "機械学習の出番は，簡単には規則化できない複雑なデータが大量にあり，そこから得られる知見が有用であることが期待されるときです．このような大量のデータは，ビッグデータ (big data)とよばれます．ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです．この大量・多様なデータから規則性を抽出したり，データを分類するモデルを獲得することで，購買記録からのお勧め商品提示のようなおなじみの機能に加えて，不審者の行動パターンの検出や，インフルエンザの流行の予想など，これまでになかったサービスや機能を実現することもできます（図1.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 99,
                                    "text": "ビッグデータの例としては，人々がブログやSNS (social networking service) に投稿する文章・画像・動画，スマートフォンなどに搭載されているセンサから収集される情報，オンラインショップやコンビニエンスストアの販売記録・IC乗車券の乗降記録など，多種多様なものです"
                                }
                            ],
                            "id": "f42156ef-004b-4d9f-bcee-e9e7ee05cc0f",
                            "question": "機械学習で用いられるビッグデータとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0103"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "4be93564-ebb2-4b67-97bd-651de92108f5",
                            "question": "回帰木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 41,
                                    "text": "音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります"
                                }
                            ],
                            "id": "8335eefc-feab-46df-a4c9-3e171ece9a8a",
                            "question": "識別ではどのようなことができますか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります"
                                }
                            ],
                            "id": "7295c1d4-5605-45fe-b4c4-c054e4f9b043",
                            "question": "最急勾配法の問題点の対処法は何がありますか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 227,
                                    "text": "後に得られる報酬ほど割り引いて計算するための係数"
                                }
                            ],
                            "id": "69278199-da19-4f66-b2a8-7d7a841a41fd",
                            "question": "割引率とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "共訓練の特徴は，学習初期の誤りに強いということが挙げられます．欠点としては，それぞれが識別器として機能し得る異なる特徴集合を見つけるのが難しいことと，場合によっては全特徴を用いた自己学習の方が性能がよいことがあること，などです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 8,
                                    "text": "学習初期の誤りに強いということ"
                                }
                            ],
                            "id": "3637baf3-a8d4-4606-8195-ec3b2f6f0c5f",
                            "question": "共訓練の特徴は何ですか"
                        }
                    ]
                }
            ],
            "title": "1410"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 64,
                                    "text": "全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています"
                                }
                            ],
                            "id": "6df5525c-52cc-49ce-998b-8b2e767b5901",
                            "question": "最急勾配法の問題点とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "CART(classification and regression tree)は，木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた決定木です．Gini Impurityは識別問題にCARTを用いるときの分類基準で，式(6.12)を用いて分類前後の集合のGini Impurity $G$を求め，式(6.13)で計算される改善度$\\Delta G$が最も大きいものをノードに選ぶことを再帰的に繰り返します．ただし，$T$はあるノードに属する要素の全体，$N(j)$は要素中のクラス$j$の割合，$T_L$は左の部分木，$P_L$は$T_L$に属するデータの割合（$L$を$R$に変えたものも同様）を示します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "木の構造を二分木に限定し分類基準としてGini Impurity （ジニ不純度）を用いた"
                                }
                            ],
                            "id": "6e078c9f-b6a2-4023-b64e-18c8edade8cc",
                            "question": "CARTは回帰木とどう違いますか"
                        }
                    ]
                }
            ],
            "title": "0612"
        },
        {
            "paragraphs": [
                {
                    "context": "Q値を推定する方法は，モデルの関する知識の前提によって大きく2つに分類されます．環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合，Q値は，動的計画法の考え方を用いて求めることができます．この方法をモデルベースの手法 と呼びます．一方，環境のモデルを持っていない場合，すなわち，状態遷移確率と報酬の確率分布が未知の場合，試行錯誤を通じて環境と相互作用をした結果を使って学習を行います．この方法をモデルフリーの手法 と呼びます．本節ではモデルベースの手法を，次節ではモデルフリーの手法を説明します．モデルベースの手法では，状態遷移確率$P(s_{t+1}|s_t, a_t)$と，報酬の確率分布$p(r_{t+1}|s_t, a_t)$が与えられているものとします．その前提で，アルゴリズム15.1に示すValue iterationアルゴリズムを実行すると，状態価値関数$V(s)$の最適値を求めることができ，それぞれの状態でQ値を最大とする行為が求まりますので，これが最適政策ということになります．アルゴリズム15.1 中の報酬の期待値$E(r|s,a)$は報酬の確率分布$p(r_{t+1}|s_t, a_t)$から求めます．このアルゴリズムは，迷路中で報酬がもらえる状態（ゴール）が1つだけある場合，まずそのゴール状態の1つ手前での最適行為が得られ，次にその1つ手前，さらにその1つ手前と，繰り返しを回る毎に正しい最適値が得られている状態がゴールを中心に広がってゆくイメージをしていただけると，わかりやすいと思います．また，モデルベースの手法には，Value iterationアルゴリズムの他にも，適当な政策を初期値として，そのもとでの状態価値関数$V(s)$を計算し，各状態で現在の知識から得られる最適行為を選び直すことを繰り返すPolicy iterationアルゴリズムもあります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 40,
                                    "text": "環境をモデル化する知識，すなわち，状態遷移確率と報酬の確率分布が与えられている場合"
                                }
                            ],
                            "id": "d9d35b81-8373-439a-8583-a2dc3be6a8ed",
                            "question": "モデルベースの手法とはどのような場合ですか"
                        }
                    ]
                }
            ],
            "title": "1509"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，この識別関数法の考え方を確率モデルに適用する，識別モデルの考え方を説明します．第4章で説明したように，データから直接に頻度を数えて事後確率$P(\\omega_i|\\bm{x})$を求めることはできません．そこで，識別モデルでは，この事後確率を特徴値の組み合わせから求めるようにモデルを作ります．つまり，特徴ベクトル$\\bm{x$}が与えられたときに，その$\\bm{x}$の値を用いて，何らかの方法で，出力$y$の確率分布を計算するメカニズムをモデル化します．いま，2値分類問題における特徴ベクトル$\\bm{x}=(x_1, \\dots, x_d)^T$に対して，各特徴の重み付き和$w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d$を考え，正例に関しては正の値，負例に関しては負の値を出力するように重みを調整することを考えます．ただし，これでは原点$\\bm{x} = \\bm{0}$に対して判定ができないので，定数$w_0$をパラメータとして加え，改めて$g(\\bm{x})=w_0 + w_1 \\cdot x_1 + \\dots + w_d \\cdot x_d = w_0 + \\bm{w} \\cdot \\bm{x}$と定義します．ここで，$g(\\bm{x})=0$とおいたものは，式の形から$d$次元空間上の平面を表しています．もし，この平面が上記のようにふるまうように調整ができたとすると，この平面上にある点は，どちらのクラスとも判別がつかず，平面の正の側（$g(\\bm{x})>0$となる側）の空間に正例，平面の負の側（$g(\\bm{x})<0$となる側）の空間に負例の空間ができるはずです．このように，特徴空間上でクラスを分割する面を識別面とよびます．また，それぞれの点の判定の確からしさは，識別面からの距離に反映されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 119,
                                    "text": "事後確率を特徴値の組み合わせから求めるようにモデルを作ります"
                                }
                            ],
                            "id": "a11c1237-0040-45d5-b7d4-5d868a769cc6",
                            "question": "識別モデルはどのように作りますか"
                        }
                    ]
                }
            ],
            "title": "0510"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 459,
                                    "text": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法"
                                }
                            ],
                            "id": "e536e67c-7364-4dc9-976f-5415c2e1b126",
                            "question": "誤差逆伝播法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習とは，正解情報が付けられていないデータを対象に行う学習です．データの集合を，以下のように定義します．この章では，この特徴ベクトルの要素がすべて数値である場合を考えます．要素がすべて数値であるということは，特徴ベクトルを$d$次元空間上の点として考えることができます．そうすると，モデル推定は，特徴空間上にあるデータのまとまりを見つける問題ということになります．データがまとまっている，ということは，共通の性質をもつようにみえる，ということなので，図11.2のように，このような個々のデータを生じさせた共通の性質を持つクラスを見つけることが目標になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "正解情報が付けられていないデータを対象に行う学習"
                                }
                            ],
                            "id": "1ad77c11-b028-4701-931c-1c2d4d965d8f",
                            "question": "教師なし学習はどういうものですか"
                        }
                    ]
                }
            ],
            "title": "1102"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章から第5章では，正解情報の付いた学習データを用いる教師あり学習の設定で，識別をおこなうモデルを学習する方法について説明します．まず第3章と第4章は，カテゴリカルデータからなる特徴ベクトルを入力として，それをクラス分けする（すなわち属するクラスラベルを出力する）識別器を作る方法について学びます（図3.1）．識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報$y_i$のペアからなります．ここでの設定は，特徴ベクトル$\\bm{x}_i$の各次元および正解情報$y_i$がいずれもカテゴリです．特にカテゴリ形式の正解情報のことをクラスとよびます．このカテゴリ特徴に対する「教師あり・識別」問題に対して，いかに納得のゆく概念モデルを獲得するか，という点に重点を置いたものが，この章で説明する概念学習です．一方，特徴が与えられたときに，それがあるクラスに属する確率を計算するモデルの獲得を目的とするものが，第4章で説明する統計的手法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 267,
                                    "text": "カテゴリ形式の正解情報"
                                }
                            ],
                            "id": "b09966fa-456c-4dc1-8836-6ed9ca8d47c1",
                            "question": "クラスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0302"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 307,
                                    "text": "ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法"
                                }
                            ],
                            "id": "d176d827-8ba6-4351-98fb-133671967626",
                            "question": "確率的最急勾配法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです"
                                }
                            ],
                            "id": "84e362f2-2651-4edc-a4dd-df97724f212e",
                            "question": "畳み込みニューラルネットワークとは何か"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "ab1d3f4d-4c2c-4882-8629-d3666cf82965",
                            "question": "回帰木とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，もとの特徴空間上の2点$\\bm{x}, \\bm{x}'$の距離に基づいて定義されるある関数$K(\\bm{x}, \\bm{x}')$を考えます．この関数をカーネル関数とよびます．そして，非線形写像を$\\phi$としたときに，以下の関係が成り立つことを仮定します．つまり，もとの空間での2点間の距離が，非線形写像後の空間における内積に反映されるという形式で，近さの情報を保存します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 80,
                                    "text": "カーネル関数"
                                }
                            ],
                            "id": "43c708b2-027c-4511-83ee-b3f67e6c89e1",
                            "question": "もとの特徴空間上の2点の距離に基づいて定義される関数はなんですか"
                        }
                    ]
                }
            ],
            "title": "0711"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 98,
                                    "text": "サポートベクトルマシン"
                                }
                            ],
                            "id": "fd47bdbc-55dc-4ee3-a352-0b79e950d61f",
                            "question": "線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "57632fab-1f47-44c1-ad0a-8867af78888a",
                            "question": "k-meansアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，事前確率 (prior probability) とよびます．クラス$\\omega_i$の事前確率は$P(\\omega_i) ~~ (i=1,\\dots,c)$ （ただし$c$はクラス数）と表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 303,
                                    "text": "入力を観測する前にもっているそれぞれのクラスの起こりやすさ"
                                }
                            ],
                            "id": "d9c60e7f-77c7-4f36-8ba2-37c8a6139b81",
                            "question": "事前確率とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0404"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとする"
                                }
                            ],
                            "id": "0fe3866a-22e0-4d19-aeb3-46b1038c7cd8",
                            "question": "深層学習では何を特徴とするのですか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワークの構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合にも対応できる方法として，確率伝播による計算法があります．このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，適用することができません．そのような場合は，確率的シミュレーションも用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 524,
                                    "text": "アークを無向とみなした結合を考えたときに，ループが形成されていれば"
                                }
                            ],
                            "id": "82c00b87-213c-4ae2-82a1-5cccd9e12577",
                            "question": "どういうときに確率的シミュレーションを使うんですか"
                        }
                    ]
                }
            ],
            "title": "0416"
        },
        {
            "paragraphs": [
                {
                    "context": "こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています．ここで，確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので，式(4.6)の対数をとって計算します．式(4.7)で計算される値を対数尤度$\\mathcal{L}(D)$とよびます．この対数尤度の値は，大きければ大きいほど学習データがそのモデルから生成された確率が高い、ということがいえます．そして，学習データが，真のモデルから偏りなく生成されたものであると仮定すると，この方法で求めたモデルは真の分布に近い，と考えることができます．したがって，式(4.7)を最大にするパラメータが求まればよいわけです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 5,
                                    "text": "モデルのパラメータが与えられたときの，学習データ全体が生成される尤度"
                                }
                            ],
                            "id": "8c7e331e-0844-46aa-b8dc-b5100122be81",
                            "question": "対数尤度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0407"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "c44800f2-a925-42b8-a94f-41b24e7ca06f",
                            "question": "サポートベクターマシンとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは，特徴値のOR結合を仮説とした機械学習は不可能なのでしょうか．もちろんそんなことはありません．仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します．決定木とは，データを分類する質問をノード（節）とし，分類結果をリーフ（葉）とする木構造の概念表現です．正例のリーフに到るノードの分岐の値をAND条件で結合し，それらをさらにOR条件で結合することで等価な論理式に変換できますが，木構造のままの方が，人間の目から見て学習結果がわかりやすいので，こちらの表現が好まれます．コンタクトレンズデータ (contact-lens.arff)（表3.1）から作成した決定木の例を図3.3に示します．図3.3の木では特徴tear-prod-rate（涙量）が最初の質問で，この値がreduced（減少）であると，コンタクトレンズは勧められない，という結論になります．この値がnormal（正常）であれば，次の特徴astigmatism（乱視）を調べる，という手順になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "仮説空間にバイアスをかけられないのなら，探索手法にバイアスをかけます．「このような探索手法で見つかった概念ならば，汎化性能が高いに決まっている」というバイアスです．ここではそのような学習手法の代表である決定木について説明します"
                                }
                            ],
                            "id": "5d31afc9-05cb-4bac-a2d3-4944c3196081",
                            "question": "決定木とはどのような学習法ですか"
                        }
                    ]
                }
            ],
            "title": "0307"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 18,
                                    "text": "事前にクラスタ数$k$を固定しなければいけないという問題点"
                                }
                            ],
                            "id": "7bf2abb2-e445-4168-b67d-e7d25b4ebbd2",
                            "question": "k-means法の問題点はなんですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 465,
                                    "text": "二乗誤差を最小にするように識別関数を調整する方法"
                                }
                            ],
                            "id": "ae454f0b-170a-48ed-920a-0413b6f0263d",
                            "question": "最小二乗法ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "dfa0d7bc-b8ad-4831-911b-8cf001afcf09",
                            "question": "マージンってなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 703,
                                    "text": "観測対象から問題設定に適した情報を選んでデータ化する処理"
                                }
                            ],
                            "id": "b310ddbd-2e0b-4308-a421-5eb05a72bed9",
                            "question": "特徴抽出とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "特徴ベクトルの次元数を減らすことです"
                                }
                            ],
                            "id": "022186a5-4ee0-4d7d-aace-88aca357a605",
                            "question": "主成分分析ってどういうことをしているんですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 132,
                                    "text": "多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点"
                                }
                            ],
                            "id": "5b5f6f17-5ade-4711-b280-08b27e24ce17",
                            "question": "誤差逆伝播法における問題点は何か"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 55,
                                    "text": "すべてのデータの重みは平等"
                                }
                            ],
                            "id": "56ad09b5-0e71-4e38-9c62-d6d5dae02d83",
                            "question": "バギングでは、個々のデータに対してどのように重みを設定しますか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，このデータからFP-木 (Frequent Pattern Tree) を作成します．FP-木は，最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．挿入アルゴリズムは以下のようになります．具体的な FP-木の作成手順は図12.7のようになります．まず，$\\{z,r\\}$がnull($\\emptyset$)を根とするFP-木に挿入されます．次に，$\\{z,x,y,s,t\\}$の挿入です．まず$z$はFP-木にあるのでカウントを1増やして2として，この$z:2$をルートとするFP-木に対して，残りの$\\{x,y,s,t\\}$を挿入します．次の$x$はFP-木にないので，新たにノードを作って$z:2$につなぎます．そして，この新しい$x:1$をルートとするFP-木に対して残りの要素を挿入する作業を再帰的に繰り返します．できあがったFP-木に対して，特徴を見出しとするヘッダテーブルを作成し，その頻度を記録しておくとともにFP-木に出現する同じ要素をリンクで結んでおきます（図12.8上）．特定の特徴は，自分より頻度の高い特徴の出現の有無に応じて，複数の枝に分かれて出現します．このリンクをたどって集めた出現数は，全体のトランザクション集合での出現数に一致します．% figure 12.8",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 53,
                                    "text": "最初にnullというカテゴリを付けた根ノード（ルート）を用意し，トランザクションを順にその木に挿入してゆきます．"
                                }
                            ],
                            "id": "f4aba473-5b65-496a-a64e-47e291e03a9a",
                            "question": "FP 木とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1215"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です"
                                }
                            ],
                            "id": "c250fb8e-fb1a-47d2-b2e1-4899ed00f6ca",
                            "question": "SVMとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，対数尤度$\\mathcal{L}(D)$を最大にするパラメータ$\\hat{\\theta}$は，$d \\mathcal{L}(D) / d\\theta = 0$の解として式(4.10)のように求めることができます．式(4.10)右辺の分子は値1をとる事例数，分母は全事例数です．このように，推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 148,
                                    "text": "推定するべき確率は，値1をとるデータ数の全データ数に対する割合という，ごく直観的な推定と一致します．この推定法を最尤推定法 (maximum likelihood estimation) とよびます"
                                }
                            ],
                            "id": "3578f573-eee3-487b-8455-96ac12feb1e4",
                            "question": "最尤推定法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0409"
        },
        {
            "paragraphs": [
                {
                    "context": "式(15.4)は，無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したものです．この再帰方程式をベルマン方程式 (Bellman equation)といいます．状態遷移確率を明示的にすると，ベルマン方程式は以下のように書き換えられます．さらに，式(15.5)を，Q値を用いて書き換えると，以下のようになります．求めるべき最適政策は，Q値を用いて，以下のように表現できます．後は，どのようにしてQ値を推定するか，という問題になります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 9,
                                    "text": "無限時刻の和で表現される状態価値関数を，隣接時刻間の再帰方程式で表したものです"
                                }
                            ],
                            "id": "3715ad1d-30fe-44bb-8e54-9f04e04c2b0d",
                            "question": "ベルマン方程式ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1508"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 164,
                                    "text": "LSTM"
                                }
                            ],
                            "id": "e112a1c5-3616-41cc-a080-58dbe3469b62",
                            "question": "リカレントニューラルネットワークで，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えた方法を何といいますか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとする"
                                }
                            ],
                            "id": "22f1f695-dfeb-44bf-9f1d-24e9d4cb907f",
                            "question": "深層学習と他の手法との大きな違いは何ですか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "図で表すと，図12.3のようになります．この a priori な原理の対偶を用いて，小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法が，Aprioriアルゴリズムです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "小さな項目集合から支持度の計算を始め，項目集合を大きくしてゆく際に，頻出でない項目集合をそれ以上拡張しないことで，調べるべき項目集合を減らす方法"
                                }
                            ],
                            "id": "32ab8fa7-cbb3-4695-90cf-5f0c8e7c3996",
                            "question": "a prioriアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1207"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 452,
                                    "text": "規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比"
                                }
                            ],
                            "id": "4a335989-f5a9-4c9d-98fa-f78e3364349f",
                            "question": "リフト値ってなんですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "制約を満たさない程度を表すので，小さい方が望ましい"
                                }
                            ],
                            "id": "2f55ddce-c259-43d1-8bf9-1b50e1af0652",
                            "question": "なぜスラック変数が小さい方が良いのですか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる概念の仮説を少なくし，その仮説の空間を探索することで概念を求める手法が開発されました．そのような手法として，FIND-Sアルゴリズムや，候補削除アルゴリズムがあります（図3.2）．FIND-Sアルゴリズムは，仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限します．このように，仮説に対して課す制約をバイアスとよびます．最初は，最も特殊な仮説（いかなる事例も正ではない）からスタートし，正例を一つずつ読み込んで，その事例の値を受け入れるように仮説を最低限一般化します．たとえば，表3.1のデータにおいて，最初の正例である1番のデータから，論理式「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ astigmatism=no $\\wedge$  tear-prod = reduced」が得られます．次の正例である3番のデータは，age, spectacle-prescrip, tear-prodの値はこの論理式に当てはまりますが，astigmatismの値が異なります．1番と3番のデータの両方が当てはまるようにするために，この論理式から，astigmatismの条件を取り除き，新たな仮説を「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ tear-prod = reduced」とします．これを続けると，5番のデータでspectacle-prescripの条件が落ち，9番のデータでageの条件が落ち，最後は16番のデータでtear-prodの条件まで落ちて，条件が何もなくなってしまいます．これでは，すべての入力が正例であるという概念になり，明らかにおかしな結果になってしまいました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 194,
                                    "text": "仮説に対して課す制約"
                                }
                            ],
                            "id": "a3d7b9dc-2bb7-4b1a-be65-a7ae4f87a9be",
                            "question": "バイアスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0305"
        },
        {
            "paragraphs": [
                {
                    "context": "階層的クラスタリングはボトムアップ的にデータをまとめるので，全体的な視点からみると，いびつなクラスタを形成してしまうことがあります．一方，全体的な視点でまとまりのよいクラスタを求める手順が，分割最適化クラスタリングです．分割最適化クラスタリングでは，データ分割のよさを評価する関数を定め，その評価関数の値を最適化することを目的とします．ところが，データ数を$N$とすると，データを分割する場合の数は，$N$に対して指数的（例えば二つのクラスタに分ける場合なら$2^N$）になるので，$N$がちょっと大きな数になると，すべての可能な分割の評価値を求めるのは実質的に不可能になります．このような場合の常套手段として，探索によって準最適解を求めるということを考えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 69,
                                    "text": "全体的な視点でまとまりのよいクラスタを求める手順"
                                }
                            ],
                            "id": "2ab4bb18-0ec7-46d8-8134-51dec0bfb5ca",
                            "question": "分割最適化クラスタリングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1107"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 24,
                                    "text": "ニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするもの"
                                }
                            ],
                            "id": "fc1670df-6602-4ba6-807e-80006b2c0925",
                            "question": "多階層ニューラルネットワークとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 15,
                                    "text": "事前学習法"
                                }
                            ],
                            "id": "987724c7-fbc0-4055-9705-a9fc892fd0af",
                            "question": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは学習です，とゆきたいところですが，その前に学習結果の評価基準を設定します．ここで扱っているデータはirisデータなので，教師あり・識別の場合の評価基準を考えます．この場合，学習データに対して正解率100\\%でも意味がありません．未知データに対してどれだけの正解率が期待できるかが評価のポイントですが，どうやって未知データで評価すればよいのでしょうか．学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法が考えられます．この方法を分割学習法とよびます．評価用に半分というのは，多すぎるように見えるかもしれませんが，評価用データがあまりに少ないと，未知データの分布と全く異なる可能性が高くなり，評価そのものが信頼できなくなります．また，学習パラメータの調整をおこなうような場合では，データを学習用・調整用・評価用と分けるケースもあります．しかし，irisデータは150事例しかないので，分割学習法で評価するのは難しそうです．このような場合，一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します(図2.7)．この方法では学習データを$m$個の集合に分割し，そのうちの$m-1$個で学習を行い，除外した残りの一つで評価を行います．そして，その除外するデータを順に交換することで，合計$m$回の学習と評価を行います．これで，全データがひととおり評価に使われ，かつその評価時に用いられる識別器は評価用データを除いて構築されたものとなっています．$m$を交差数とよび，技術論文では交差数$m$を10とするケース (10-fold CV) や，データの個数とするケースがよく見られます．$m$がデータの個数の場合を一つ抜き法(leave-one-out method)とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 179,
                                    "text": "学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法"
                                }
                            ],
                            "id": "417f552c-6a9d-4ada-a27f-357f1b300192",
                            "question": "分割学習法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0206"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 411,
                                    "text": "回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫"
                                }
                            ],
                            "id": "5910f6af-333b-4c94-8238-8917b96c0b64",
                            "question": "正則化ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 116,
                                    "text": "最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成"
                                }
                            ],
                            "id": "49f366f7-3454-4ee1-84f4-995ca4a1c0b1",
                            "question": "ブースティングでは、どのように識別器を作成するのですか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 300,
                                    "text": "与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする"
                                }
                            ],
                            "id": "2b788249-964f-4f30-a86d-91d557ee5979",
                            "question": "Ward法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "そうすると，式(13.2)は式(13.3)のように書き換えることができます．式(13.3)右辺の和の部分の最大値を求めるには，先頭$t=1$からスタートして，その時点での最大値を求めて足し込んでゆくという操作を$t$を増やしながら繰り返すことになります．このような手順をビタビアルゴリズムとよびます．このような制限を設け，対数線型モデルを系列識別問題に適用したものを条件付き確率場（Conditional Random Field: CRF）とよびます．CRFの学習は，対数線型モデルほど簡単ではありませんが，識別の際の手順と同様に，素性関数の値が1つ前の出力にしか影響されないという条件のもとで，重複する計算をまとめることができるという性質を利用します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "条件付き確率場（Conditional Random Field: CRF）"
                                }
                            ],
                            "id": "28d18c88-1275-438c-b6cf-a4e6a37a0288",
                            "question": "CRFは何の略ですか"
                        }
                    ]
                }
            ],
            "title": "1306"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 688,
                                    "text": "識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした"
                                }
                            ],
                            "id": "19c77161-5013-4f62-bed8-f24ae7c79df9",
                            "question": "識別では、なぜすべてのデータをきれいに分離しないのですか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 503,
                                    "text": "誤差逆伝播法"
                                }
                            ],
                            "id": "526133aa-fc6d-4ffa-9a32-eaeff9e2c56f",
                            "question": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法はなんですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 306,
                                    "text": "トレードオフの関係"
                                }
                            ],
                            "id": "00e74f4c-36f8-4141-ae97-3a0f1b83391a",
                            "question": "バイアスと分散はどのような関係ですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，出力$o_j$を重み$w_i$で偏微分したものは以下の合成微分で求めます．第1項はシグモイド関数の微分です．第2項は入力の重み付き和の微分です．従って，出力層の重みの更新式は以下のようになります．通常，ニューラルネットワークの学習は確率的最急勾配法を用いるので，式8.7の全データに対して和をとる操作を削除します．また，中間層は，この修正量を重み付きで足し合わせます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 45,
                                    "text": "シグモイド関数の微分"
                                }
                            ],
                            "id": "3fff2f83-1537-445b-8603-4a330a05409c",
                            "question": "1つ目の式の右辺の第1項は何ですか"
                        }
                    ]
                }
            ],
            "title": "0808"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，特徴数削減の手法として，主成分分析 (principal component analysis: PCA) を紹介します．図2.5に，2次元から1次元への削減を例として，主成分分析の考え方を示します．主成分分析とは，相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作です．次元削減の対象である高次元特徴空間上にデータがどのように散らばっているかという情報は，もとのデータの統計的性質をあらわす共分散行列によって表現することができるので，この共分散行列の情報を基にして，低次元空間への写像をおこなう行列を作ってゆきます．学習データ$\\{\\bm{x} | \\bm{x} \\in D\\}$の共分散行列$\\Sigma$は式(2.1)を用いて計算されます．ここで，$\\bm{\\mu}$は$D$の平均ベクトル，$N$は$D$の要素数です．平均ベクトル$\\bm{\\mu}$は式(2.2)を用いて計算されます．図2.5左上に示すような2次元データの場合，平均ベクトルを$\\bm{\\mu}=(\\bar{x_1}, \\bar{x_2})^T$とすると，共分散行列$\\Sigma$は式(2.3)のようになります．対角成分は，次元ごとの散らばり具合を表す分散に対応し，非対角成分は次元間の相関を表します．次に，この共分散行列の固有値と固有ベクトルを求めると，固有値の大きい順にその対応する固有ベクトルの方向が，データの散らばりが大きい（すなわち，識別するにあたって情報が多い）方向となります．固有ベクトルどうしは直交するので，固有値の大きい順に軸として採用し，特徴空間を構成すると，たとえば上位$n$位までなら$n$次元空間が構成でき，これらはもとの多次元特徴空間のデータの散らばりを最もよく保存した$n$次元空間ということになります．特徴空間の次元数が下がれば下がるほど，学習において推定するべきパラメータ数が少なくなるので，学習結果の信頼性が高まります．もっとも，もとのデータの情報が大きく損なわれるほどに次元を削減してしまっては意味がないので，そのあたりの調整は難しいところです．主成分分析によって構成した軸では，対応する固有値が分散になるので，「すべての軸の固有値の和」に対する「採用した軸の固有値の和」の比（累積寄与率）を計算することで，次元削減後の空間が，もとのデータの情報をどの程度保存しているのか，見当をつけることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 113,
                                    "text": "相関が高い特徴を複数含むような冗長な高次元空間を，冗長性の少ない低次元空間に写像する行列を求める操作"
                                }
                            ],
                            "id": "db33a4ef-d185-4c4c-84fa-27a269ba2adb",
                            "question": "主成分分析って何ですか"
                        }
                    ]
                }
            ],
            "title": "0205"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムの中で，詳しい説明のない「特徴集合A中で最も分類能力の高い特徴」を決定する方法について説明します．分類能力が高いとは，「その特徴を使った分類を行うことによって，なるべくきれいに正例と負例に分かれる」ということです．いいかえると，乱雑さが少なくなるように分類を行うということですね．乱雑さの尺度として，エントロピーを用います．学習データ集合$D$の乱雑さを計算するために，まず正例の割合: $P_+$,  負例の割合: $P_-$を計算し，それを元に式(3.1)によって，その集合の乱雑さ（エントロピー）$E(D)$を求めます．エントロピーの値は$P_+=1$または$P_-=1$のとき最小値0となり，$P_+=P_-=0.5$のとき，最大値1となるので，エントロピーの値が小さいほど，集合が乱雑でない，すなわち整っている（同じクラスのものが大半を占めている）ということになります．このエントロピーの減り具合を計算したいのですが，単純に引き算はできません．エントロピーは集合に対して定義できるものです．分類前は1つの集合で，分類後はその特徴値の種類数だけ集合ができます．そこで，分類後の集合の要素数の割合で重みを付けて計算します．この値を情報獲得量と定義します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 246,
                                    "text": "集合の乱雑さ"
                                }
                            ],
                            "id": "2dffaf5a-d5b1-4ec4-bb38-8f89edff09b7",
                            "question": "エントロピーって何ですか"
                        }
                    ]
                }
            ],
            "title": "0311"
        },
        {
            "paragraphs": [
                {
                    "context": "アンサンブル学習とは，識別器を複数組み合わせ，それらの結果を統合することで，個々の識別器よりも性能を向上させる方法です（図10.1）．ここでの問題設定は識別問題ですが，アンサンブル学習の考え方は，ほぼそのまま回帰問題にも適用できます．アンサンブル学習の説明には，「三人寄れば文殊の知恵」ということわざがよく引き合いに出されます．確かに，一つの識別器を用いて出した結果よりは，多数の識別器が一致した結果のほうが，信用できそうな気はします．そのような直観的な議論ではなく，本当に多数が出した結論の方が信用できるのかどうかを検討してみましょう，ここで，同じ学習データを用いて，異なる識別器を$L$個の作成したとします．仮定として，識別器の誤り率$\\epsilon$はすべて等しく，その誤りは独立であるとします．誤りが独立であるとは，評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということです．このような仮定をおくと，この識別器集合に，ある評価用データを与えたとき，識別器が誤る数が$m$個になる確率は二項分布$B(m; \\epsilon, L)$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 363,
                                    "text": "評価用のデータそれぞれに対して，それぞれの識別器が誤る確率が独立であるということで，多くの識別器がいっしょに誤ってしまうようなデータはない，ということで"
                                }
                            ],
                            "id": "e1c7a720-38a7-47c1-b00d-33d42d2d7588",
                            "question": "誤りが独立であるとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1001"
        },
        {
            "paragraphs": [
                {
                    "context": "データから規則や知見を得る機械学習技術のなかでも，特に深層学習 (deep learning)は，高い性能を実現する方法として近年注目を集めています．深層学習は，一般に隠れ層を多くもつニューラルネットワーク（図1.3）によって実装されています．深層学習が他の機械学習手法と異なるのは，深い階層構造をとることによって，従来は人手でプログラムされていた特徴抽出段階の処理を，学習の対象として取り込んでいるところでです．近年の深層学習の流行を見ると，他の機械学習技術はもう不要に見えるかもしれません．しかし，深層学習がその強さを発揮しているのは，音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題であるという傾向があります．さまざまな問題に対して機械学習アルゴリズムの性能を競うサイトでは，深層学習と並んで勾配ブースティングなどの手法が上位を占めることがあります．また一方で，性能は多少低くてもよいので判定結果に至るプロセスがわかりやすい手法や，運用後のチューニングが容易な手法が好まれる場合もあり，さまざまな状況でさまざまな問題に取り組むためには，深層学習だけではなく機械学習手法全般に関して理解しておくことが必要であるといえます．本書では機械学習全般に関して，設定した問題に対する基本的な手法の概要と，フリーソフトを用いた例題の解法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 270,
                                    "text": "音声・画像・自然言語など空間的・時間的に局所性を持つ入力が対象で，かつ学習データが大量にある問題"
                                }
                            ],
                            "id": "8e5008b1-79e7-48b2-b5bb-752ef1b59355",
                            "question": "深層学習が得意な問題はなんですか"
                        }
                    ]
                }
            ],
            "title": "0104"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\bm{w}$は$d+1$次元の重みベクトルとします．また，$\\eta$は学習係数で，適当な小さい値を設定します．このアルゴリズムは，学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します．これをパーセプトロンの収束定理とよびます．一方，学習データが線形分離不可能な場合にはこのアルゴリズムを適用することができません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合はこのアルゴリズムは停止しません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 210,
                                    "text": "全ての誤りがなくなることが学習の終了条件なので"
                                }
                            ],
                            "id": "11d91fa7-5dca-4c15-9c99-177d1f26180c",
                            "question": "なぜ線形分離不可能なときパーセプトロンの学習アルゴリズムは使えないのですか"
                        }
                    ]
                }
            ],
            "title": "0507"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "281b52fa-533f-4992-96df-8f4be97294eb",
                            "question": "マージンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 91,
                                    "text": "計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズム"
                                }
                            ],
                            "id": "ff47398d-6661-4dc6-b130-9cb7ffa76d2d",
                            "question": "ニューラルネットワークとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 428,
                                    "text": "学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります"
                                }
                            ],
                            "id": "15bbae2b-794b-444a-95a3-dd5712b1c0bf",
                            "question": "ドロップアウトで過学習が防げるのはなぜですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 272,
                                    "text": "同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させる"
                                }
                            ],
                            "id": "a114c431-cbc9-4d1d-a35a-dfb4fb8e3122",
                            "question": "なぜ割引率が必要なのですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1037,
                                    "text": "正例がどれだけ正しく判定されているかという指標"
                                }
                            ],
                            "id": "94d17e91-a225-46d9-a81b-6fa0ea4754fe",
                            "question": "再現率とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 116,
                                    "text": "最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします"
                                }
                            ],
                            "id": "a6870171-d702-4661-bf51-a6772bdd125d",
                            "question": "ブースティングで、個々のデータに対してどのように重みを設定するのですか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 129,
                                    "text": "識別に有効な特徴が入力の線形結合で表される，という保証はないからです"
                                }
                            ],
                            "id": "874ea9fc-3a21-486b-aba0-a758f9874757",
                            "question": "多階層ニューラルネットワークにおいて、特徴抽出層を3階層ニューラルネットワークの入力側に付け加えていけないのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習(self-training)は，最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです(図14.5)自分が出した結果を信じて，再度自分を学習させるというところが自己学習と呼ばれる理由です．繰り返しによって学習データが増加し，より信頼性の高い識別器ができることをねらっています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 21,
                                    "text": "最も単純な半教師あり学習アルゴリズムで，教師ありデータで作成した識別器の出力のうち，確信度の高い結果を信じて，そのデータを教師ありデータに取り込んで，自分を再度学習させるということを繰り返すものです"
                                }
                            ],
                            "id": "5d6a2fd3-bef9-4ac0-a6c8-db90eed37a89",
                            "question": "自己学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1407"
        },
        {
            "paragraphs": [
                {
                    "context": "多階層ニューラルネットワークは，図9.1のようなニューラルネットワークで，入力に近い側の処理で，特徴抽出をおこなおうとするものです．これはよく用いられる3階層ニューラルネットワークの入力側に，もう1層の特徴抽出層を付け加えればよい，というものではありません．識別に有効な特徴が入力の線形結合で表される，という保証はないからです．それでは，もう1層加えて非線形にすればよいかというと，隣接するデータの関連性（音声の場合の時系列のデータの関連性・画像の場合の近接する空間的なデータの関連性など）が考慮されていないので，それでも十分ではないでしょう．つまり，特徴抽出を学習するには十分多くの層を持つニューラルネットワークが必要だということになります．第8章で説明したニューラルネットワークの学習手法である誤差逆伝播法は多階層構造でもそのまま適用できます．そうすると，深層学習でも最初からニューラルネットワークを多階層で構成すれば，それで問題が解決するのではないか，と思われるかもしれません．しかし，一般に階層が多いニューラルネットワークの学習には問題点があります．第8章で説明した誤差逆伝播法は，入力層に向かうにつれ修正量が少なくなり，多階層では入力側の重みはほとんど動かない，ということがわかっています（図9.3）．アルゴリズム8.1の修正量$\\delta$には$出力値 \\times (1-出力値) \\quad ただし，0<出力値<1$が掛けられますが，この値は最大でも$1/4$です．この値を1階層上の修正量の重み付き和にかけるのですが，重みは大きい値を取るノード以外は小さくなるように学習されます（正則化の議論を思い出してください）ので，この値もそれほど大きくはなりません．また，学習係数$\\eta$を大きくしても値が振動するだけなので，問題の解決にはなりません．3層のfeed forward型ではこの修正量の減少はあまり問題になりませんでしたが，階層が4層，5層と増えてゆくと修正量が急激に減ってしまいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 356,
                                    "text": "多階層構造でもそのまま適用できます"
                                }
                            ],
                            "id": "d20707ef-35ab-4ce6-bcdc-75afa7139d66",
                            "question": "誤差逆伝搬法は多階層構造でも利用できますか"
                        }
                    ]
                }
            ],
            "title": "0906"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで紹介する局所異常因子 (LOF: Local Outlier Factor)の考え方は，単純にいうと，近くにデータがないか，あるは極端に少ないものを外れ値とみなす，というものです．ただし，この「近く」という概念は，データの散らばり具合によって異なるので，一定の閾値をあらかじめ定めておくことはできません．そこで，それぞれのデータにとっての「周辺」を，$k$番目までに近いデータがある範囲と定義し，周辺にあるデータまでの距離の平均を，「周辺密度」として定義します（図11.13）．そして，あるデータの周辺密度が，近くの$k$個のデータの周辺密度の平均と比べて極端に低いときに，そのデータを外れ値とみなします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 54,
                                    "text": "近くにデータがないか，あるは極端に少ないものを外れ値とみなす"
                                }
                            ],
                            "id": "77066af5-e566-4dec-9c07-a3d1904e6bde",
                            "question": "局所異常因子では、どのように外れ値を検知するのですか"
                        }
                    ]
                }
            ],
            "title": "1112"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った方法です"
                                }
                            ],
                            "id": "878b7343-86ec-4c36-9724-278a621aaf20",
                            "question": "モデル木とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "その政策に従って行動したときの累積報酬の期待値で評価"
                                }
                            ],
                            "id": "229b72ca-a04e-49a9-b41c-354dadf60b07",
                            "question": "政策の良さはどのように評価されますか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 388,
                                    "text": "正解が付いていない場合の学習"
                                }
                            ],
                            "id": "8b740ee5-61d4-4a00-8380-7e1eca6c5038",
                            "question": "教師なし学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った方法です"
                                }
                            ],
                            "id": "86d39f45-af0f-49b1-9d87-748bbb6012e6",
                            "question": "モデル木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 170,
                                    "text": "最も遠い事例対の距離を類似度とする"
                                }
                            ],
                            "id": "22571ad8-cca3-4dd1-9770-cbab848bd631",
                            "question": "完全連結法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります"
                                }
                            ],
                            "id": "334bdbbe-98e2-4fad-98ae-1df2cd7f749b",
                            "question": "特徴ベクトルの次元が増えるとどうなるのか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 383,
                                    "text": "重みの修正量が層を戻るにつれて小さくなってゆく"
                                }
                            ],
                            "id": "926ba3ea-35aa-4a27-a2df-c727eb6a2f03",
                            "question": "勾配消失問題とは何か"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 232,
                                    "text": "クラスタの重心間の距離を類似度とする"
                                }
                            ],
                            "id": "43bda423-cd32-4df2-8547-7b879dd96b66",
                            "question": "重心法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "ここまで見てきたように，特徴ベクトルが数値であってもラベルであっても，教師ありデータで作成した識別器のパラメータを，教師なしデータを用いて調整してゆく，というのが半教師あり学習の基本的な進め方です（この章の最後に紹介するYATSIアルゴリズムは例外です）．識別器を作成するアルゴリズムはこれまで紹介してきたものを問題に応じて用いればよいのですが，信用できる出力をする教師なしデータを次回の識別器作成に取り込むためには，ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切です．一方，繰り返しアルゴリズムに関して，単純に終了のための閾値チェックをするだけなのか，識別器のパラメータを繰り返しの度に変化させるか，識別器で使う特徴に制限をかけるか，など様々な設定が可能です．以下では，繰り返しアルゴリズムの違いによって生じる，様々な半教師あり学習手法について説明してゆきます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 209,
                                    "text": "ナイーブベイズ識別器のような，その識別結果に確信度を伴うものが適切"
                                }
                            ],
                            "id": "685898d3-3303-441f-87c0-f806ef28a239",
                            "question": "半教師あり学習の識別器に適切なのはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1406"
        },
        {
            "paragraphs": [
                {
                    "context": "パターンマイニングで扱うデータは，一般的には\\ruby{疎}{まばら}なデータです．典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録をトランザクションとよびます．スーパーで揃えている商品の種類数（こちらは数千から数万）に比べて，一回の買い物で客が買う商品の種類数（こちらはせいぜい数十）は桁違いに少ないので，各トランザクションでは特徴ベクトルのほとんどの次元の値が空白で，ごく小数の次元のデータが埋まっている状況になります．このようなデータを疎なデータとよびます．イメージしやすいようにごく小さな例を示しましょう．商品点数が$\\{ミルク, パン, バター, 雑誌\\}$の4点，トランザクションが6件のデータを表12.1に示します．ここで，t は各トランザクションにおいて，その商品が買われたことを意味します．以下，商品を項目とよびます．ざっと見たところ，パンを買った人が多いようです．また，ミルクとパンが一緒に売れることも多いようです．「多いようだ」「少ないようだ」という定性的な判断ではなく，定量的な基準を決めて，その基準を超えるパターンを見つけましょう．データ中によく現れるということは，「全データに対して，ある項目集合が出現する割合が一定以上である」と解釈します．これを支持度 (support)とよび，式(12.1)で計算します．ただし，$T$は全トランザクション件数，$T_{\\mbox{items}}$は項目集合 items が出現するトランザクション件数です．表12.1の項目集合$\\{ミルク, パン\\}$の支持度は，$\\mbox{support}(\\{ミルク, パン\\})= 3/6 =0.5$となります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 42,
                                    "text": "典型的なものはスーパーマーケットの売上記録で，そのスーパーで扱っている商品点数が特徴ベクトルの次元数となり，各次元の値はその商品が買われたかどうかを記録したものとします．そうすると，各データは一回の買い物で同時に買われたものの集合になります．この一回分の記録"
                                }
                            ],
                            "id": "1d85c8e1-b1c7-4ef5-9c16-cb0716084da1",
                            "question": "トランザクションとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1203"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 10,
                                    "text": "自分が出した誤りを指摘してくれる他人がいない"
                                }
                            ],
                            "id": "9579e008-6b67-4c5d-b5ad-2232f3388fd9",
                            "question": "自己学習の問題点は何がありますか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "7c22ec55-ecbc-4eb2-81fc-02891e1fb114",
                            "question": "深層学習の定義はなにか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは前節で定式化した問題を，ラグランジュの未定乗数法を用いて解決する方法を説明します．ラグランジュの未定乗数法を用いると，$g(\\bm{x})=0$という条件の下で$f(\\bm{x})$の最小値（あるいは最大値）を求める問題は，$L(\\bm{x}, \\lambda)=f(\\bm{x})-\\lambda g(\\bm{x})$ （ただし$\\lambda$はラグランジュ乗数）という新しいラグランジュ関数を導入し，この関数の極値を求めるという問題に置き換えることができます．ラグランジュ関数の$\\bm{x}$に関する偏微分を0とすると，式(7.8)のようになります．ここでは，ラグランジュの未定乗数法を不等式制約条件で用います．そうすると，式(7.6)，式(7.7)の制約付きの最小化問題は，ラグランジュ乗数$\\alpha_i$を導入して，以下の関数$L$の最小値を求めるという問題に置き換えることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 288,
                                    "text": "ラグランジュの未定乗数法を不等式制約条件"
                                }
                            ],
                            "id": "65518e8b-5f96-4689-ad68-639a1e9a71d0",
                            "question": "マージンを最大にする識別面の計算はどうするの"
                        }
                    ]
                }
            ],
            "title": "0704"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに説明してきた識別問題では，何を特徴とするかは既に与えられたものとしてきました．音声処理や画像処理では，これまでの経験や分析の結果，どのような特徴が識別に役立つかが分かってきていて，識別問題の学習をおこなう対象は，それらの特徴からなるベクトルで表現されました．一方，深層学習は，どのような特徴を抽出するのかもデータから機械学習しようとするものです（図9.2）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 143,
                                    "text": "どのような特徴を抽出するのかもデータから機械学習しようとするものです"
                                }
                            ],
                            "id": "a836682d-8fc0-4a67-a42a-290ba6af8762",
                            "question": "深層学習におけるこれまでの識別問題との差はなにか"
                        }
                    ]
                }
            ],
            "title": "0902"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 622,
                                    "text": "識別境界線と最も近いデータとの距離"
                                }
                            ],
                            "id": "24e42384-cb93-4503-8028-3dfcd8c069fd",
                            "question": "マージンの定義はなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "上で定義したように，本章で扱う半教師あり学習は教師あり／なしの混在型データに対する識別学習です．基本的には教師ありデータで識別器を作成して，教師なしデータをできるだけ識別器の性能向上に役立てるという方針で学習手順を考えます．このような目的で教師なしデータを用いる際には，教師なしデータの性質にある程度の制約があります．まず，入力が数値のベクトルである場合を考えましょう．図14.2左の図のように，データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していないような状況では，教師ありデータの分布から教師なしデータのターゲットを比較的高い精度で推測でき，それらが識別器の性能向上に役立ちそうです．一方，図14.2右の図のように，明確なクラスタが確認されず，識別境界は存在しそうだけど，どのデータに教師信号が付いているかで推定される識別境界の位置が大きく異なりそうなデータは，間違った方に分類された教師なしデータがかえって識別器の性能を下げる振る舞いをするかもしれません．確率的モデルの生成的手法のことばで表現すると，正解なしデータから得られる$p(\\bm{x})$に関する情報が，$P(y|\\bm{x})$の推定に役立つことが，半教師あり学習が成立する条件です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 198,
                                    "text": "データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していない"
                                }
                            ],
                            "id": "07208161-df75-4c6c-9640-16298982d0d1",
                            "question": "半教師あり学習に適した数値特徴データとはどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1402"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 70,
                                    "text": "モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象"
                                }
                            ],
                            "id": "db0f17cd-4526-4ac3-974b-9b784eec5592",
                            "question": "過学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの"
                                }
                            ],
                            "id": "bcc3bd7a-9296-4df6-b678-ff30b197514d",
                            "question": "X-meansアルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 444,
                                    "text": "隠れ層"
                                }
                            ],
                            "id": "fd3f609e-bc34-430b-9d11-239139fda70f",
                            "question": "中間層の別名は何か"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので"
                                }
                            ],
                            "id": "dda44f73-bb2d-4152-8d83-b7d69bdc3ccd",
                            "question": "活性化関数にReLUを用いると誤差が失われないのはなぜですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，あるいはまったく与えられていないかのいずれかでした．その中間的な設定として，学習データの一部にだけ正解が与えられている場合が考えられます．学習データに正解を与えるのは人間なので，正解付きのデータを作成するにはコスト（費用・時間）がかかります．一方，正解なしのデータならば，容易にかつ大量に入手可能であるという状況があります．たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります．このような状況で，正解付きデータから得られた識別器の性能を，正解なしデータを使って向上させる問題を半教師あり学習 (semi-supervised learning) といいます．半教師あり学習は主として識別問題に対して用いられます．半教師あり学習の代表的な手法のアイディアを図1.11に示します．図1.11左のように，全データの中で正解の付加されたデータを丸・バツで表し，正解のないデータを三角形で表します．最初は丸・バツが付いたデータだけから識別器を作り，たとえば，その中間あたりに境界直線を引いたものとします．これに従って三角形のデータを分類しますが，境界線近辺のデータはあまり信用せず，境界線から大きく離れたものを確信度が高いとみなして正解を付与します．今度は，これらの新しく正解を付与されたデータも加えて，再度識別境界を計算します．これを，新しい正解付きデータが増えなくなるまで繰り返します．この学習法は，識別するべきクラスがうまくまとまっているようなデータや，識別結果によって有効な特徴が増えてゆくような，やや特殊なデータに対して適用するときにうまくゆきます．この手法を第14章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 82,
                                    "text": "学習データの一部にだけ正解が与えられている場合"
                                }
                            ],
                            "id": "752f88ee-c3f5-4532-a3b8-ecbf76d3a8a2",
                            "question": "半教師あり学習はどんなときに使われますか"
                        }
                    ]
                }
            ],
            "title": "0117"
        },
        {
            "paragraphs": [
                {
                    "context": "候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です"
                                }
                            ],
                            "id": "af0d8dad-86ef-4ea8-af79-2a52d9ab7b45",
                            "question": "候補削除アルゴリズムってなんですか"
                        }
                    ]
                }
            ],
            "title": "0306"
        },
        {
            "paragraphs": [
                {
                    "context": "上で定義したように，本章で扱う半教師あり学習は教師あり／なしの混在型データに対する識別学習です．基本的には教師ありデータで識別器を作成して，教師なしデータをできるだけ識別器の性能向上に役立てるという方針で学習手順を考えます．このような目的で教師なしデータを用いる際には，教師なしデータの性質にある程度の制約があります．まず，入力が数値のベクトルである場合を考えましょう．図14.2左の図のように，データがクラスタを形成していると見なすことができ，同一クラスタ内に異なるクラスのターゲットが混在していないような状況では，教師ありデータの分布から教師なしデータのターゲットを比較的高い精度で推測でき，それらが識別器の性能向上に役立ちそうです．一方，図14.2右の図のように，明確なクラスタが確認されず，識別境界は存在しそうだけど，どのデータに教師信号が付いているかで推定される識別境界の位置が大きく異なりそうなデータは，間違った方に分類された教師なしデータがかえって識別器の性能を下げる振る舞いをするかもしれません．確率的モデルの生成的手法のことばで表現すると，正解なしデータから得られる$p(\\bm{x})$に関する情報が，$P(y|\\bm{x})$の推定に役立つことが，半教師あり学習が成立する条件です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 479,
                                    "text": "正解なしデータから得られる$p(\\bm{x})$に関する情報が，$P(y|\\bm{x})$の推定に役立つことが，半教師あり学習が成立する条件"
                                }
                            ],
                            "id": "f799c680-a6fb-4e7f-8737-b8021ee1dcbf",
                            "question": "半教師あり学習が成立する条件は何ですか"
                        }
                    ]
                }
            ],
            "title": "1402"
        },
        {
            "paragraphs": [
                {
                    "context": "タスクに特化したディープニューラルネットワークの代表的なものが，画像認識でよく用いられる畳み込みニューラルネットワーク(convolutional neural network; CNN)です．CNNは，畳み込み層と，プーリング層を交互に配置し，最後のプーリング層の出力を受ける通常のニューラルネットワークを最終出力側に配置したものです（図9.8）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 32,
                                    "text": "画像認識"
                                }
                            ],
                            "id": "f9b8bd86-734b-4829-8a58-fab46ba64878",
                            "question": "畳み込みニューラルネットワークはどのような分野でよく使われるか"
                        }
                    ]
                }
            ],
            "title": "0912"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 340,
                                    "text": "特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する"
                                }
                            ],
                            "id": "9130a1cf-9e76-4304-aac6-4cb2fa71e81d",
                            "question": "表現学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "突然ですが，現在の気象に関する情報が何も知らされていない状況で，weather.nominalデータだけが与えられて，今日この人がゴルフをするかどうかと尋ねられたらどう答えますか．weather.nominalデータを眺めると，全14事例のうちyesが9事例，noが5事例です．したがって，yesと答えた方が正解する確率が高そうです．この場合はあまり確信を持ってyesと答えられるとはいえませんが，プロゴルファーのような人の1年分のデータが与えられて，yesが360事例，noが5事例だったら，躊躇なくyesと答える人が多いでしょう．この判断は，それぞれのクラスの起こりやすさの確率に基づいたものです，この入力を観測する前にもっているそれぞれのクラスの起こりやすさを，事前確率 (prior probability) とよびます．クラス$\\omega_i$の事前確率は$P(\\omega_i) ~~ (i=1,\\dots,c)$ （ただし$c$はクラス数）と表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 334,
                                    "text": "事前確率"
                                }
                            ],
                            "id": "ff24893a-6315-4abb-a9ec-149afc55a91e",
                            "question": "入力を観測する前に持っているそれぞれのクラスの起こりやすさを何と言いますか"
                        }
                    ]
                }
            ],
            "title": "0404"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 8,
                                    "text": "報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習"
                                }
                            ],
                            "id": "f2847520-d192-499c-a4ad-c01041526371",
                            "question": "強化学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，復元抽出によって作られた個々のデータ集合は，もとの学習データ集合と約$2/3$のデータを共有しているので，とくに，元のデータのまとまりがよい場合，それほど極端に異なった識別器にはなりません．ここで説明するランダムフォレスト(RandomForest)は，識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 169,
                                    "text": "識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法"
                                }
                            ],
                            "id": "d004318d-7fef-416e-a455-7853754e8aef",
                            "question": "ランダムフォレストとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1007"
        }
    ],
    "version": "1.0"
}