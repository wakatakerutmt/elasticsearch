{
    "data": [
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "真のモデルとの距離"
                                }
                            ],
                            "id": "bbd2ff64-2f26-41aa-95a8-705e1106b6a2",
                            "question": "バイアスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "候補削除アルゴリズムは，FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法です．しかし，候補削除アルゴリズムでも表現できる仮説の制約は同じなので，FIND-Sアルゴリズムと同じ手順で，概念の学習に失敗します．これらのアルゴリズムが，概念の学習に失敗する理由は，仮説に対するバイアスが強すぎて求めるべき概念が仮説の空間の中に存在しないことです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "FIND-Sアルゴリズムに加えて，負例に対して最も一般的な仮説（いかなる事例も正）を最低限特殊化するという処理を加えて，仮説の空間を特殊な論理式と一般的な論理式で挟むことによって，候補を絞り込む方法"
                                }
                            ],
                            "id": "a7f06716-ba6a-4277-aa29-c066d9f4f89d",
                            "question": "候補削除アルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0306"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 8,
                                    "text": "報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習"
                                }
                            ],
                            "id": "28c5e4be-c8f6-49f1-b342-31f324afc2dc",
                            "question": "強化学習とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは回帰モデルの評価について考えます．教師付き学習においては，未知データに対する誤差が問題となります．この回帰式は未知データに対してもうまく値を予測してくれるのでしょうか．回帰問題の評価は，交差確認法との相性はあまりよくありません．識別問題では，交差確認に用いるデータの部分集合は，そこに含まれるクラスの割合が，全体の割合と整合するように分割すれば，一回ごとの評価値がそれほど極端にはぶれず，ある程度適切な評価が行えます．しかし，回帰では何をもって部分集合の構成が近いかを定義することが難しくなります．したがって，計算能力に余裕があれば，一つ抜き法で評価することをお勧めします．そこでの評価指標は，学習の基準に合わせると平均二乗誤差ということになります．しかし，この値はデータが異なれば，スケールがまったく異なるので，結果がよいものかどうか直観的にはわかりにくいものです．そこで，回帰の場合は，正解と予測とがどの程度似ているかを表す相関係数や，式(6.6)で計算できる決定係数で評価します．決定係数は，「正解との離れ具合」と「平均との離れ具合」の比を1から引いたものですが，式変形により相関係数の二乗と一致するので，$R^2$とも表記されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 453,
                                    "text": "「正解との離れ具合」と「平均との離れ具合」の比を1から引いたもの"
                                }
                            ],
                            "id": "494bbb92-86f8-4177-bceb-5b16b4624ed9",
                            "question": "決定係数の定義は何ですか"
                        }
                    ]
                }
            ],
            "title": "0605"
        },
        {
            "paragraphs": [
                {
                    "context": "もうひとつのタスクに特化した構造をもつニューラルネットワークとして，中間層の出力が時間遅れで自分自身に戻ってくる構造をもつリカレントニューラルネットワーク（図9.10(a)）があります．リカレントニューラルネットワーは時系列信号や自然言語などの系列パターンを扱うことができます．このリカレントニューラルネットワークへの入力は，特徴ベクトルの系列$\\bm{x}_1,\\bm{x}_2,\\dots, \\bm{x}_T $という形式になります．たとえば，動画像を入力して異常検知を行ったり，ベクトル化された単語系列を入力して品詞列を出力するようなタスクが具体的に考えられます．これらに共通していることは，単純に各時点の入力からだけでは出力を決めることが難しく，それまでの入力系列の情報が何らかの役に立つという点です．リカレントニューラルネットワークの中間層は，入力層からの情報に加えて，一つ前の中間層の活性化状態を入力とします．この振舞いを時間方向に展開したものが，図9.10(b)です．時刻$t$における出力は，時刻$t-1$以前のすべての入力を元に計算されるので，これが深い構造をもっていることがわかります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 61,
                                    "text": "リカレントニューラルネットワーク"
                                }
                            ],
                            "id": "d2d3144f-eac4-49a3-8e6b-4747bbb695e6",
                            "question": "時系列信号や自然言語などの系列パターンを扱うことができるニューラルネットワークは何ですか"
                        }
                    ]
                }
            ],
            "title": "0916"
        },
        {
            "paragraphs": [
                {
                    "context": "強化学習とは，「報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習」と定義することができます（図15.2）．実世界で行為を行う意思決定エージェントというと，ロボットが思いつきます．バーチャルな世界で思いつきやすいのは，将棋や囲碁などを行うプログラムでしょうか．強化学習は，このような意思決定を行うエージェントを賢くする学習法です．エージェントには，環境についての情報が与えられます．たとえば，ロボットでは，センサ・カメラ・マイクなどからの入力が環境となります．多種多様な環境を連続的に考えるのは難しいので，環境は離散的な状態の集合$S=\\{s|s \\in S\\}$でモデル化できると仮定します．時刻$t$で，ある状態$s_t$において，エージェントが行為$a_t$を行うと，報酬$r_{t+1}$が得られ，状態$s_{t+1}$に遷移します．一般に，状態遷移は確率的で，その確率は遷移前の状態にのみ依存すると考えます．このような問題の定式化をマルコフ決定過程(Markov Decision Process: MDP)とよびます．また，強化学習で考えている問題では，報酬$r$はたまにしか与えられません．将棋やチェスなどのゲームを考えると，個々の手が良いか，悪いかはその手だけでは判断できず，最終的に勝ったときに報酬が与えられます．ロボットが迷路を移動する問題でも，個々の道の選択には報酬は与えられず，ゴールにだとりついた段階で報酬が与えられます．この場合，回り道をすれことを避けるために，選択毎にマイナスの報酬を与える場合もあります．このように定式化すると，強化学習は，なるべく多くの報酬を得ることを目的として，状態(ラベル)または状態の確率分布（連続値）を入力として，行為（ラベル）を出力する関数を学習することと定義できます．ただし強化学習は，その設定上，これまでの教師あり／教師なし学習とは違う問題になります．他の機械学習手法との違いは以下のようになります．\\begin{itemize}\\item 教師信号が間接的\\\\　「何が正解か」ではなく，時々報酬の形で与えられる\\item 報酬が遅れて与えられる\\\\　例)将棋の勝利，迷路のゴール\\item 探求が可能\\\\　エージェントが自分で学習データの分布を変えられる\\item 状態が確定的でない場合がある\\\\　確率分布でそれぞれの状態にいる確率を表す\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 8,
                                    "text": "報酬を得るために，環境に対して何らかの行為を行う意思決定エージェントの学習"
                                }
                            ],
                            "id": "3ca1d4de-5dcf-43ff-a78d-f9ca3b291861",
                            "question": "強化学習はどのような学習法ですか"
                        }
                    ]
                }
            ],
            "title": "1502"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，このように少ないデータでも学習が行えるように尤度計算の方法を単純にしても，学習データが少ないがゆえに生じる問題がまだあります．$n_i$を「学習データ中で，クラス$\\omega_i$に属するデータ数」，$n_{j}$を「クラス$\\omega_i$のデータ中で，ある特徴が値$x_j$をとるデータ数」としたとき，ナイーブベイズ識別に用いる尤度は，式(4.13)で最尤推定されます．ここで$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になるというゼロ頻度問題が生じます．たとえば，表3.3に示したweather.nominalデータでは， play=no のクラスで，outlook=overcast を特徴とする事例がありません．このようなゼロ頻度問題へ対処するには，確率のm推定という考え方を用います．これは$m$個の仮想的なデータがすでにあると考え，それらによって各特徴値の出現は事前にカバーされているという状況を設定します．各特徴値の出現割合$p$を事前に見積り，事前に用意する標本数を$m$とすると，尤度は式(4.14)で推定されます．このときの$m$を，等価標本サイズとよびます．この工夫によって，$n_j = 0$のときでも，式(4.14)の右辺の値が0にならず，ゼロ頻度問題が回避できることになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "$n_{j}$が0の場合，この特徴値に対する尤度が$0$になり，その結果，この特徴値を含むすべての事例の確率が$0$になる"
                                }
                            ],
                            "id": "6aaaf07d-b7a1-47d2-94f2-f1771098f153",
                            "question": "ゼロ頻度問題って何ですか"
                        }
                    ]
                }
            ],
            "title": "0411"
        },
        {
            "paragraphs": [
                {
                    "context": "人間の神経回路網は，3 階層のフィードフォワード型ネットワークよりはるかに複雑な構造をもっているはずです．そこで，誤差逆伝播法による学習が流行した1980 年代後半にも，ニューラルネットワークの階層を深くして性能を向上させようとする試みはなされてきました．しかし，多段階に誤差逆伝播法を適用すると，誤差が小さくなって消失してしまうという問題点があり，深い階層のニューラルネットワークの学習はうまくはゆきませんでした．しかし，2006 年頃に考案された事前学習法をきっかけに階層の深いディープニューラルネットワークに関する研究が盛んになり，様々な成果をあげてきました．ディープニューラルネットワークは，フィードフォワードネットワークの中間層を多層にして，性能を向上させたり，適用可能なタスクを増やそうとしたものです．しかし，誤差逆伝播法による多層ネットワークの学習は，重みの修正量が層を戻るにつれて小さくなってゆく勾配消失問題に直面し，思うような性能向上は長い間実現できませんでした．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 212,
                                    "text": "2006 年頃に考案された事前学習法"
                                }
                            ],
                            "id": "4230436d-93c8-44ff-ba75-34bc746ded2e",
                            "question": "階層の深いディープニューラルネットワークに関する研究が盛んになったきっかけはなんですか"
                        }
                    ]
                }
            ],
            "title": "0810"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで説明した最急勾配法は，最適化問題によく用いられる手法ですが，いくつか欠点もあります．まず，式(5.17)からわかるように，全データに対する誤差を計算してから重みを更新するので，一回の重みの更新に時間がかかり，データ数が多いと収束が遅くなります．また，最大の欠点として，最適化対象の関数がいくつかの局所的最適解をもつとき，その局所的最適解に陥りやすいといわれています．対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります．これらの問題に対処するために，重みの更新を全データで一括におこなうのではなく，ランダムに学習データを一つ選び，その学習データに対して重みを更新するという方法が考えられます．この方法を確率的最急勾配法とよびます．重みの更新式は，式(5.18)のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "対処法としては，初期値を変えて何回か試行するという方法が取られていますが，データの次元数が高い場合，その特徴空間にまんべんなく初期値を設定することは難しくなります"
                                }
                            ],
                            "id": "9e0a560f-7d8e-4dca-b47c-989b296763aa",
                            "question": "最急勾配法の問題点の対処法は何がありますか"
                        }
                    ]
                }
            ],
            "title": "0514"
        },
        {
            "paragraphs": [
                {
                    "context": "与えられたデータをまとまりに分ける操作を，クラスタリングといいます．「まとまりに分ける」という部分をもう少し正確にいうと，分類対象の集合を，内的結合と外的分離が達成されるような部分集合に分割することになります．つまり，一つのまとまりと認められるデータは相互の距離がなるべく近くなるように（内的結合），一方で，異なったまとまり間の距離はなるべく遠くなるように（外的分離），データを分けるということです．このようなデータの分割は，個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る階層的手法と，全体のデータの散らばり（トップダウン的情報）から最適な分割を求めてゆく分割最適化手法とに分類できます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 213,
                                    "text": "個々のデータをボトムアップ的にまとめてゆくことによってクラスタを作る"
                                }
                            ],
                            "id": "c984cd9b-6018-4f5b-b56e-158f3be66617",
                            "question": "階層的手法とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1103"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです"
                                }
                            ],
                            "id": "3002d377-afd5-41dc-84ac-73d4a1102fe1",
                            "question": "なぜ重みを更新するのですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "ナイーブベイズ識別器の「すべての特徴が，あるクラスのもとで独立」であるという仮定は，一般的には成り立ちません．だからといって，必ずしもすべての特徴が依存し合っているということでもありません．あいだをとって，「特徴の部分集合が，あるクラスのもとで独立である」と仮定することが現実的です．このような仮定を表現したものが，ベイジアンネットワークです．ベイジアンネットワークとは，確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデルです．依存関係は，アークに付随する条件付き確率表で定量的に表現されます．ベイジアンネットワークでは，確率変数間に条件付き独立の仮定を設けます．この仮定は，確率変数（ノード）の値は，その親（アークの元）となる確率変数の値以外のものには影響を受けないというものです．数式で表すと，確率変数の値$\\{z_1,\\dots,z_n\\}$の結合確率は，以下のように計算されます．ただし$\\mbox{Parents}(Z_i)$は，値$z_i$をとる確率変数を表すノードの親ノードの値です．親ノードは複数になる場合もあります．これらのパターンを組み合わせて，図4.9のようなベイジアンネットワークを構成することができます．ベイジアンネットにおけるノードの値の確率計算は，この3パターンと，そのバリエーション（親や子の数が異なる場合）だけなので，この計算を順に行うことで，ネットワーク全体の確率計算が行えます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 186,
                                    "text": "確率的に値を変化させる変数（以下，確率変数とよびます．本書の設定では，特徴やクラスにあたります）をノード，それらの間の依存関係をアーク（片方向の矢印）でグラフィカルに表した確率モデル"
                                }
                            ],
                            "id": "63fcbf45-a0d4-452c-bc30-2203ce1c821b",
                            "question": "ベイジアンネットワークって何ですか"
                        }
                    ]
                }
            ],
            "title": "0412"
        },
        {
            "paragraphs": [
                {
                    "context": "モデル木は，回帰木と線形回帰の双方のよいところを取った方法です．CARTはリーフの値が定数であったのに対して，モデル木ではリーフの値を線形回帰式とします．回帰木と同じ考え方で，データの出力値が近い区間を切り出せる特徴を選んでデータを分割してゆき，分割後のデータに対して線形回帰式を求めます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "回帰木と線形回帰の双方のよいところを取った方法"
                                }
                            ],
                            "id": "de636fe4-4036-482d-9261-f4779fe5e5ad",
                            "question": "モデル木って何ですか"
                        }
                    ]
                }
            ],
            "title": "0614"
        },
        {
            "paragraphs": [
                {
                    "context": "本章で扱う回帰問題は，過去の経験をもとに今後の生産量を決めたり，信用評価を行ったり，価格を決定したりする問題です過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習することが目標です．回帰問題は，正解付きデータから，「数値特徴を入力として数値を出力する」関数を学習する問題と定義できます(図6.1)．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 56,
                                    "text": "過去のデータに対するこれらの数値が学習データの正解として与えられ，未知データに対しても妥当な数値を出力してくれる関数を学習すること"
                                }
                            ],
                            "id": "32bcee12-14ac-45da-b354-67247936b81a",
                            "question": "回帰問題の学習目的はなんですか"
                        }
                    ]
                }
            ],
            "title": "0601"
        },
        {
            "paragraphs": [
                {
                    "context": "Grid search は，パラメータの可能な値をリストアップし，そのすべての組み合わせについて性能を評価して最も性能の高い組み合わせを求めます．図7.3のようなパラメータを組み合わせる空間をグリッドとよびます．SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 106,
                                    "text": "SVMでは，たとえばRBFカーネルの範囲を制御する$\\gamma$と，スラック変数の重み$C$は連続値をとるので，手作業でいろいろ試すのは手間がかかります．そこで，グリッドを設定して，一通りの組み合わせで評価実験をおこないます．"
                                }
                            ],
                            "id": "7e9935a7-3ce9-4691-87f2-81c1dcd7522e",
                            "question": "グリッドサーチはなんのためにやるんですか"
                        }
                    ]
                }
            ],
            "title": "0717"
        },
        {
            "paragraphs": [
                {
                    "context": "ラベル伝搬法の考え方は，特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築するというものです．近くのノードは同じクラスになりやすいという仮定で，正解なしデータの予測を行います．評価関数は式(14.1)に示すもので，この評価関数の最小化をおこないます．$f_i$は$i$番目のノードの予測値，$y_i$は$i$番目のノードの正解ラベル{ -1, 0, 1}，$w_{ij}$は$i$番目のノードと$j$番目のノードの結合の有無を表します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 12,
                                    "text": "特徴空間上のデータをノードとみなし，類似度に基づいたグラフ構造を構築する"
                                }
                            ],
                            "id": "e6ed907c-7f02-495b-a230-c30655607a6e",
                            "question": "ラベル伝搬法の考え方はどのようなものですか"
                        }
                    ]
                }
            ],
            "title": "1412"
        },
        {
            "paragraphs": [
                {
                    "context": "関数を学習するためのデータは，すべての要素が数値である特徴ベクトルと，その出力値（スカラーの場合も，ベクトルの場合もあります）の対として与えられます．識別問題との違いは，正解情報$y$が数値であるということです．特に数値型の正解情報のことをターゲットとよびます．しかし，回帰と「数値特徴を入力としてクラスラベルを出力する」識別問題との境界はそれほど明確ではありません．例えば，クラスによって異なる値をとるクラス変数を導入し，入力からクラス変数の値を予測する問題と考えると，識別問題を回帰問題として考えることもできます．実際，カーネル法など共通して使われる手法も多くあり，混乱しそうになってしまうのですが，まずはここでは「数値特徴を入力として数値を出力する」手法の習得に集中し，その全体像が見えてから，他の問題との関係を考えてゆきましょう．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "正解情報$y$が数値であるということ"
                                }
                            ],
                            "id": "03a42118-a521-47b2-9c30-6cfde665544e",
                            "question": "回帰問題が識別問題と異なるのはどのような点ですか"
                        }
                    ]
                }
            ],
            "title": "0602"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 503,
                                    "text": "誤差逆伝播法"
                                }
                            ],
                            "id": "cf1b795b-48f1-40ac-9850-3d04e6bd7915",
                            "question": "出力層の誤差を求めて、その誤差を中間層に伝播させて学習を行う手法をなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "上記の例は，普通の条件付き確率をベイジアンネットワークで表現したものです．しかし，ベイジアンネットワークの利点は，変数間の独立性を表現できることです．以下では，独立性を表現する基本パターンと，それぞれの確率計算の例を示します．最初のパターンはHead-to-tail connectionで，これは三つのノードが直線上に並んだものです．図4.6に，「曇っている」(Cloudy)，「雨が降った」(Rain)，「芝生が濡れている」(Wet grass)がHead-to-tail connectionでつながっている例を示します．これは，真ん中のノードの値が与えられると，左のノードと右のノードが独立になるパターンです．もし，Rainの値が定まっていれば，Wet grassの値はCloudyの値とは無関係に，RainからのWet grassへのアークに付随している条件付き確率表のみから定まります．一方，Rainの値がわからないときは，Rainの値はCloudyの値に影響され，Wet grassの値はRainの値に影響されるので，CloudyとWet grassは独立ではありません．何も情報がない状態での「芝生が濡れている」確率は以下のようになります．まず，「曇っている」の事前確率$P(C)$を使って「雨が降った」確率$P(R)$を求め，それを使って「芝生が濡れている」確率$P(W)$を求めます．ここで，「曇っている」ことが観測されたとします．そうすると，その条件の下で「芝生が濡れている」確率$P(W|C)$は，以下のようになります．つまり，「曇っている」ことの観測が，「芝生が濡れている」確率を変化させているので，これらは独立していないことになります．なお，確率伝播の計算は，逆方向にも可能です．「芝生が濡れている」ことがわかったときに，その日が「曇っている」確率は以下のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 57,
                                    "text": "変数間の独立性を表現できること"
                                }
                            ],
                            "id": "48db9f4e-3e79-43cc-807b-fdaaf1428810",
                            "question": "ベイジアンネットワークの利点とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0413"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "73c22173-a11a-4a8a-9d8c-1b8fed3b0714",
                            "question": "回帰木とはなに"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "引数が負のときは0，0以上のときはその値を出力"
                                }
                            ],
                            "id": "9759146c-1b58-434b-b89a-6297c1aa7f06",
                            "question": "ReLUとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "前項で説明した基底関数ベクトルによる非線形識別面は，重みパラメータに対しては線形で，入力を非線形変換することによって実現されています．一方，ここで説明するニューラルネットワークによる非線形識別面は，非線形な重みパラメータによって実現されています．なぜノードを階層的に組むと非線形識別面が実現できるかというと，図8.4に示すように，個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるからです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 165,
                                    "text": "個々のノードの出力であるロジスティック関数を重みを加えて足し合わせることで，データの境界の形に合わせた識別面を構成することができるから"
                                }
                            ],
                            "id": "a02e7a2f-de27-48a4-804f-45e82932a0c6",
                            "question": "ノードを階層的に組むと非線形識別面が実現できるのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0803"
        },
        {
            "paragraphs": [
                {
                    "context": "第3章から第5章では，正解情報の付いた学習データを用いる教師あり学習の設定で，識別をおこなうモデルを学習する方法について説明します．まず第3章と第4章は，カテゴリカルデータからなる特徴ベクトルを入力として，それをクラス分けする（すなわち属するクラスラベルを出力する）識別器を作る方法について学びます（図3.1）．識別問題は教師あり学習なので，学習データは特徴ベクトル$\\bm{x}_i$と正解情報$y_i$のペアからなります．ここでの設定は，特徴ベクトル$\\bm{x}_i$の各次元および正解情報$y_i$がいずれもカテゴリです．特にカテゴリ形式の正解情報のことをクラスとよびます．このカテゴリ特徴に対する「教師あり・識別」問題に対して，いかに納得のゆく概念モデルを獲得するか，という点に重点を置いたものが，この章で説明する概念学習です．一方，特徴が与えられたときに，それがあるクラスに属する確率を計算するモデルの獲得を目的とするものが，第4章で説明する統計的手法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 267,
                                    "text": "カテゴリ形式の正解情報のこと"
                                }
                            ],
                            "id": "35ec5ac7-4937-40de-99e7-85b722c05307",
                            "question": "クラスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0302"
        },
        {
            "paragraphs": [
                {
                    "context": "協調フィルタリングの前提は，どの個人がどの商品を購入したかが記録されているデータがあることです．そして，新規ユーザが，ある商品を購入したときに，記録済みのデータと購入パターンが似ているデータを探して，検索結果のユーザが購入していて，かつ新規ユーザが購入していない商品を推薦するというのが，基本的な考え方です．しかし，この個人別の購入データは，前節までのトランザクションデータとどうように，まばらに値が入っているデータです．購入パターンが似ているユーザを探す際に，データをベクトルとみなして，コサイン類似度による計算をおこなっても，ほとんど一致する項目数を数えているに過ぎないような状況になってしまいます．そこで，購入データをもっと低次元の行列に分解し，ユーザ・商品の特徴を低次元のベクトルで抽出する方法が考えられました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 14,
                                    "text": "どの個人がどの商品を購入したかが記録されているデータ"
                                }
                            ],
                            "id": "4170672a-6ba6-4399-aea4-866d728bbb78",
                            "question": "協調フィルタリングではどのようなデータが必要ですか"
                        }
                    ]
                }
            ],
            "title": "1219"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，線形回帰式の重みに注目します．一般的に，入力が少し変化したときに，出力も少し変化するような線形回帰式が，汎化能力という点では望ましいと思われます．このような性質を持つ線形回帰式は，重みの大きさが全体的に小さいものです．逆に，重みの大きさが大きいと，入力が少し変わるだけで出力が大きく変わり，そのように入力の小さな変化に対して大きく変動する回帰式は，たまたま学習データの近くを通っているとしても，未知データに対する出力はあまり信用できないものだと考えられます．また，重みに対する別の観点として，予測の正確性よりは学習結果の説明性が重要である場合があります．製品の品質予測などの例を思い浮かべればわかるように，多くの特徴量からうまく予測をおこなうよりも，どの特徴が製品の品質に大きく寄与しているのかを求めたい場合があります．線形回帰式の重みとしては，値として0となる次元が多くなるようにすればよいことになります．つまり，回帰式中の係数$\\bm{w}$に関して，大きな値を持つものがなるべく少なくなる，あるいは値0となるものが多くなるような方法が必要になります．そのための工夫を正則化とよび，誤差の式に正則化項と呼ばれる項を追加することで実現します．パラメータ$\\bm{w}$の二乗を正則化項とするものをRidge回帰とよびます．Ridge回帰に用いる誤差評価式を式(6.7)に示します．ここで，$\\lambda$は正則化項の重みで，大きければ性能よりも正則化の結果を重視，小さければ性能を重視するパラメータとなります．最小二乗法でパラメータを求めたときと同様に，$\\bm{w}$で微分した値が0となるときの$\\bm{w}$の値を求めると，式(6.8)のようになります．Ridgeは山の尾根という意味で，単位行列が尾根のようにみえるところから，このように名付けられたといわれています．一般に，Ridge回帰は，パラメータの値が小さくなるように正則化されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "入力が少し変化したときに，出力も少し変化する"
                                }
                            ],
                            "id": "fecb71e9-1b4e-403b-9592-831648ff7bc7",
                            "question": "汎化能力という点で望ましい線形回帰式の性質はなんですか"
                        }
                    ]
                }
            ],
            "title": "0606"
        },
        {
            "paragraphs": [
                {
                    "context": "そして，問題となる結合重みの学習ですが，単純な誤差逆伝播法ではやはり勾配消失問題が生じてしまいます．この問題に対する対処法として，リカレントニューラルネットワークでは，中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をします．この方法を，長・短期記憶（Long Short-Term Memory; LSTM）とよび，メモリユニットをLSTMセルとよびます．LSTMセルは，入力層からの情報と，時間遅れの中間層からの情報を入力として，それぞれの重み付き和に活性化関数をかけて出力を決めるところは通常のユニットと同じです．通常のユニットとの違いは，内部に情報の流れを制御する3つのゲート（入力ゲート・出力ゲート・忘却ゲート）をもつ点です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 164,
                                    "text": "LSTM"
                                }
                            ],
                            "id": "f6e94636-d09e-46c4-9005-9a2f8b2787ba",
                            "question": "中間層のユニットを，記憶構造をもつ特殊なメモリユニットに置き換えるという工夫をなんというか"
                        }
                    ]
                }
            ],
            "title": "0917"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習では，学習に用いられるデータに正解情報が付いていません．入力ベクトル$\\bm{x}_i$の次元数に関しては，教師あり学習の場合と同様に，$d$次元の固定長ベクトルで，各要素は数値あるいはカテゴリのいずれかの値をとると考えておきます．教師なし学習は，入力データに潜む規則性を学習することを目的とします．ここで着目すべき規則性としては，2通り考えられます．一つめは，入力データ全体を支配する規則性で，これを学習によって推定するの問題がモデル推定 (model estimation)です．もう一つは，入力データの部分集合内あるいはデータの部分集合間に成り立つ規則性で，通常は多数のデータの中に埋もれてみえにくくなっているものです．これを発見する問題がパターンマイニング (pattern mining) です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 130,
                                    "text": "入力データに潜む規則性"
                                }
                            ],
                            "id": "0639cff6-08c7-45b6-b641-24d37245181c",
                            "question": "教師なし学習では何を学習しますか"
                        }
                    ]
                }
            ],
            "title": "0113"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，一般の機械学習の問題では，どのクラスが出やすいかという事前確率や，各クラスから生じる特徴の尤もらしさを表す尤度はわかりません．そこで，この事前確率や尤度を計算する確率モデルを仮定し，そのパラメータを学習データに最も合うように調整することを考えます．それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します．学習データ全体$D$が生成される確率$P(D)$は，個々の事例$\\{\\bm{x}_1,\\dots,\\bm{x}_N\\}$の独立性、すなわち i.i.d. を仮定すると，式(4.5)のように，個々の事例が生成される確率の積で求めることができます．$P$は，データの生成確率を何らかのパラメータに基づいて計算するモデルです．ある程度複雑なモデルでは，パラメータが複数あることが一般的なので，これらのパラメータをまとめて$\\bm{\\theta}$と表記して明示すると，式(4.5)は式(4.6)のように書けます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 128,
                                    "text": "それぞれのクラスは，特徴ベクトルを生成する何らかの確率分布をもっていて，学習データはその確率分布から，各事例独立に生成されたものと仮定します．この仮定をi.i.d. (independent and identically distributed) と表記します"
                                }
                            ],
                            "id": "5c95cd20-82c0-44ed-9ef9-2243db944c9f",
                            "question": "i.i.d. (independent and identically distributed) とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0406"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，簡単なカーネルについてその非線形変換$\\phi$を求めてみましょう．特徴ベクトルを2次元として多項式カーネル(p=2)を展開します．したがって，$\\bm{x}=(x_1, x_2)$のとき，$\\phi(\\bm{x})=(x_1^2, x_2^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1, \\sqrt{2} x_2, 1) )$となります．この変換の第3項に注目してください．特徴の積の項が加わっています．積をとるということは，2つの特徴が同時に現れるときに大きな値になります．すなわち，共起の情報が加わったことになります．このような，非線形変換で線形分離可能な高次元にデータを飛ばしてしまい，マージン最大化基準で信頼できる識別面を求めるというSVMの方法は非常に強力で，文書分類やバイオインフォマティックスなど様々な分野で利用されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 351,
                                    "text": "文書分類やバイオインフォマティックスなど"
                                }
                            ],
                            "id": "002f0b30-e670-4377-bb3d-5ce65a36539b",
                            "question": "SVMの方法は例えば何に使われていますか"
                        }
                    ]
                }
            ],
            "title": "0713"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "d691d03f-a52d-44f4-9b83-634cda06669c",
                            "question": "k-平均法って何ですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "教師なし学習の実用的な応用例として，異常検出があります．異常検出の問題設定は，入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです．ここでは，最も基礎的な異常検出として，外れ値の検出について説明します．外れ値は，学習データに含まれるデータの中で，ほかと大きく異なるデータを指します．たとえば，全体的なデータのまとまりから極端に離れたデータや，教師ありデータの中で，一つだけほかのクラスのデータに紛れ込んでしまっているようなデータです．これらは，計測誤りや，教師信号付与作業上でのミスが原因で生じたと考えられ，学習をおこなう前に除去しておくのが望ましいデータです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 39,
                                    "text": "入力$\\{\\bm{x}_i\\}$に含まれる異常値を，教師信号なしで見つけることです"
                                }
                            ],
                            "id": "31828a43-e9af-4199-9835-d4cc1657cd57",
                            "question": "異常検出とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1111"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは，この最尤推定法を使って，式(4.4)の尤度を具体的に求める方法をみてゆきましょう．多次元ベクトル$\\bm{x}$を要素に分けて表記すると，式(4.11)のようになります．尤度$ P(x_{1}, \\dots, x_d \\vert \\omega_i)$を統計で求めるためには，学習データ中からクラス$\\omega_i$に属するデータを取り出し，そのデータに対してすべての特徴値の組み合わせが，それぞれ何回起こっているかをカウントすることになります．式(4.1)のところでの考察に比べると，条件部にあてはまるデータがない，ということはないので少しはましですが，結論部にあてはまるデータが，統計をとれるほとに十分に揃っているということはなかなか望めそうにありません．そこで，各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法をナイーブベイズ識別法 (naive Bayes classifier)または単純ベイズ識別法とよびます．識別の結果を$C_{\\mbox{NB}}$とすると，ナイーブベイズ識別法は式(4.12)のように定義できます．この$P(x_{j} \\vert \\omega_i)$であれば，クラス$\\omega_i$のデータの中で，特徴値$x_{j}$をとるデータの頻度を数えることで確率を推定する，最尤推定をおこなうことで求めることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 339,
                                    "text": "各特徴はほかの特徴とは独立に値を決定するものと仮定をすると，特定の特徴について，特定の値をとるデータを集めることになるので，すべての特徴値の組み合わせに対するデータよりはかなり少ない量のデータで学習ができます．このような，特徴の独立性を仮定した識別法"
                                }
                            ],
                            "id": "cff42a97-ee69-4827-bc8e-d5b575e6dd8e",
                            "question": "ナイーブベイズ識別法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0410"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "b75d288b-1382-4f5c-b5aa-2056f71af0f6",
                            "question": "DNNとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 398,
                                    "text": "識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたもの"
                                }
                            ],
                            "id": "9969d750-0f40-46a2-95b6-d944714897af",
                            "question": "二乗誤差とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "こちらは，モデルのパラメータが与えられたときの，学習データ全体が生成される尤度を表しています．ここで，確率は1以下であり，それらの全学習データ数回の積はとても小さな数になって扱いにくいので，式(4.6)の対数をとって計算します．式(4.7)で計算される値を対数尤度$\\mathcal{L}(D)$とよびます．この対数尤度の値は，大きければ大きいほど学習データがそのモデルから生成された確率が高い、ということがいえます．そして，学習データが，真のモデルから偏りなく生成されたものであると仮定すると，この方法で求めたモデルは真の分布に近い，と考えることができます．したがって，式(4.7)を最大にするパラメータが求まればよいわけです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 5,
                                    "text": "モデルのパラメータが与えられたときの，学習データ全体が生成される尤度"
                                }
                            ],
                            "id": "f21d3f45-035d-492d-8338-548137d03636",
                            "question": "対数尤度とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0407"
        },
        {
            "paragraphs": [
                {
                    "context": "式(7.7)がすべてのデータを正しく識別できる条件なので，この制約を弱める変数（スラック変数とよびます）$\\xi_i ~ (\\ge 0)$を導入して，$i$番目のデータが制約を満たしていない程度を示します．$\\xi_i$は制約を満たさない程度を表すので，小さい方が望ましいものです．この値を上記SVMのマージン最大化（$|| \\bm{w} ||^2$の最小化）問題に加えます．これをラグランジュの未定乗数法で解くと，結論として同じ式が出てきて，ラグランジュ乗数$\\alpha_i$に$0 \\le \\alpha_i \\le C$という制約が加わります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "制約を満たさない程度を表すので，小さい方が望ましい"
                                }
                            ],
                            "id": "97bb085c-d00a-4a98-a286-76342d0636eb",
                            "question": "スラック変数が小さいほうがいいのはなぜか"
                        }
                    ]
                }
            ],
            "title": "0708"
        },
        {
            "paragraphs": [
                {
                    "context": "ただし，このままでは，$g(\\bm{x})$は$\\bm{x}$の値次第で，極端に大きな（あるいは小さな）値となる可能性があり，確率と対応づけることが難しくなります．$g(\\bm{x})$の望ましいふるまいは，出力範囲が0以上1以下ので，正例に属する$\\bm{x}$には1に近い値を，負例に属する$\\bm{x}$には0に近い値を出力することです．このようなふるまいは，変換$1/(1+e^{-g(\\bm{x})})$を行い，これを式(5.10)に示すように事後確率$p(\\oplus|\\bm{x})$（ただし，$\\oplus$は正のクラス）と対応付けることで実現できます．この場合，$\\bm{x}$が負のクラスになる確率は$p(\\ominus|\\bm{x}) = 1 - p(\\oplus|\\bm{x})$（ただし，$\\ominus$は負のクラス）で求められます．式(5.10)はシグモイド関数（図5.6）とよばれるもので，$g(\\bm{x}) = w_0+\\bm{w}\\cdot\\bm{x}$がどのような値をとっても，シグモイド関数の値は0から1の間となります．また，$g(\\bm{x})=0$のとき，式(5.10)の値は0.5となり，これは確率を表現するのに適しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 310,
                                    "text": "$p(\\ominus|\\bm{x}) = 1 - p(\\oplus|\\bm{x})$（ただし，$\\ominus$は負のクラス）"
                                }
                            ],
                            "id": "c881ca20-383a-4c7f-bc4d-99ebc4c693ec",
                            "question": "入力が負例のときは、確率はどう求められますか"
                        }
                    ]
                }
            ],
            "title": "0511"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 85,
                                    "text": "2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの"
                                }
                            ],
                            "id": "2a2b1d42-1d4d-4062-9069-91b4786f9956",
                            "question": "X-meansアルゴリズムとはなんですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，ID3アルゴリズムにおける過学習について考えてみます．過学習とは，文字通りの意味は学習しすぎるということですが，機械学習においては，モデルが学習データに特化しすぎたために，未知データに対して性能が下がる現象を指します．ID3アルゴリズムのバイアスは単純に表現すると，「小さい木を好む」となります．なぜ小さな木を結果とするのでしょうか．これはオッカムの剃刀 (Occam's razor) と呼ばれる「データに適合する最も単純な仮説を選べ」という基準に基づいています．長い仮説なら表現能力が高いので，偶然に学習データを説明できるかもしれないのですが，短い仮説だと，表現能力が低いので，偶然データを説明できる確率は低くなります．もし，ID3アルゴリズムによって，小さな木で学習データが説明できたとすると，これは偶然である確率は相当低くなります．すなわち偶然でなければ必然である，というわけです．ところが，このバイアスを持って学習を行ったとしても，最後の1例までエラーがなくなるように決定木を作成してしまうと，その決定木は成長しすぎて，学習データに適応しすぎた過学習になりがちです．そこで，過学習への対処として，適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法があります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 507,
                                    "text": "適当なところで決定木の成長を止める方法（葉の要素数を一定値以下にならないようにする）や，完全に学習させた後，枝刈りするという方法"
                                }
                            ],
                            "id": "7c4e672d-dbd6-4d1d-bd28-8ae66181e879",
                            "question": "過学習を避けるためにはどのような方法がありますか"
                        }
                    ]
                }
            ],
            "title": "0313"
        },
        {
            "paragraphs": [
                {
                    "context": "これまでに述べてきた機械学習の分類では，学習データすべてに対して正解が与えられているか，あるいはまったく与えられていないかのいずれかでした．その中間的な設定として，学習データの一部にだけ正解が与えられている場合が考えられます．学習データに正解を与えるのは人間なので，正解付きのデータを作成するにはコスト（費用・時間）がかかります．一方，正解なしのデータならば，容易にかつ大量に入手可能であるという状況があります．たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります．このような状況で，正解付きデータから得られた識別器の性能を，正解なしデータを使って向上させる問題を半教師あり学習 (semi-supervised learning) といいます．半教師あり学習は主として識別問題に対して用いられます．半教師あり学習の代表的な手法のアイディアを図1.11に示します．図1.11左のように，全データの中で正解の付加されたデータを丸・バツで表し，正解のないデータを三角形で表します．最初は丸・バツが付いたデータだけから識別器を作り，たとえば，その中間あたりに境界直線を引いたものとします．これに従って三角形のデータを分類しますが，境界線近辺のデータはあまり信用せず，境界線から大きく離れたものを確信度が高いとみなして正解を付与します．今度は，これらの新しく正解を付与されたデータも加えて，再度識別境界を計算します．これを，新しい正解付きデータが増えなくなるまで繰り返します．この学習法は，識別するべきクラスがうまくまとまっているようなデータや，識別結果によって有効な特徴が増えてゆくような，やや特殊なデータに対して適用するときにうまくゆきます．この手法を第14章で説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 206,
                                    "text": "たとえば，ある製品の評価をしているブログエントリーのPN判定をおこなう問題では，正解付きデータを1,000件作成するのはなかなか大変ですが，ブログエントリーそのものは，webクローラプログラムを使えば，自動的に何万件でも集まります"
                                }
                            ],
                            "id": "4c1f98ac-a2c2-436b-94c3-4bf049900dce",
                            "question": "半教師あり学習に適した状況はどのように作られますか"
                        }
                    ]
                }
            ],
            "title": "0117"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 337,
                                    "text": "学習データに正解が付いている場合の学習"
                                }
                            ],
                            "id": "b3a44f01-1ba2-4525-914b-a9c8e0043354",
                            "question": "教師あり学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "特徴ベクトルの次元数を減らすこと"
                                }
                            ],
                            "id": "3122fae0-cdcf-43ca-aac1-2c06df2d7245",
                            "question": "主成分分析ってどういうことをしているんですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 193,
                                    "text": "特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習"
                                }
                            ],
                            "id": "a6e1032f-8b23-4caa-8928-347b66cf9725",
                            "question": "深層学習とはどのようなものか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "作成する識別器に対して，誤りを減らすことに特化させるために，個々のデータに対して重みを設定します．バギングではすべてのデータの重みは平等でした．一方，ブースティングのアイディアは，各データに重みを付け，そのもとで識別器を作成します．最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします．そのように重みが変更されたデータ集合に対して，次の識別器を学習するというやりかたで，異なる識別器を逐次的に作成してゆきます．後から作られる識別器は，前段の識別器が誤ったデータを優先的に識別するようになるので，前段の識別器とは異なり，かつその弱いところを補うような相補的働きをします（図10.5）．ブースティングに用いる識別器の学習アルゴリズムは，基本的にはデータの重みを識別器作成の基準として取り入れている必要があります．ただし，学習アルゴリズムが重みに対応していない場合は，重みに比例した数を復元抽出してデータ集合を作ることで対応可能です．このように，前段での誤りに特化して逐次的に作成された識別器は，もとの学習データをゆがめて作成されているので，未知の入力に対しては，もとの学習データに忠実に作られた識別器（たとえば，図10.5の識別器1）とは，信頼性が異なります．したがって，バギングのように単純な多数決で結論を出すわけにはゆきません．各識別器に対してその識別性能を測定し，その値を重みとする重み付き投票で識別結果を出します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 116,
                                    "text": "最初は，平等な重みが付いた学習データによって識別器を作成し，その識別器によって誤識別されたデータに対して，その重みを増やします"
                                }
                            ],
                            "id": "4fe911bb-df63-4e79-a906-12f3a3cdd064",
                            "question": "ブースティングで、個々のデータに対してどのように重みを設定するのですか"
                        }
                    ]
                }
            ],
            "title": "1012"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 191,
                                    "text": "さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表した"
                                }
                            ],
                            "id": "41c7fb87-bf12-4780-b34e-575051a5847b",
                            "question": "BICとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "次に，学習データが線形分離可能でない場合を考えます．前節と同様に線形識別面を設定するのですが，その際，間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶこととします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 51,
                                    "text": "間違ったデータがあってもよいので，それらが識別面からあまり離れていないような識別面を選ぶ"
                                }
                            ],
                            "id": "7b8eee9e-083b-4323-8c5b-15e57a3e7513",
                            "question": "ソフトマージンとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0707"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，$\\bm{x}$は特徴ベクトルに$x_0=1$を加えた$d+1$次元ベクトル，$\\bm{w}$は$d+1$次元の重みベクトルとします．また，$\\eta$は学習係数で，適当な小さい値を設定します．このアルゴリズムは，学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します．これをパーセプトロンの収束定理とよびます．一方，学習データが線形分離不可能な場合にはこのアルゴリズムを適用することができません．全ての誤りがなくなることが学習の終了条件なので，データが線形分離不可能な場合はこのアルゴリズムは停止しません．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 111,
                                    "text": "学習データが線形分離可能な場合には必ず識別境界面を見つけて停止します"
                                }
                            ],
                            "id": "08fd24e2-434b-43e0-95bf-6d982a944ca4",
                            "question": "パーセプトロンの学習アルゴリズムはどのようなときに停止しますか"
                        }
                    ]
                }
            ],
            "title": "0507"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 94,
                                    "text": "引数が負のときは0，0以上のときはその値を出力"
                                }
                            ],
                            "id": "817e4f6b-533d-4cee-9c36-13a729823d2b",
                            "question": "ReLuってなんですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，式(4.2)に基づいて得られた事後確率の計算式を，記号を変えてもう一度見直してみます．式(5.3)の分子は，生成モデルとよばれる考え方で解釈することができます．まず，あるクラス$\\omega_i$が確率$P(\\omega_i)$で選ばれ，そのクラスから特徴ベクトル$\\bm{x}$が確率$p(\\bm{x} \\vert \\omega_i)$に基づいて生成されたという考え方です．これは式(5.4)の分子である特徴ベクトルとクラスの同時確率$p(\\omega_i, \\bm{x})$を求めていることになります．この生成モデルアプローチは，（学習データとは別に，何らかの方法で）事前確率がかなり正確に分かっていて，それを識別に取り入れたい場合には有効です．しかし，そうでない場合は，推定するべきパラメータは，$P(\\omega_i|\\bm{x})$を直接推定するよりも増えてしまいます．同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなるのが一般的です．つまり，生成モデルアプローチは，本来解くべき問題を，あえて難しい問題にしてしまっているのではないかという疑問が出てくるわけです．この問題への対処法として，次節では，$P(\\omega_i|\\bm{x})$を直接推定する方法について説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 394,
                                    "text": "同じ量のデータを用いて複数のパラメータを推定する場合，パラメータの量が増えるほど，問題が難しくなる"
                                }
                            ],
                            "id": "31415468-e10f-45bf-9bbb-3b449d207b2e",
                            "question": "なぜ生成モデルアプローチは問題を難しくしているといえるのですか"
                        }
                    ]
                }
            ],
            "title": "0504"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習における初期パラメータの調整で必要な，入力の情報をなるべく失わない，より少ないノードへの写像を学習する手段として，AutoencoderとRestricted Bolzmann Machine(RBM)がよく使われます．ここでは第8章で説明したフィードフォワード型のニューラルネットワークを用いたAutoencoderについて説明します．Autoencoderは，図9.5のように，3階層のフィードフォワード型のニューラルネットワークで自己写像を学習するものです．自己写像の学習とは，$d$次元の入力${\\bf f}$と，同じく$d$次元の出力${\\bf y}$の距離（誤差と解釈してもよいです）の全学習データに対する総和が最小になるように，ニューラルネットワークの重みを調整することです．距離は通常，ユークリッド距離が使われます．また，入力が0または1の2値であれば，出力層の活性化関数としてシグモイド関数が使えるのですが，入力が連続的な値を取るとき，その値を再現するために出力層では恒等関数を活性化関数として用います．すなわち，中間層の出力の重み付き和をそのまま出力します．Autoencoderではこのようにして得られた中間層の値を新たな入力として，1階層上にずらして同様の表現学習を行います．この手順を積み重ねると，入力に近い側では単純な特徴が，階層が上がってゆくにつれ複雑な特徴が学習されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 355,
                                    "text": "ユークリッド距離"
                                }
                            ],
                            "id": "7ca66d8d-d811-475d-a8a5-f298c073a117",
                            "question": "自己写像の学習において使われる入力と出力の距離は何ですか"
                        }
                    ]
                }
            ],
            "title": "0908"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 43,
                                    "text": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディア"
                                }
                            ],
                            "id": "cc6f6f4c-60e6-418e-ad40-bde265c5b4a5",
                            "question": "事前学習法の考え方は何ですか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 6,
                                    "text": "識別における決定木の考え方を回帰問題に適用する方法"
                                }
                            ],
                            "id": "81e7273d-1701-4018-b567-fd0fea74d180",
                            "question": "回帰木ってなんですか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 452,
                                    "text": "規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比"
                                }
                            ],
                            "id": "3436de4e-7fc9-4f20-ae0d-f6e451daa594",
                            "question": "リフト値とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "この節では，教師あり／教師なしのそれぞれのデータから規則を学習する方法を考えてみます．規則の学習では，学習データが教師ありか教師なしかで，問題設定や学習アルゴリズムが異なります．教師ありデータからの規則の学習では，結論部はclass特徴と決まっていました．今から考える教師なしデータの場合，どの特徴（あるいはその組合せ）が結論部になるのかはわかりません．むしろ，結果として得られた規則のそれぞれが，異なった条件部・結論部をもつことが多くなります．たとえば，「商品Aを購入したならば，商品Bを購入することが多い」，「商品Cを購入したならば，商品Dと商品Eを購入することが多い」といった規則になります．まず，規則の有用性をどのようにして評価するか，ということです．ここでは，確信度(confidence)とLift値を紹介します．確信度は，規則の条件部が起こったときに結論部が起こる割合を表し，式(12.3)で計算します．この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます．リフト値は，規則の結論部だけが単独で起こる割合と，条件部が起こったときに結論部が起こる割合との比を表し，式(12.4)で計算します．この値が高いほど，得られる情報の多い規則であることを表しています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 409,
                                    "text": "この割合が高いほど，この規則にあてはまる事例が多いと見なすことができます"
                                }
                            ],
                            "id": "26b4cbd5-07e2-4633-882d-b7b4282184ac",
                            "question": "確信度の値からどのようなことがわかりますか"
                        }
                    ]
                }
            ],
            "title": "1209"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で典型的な例としてあげた形態素解析は，単語の系列を入力として，それぞれの単語に品詞を付けるという問題です(図13.1)．形態素の列はある言語の文を構成するので，その言語の文法に従った並び方が要求されます．たとえば，日本語の形態素列は，形容詞の後には名詞がくることが多い，助詞の前には名詞がくることが多いなどの傾向が，明らかに存在します．また，地名・人名・組織名・日時などの特定の表現を文中から特定の表現を抜き出す固有表現抽出（チャンキングとも呼びます）も，系列ラベリングの典型的な問題です．1単語が1表現になっていれば形態素解析と同じ問題ですが，複数の単語で一つの表現になっている場合があるので，その並びにラベルを付けます．ラベルの付け方は，その表現の開始を表すB (Beginning)，2単語目以降の表現の構成要素を指すI (Inside)，表現外の単語を表すO (Outside)の3種類になります．これは，Iの前は必ずBかIであることや，BやIの連続出現数にそれぞれおおよその上限数があることなど，出力の並びに一定の制約があります．このラベル方式にはIOB2タグという名前がついています．たとえば，文中から「人を指す表現」を抽出した結果は，図13.2のようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 21,
                                    "text": "単語の系列を入力として，それぞれの単語に品詞を付けるという問題"
                                }
                            ],
                            "id": "779cefe9-3abd-47cb-8063-322368036ddd",
                            "question": "形態素解析とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1303"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 16,
                                    "text": "各状態でどの行為を取ればよいのかという意思決定規則"
                                }
                            ],
                            "id": "219aa551-31ac-4421-bee1-691e3b8a5445",
                            "question": "政策とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "概念学習手法が研究されていた初期の頃には，概念の表現形式を限定することで，データに当てはまる概念の仮説を少なくし，その仮説の空間を探索することで概念を求める手法が開発されました．そのような手法として，FIND-Sアルゴリズムや，候補削除アルゴリズムがあります（図3.2）．FIND-Sアルゴリズムは，仮説の表現を，特徴に対する制約を論理積(AND)で結合したものに制限します．このように，仮説に対して課す制約をバイアスとよびます．最初は，最も特殊な仮説（いかなる事例も正ではない）からスタートし，正例を一つずつ読み込んで，その事例の値を受け入れるように仮説を最低限一般化します．たとえば，表3.1のデータにおいて，最初の正例である1番のデータから，論理式「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ astigmatism=no $\\wedge$  tear-prod = reduced」が得られます．次の正例である3番のデータは，age, spectacle-prescrip, tear-prodの値はこの論理式に当てはまりますが，astigmatismの値が異なります．1番と3番のデータの両方が当てはまるようにするために，この論理式から，astigmatismの条件を取り除き，新たな仮説を「age=young $\\wedge$ spectacle-prescrip=myope $\\wedge$ tear-prod = reduced」とします．これを続けると，5番のデータでspectacle-prescripの条件が落ち，9番のデータでageの条件が落ち，最後は16番のデータでtear-prodの条件まで落ちて，条件が何もなくなってしまいます．これでは，すべての入力が正例であるという概念になり，明らかにおかしな結果になってしまいました．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 194,
                                    "text": "仮説に対して課す制約"
                                }
                            ],
                            "id": "d929dcbe-db35-44c2-8758-d99bf39b4c3c",
                            "question": "バイアスとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0305"
        },
        {
            "paragraphs": [
                {
                    "context": "k-Meansアルゴリズムにおける，事前にクラスタ数$k$を固定しなければいけないという問題点を回避する手法として，クラスタ数を適応的に決定する方法が考えられます．最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもので，この方法をX-meansアルゴリズムとよびます．このアルゴリズムの勘所は，さらなる分割が適当であるかどうかの判断k基準として，分割前後でモデルのよさを定量的に表したBIC (Bayesian information criterion) 値を比べるという点です．BICは以下の式で得られる値です．ただし，$\\log L$はモデルの対数尤度（各クラスタの正規分布を所属するデータから最尤推定し，その分布から各データが出現する確率の対数値を得て，それを全データについて足し合わせたもの）$q$はモデルのパラメータ数（クラスタ数に比例），$N$はデータ数です．BICは小さいほど，得られたクラスタリング結果がデータをよく説明しており，かつ詳細になりすぎていない，ということを表します．モデルを詳細にすればするほど（クラスタリングの場合はクラスタ数を増やせば増やすほど），対数尤度を大きくすることができます．極端な場合，1クラスタに1データとすると，そのクラスタの正規分布の平均値がそのデータになるので，その分布の最も大きい値をとる（BICを小さくする方向に寄与する）ことになります．しかし，その場合，$q$の値が大きくなるので，BIC全体としては大きな値となってしまい，対数尤度とクラスタ数のバランスが取れたものが選ばれるという仕組みになっています．クラスを表すモデルのよさを定量的に表現する基準はひとつではありませんが，ここで示したBICや，同様の目的で使われるAIC(Akaike information criterion)は，モデルの対数尤度とパラメータの複雑さのバランスを取った式で，その評価値を表していることから，さまざまなモデル選択の状況で用いられています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 82,
                                    "text": "最初は2分割から始まって，得られたクラスタに対して分割が適当でないと判断されるまで，k-means法によるクラスタリングを繰り返すというもの"
                                }
                            ],
                            "id": "de06ed5d-57a0-4414-accc-c034017a43b8",
                            "question": "x-meansアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1110"
        },
        {
            "paragraphs": [
                {
                    "context": "この問題を解決する手法として，事前学習法(pre-training)が考案されました．誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておくというアイディアです．この事前学習は，入力$\\bm{x}$の情報をなるべく失わないように，入力層側から1層ずつ順に教師なし学習で行います(図9.4)．入力層から上位に上がるにつれノードの数は減るので，うまく特徴となる情報を抽出しないと情報を保持することはできません．このプロセスで，元の情報を保持しつつ，抽象度の高い情報表現を獲得してゆくことを階層を重ねて行うことが深層学習のアイディアです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 15,
                                    "text": "事前学習法"
                                }
                            ],
                            "id": "ac881286-6fab-4272-a6ac-06599e15bd02",
                            "question": "誤差逆伝播法を用いた教師あり学習を行う前に，何らかの方法で重みの初期パラメータを適切なものに事前調整しておく方法は何といいますか"
                        }
                    ]
                }
            ],
            "title": "0907"
        },
        {
            "paragraphs": [
                {
                    "context": "ノードを階層状に組むことによって，複雑な非線形識別面が実現することが可能なことはわかりました．問題は，その非線形識別面のパラメータをどのようにしてデータから獲得するか，ということです．もし，入力層から中間層への重みが固定されていると仮定すると，各学習データに対して，各中間層の値が決まります．それを新たな学習データとみなして，もとのターゲットを教師信号とするロジスティック識別器の学習を行えば，中間層から出力層への重みの学習を行うことができます．しかし，この方法では中間層が出力すべき値がわからないので，入力層から中間層への重みを学習することができません．出力すべき値はわかりませんが，望ましい値との差であれば計算することができます．出力層では，出力値とターゲットとの差が，望ましい値との差になります．もし，出力層がすべて正しい値を出していないとすると，その値の算出に寄与した中間層の重みが，出力層の更新量に応じて修正されるべきです．この，重みと出力層の更新量の積が，中間層が出力すべき値との差になります．このように，出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法(図8.5)を{\\bf 誤差逆伝播法}とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 459,
                                    "text": "出力層の誤差を求めて，その誤差を中間層に伝播させて学習を行う手法"
                                }
                            ],
                            "id": "29fc1428-2b87-43f7-9803-16ddd1cc8604",
                            "question": "誤差逆伝播法とはどういう手法ですか"
                        }
                    ]
                }
            ],
            "title": "0805"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 428,
                                    "text": "学習時の自由度を意図的に下げていること"
                                }
                            ],
                            "id": "6b0fbe28-820e-4ef4-ae03-281f5c1fae80",
                            "question": "ドロップアウトによって過学習が生じにくくなっている理由はなんですか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "それでは学習です，とゆきたいところですが，その前に学習結果の評価基準を設定します．ここで扱っているデータはirisデータなので，教師あり・識別の場合の評価基準を考えます．この場合，学習データに対して正解率100\\%でも意味がありません．未知データに対してどれだけの正解率が期待できるかが評価のポイントですが，どうやって未知データで評価すればよいのでしょうか．学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法が考えられます．この方法を分割学習法とよびます．評価用に半分というのは，多すぎるように見えるかもしれませんが，評価用データがあまりに少ないと，未知データの分布と全く異なる可能性が高くなり，評価そのものが信頼できなくなります．また，学習パラメータの調整をおこなうような場合では，データを学習用・調整用・評価用と分けるケースもあります．しかし，irisデータは150事例しかないので，分割学習法で評価するのは難しそうです．このような場合，一般的には交差確認法(cross validation method: CV法)とよばれる方法を用いて評価します(図2.7)．この方法では学習データを$m$個の集合に分割し，そのうちの$m-1$個で学習を行い，除外した残りの一つで評価を行います．そして，その除外するデータを順に交換することで，合計$m$回の学習と評価を行います．これで，全データがひととおり評価に使われ，かつその評価時に用いられる識別器は評価用データを除いて構築されたものとなっています．$m$を交差数とよび，技術論文では交差数$m$を10とするケース (10-fold CV) や，データの個数とするケースがよく見られます．$m$がデータの個数の場合を一つ抜き法(leave-one-out method)とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 179,
                                    "text": "学習データが大量にある場合は，半分を学習用，残り半分を評価用として分ける方法"
                                }
                            ],
                            "id": "ebb974ee-ba1a-4ef5-97d1-aec355ad86eb",
                            "question": "分割学習法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0206"
        },
        {
            "paragraphs": [
                {
                    "context": "マルコフ決定過程における学習は，各状態でどの行為を取ればよいのかという意思決定規則を獲得してゆくプロセスです．意思決定規則のことを政策$\\pi$と呼び，状態から行為への関数の形で表現します．政策の良さは，その政策に従って行動したときの累積報酬の期待値で評価します．状態$s_t$から政策$\\pi$に従って行動したときに得られる累積報酬の期待値は式(15.2)のように計算できます．ただし，$\\gamma (0 \\le \\gamma < 1)$は割引率で，後に得られる報酬ほど割り引いて計算するための係数です．この係数を累積計算に組み込むことで，同じ報酬にたどり着けるのであればより短い手順を優先することになり，ずっと先の報酬を小さくすることで累積計算を収束させることにもなります．累積報酬の期待値を全ての状態に対して最大となる政策を最適政策$\\pi^*$といいます．マルコフ決定過程における学習の目標は，この最適政策$\\pi^*$を獲得することです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 102,
                                    "text": "その政策に従って行動したときの累積報酬の期待値で評価"
                                }
                            ],
                            "id": "4ef9115a-30b8-4de9-b004-78bc8e2b707c",
                            "question": "政策の良さはどのように評価されますか"
                        }
                    ]
                }
            ],
            "title": "1506"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 542,
                                    "text": "入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するもの"
                                }
                            ],
                            "id": "9386663e-368b-4ee0-8c5f-5c16bb8fd0d6",
                            "question": "回帰とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "分割最適化クラスタリングの代表的手法であるk-meansクラスタリング (k-平均法)では，クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します（図11.6）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 46,
                                    "text": "クラスタ数$k$をあらかじめ与え，各クラスタの平均ベクトルを乱数で生成し（あるいはランダムに$k$個データを選んで平均ベクトルとし），各データを最も近い平均ベクトルを持つクラスタに所属させた後，それぞれのクラスタの平均ベクトルを，その所属するデータから再計算します．この手順を，全クラスタの平均ベクトルが動かなくなるまで繰り返します"
                                }
                            ],
                            "id": "a0103298-0c56-4d3d-a537-2df1ee46829f",
                            "question": "k-meansアルゴリズムとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1108"
        },
        {
            "paragraphs": [
                {
                    "context": "前節の誤り訂正学習は，学習データが線形分離可能であることを前提にしていました．しかし，現実のデータではそのようなことを保証することはできません．むしろ，線形分離不可能な場合の方が多いと思われます．そこで，統計的手法での対数尤度最大化のように，識別関数法でもなんらかの基準で識別関数の良さを定量的に表して，それを最大化するという方法で，線形分離不可能なデータにも対処することを考えます．しかし，識別関数の「良さ」を定義するよりは，「悪さ」を定義する方が簡単なので，識別関数が誤る度合いを定量的に表して，それを最小化するという方法を考えます．個々のデータに対する識別関数の「悪さ」は，その出力と教師信号との差で表すことができます．しかし，データ集合に対して，この「悪さ」を足し合わせてしまうと，出力よりも教師信号が大きい場合と，出力よりも教師信号が小さい場合の効果が打ち消し合ってしまいます．そこで，識別関数の出力と教師信号との差の2乗を，全データに対して足し合わせたものを識別関数の「悪さ」と定義し，この量を二乗誤差と呼びます．この二乗誤差を最小にするように識別関数を調整する方法が，最小二乗法による学習です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 465,
                                    "text": "二乗誤差を最小にするように識別関数を調整する方法"
                                }
                            ],
                            "id": "696d9872-86e0-445f-a625-38a118683a0c",
                            "question": "最小二乗法って何ですか"
                        }
                    ]
                }
            ],
            "title": "0508"
        },
        {
            "paragraphs": [
                {
                    "context": "ID3アルゴリズムで用いた情報獲得量は，値の種類が多い特徴ほど大きな値になる傾向があります．一般に，その性質は悪いものではないのですが，値の種類が極端に多い場合には問題があります．例えば表3.3の特徴として，日付(date)があったとし，その値が全てのデータで異なっているとします．この場合，dateによって分割した集合は要素数が1となって，そのエントロピーは0となりますので，$\\mbox{Gain}(D, date)$の値は最大値である$E(D)$になって，この特徴が決定木のルートに選ばれることになります．こうして出来た決定木ではテスト例は分類できません．そこで，分割の程度を式(3.3)によって評価し，分割が少ない方が有利になるように式(3.4)で定義された獲得率を用いて特徴を選択することもあります．また，学習データの性質や学習の目的によって，データの乱雑さを評価する基準も変化することがあります．データの乱雑さを不純度(impurity)と定義すると，先述のエントロピー以外にいくつかの可能性を考えることができます．式(3.5)で計算されるGini Inpurityは，分割後のデータの分散を表します．この性質は回帰木の作成で用いますので，そこで再度，解説します．また，Gini Inpurityの平方根を取って，最大値がGini Inpurityと同じ0.5になるように係数を補正したものをRootGiniImpurityとして，式(3.6)で定義します．いずれも，分割前後の値の差によって選ぶ特徴を決めるのですが，獲得率やジニ不純度は，正例・負例の数に偏りがあると，多数派の性能の影響が大きくなってしまいます．一方，RootGiniImpurityは，分割前のGini Inpurityと，分割後の重み付きGini Inpurityの比を計算していることになり，正例・負例の数に偏りがあっても，分割基準としては影響を受けないようになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 491,
                                    "text": "分割後のデータの分散"
                                }
                            ],
                            "id": "e26340c2-8a74-4095-bc52-42a4288b4ef8",
                            "question": "Gini Inpurityは何を表しますか"
                        }
                    ]
                }
            ],
            "title": "0315"
        },
        {
            "paragraphs": [
                {
                    "context": "まず，最も基本的な識別関数法である誤り訂正学習から説明を始めます．1943年に，McCullochとPittsは神経細胞の数理モデル（図5.5）を組み合わせて，任意の論理関数が計算可能であることを示しました．図5.5に基づいた計算モデルを単層パーセプトロンとよびます．このモデルを単独で考えると，入力の重み付き和を計算して，その値と閾値を比べて出力を決めるということをしています．閾値との比較をしている部分は，$x_0 = 1$という固定した入力を仮定し，この入力に対する重みを$w_0 = - \\theta$とすることで，その他の入力の重み付き和に組み込むことができます．これは$d$次元の特徴空間上で，$g(\\bm{x}) = w_0+w_1 x_1+\\dots+w_d x_d = 0$という識別超平面を設定し，入力がこの識別超平面のどちら側にあるのかを計算していることと等価になります．もし，与えられた学習データが特徴空間上で線形分離可能ならば（超平面で区切ることができるならば），以下に示すパーセプトロンの学習アルゴリズムで，線形分離面を見つけることができます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 148,
                                    "text": "入力の重み付き和を計算して，その値と閾値を比べて出力を決める"
                                }
                            ],
                            "id": "b79e2f3b-09f6-490d-9610-61c2a94ab925",
                            "question": "単層パーセプトロンはどうやって出力を決めるんですか"
                        }
                    ]
                }
            ],
            "title": "0506"
        },
        {
            "paragraphs": [
                {
                    "context": "前節で述べた誤り訂正学習は，特徴空間上では線形識別面を設定することに相当します．この識別器を神経細胞のニューロンとみなし，脳の構造に倣って階層状に重ねることで，非線形識別面が実現できます．図8.3のようにパーセプトロンをノードとして，階層状に結合したものを多層パーセプトロンあるいはニューラルネットワークとよびます．脳のニューロンは，一般には図8.3のようなきれいな階層状の結合をしておらず，階層内の結合，階層を飛ばす結合，入力側に戻る結合が入り乱れていると考えられます．このような任意の結合を持つニューラルネットワークモデルを考えることもできるのですが，学習が非常に複雑になるので，まずは図8.3のような3階層のフィードフォワード型ネットワークを対象とします．一般に3階層のフィードフォワード型モデルでは，入力層を特徴ベクトルの次元数用意して，それぞれの次元を入力値とし，出力層を識別対象のクラス数だけ用意します．そして，入力層・出力層の数に応じた適当な数の中間層を用意します．中間層は隠れ層(hidden layer)とも呼ばれます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 444,
                                    "text": "隠れ層"
                                }
                            ],
                            "id": "7aeeddfc-6fb0-47ca-a733-8325784b41ac",
                            "question": "中間層を言い換えるとなんといいますか"
                        }
                    ]
                }
            ],
            "title": "0802"
        },
        {
            "paragraphs": [
                {
                    "context": "回帰木とは，識別における決定木の考え方を回帰問題に適用する方法です．このような判断をしたから，この値が求まったというように，結果に説明を付けやすくなるのが特徴です．決定木では，同じクラスの集合になるように，特徴の値によって学習データを分割してゆく，という考え方でした．それに対して，回帰では，出力値の近いデータが集まるように，特徴の値によって学習データを分割してゆきます．結果として得られる回帰木は図6.5のように，特徴をノードとし，出力値をリーフとするものになります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 0,
                                    "text": "回帰"
                                }
                            ],
                            "id": "6020bfb5-7058-433e-be9e-45876e4ec72c",
                            "question": "出力値の近いデータが集まるように，特徴の値によって学習データを分割していくことをなんと言いますか"
                        }
                    ]
                }
            ],
            "title": "0611"
        },
        {
            "paragraphs": [
                {
                    "context": "識別は，入力をあらかじめ定められたクラスに分類する問題です．典型的な識別問題には，音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定などがあります．ここで，識別問題としてもっとも単純な，2値分類問題を考えてみましょう．2値分類問題とは，たとえば，ある病気かそうでないか，迷惑メールかそうでないかなど，入力を2クラスに分類する問題です．さらに，入力を数値のみを要素とするベクトルと仮定します．入力ベクトルが2次元の場合，学習データは図1.7(a)に示すように，平面上の点集合になります．クラスの値に対応させて，それぞれ丸とバツで表しました．最も単純に考えると，識別問題はこの平面上で二つのクラスを分ける境界線を決めるという問題になります．未知のデータが入力されたとき，この境界線のどちらにあるかを調べるだけで，属するクラスを解答できるので，このことによって，識別能力を身につけたとみなすことができます．境界線として1本の直線を考えてみましょう．図1.7(b)に示すように，このデータでは切片や傾きをどのように調整しても1本の直線で二つのクラスをきれいに分離することはできません．一方，図1.7(c)のように複雑に直線を組み合わせると，すべての学習データに対して同じクラスに属するデータが境界線の片側に位置するようになり，きれいに分離できたことになります．しかしここで，「識別とは図1.7(c)のようなすべての学習データをきれいに分離する，複雑な境界線を探すことだ」という早とちりをしてはいけません．識別の目的は，学習データに対して100\\%の識別率を達成することではなく，未知の入力をなるべく高い識別率で分類するような境界線を探すことでした．もう一度図1.7(a)に戻って，二つのクラスの塊をぼんやりとイメージしたとき，その塊を区切る線として図1.7(b)，(c)いずれがよいと思えるでしょうか．おそらく大半の人が(b)を支持するでしょう．これが我々人間が身につけている汎化能力で，そのようなことを学習結果に反映させるように，学習アルゴリズムを考える必要があります．しかし，一般的な識別問題は，このような単純なものばかりではありません．識別結果が一つのクラスになるとは限らない場合があります．たとえば，受信した電子メールに対して，重要・緊急・予定・締切...などのタグを付与する自動タグ付けを識別問題に当てはめると，一つの入力に対して，複数の出力の可能性がある問題設定になります．また，どのクラスにも当てはまらない入力が入ってくる可能性もあります．たとえば，スキャンした文書に対して文字認識をおこなっているときに，文字コードにない記号が含まれている場合があるかもしれません．本書で扱う識別は，これらの複数出力の可能性や，識別不可能な入力の問題を除外して，すべての入力に対してあらかじめ決められたクラスのうちの一つを出力とする，というように単純化します．識別の代表的な手法には決定木，ナイーブベイズ識別， ロジスティック識別，サポートベクトルマシン，ニューラルネットワークなどがあります．これらを第3章から第8章で説明します．近年注目を集めている深層学習は，主としてこの識別問題に適用されて高い性能を実現しています．深層学習に関しては第9章で説明します．また，第10章では複数の識別器を組み合わせる手法を，第13章では系列データの識別手法を扱います．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 41,
                                    "text": "音声や文字の認識，レビュー文章のPN判定（positive（ほめている）か negative（けなしている）か），疾病の有無の判定など"
                                }
                            ],
                            "id": "bce5f870-6f7a-4bca-992c-ffac22b699f7",
                            "question": "識別ではどのようなことができますか"
                        }
                    ]
                }
            ],
            "title": "0111"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 183,
                                    "text": "半分の領域で勾配が1になるので"
                                }
                            ],
                            "id": "62d636c6-c845-40e3-a02c-e39653090385",
                            "question": "ReLUだと勾配消失が起こらないのはなぜですか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1037,
                                    "text": "正例がどれだけ正しく判定されているかという指標"
                                }
                            ],
                            "id": "f377acb4-23f2-42cc-a401-c199b702efa1",
                            "question": "再現率って何ですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法"
                                }
                            ],
                            "id": "27d28285-acf6-4179-8015-c0b41b1f2e80",
                            "question": "サポートベクターマシンとはなんですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "異なった振る舞いをする識別器を複数作るための最初のアイディアは，異なった学習データを複数用意する，ということです．ここまでの教師あり学習の説明からもわかるように，学習データが異なれば，たいていその学習結果も異なります．バギング(Bagging)はこのアイディアに基づいて，学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するものです（図10.3}）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 136,
                                    "text": "学習データから復元抽出することで，もとのデータと同じサイズの独立なデータ集合を作成し，各々のデータ集合に対して同じアルゴリズムで識別器を作成するもの"
                                }
                            ],
                            "id": "dbc10dd4-84dc-4377-9a1e-78e7dc8c506d",
                            "question": "バギングとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1004"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 479,
                                    "text": "アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築すること"
                                }
                            ],
                            "id": "68ed703b-8690-4d00-ae8f-b348b857b207",
                            "question": "機械学習の基本的な定義はなんですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "さて，カーネル関数が正定値関数という条件を満たすときには，このような非線形変換$\\phi$が存在することがわかっています．そのようなカーネル関数の例としては，以下のような多項式カーネル関数や以下のようなガウシアンカーネル関数などがあります．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 3,
                                    "text": "カーネル関数が正定値関数という条件を満たすとき"
                                }
                            ],
                            "id": "11c49a04-a4ac-4572-ac82-d09139551b06",
                            "question": "非線形変換が存在するのはどういう条件の時ですか"
                        }
                    ]
                }
            ],
            "title": "0712"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 202,
                                    "text": "線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法"
                                }
                            ],
                            "id": "84605bd3-b386-423a-8d7c-cc3bc7929f7e",
                            "question": "サポートベクトルマシンはどういう手法ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする"
                                }
                            ],
                            "id": "d83981b3-bd86-4613-ad99-744829b67d8c",
                            "question": "機械学習では何ができますか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 248,
                                    "text": "線形で識別できないデータに対応するため"
                                }
                            ],
                            "id": "93cc980c-a7d2-4cd0-a9fa-4be73ad8acb4",
                            "question": "なぜペナルティを設定するのですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "しかし，通常はそう簡単にはゆきません．図14.3の例で示したような商品の評価を行う文書にしても，褒める言葉やけなす言葉は様々なバリエーションがあります．顔文字を使ったり，略語を使ったりもするでしょう．そのような場合，正解付きデータと特徴語のオーバーラップが多い正解なしデータにラベル付けを行って正解ありデータに取り込んでしまった後，新たな頻出語を特徴語とすることによって，連鎖的に特徴語を増やしてゆくという手段が考えられます（図14.4）．つまり，ラベル特徴の場合，教師ありデータと教師なしデータにラベル値のオーバーラップが全く見られないデータでは，半教師あり学習は役に立ちませんが，教師なしデータの一部とでも適当なオーバーラップがあれば，その一部の教師なしデータが他の教師なしデータを徐々に巻き込んでゆく可能性があります．通常，自然言語で書かれたデータはこの後者の仮定を満たすことが多いので，半教師あり学習は文書分類問題によく適用されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 397,
                                    "text": "半教師あり学習は文書分類問題によく適用されます"
                                }
                            ],
                            "id": "1001dfd7-c35d-423b-a0d2-53661be1d883",
                            "question": "半教師あり学習は何によく使われますか"
                        }
                    ]
                }
            ],
            "title": "1405"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 98,
                                    "text": "サポートベクトルマシン"
                                }
                            ],
                            "id": "48fe8554-ac51-4187-aaf2-e86740bda008",
                            "question": "線形モデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法はなに"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "最後のステップは，学習結果の可視化です．識別結果からいくつかの評価指標の値を計算し，表やグラフとして表示します．まず，説明を単純にするために2クラス識別問題の評価法を考えます．2クラス問題では，入力がある概念に当てはまるか否かを判定します．たとえば，ある病気か否か，迷惑メールか否か，というような問題です．設定した概念に当てはまる学習データを正例 (positve) ，当てはまらないデータを負例 (negative) といいます．迷惑メールの識別問題では，迷惑メールが正例なので，これをpositiveと見なすのは少し変な気がしますが，惑わされないでください．あくまでも設定した概念に当てはまるか否かで，正例・負例が決まります．さて，識別器を作成し，テストデータでその評価を行うと，その結果は表2.3で表すことができます．正例を正解+，負例を正解-，識別器が正と判定したものを予測+，負と判定したものを予測-とします．この表のことを混同行列(confusion matrix)あるいは分割表(contingency table)とよびます．実は，機械学習の評価は，正解率を算出して終わり，というほど単純なものではありません．たとえば，正例に比べて負例が大量にあるデータを考えてみましょう．もし，正例のデータがでたらめに判定されていても，負例のデータがほとんど正確に判定されていたとしたら，正解率は相当高いものになります．そのような状況を見極めるために，機械学習の結果は様々な指標で評価する必要があります．有効な指標を紹介する前に，まず，混同行列の各要素に表2.4に示す名前をつけておきます．たとえば，左上の要素は，正例に対して識別器が正 (positive) であると正しく (true) 判定したので，true positive といいます．一方，右上の要素は，正例に対して識別器が負 (negative) であると間違って (false) 判定したので，false negativeとよびます．前の語が判定の成否 (true or false) を，後の語が判定結果 (positive or negative) を表します．これらの定義を用いると，正解率 Accuracy は式(2.5)のように定義できます．また，識別器が正例と判断したときに，それがどれだけ信頼できるかという指標を表すために，精度(precision)が式(2.6)のように定義されます．さらに，正例がどれだけ正しく判定されているかという指標を表すために，再現率(recall)が式(2.7)のように定義されます．精度と再現率を総合的に判断するために，その調和平均をとったものをF値(F-measure)とよび，式(2.8)のように定義されます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 1037,
                                    "text": "正例がどれだけ正しく判定されているかという指標"
                                }
                            ],
                            "id": "3647cd85-6ec2-4f80-941a-e827e2944816",
                            "question": "再現率とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0209"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 388,
                                    "text": "正解が付いていない場合の学習"
                                }
                            ],
                            "id": "46565d8c-ad70-4427-bc9c-0203b5700585",
                            "question": "教師なし学習って何ですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "ニューラルネットワークの階層を深くすると，それだけパラメータも増えるので，過学習の問題がより深刻になります．そこで，ランダムに一定割合のユニットを消して学習を行うドロップアウト（図9.7）を用いると，過学習が起きにくくなり，汎用性が高まることが報告されています．ドロップアウトによる学習では，まず各層のユニットを割合$p$でランダムに無効化します．たとえば$p=0.5$とすると，半数のユニットからなるニューラルネットワークができます．そして，このネットワークに対して，ミニバッチ一つ分のデータで誤差逆伝播法による学習を行います．対象とするミニバッチのデータが変わるごとに無効化するユニットを選び直して学習を繰り返します．学習後，得られたニューラルネットワークを用いて識別を行う際には，重みを$p$倍して計算を行います．これは複数の学習済みネットワークの計算結果を平均化していることになります．このドロップアウトによって過学習が生じにくくなっている理由は，学習時の自由度を意図的に下げていることにあります．自由度が高いと結合重みが不適切な値（学習データに対しては正解を出力できるが，汎用性がないもの）に落ち着いてしまう可能性が高くなりますが，自由度が低いと，正解を出力するための結合重みの値は特定の値に限定されやすくなります（図9.7）．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 100,
                                    "text": "過学習が起きにくくなり，汎用性が高まることが報告されています"
                                }
                            ],
                            "id": "37ad62b2-99c0-4be2-81ee-b6ec62e2c08c",
                            "question": "多階層学習においてドロップアウトを用いるとどうなるか"
                        }
                    ]
                }
            ],
            "title": "0911"
        },
        {
            "paragraphs": [
                {
                    "context": "次に考えられる手法としては，区分線形よりももっとなめらかな非線形関数を用いて回帰式を得られないか，ということになるのですが，一般に非線形式ではデータにフィットしすぎてしまうため，過学習が問題になります．そこで，SVMのところでも説明したカーネル法を使って，非線形空間へ写像した後に，線形識別を正規化項を入れながら学習するというアプローチが有効になります．この手法については，文献\\cite{akaho08}に詳しく書かれています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 62,
                                    "text": "一般に非線形式ではデータにフィットしすぎてしまうため"
                                }
                            ],
                            "id": "5e55e781-ff55-47bf-bc43-0eaccaa212e4",
                            "question": "なぜ回帰問題にカーネル法を使うんですか"
                        }
                    ]
                }
            ],
            "title": "0616"
        },
        {
            "paragraphs": [
                {
                    "context": "深層学習はDeep learningの訳語で，日本語の文献ではディープラーニングとそのままの単語でよばれることもあります．第1章で述べたように，深層学習は機械学習の一手法で，高い性能を発揮する事例が多いことから，近年，音声認識・画像認識・自然言語処理などの分野で大いに注目を集めています．本章では，深層学習の基本的な考え方を説明します．深層学習をごく単純に定義すると，図9.1のような，特徴抽出前の信号を入力をとする多階層ニューラルネットワークの学習，ということになります．とくに深層学習に用いるニューラルネットワークをDeep Neural Network (DNN) とよびます．深層学習は，表現学習(representation learning)とよばれることもあります．特徴抽出前の生データに近い信号から，その内容を表現する特徴を学習する，というところがポイントです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 240,
                                    "text": "深層学習に用いるニューラルネットワーク"
                                }
                            ],
                            "id": "e6269529-ccdb-41c4-a7b7-b74543b916d6",
                            "question": "ディープニューラルネットワークとは何ですか"
                        }
                    ]
                }
            ],
            "title": "0901"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，次元削減と標準化を紹介します．次元削減とは，特徴ベクトルの次元数を減らすことです．せっかく用意した特徴を減らすと聞くと，不思議な感じがするかもしれません．しかし一般的には，多次元の特徴には冗長性が多く含まれます．また，次元数が増えれば増えるほど，学習データが高次元空間上にまばらに存在することになり，そのようなまばらなデータから得られたモデルは，一般的に汎化能力が低いことがわかっています．これを「次元の呪い」とよびます．したがって，特徴ベクトルの次元削減は，より汎化能力の高いモデルを学習するという観点から，重要な前処理ということになります．また，特徴の値の範囲を揃えておく標準化 (standardization)も，前処理としては重要な処理です．一般に，特徴はそれぞれ独立の基準で計測・算出するので，その絶対値や分散が大きく異なります．これをベクトルとして組み合わせて，そのまま学習をおこなうと，絶対値の大きい特徴量の寄与が大きくなりすぎるという問題があるので，値のスケールを合わせる必要があります．また，入力の平均値を特定の値に合わせておくと，学習対象のパラメータの初期値を個別のデータに合わせて調整する必要がなくなります．このようなことを目的として，一般的には式(2.4)に従ってそれぞれの次元の平均値を0に，標準偏差を1に揃えます．この処理を標準化とよびます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 27,
                                    "text": "特徴ベクトルの次元数を減らすこと"
                                }
                            ],
                            "id": "862236ed-425e-4797-b0b0-a648f8b6cde4",
                            "question": "次元削減って何ですか"
                        }
                    ]
                }
            ],
            "title": "0204"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，もう少し詳細に機械学習の中身をみてゆきましょう．機械学習で扱うのは，人手では規則を記述することが難しい問題です．すなわち，解法が明確にはわかっていない問題であると言い換えることができます．ここでは，機械学習で対象とする問題をタスク (task)とよびます．たとえば，人間は文字や音声の認識能力を学習によって身につけますが，どうやって認識をおこなっているかを説明することはできません．人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術が，機械学習の一種であるパターン認識 (pattern recognition) です．ここでのアイディアは，明示的にその手順は記述できないけれども，データ（この場合は入力とその答え）は大量に用意することができるので，そのデータを使って人間の知的活動（場合によってはそれを超えるもの）のモデルを作成しようというものです．これ以降，機械学習のために用いるデータを学習データ (training data)とよびます．ここまでをまとめると，機械学習の基本的な定義は，アルゴリズムとして明示的に解法が与えられないタスクに対して，そのタスクを遂行するためのモデルを，学習データから構築することとなります．また，学習データも一見多様に見えますが，金額やセンサーからの入力のような「数値データ」，あるいは商品名や性別のような「カテゴリカルデータ」が並んだものであるとすると，数値データの並びからなるデータ，カテゴリカルデータの並びからなるデータ，それらが混合したデータというように整理して考えることができます（図1.4）．観測対象から問題設定に適した情報を選んでデータ化する処理は，特徴抽出とよばれます．機械学習の役割をこのように位置付けると，図1.4中の「機械学習」としてまとめられた中身は，タスクの多様性によらず，目的とする規則・関数などのモデルを得るために，どのような学習データに対して，どのようなアルゴリズムを適用すればよいか，ということを決める学習問題と，その学習の結果得られたモデルを，新たに得られる入力に対して適用する実運用段階に分割して考えることができます（図1.5）．本書の対象は，主として図1.5の学習問題と定義された部分ですが，いかに実運用の際によい性能を出すか，すなわち，学習段階ではみたことのない入力に対して，いかによい結果を出力するかということを常に考えることになります．この能力は，「学習データからいかに一般化されたモデルが獲得されているか」ということになるので汎化 (generalization) 能力といいます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 196,
                                    "text": "人間の認識能力を何らかの手法でモデル (model)化して，コンピュータでその能力を再現しようとする技術"
                                }
                            ],
                            "id": "b6f1a220-00fe-4d99-8d41-2e613c905e11",
                            "question": "パターン認識とはどういうものですか"
                        }
                    ]
                }
            ],
            "title": "0105"
        },
        {
            "paragraphs": [
                {
                    "context": "バギングでは，不安定な学習アルゴリズムを用いて，異なる識別器を作成しました．しかし，復元抽出によって作られた個々のデータ集合は，もとの学習データ集合と約$2/3$のデータを共有しているので，とくに，元のデータのまとまりがよい場合，それほど極端に異なった識別器にはなりません．ここで説明するランダムフォレスト(RandomForest)は，識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法です．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 169,
                                    "text": "識別器の学習アルゴリズムに，その結果が大きく異なる仕組みを入れる方法"
                                }
                            ],
                            "id": "16a9c8a4-bf12-46fa-9ad7-727fbc37da21",
                            "question": "ランダムフォレストとは何ですか"
                        }
                    ]
                }
            ],
            "title": "1007"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，前節で説明した学習データと出力を基準に機械学習の分類を試みます．機械学習にはさまざまなアルゴリズムがあり，その分類に関してもさまざまな視点があります．機械学習の入門的な文献では，モデルの種類に基づく分類が行われていることが多いのですが，そもそもそのモデルがどのようなものかというイメージをもっていない初学者には，なかなか納得しにくい分類にみえてしまいます．そこで本書では，入力である学習データの種類と出力の種類の組合せで機械学習のタスクの分類をおこない，それぞれに分類されたタスクを解決する手法としてモデルを紹介します．まず，学習データにおいて，正解（各データに対してこういう結果を出力して欲しいという情報）が付いているか，いないかで大きく分類します（図1.6）．学習データに正解が付いている場合の学習を教師あり学習  (supervised learning) ，正解が付いていない場合の学習を教師なし学習  (unsupervised learning)とよびます．また，少し曖昧な定義ですが，それらのいずれにも当てはまらない手法を中間的学習 とよぶことにします．教師あり／なしの学習については，それぞれの出力の内容に基づいてさらに分類をおこないます．教師あり学習では，入力の分類結果をカテゴリとして出力するものを識別とし，入力から予測される数値を出力するものを回帰とします．一方，教師なし学習では観点を変えて，入力となるデータ集合全体を説明する情報を出力するものをモデル推定とし，入力となるデータ集合の一部から得られる特徴的な情報を出力するものをパターンマイニングとします．中間的学習に関しては，何が「中間」であるのかに着目します．学習データが中間である場合（すなわち，正解付きの学習データと正解なしの学習データが混在している場合）である半教師あり学習という設定と，正解が間接的・確率的に与えられるという意味で，教師あり／なしの中間的な強化学習という設定を取り上げます．以下では，それぞれの分類について，その問題設定を説明します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 388,
                                    "text": "正解が付いていない場合の学習"
                                }
                            ],
                            "id": "cb6ca052-5897-46b1-ae77-001b60334872",
                            "question": "教師なし学習とはなんですか"
                        }
                    ]
                }
            ],
            "title": "0109"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでは，ベイジアンネットワークがすでにできている，すなわち図4.9に示したようなネットワークの構造と，全アークに対応する条件付き確率表が得られているものとして，それを用いて識別を行う手順を説明します．一般にクラスは親ノードに，特徴は子孫ノードに配置します．求めるものは，特徴を現すノードの値が与えられたもとで，クラスを表すノードが真となる確率ですが，ここではネットワーク中の一部のノードの値が与えられたときに，値が与えられていないノードが真となる確率を求める問題に一般化して考えます．ここで，値が真となる確率を知りたいノードが表す変数を，目的変数とよびます．目的変数以外のすべての変数の値が観測された場合（実際は，目的変数の親ノードの値が観測された場合，あるいは，さらなる親ノードの値から計算可能な場合）は，目的変数から遠い順に条件付き確率表を使ってノードの値を計算することで，目的変数の値が求まります．しかし，効率を求める場合や，一部のノードの値しか観測されなかった場合にも対応できる方法として，確率伝播による計算法があります．このようにノード間の独立性を使いながら，確率を伝搬させて任意のノードの確率を求めることができます．ただし，この方法はアークを無向とみなした結合を考えたときに，ループが形成されていれば値が収束しないことがあるので，適用することができません．そのような場合は，確率的シミュレーションも用いられます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 247,
                                    "text": "値が真となる確率を知りたいノードが表す変数"
                                }
                            ],
                            "id": "8c0375ec-4f9f-456f-956b-d1a66caf0245",
                            "question": "ここでいう目的変数って何ですか"
                        }
                    ]
                }
            ],
            "title": "0416"
        },
        {
            "paragraphs": [
                {
                    "context": "一般に，特徴空間の次元数$d$が大きい場合は，データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります．そこで，この性質を逆手にとって，低次元の特徴ベクトルを高次元に写像し(図7.3参照)，線形分離の可能性を高めてしまい，その高次元空間上でSVMを使って識別超平面を求めるという方法が考えられます．この方法は，むやみに特徴を増やして次元を上げる方法とは違います．識別に役立つ特徴で構成された$d$次元空間に対して，もとの空間におけるデータ間の距離関係を保存する方式で高次元に非線形変換したならば，その高次元空間上での線形識別器の性能は，もとの空間での複雑な非線形識別器の性能に相当することがわかっています．一方，識別に無関係な特徴を持ち込むと，データが無意味な方向に\\ruby{疎}{まばら}に分布し，もとの分布の性質がこわされやすくなってしまいます．しかし問題は，もとの空間におけるデータ間の距離関係を保存するような，そんな都合の良い非線形写像が見つかるかということです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 23,
                                    "text": "データがまばらに分布することになるので，線形分離面が存在する可能性が高くなります"
                                }
                            ],
                            "id": "4c7af6c2-86ed-49af-b4a2-40c57136aa26",
                            "question": "特徴空間の次元数$d$が大きい場合はどうなりますか"
                        }
                    ]
                }
            ],
            "title": "0710"
        },
        {
            "paragraphs": [
                {
                    "context": "この章では，数値データからなる特徴ベクトルとその正解クラスの情報からクラスを識別できる，神経回路網のモデルを作成する方法について説明します．ニューラルネットワークは，図8.1のような計算ユニットを，階層的に組み合わせて，入力から出力を計算するメカニズムです．このモデルは生物の神経細胞のモデルであると考えられています．神経細胞への入力はそれぞれ重み$w$をもっていて，入力があるとそれらの重み付き和が計算されます．そして，その結果が一定の閾値$\\theta$を超えていれば，この神経細胞が出力信号を出し，それが他の神経細胞へ(こちらも重み付きで)伝播されます．人間の脳はこのようにモデル化されるニューロンと呼ばれる神経細胞がおよそ$10^{11}$個集まったものです．これらの神経細胞がシナプス結合と呼ばれる方法で互いに結びついていて，この結合が変化することで学習が行われます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 129,
                                    "text": "このモデルは生物の神経細胞のモデルであると考えられています"
                                }
                            ],
                            "id": "c40b3d8a-adc2-4ec2-a9cb-6adcd9249092",
                            "question": "ニューラルとはどういう意味か"
                        }
                    ]
                }
            ],
            "title": "0801"
        },
        {
            "paragraphs": [
                {
                    "context": "勾配消失問題への別のアプローチとして，ユニットの活性化関数を工夫する方法があります．シグモイド関数ではなく，rectified linear関数とよばれる$f(x)=\\max(0,x)$（引数が負のときは0，0以上のときはその値を出力）を活性化関数とした ユニットを，ReLU(rectified linear unit)（図8.8）とよびます．ReLuを用いると，半分の領域で勾配が1になるので，誤差が消失しません．また，多くのユニットの出力が0であるスパース（疎ら）なネットワークになる点や，勾配計算が高速に行える点などが，ディープニューラルネットワークの学習に有利に働くので，事前学習なしでも学習が行えることが報告されています．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 19,
                                    "text": "ユニットの活性化関数を工夫する方法があります"
                                }
                            ],
                            "id": "4a176a40-7fce-4daf-afb8-4a77794d60c1",
                            "question": "勾配消失問題を解決する手法はあるのか"
                        }
                    ]
                }
            ],
            "title": "0811"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，クラスタ間の類似度計算に関して，いくつか異なる計算方法があり，そのいずれを採用するかによって，出来上がるクラスタの性質に違いが生じます．\\begin{itemize}\\item 単連結法(SINGLE)\\\\最も近い事例対の距離を類似度とする．クラスタが一方向に伸びやすくなる傾向がある．\\item 完全連結法(COMPLETE)\\\\最も遠い事例対の距離を類似度とする．クラスタが一方向に伸びるのを避ける傾向がある．\\item 重心法(CENTROID)\\\\クラスタの重心間の距離を類似度とする．クラスタの伸び方型は単連結と完全連結の間をとったようになる．\\item Ward法(WARD)\\\\与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする．比較的良い結果が得られることが多いので，階層的クラスタリングでよく用いられる類似度基準である．\\end{itemize}",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 300,
                                    "text": "与えられたクラスタを融合したときのクラスタ内のデータと，クラスタ中心との距離の二乗和を求め，そこか各クラスタについて同様に求めた値を引いたものを類似度とする"
                                }
                            ],
                            "id": "c870fb24-1567-401f-a890-b2ac1d0f4d73",
                            "question": "Ward法とはなんですか"
                        }
                    ]
                }
            ],
            "title": "1106"
        },
        {
            "paragraphs": [
                {
                    "context": "第7章から第10章では，教師あり学習全般に用いることができる，発展的手法について説明します．大半は識別問題を題材に説明しますが，ほとんどの手法は，少しの工夫で回帰問題にも適用できます．この章では，サポートベクトルマシン(support vector machine: SVM)とよばれる学習手法について説明します．扱う問題は，主として数値特徴に対する「教師あり・識別」問題です．サポートベクトルマシンは，線形のモデルを使ってなるべく学習データに特化しすぎないような識別面を求める方法です．そして，線形で識別できないデータに対応するために，誤識別に対してペナルティを設定することで対応する手法と，学習データを高次元の空間に写して，線形識別手法を適用するという手法について説明します．いま，図7.1に示すような特徴空間上で平面によって分離することができる学習データがあるとします．この学習データに対して，識別率100\\%で識別をおこなえる直線は無数に存在します．図7.1(a)，(b)のどちらの識別境界線（図の実線）も，学習データに関しては識別率100\\%ですが，未知データに対しては，図7.1(a)の識別境界線より図7.1(b)の識別境界線の方が，よい結果が出そうです．図の全体を眺めたとき，データの塊が見えたなら，その塊の真ん中に位置する識別境界線が，最も汎化性能が高いことが予想できます．この漠然とした「汎化性能の高さ」を定量的に表すために，識別境界線と最も近いデータとの距離を考えます．この距離のことをマージン（図の実線と図の点線の距離）といいます．マージンが広いほうが，学習データと識別境界線の間に未知データが入る余地があることになるので，学習データからちょっとずれただけの未知データが，別クラスに識別されるということが少なくなります．これが，汎化性能の高さにつながります．この学習データからのマージンが最大となる識別境界線（一般には識別超平面）を求める手法がサポートベクトルマシンです．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 792,
                                    "text": "学習データからのマージンが最大となる識別境界線"
                                }
                            ],
                            "id": "33eedf33-9d17-4375-9de2-4fd4783e07b4",
                            "question": "超平面とは何ですか"
                        }
                    ]
                }
            ],
            "title": "0701"
        },
        {
            "paragraphs": [
                {
                    "context": "ここで，バイアスと分散の関係を考えてみます．バイアスは真のモデルとの距離，分散は学習結果の散らばり具合と理解してください．線形回帰式のような単純なモデルは，個別のデータに対する誤差は比較的大きくなってしまう傾向があるのですが，学習データが少し変動しても，結果として得られるパラメータはそれほど大きく変動しません．これをバイアスは大きいが，分散は小さいと表現します．逆に，複雑なモデルは個別のデータに対する誤差を小さくしやすいのですが，学習データの値が少し異なるだけで，結果が大きく異なることがあります．これをバイアスは小さいが，分散は大きいと表現します．このバイアスと分散は，片方を減らせば片方が増える，いわゆるトレードオフの関係にあります．このテーマは第3章の概念学習でも出てきました．概念学習では，バイアスを強くすると，安定的に解にたどり着きますが，解が探索空間に含まれず，学習が失敗する場合がでてきてしまいました．一方，バイアスを弱くすると，探索空間が大きくなりすぎるので，探索方法にバイアスをかけました．探索方法にバイアスをかけてしまうと，最適な概念（オッカムの剃刀に従うと，最小の表現）が求めることが難しくなり，学習データのちょっとした違いで，まったく異なった結果が得られることがあります．回帰問題でも同様の議論ができます．線形回帰式に制限すると，求まった平面は，一般的には学習データ内の点をほとんど通らないのでバイアスが大きいといえます．一方，「学習データの個数-1」次の高次回帰式を仮定すると，「係数の数」と「学習データを回帰式に代入した制約の数」が等しくなるので，連立方程式で解くことで，重みの値が求まります．つまり，「学習データの個数-1」次の回帰式は，全学習データを通る式にすることができます．これが第1章の図1.8(c)に示したような例になります．この図からもわかるように，データが少し動いただけでこの高次式は大きく変動します．つまり，バイアスが弱いので学習データと一致する関数が求まりますが，変動すなわち結果の分散はとても大きくなります．このように，機械学習は常にバイアス－分散のトレードオフを意識しなければなりません．前節で説明した正則化はモデルそのものを制限するよりは少し緩いバイアスで，分散を減らすのに有効な役割を果たします．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 288,
                                    "text": "片方を減らせば片方が増える"
                                }
                            ],
                            "id": "9ed362ab-f2ac-49b1-87dc-34891e35e70f",
                            "question": "トレードオフってなんですか"
                        }
                    ]
                }
            ],
            "title": "0610"
        },
        {
            "paragraphs": [
                {
                    "context": "自己学習の問題点は，自分が出した誤りを指摘してくれる他人がいない，というたとえができます．そこで，判断基準が異なる識別器を2つ用意して，お互いが教え合うというたとえで半教師あり学習を実現する方法が，共訓練(Co-training)です．共訓練は，異なった特徴を用いて識別器を2つ作成し，相手の識別結果を利用して，それぞれの識別器を学習させるアルゴリズムです．まず，教師付きデータの分割した特徴から識別器1と識別器2を作成し，教師なしデータをそれぞれで識別します．識別器1の確信度上位$k$個を教師付きデータとみなして，識別器2を学習します，その後，1と2の役割を入れ替え，精度の変化が少なくなるまで繰り返します．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 10,
                                    "text": "自分が出した誤りを指摘してくれる他人がいない"
                                }
                            ],
                            "id": "355a9bed-ad3d-4b0a-9f5b-e3dc174e33c8",
                            "question": "自己学習の問題点は何ですか"
                        }
                    ]
                }
            ],
            "title": "1409"
        },
        {
            "paragraphs": [
                {
                    "context": "ここでもう一度，表12.1の小規模データに戻って，項目集合を絞り込む方法を考えましょう．今度は図示しやすいように，項目に通し番号を付けて，$\\{0,1,2,3\\}$と表します．すべての可能な項目の組合せは$2^4-1=15$で，図12.2に丸で示すもの（ただし，最上段の空集合$\\emptyset$は除く）になります．ここで，a prioriな原理として，ある項目集合が頻出ならば，その部分集合も頻出であるを考えます．そうすると，上で述べたa prioriな原理から，ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でないが成り立ちます．",
                    "qas": [
                        {
                            "answers": [
                                {
                                    "answer_start": 234,
                                    "text": "ある項目集合が頻出でないならば，その項目集合を含む集合も頻出でない"
                                }
                            ],
                            "id": "61768cf6-c1c5-4d85-8524-7eed1c3699a9",
                            "question": "a priori な原理の対偶とは何ですか"
                        }
                    ]
                }
            ],
            "title": "1205"
        }
    ],
    "version": "1.0"
}